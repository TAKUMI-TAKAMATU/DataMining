{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize, sent_tokenize\n",
    "# ＜使用しているNLTKライブラリの説明＞\n",
    "# nltk.corpus.stopwords: 文章を特徴付ける要素として不適切なものを除外するためのブラックリスト。通称ストップワード。\n",
    "# nltk.sent_tokenize: 文章(doc)を文(sentence)に分割する。\n",
    "# nltk.wordpunct_tokenize: 文(sentence)を単語(word)に分割する。通称トークン化。\n",
    "# nltk.lemmatize: 単語(word)を基本形(らしきもの)に修正する。通称ステミング。\n",
    "\n",
    "import numpy as np   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"ch\"\n",
    "remove_tags = ('.//style', './/script', './/noscript')\n",
    "\n",
    "docs3 = [''] * 13\n",
    "for i in range(0,13):\n",
    "    with open(filepath +str(i)+\".html\" , mode='rb') as f:\n",
    "        docs3[i] = html.fromstring(f.read())\n",
    "        for remove_tag in remove_tags:\n",
    "            for tag in docs3[i].findall(remove_tag):\n",
    "                tag.drop_tree()\n",
    "        docs3[i] = docs3[i].text_content().strip()\n",
    "\n",
    "docs2 = preprocess_docs(docs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codebook =  ['preface', 'book', 'natural', 'language', 'processing', '\"', 'mean', 'used', 'everyday', 'communication', 'human', ';', 'like', 'english', 'hindi', 'portuguese', 'contrast', 'artificial', 'programming', 'mathematical', 'notation', 'evolved', 'pas', 'generation', 'hard', 'pin', 'explicit', 'rule', 'take', '—', 'nlp', 'short', 'wide', 'sense', 'cover', 'kind', 'computer', 'manipulation', 'one', 'extreme', 'could', 'simple', 'counting', 'word', 'frequency', 'compare', 'different', 'writing', 'style', 'involves', 'understanding', 'complete', 'utterance', 'least', 'extent', 'able', 'give', 'useful', 'response', 'technology', 'based', 'becoming', 'increasingly', 'widespread', 'example', 'phone', 'handheld', 'support', 'predictive', 'text', 'handwriting', 'recognition', 'web', 'search', 'engine', 'access', 'information', 'locked', 'unstructured', 'machine', 'translation', 'allows', 'u', 'retrieve', 'written', 'chinese', 'read', 'spanish', 'analysis', 'enables', 'detect', 'sentiment', 'tweet', 'blog', 'providing', '-', 'interface', 'sophisticated', 'stored', 'ha', 'come', 'play', 'central', 'role', 'multilingual', 'society', 'provides', 'highly', 'accessible', 'introduction', 'field', 'individual', 'study', 'textbook', 'course', 'computational', 'linguistics', 'supplement', 'intelligence', 'mining', 'corpus', 'intensely', 'practical', 'containing', 'hundred', 'fully', 'worked', 'graded', 'exercise', 'python', 'together', 'open', 'source', 'library', 'called', 'toolkit', '(', 'nltk', ').', 'includes', 'extensive', 'software', 'data', 'documentation', 'freely', 'downloadable', 'http', '://', 'org', '/.', 'distribution', 'provided', 'window', 'macintosh', 'unix', 'platform', 'strongly', 'encourage', 'download', 'try', 'along', 'way', 'audience', 'important', 'scientific', 'economic', 'social', 'cultural', 'reason', 'experiencing', 'rapid', 'growth', 'theory', 'method', 'deployed', 'variety', 'new', 'range', 'people', 'working', 'knowledge', 'within', 'industry', 'interaction', 'business', 'development', 'academia', 'area', 'humanity', 'computing', 'science', 'many', 'known', 'name', '.\")', 'intended', 'diverse', 'want', 'learn', 'write', 'program', 'analyze', 'regardless', 'previous', 'experience', ':', 'early', 'chapter', 'suitable', 'reader', 'prior', 'long', \"'\", 'afraid', 'tackle', 'concept', 'develop', 'skill', 'full', 'copy', 'need', 'general', 'see', 'list', 'resource', 'doc', 'experienced', 'programmer', 'quickly', 'enough', 'using', 'get', 'immersed', 'relevant', 'feature', 'carefully', 'explained', 'exemplified', 'appreciate', 'suitability', 'application', 'index', 'help', 'locate', 'discussion', 'already', 'dreaming', 'skim', 'dig', 'interesting', 'material', 'start', '1', '..', 'soon', 'applying', 'fascinating', 'domain', 'emphasis', 'real', 'grasp', 'value', 'test', 'idea', 'implementation', 'learnt', 'teach', 'unlike', 'provide', 'illustration', 'approach', 'taken', 'also', 'principled', 'theoretical', 'underpinnings', 'shy', 'away', 'careful', 'linguistic', 'tried', 'pragmatic', 'striking', 'balance', 'identifying', 'connection', 'tension', 'finally', 'recognize', 'unless', 'pleasurable', 'include', 'entertaining', 'sometimes', 'whimsical', 'note', 'reference', 'work', 'coverage', 'selective', 'presented', 'tutorial', 'please', 'consult', 'substantial', 'quantity', 'searchable', 'available', '/', 'advanced', 'content', 'introductory', 'intermediate', 'directed', 'algorithm', 'implemented', 'examine', 'code', 'linked', '/,', 'cited', 'digging', 'manipulate', 'key', 'describe', 'analyse', 'structure', 'standard', 'format', 'evaluate', 'performance', 'technique', 'depending', 'background', 'motivation', 'interested', 'gain', 'set', 'iii', 'goal', 'art', 'engineering', 'manipulating', 'large', 'exploring', 'model', 'testing', 'empirical', 'claim', 'modeling', 'discovery', 'building', 'robust', 'system', 'perform', 'task', 'technological', 'table', 'gained', 'reading', 'organization', 'organized', 'order', 'conceptual', 'difficulty', 'starting', 'show', 'explore', 'body', 'tiny', '3', 'followed', 'structured', '4', ')', 'consolidates', 'topic', 'scattered', 'across', 'preceding', 'pace', 'pick', 'move', 'series', 'covering', 'fundamental', 'tagging', 'classification', 'extraction', '5', '7', 'next', 'three', 'look', 'parse', 'sentence', 'syntactic', 'construct', 'representation', 'meaning', '8', '10', 'final', 'devoted', 'managed', 'effectively', '11', 'concludes', 'afterword', 'briefly', 'discussing', 'past', 'future', 'switch', 'presentation', 'driver', 'use', 'often', 'employ', 'introduced', 'systematically', 'purpose', 'delving', 'detail', 'learning', 'idiomatic', 'expression', 'foreign', 'buy', 'nice', 'pastry', 'without', 'first', 'intricacy', 'question', 'formation', 'supporting', 'end', 'consolidating', 'according', 'following', 'scheme', '☼', 'easy', 'involve', 'minor', 'modification', 'supplied', 'sample', 'activity', '◑', 'aspect', 'depth', 'requiring', 'design', '★', 'difficult', 'ended', 'challenge', 'force', 'think', 'independently', 'skip', 'section', 'online', 'extra', 'pointer', 'version', 'yet', 'powerful', 'excellent', 'functionality', 'downloaded', 'free', 'installers', 'five', 'line', 'process', 'file', 'txt', 'print', 'ending', 'ing', '>>>', '(\"', '\"):', '...', 'split', '():', 'endswith', \"('\", \"'):\", 'illustrates', 'main', 'whitespace', 'nest', 'thus', 'fall', 'inside', 'scope', 'ensures', 'performed', 'second', 'object', 'oriented', 'variable', 'entity', 'certain', 'defined', 'attribute', 'sequence', 'character', 'string', 'operation', '()', 'break', 'apply', 'period', 'e', '().', 'third', 'argument', 'expressed', 'parenthesis', 'instance', \"')\", 'indicate', 'wanted', 'something', 'else', 'importantly', 'readable', 'much', 'fairly', 'guess', 'doe', 'even', 'never', 'chose', 'shallow', 'curve', 'syntax', 'semantics', 'transparent', 'good', 'handling', 'interpreted', 'facilitates', 'interactive', 'exploration', 'permit', 'encapsulated', 'easily', 'dynamic', 'added', 'fly', 'typed', 'dynamically', 'facilitating', 'including', 'component', 'graphical', 'numerical', 'connectivity', 'heavily', 'research', 'education', 'around', 'world', 'praised', 'productivity', 'quality', 'maintainability', 'collection', 'success', 'story', 'posted', 'defines', 'infrastructure', 'build', 'basic', 'class', 'representing', 'performing', 'part', 'speech', 'parsing', 'combined', 'solve', 'complex', 'problem', 'addition', 'website', 'api', 'every', 'module', 'function', 'specifying', 'parameter', 'giving', 'usage', 'updated', 'significant', 'change', 'statement', 'return', 'iterators', 'instead', 'save', 'memory', ');', 'integer', 'division', 'floating', 'point', 'number', 'unicode', 'formatted', 'detailed', 'dev', 'whatsnew', '0', 'html', 'utility', '2to3', 'py', 'convert', '2', 'pervasive', 'type', 'initialised', 'fromstring', 'contextfreegrammar', 'cfg', 'weightedgrammar', 'pcfg', 'batch_tokenize', 'tokenize_sents', '();', 'corresponding', 'batch', 'tagger', 'parser', 'classifier', 'removed', 'favour', 'external', 'package', 'maintained', 'adequately', 'github', 'com', 'wiki', 'porting', 'requirement', 'install', 'several', 'current', 'instruction', 'assumes', 'later', '6', '.)', 'subsequent', 'release', 'backward', 'compatible', 'contains', 'analyzed', 'processed', 'numpy', ':(', 'recommended', 'multidimensional', 'array', 'linear', 'algebra', 'required', 'probability', 'clustering', 'matplotlib', '2d', 'plotting', 'visualization', 'produce', 'graph', 'bar', 'chart', 'stanford', 'tool', 'scale', 'edu', '/).', 'networkx', 'optional', 'storing', 'network', 'consisting', 'node', 'edge', 'visualizing', 'semantic', 'graphviz', 'prover9', 'automated', 'theorem', 'prover', 'equational', 'logic', 'inference', 'wa', 'originally', 'created', '2001', 'department', 'university', 'pennsylvania', 'since', 'developed', 'expanded', 'dozen', 'contributor', 'adopted', 'serf', 'basis', 'project', 'viii', 'accessing', 'standardized', 'lexicon', 'tokenize', 'stem', 'tokenizers', 'stemmer', 'collocation', 'chi', 'squared', 'wise', 'mutual', 'tag', 'n', 'gram', 'backoff', 'brill', 'hmm', 'tnt', 'classify', 'cluster', 'tbl', 'decision', 'tree', 'maximum', 'entropy', 'naive', 'bayes', 'em', 'k', 'chunking', 'chunk', 'regular', 'named', 'ccg', 'unification', 'probabilistic', 'dependency', 'interpretation', 'sem', 'lambda', 'calculus', 'checking', 'evaluation', 'metric', 'precision', 'recall', 'agreement', 'coefficient', 'estimation', 'smoothed', 'app', 'chat', 'concordancer', 'wordnet', 'browser', 'chatbots', 'fieldwork', 'toolbox', 'sil', 'designed', 'four', 'primary', 'mind', 'simplicity', 'intuitive', 'framework', 'block', 'user', 'getting', 'bogged', 'tedious', 'house', 'keeping', 'usually', 'associated', 'annotated', 'consistency', 'uniform', 'consistent', 'guessable', 'extensibility', 'accommodated', 'alternative', 'competing', 'modularity', 'needing', 'understand', 'rest', 'contrasting', 'non', 'potentially', 'deliberately', 'avoided', 'encyclopedic', 'continue', 'evolve', 'efficient', 'meaningful', 'optimized', 'runtime', 'optimization', 'lower', 'level', 'c', '++.', 'would', 'make', 'le', 'avoid', 'clever', 'trick', 'believe', 'clear', 'preferable', 'ingenious', 'indecipherable', 'instructor', 'taught', 'confines', 'single', 'semester', 'undergraduate', 'postgraduate', 'found', 'side', 'subject', 'span', 'time', 'focus', 'exclusion', 'deprive', 'student', 'excitement', 'automatically', 'simply', 'linguist', 'manage', 'address', 'making', 'feasible', 'amount', 'practice', 'fraction', 'syllabus', 'deal', 'rather', 'dry', 'brings', 'life', 'possible', 'view', 'step', 'demonstration', 'performs', 'special', 'input', 'effective', 'deliver', 'entering', 'session', 'observing', 'modifying', 'issue', 'assignment', 'simplest', 'fragment', 'specified', 'answer', 'concrete', 'spectrum', 'flexible', 'graduate', 'widely', 'datasets', '),', 'extensible', 'architecture', 'additional', 'teaching', 'unique', 'comprehensive', 'context', 'apart', 'tight', 'coupling', 'completing', 'ready', 'attempt', 'jurafsky', 'martin', 'prentice', 'hall', '2008', 'present', 'unusual', 'beginning', 'trivial', 'introducing', 'control', 'comprehension', 'conditionals', 'idiom', 'place', 'systematic', 'loop', 'forth', 'ground', 'conventional', 'expecting', 'sake', 'two', 'plan', 'illustrated', 'ix', 'presumes', 'whereas', 'devote', 'remaining', '9', 'management', 'lexical', 'raw', 'categorizing', 'extracting', 'analyzing', 'grammar', 'managing', 'total', '18', '36', 'suggested', 'approximate', 'lecture', 'per', 'convention', 'typographical', 'bold', '--', 'indicates', 'term', 'italic', 'paragraph', 'refer', 'url', 'filename', 'extension', 'constant', 'width', 'listing', 'well', 'element', 'keywords', 'command', 'literally', 'replaced', 'determined', 'metavariables', 'icon', 'signifies', 'tip', 'suggestion', 'caution', 'warning', 'job', 'done', 'may', 'contact', 'permission', 'youõre', 'reproducing', 'portion', 'us', 'require', 'selling', 'distributing', 'cd', 'rom', 'oõreilly', 'answering', 'citing', 'quoting', 'incorporating', 'productõs', 'attribution', 'title', 'author', 'publisher', 'isbn', 'ònatural', 'steven', 'bird', 'ewan', 'klein', 'edward', 'loper', 'reilly', 'medium', '978', '596', '51649', 'ó', 'feel', 'outside', 'fair', 'given', '@', 'oreilly', 'acknowledgment', 'indebted', 'feedback', 'earlier', 'draft', 'doug', 'arnold', 'michaela', 'atterer', 'greg', 'aumann', 'kenneth', 'beesley', 'bethard', 'ondrej', 'bojar', 'chris', 'cieri', 'robin', 'cooper', 'grev', 'corbett', 'james', 'curran', 'dan', 'garrette', 'jean', 'mark', 'gawron', 'hellmann', 'nitin', 'indurkhya', 'liberman', 'peter', 'ljunglöf', 'stefan', 'müller', 'munn', 'joel', 'nothman', 'adam', 'przepiorkowski', 'brandon', 'rhodes', 'stuart', 'robinson', 'jussi', 'salmela', 'kyle', 'schlansker', 'rob', 'speer', 'richard', 'sproat', 'thankful', 'colleague', 'comment', 'participant', 'summer', 'school', 'brazil', 'india', 'usa', 'exist', 'member', 'developer', 'community', 'expertise', 'extending', 'grateful', 'national', 'foundation', 'consortium', 'clarence', 'dyason', 'fellowship', 'edinburgh', 'melbourne', 'thank', 'julie', 'steele', 'abby', 'fox', 'loranah', 'dimant', 'team', 'organizing', 'review', 'cheerfully', 'customizing', 'production', 'meticulous', 'editing', 'preparing', 'revised', 'edition', 'michael', 'korobov', 'leading', 'effort', 'port', 'antoine', 'trux', 'owe', 'huge', 'debt', 'gratitude', 'mimo', 'jee', 'love', 'patience', 'year', 'hope', 'child', 'andrew', 'alison', 'kirsten', 'leonie', 'maaike', 'catch', 'enthusiasm', 'computation', 'page', 'associate', 'professor', 'senior', 'completed', 'phd', 'phonology', '1990', 'supervised', 'moved', 'cameroon', 'conduct', 'grassfields', 'bantu', 'auspex', 'institute', 'recently', 'spent', 'director', 'led', 'r', '&', 'create', 'database', 'established', 'group', 'curriculum', '2009', 'president', 'association', 'informatics', 'formal', 'cambridge', '1978', 'sussex', 'newcastle', 'upon', 'tyne', 'took', 'position', 'involved', 'establishment', '1993', 'closely', 'ever', '2000', '–', '2002', 'leave', 'act', 'manager', 'edify', 'corporation', 'santa', 'clara', 'responsible', 'spoken', 'dialogue', 'european', 'founding', 'coordinator', 'excellence', 'elsnet', 'went', 'ta', 'share', 'helped', 'documenting', 'epydoc', 'doctest', 'royalty', 'sale', 'figure', 'xiv', 'july', '2007', 'document', 'copyright', '©', '2019', 'distributed', '[', '/],', 'creative', 'common', 'noncommercial', 'derivative', 'united', 'state', 'license', 'creativecommons', 'nc', 'nd', '/].', 'built', 'wed', 'sep', '40', '48', 'acst', 'hand', 'million', 'assuming', 'achieve', 'combining', 'extract', 'phrase', 'sum', 'divided', 'quite', 'linguistically', 'motivated', 'necessarily', 'explaining', 'closer', 'flag', 'mix', 'front', 'authentic', 'taste', 'elementary', 'familiarity', 'repeat', 'miss', 'anything', 'completely', 'raise', 'addressed', 'familiar', 'day', 'treat', 'started', 'interpreter', 'friendly', 'thing', 'directly', 'running', 'environment', 'idle', 'mac', 'find', '→', 'macpython', 'run', 'shell', 'typing', 'installed', 'blurb', 'check', '):', 'default', 'oct', '15', '2014', '22', '01', '37', 'gcc', 'apple', 'llvm', 'clang', '503', ')]', 'darwin', '\",', 'credit', 'unable', 'probably', 'correctly', 'visit', 'older', 'operator', 'round', 'fractional', 'result', 'downwards', 'expected', 'behavior', '__future__', 'import', 'prompt', 'waiting', 'copying', '\">>>\"', 'let', 'begin', 'calculator', '+', '*', 'finished', 'calculating', 'displaying', 'reappears', 'another', 'turn', 'enter', 'asterisk', '(*)', 'multiplication', 'slash', '(/)', 'bracketing', 'demonstrate', 'interactively', 'experimenting', 'various', 'nonsensical', 'handle', '\"<', 'stdin', '>\",', '^', 'syntaxerror', 'invalid', 'produced', 'error', 'plus', 'sign', 'occurred', '<', '>,', 'stand', '\").', 'going', 'follow', 'selecting', 'shown', 'downloading', 'browse', 'tab', 'downloader', 'grouped', 'select', 'labeled', 'obtain', 'consists', '30', 'compressed', '100mb', 'disk', 'space', '.,', 'nearly', 'ten', 'size', 'continues', 'expand', 'load', 'tell', '*.', 'say', 'item', '.\"', 'printing', 'welcome', 'message', 'output', 'care', 'spelling', 'punctuation', 'right', 'remember', '>>>.', '***', 'loading', 'text1', '...,', 'text9', 'sent1', 'sent9', \"()'\", 'sent', 'moby', 'dick', 'herman', 'melville', '1851', 'text2', 'sensibility', 'jane', 'austen', '1811', 'text3', 'genesis', 'text4', 'inaugural', 'text5', 'text6', 'monty', 'holy', 'grail', 'text7', 'wall', 'street', 'journal', 'text8', 'personal', 'man', 'thursday', 'g', 'chesterton', '1908', '>', 'searching', 'concordance', 'occurrence', 'monstrous', 'placing', '\")', 'match', 'ong', 'former', 'came', 'towards', 'psalm', 'touching', 'bulk', 'whale', 'ork', 'heathenish', 'club', 'spear', 'thick', 'gazed', 'wondered', 'cannibal', 'savage', 'hav', 'survived', 'flood', 'mountainous', 'himmal', 'might', 'scout', 'fable', 'still', 'worse', 'de', 'th', 'radney', '.\\'\"', '55', 'picture', 'shall', 'ere', 'l', 'scene', 'connexion', 'fo', 'ght', 'rummaged', 'cabinet', 'telling', 'bone', 'oftentimes', 'cast', 'dead', 'particular', 'fast', 'arrow', 'ctrl', 'alt', 'p', 'modify', 'searched', 'included', 'affection', 'lived', 'back', '1789', 'nation', 'terror', 'god', 'differently', 'np', 'unconventional', 'im', 'ur', 'lol', 'uncensored', '!)', 'little', 'examining', 'richness', 'diversity', 'broader', 'saw', '___', 'appear', 'similar', 'appending', 'inserting', 'maddens', 'doleful', 'gamesome', 'subtly', 'uncommon', 'untoward', 'exasperate', 'loving', 'passing', 'mouldy', 'christian', 'true', 'mystifying', 'imperial', 'modifies', 'contemptible', 'heartily', 'exceedingly', 'remarkably', 'vast', 'great', 'amazingly', 'extremely', 'sweet', 'observe', 'positive', 'connotation', 'intensifier', 'common_contexts', 'shared', 'enclose', 'square', 'bracket', 'separate', 'comma', '([\"', '\"])', 'a_pretty', 'is_pretty', 'am_glad', 'be_glad', 'a_lucky', 'pair', 'occurs', 'display', 'however', 'determine', 'location', 'appears', 'positional', 'displayed', 'dispersion', 'plot', 'stripe', 'represents', 'row', 'entire', 'pattern', 'last', '220', 'constructed', 'joining', 'liberty', 'constitution', 'predict', 'quote', 'exactly', 'dispersion_plot', 'citizen', 'democracy', 'freedom', 'duty', 'america', 'presidential', 'investigate', 'installation', 'google', 'ngrams', 'fun', 'generating', 'random', 'seen', 'generate', 'nothing', 'go', 'brother', 'hairy', 'whose', 'top', 'reach', 'unto', 'heaven', 'ye', 'sow', 'land', 'egypt', 'bread', 'month', 'earth', 'thy', 'wage', 'made', 'father', 'isaac', 'old', 'kissed', 'laban', 'cattle', 'midst', 'esau', 'born', 'phichol', 'chief', 'butler', 'son', 'reinstated', 'vocabulary', 'obvious', 'fact', 'emerges', 'differ', 'count', 'jump', 'experiment', 'though', 'studied', 'trying', 'finding', 'length', 'finish', 'symbol', 'len', '44764', '44', '764', 'token', 'technical', ':)', 'distinct', 'contain', 'pose', 'slightly', 'duplicate', 'collapsed', 'screen', 'sorted', '))', \"['!\", \"',\", '\"\\'\",', \"'(',\", \"')',\", \"',',\", \"',)',\", \"'.\", \")',\", \"':',\", \"';',\", \"';)',\", \"'?\", 'abel', 'abelmizraim', 'abidah', 'abide', 'abimael', 'abimelech', 'abr', 'abrah', 'abraham', 'abram', 'accad', 'achbor', 'adah', '...]', '2789', 'wrapping', 'continuing', 'capitalized', 'precede', 'lowercase', 'discover', 'indirectly', 'asking', 'although', '789', 'form', 'specific', 'considered', 'generally', 'call', 'calculate', 'measure', '%', 'equivalently', '16', 'average', '06230453042623537', 'compute', 'percentage', 'smote', '100', '4643016433938312', 'calculation', 'keep', 'retyping', 'formula', 'lexical_diversity', 'define', 'keyword', 'def', 'encountering', 'colon', 'expects', 'indented', 'indentation', 'hitting', 'blank', 'definition', 'specify', 'placeholder', 'actual', 'reoccurs', 'similarly', 'know', 'ahead', '13477005109975562', '80', \"'),\", 'recap', 'close', 'encountered', '(),', 'always', 'add', 'empty', 'talking', 'mention', 'outset', 'newcomer', 'power', 'creativity', 'worry', 'bit', 'confusing', 'tabulating', 'repetitive', 'genre', 'hobby', '82345', '11935', '145', 'humor', '21695', '5017', '231', 'fiction', '14470', '3233', '223', 'press', 'reportage', '100554', '14394', '143', 'romance', '70022', '8452', '121', 'religion', '39399', '6373', '162', 'brown', 'moment', 'represent', 'case', 'opening', '=', \"['\", 'ishmael', \"'.']\", 'equal', 'quoted', 'separated', 'surrounded', 'bracketed', 'store', 'inspect', 'ask', 'sent2', '…', '*).', 'family', 'dashwood', 'settled', 'sent3', 'ex1', \"'].\", ')),', \"').\", 'pleasant', 'surprise', 'adding', 'creates', 'everything', \"']\", 'concatenation', 'combine', 'concatenate', 'either', 'pre', 'sent4', 'fellow', \"'-',\", 'senate', 'representative', 'append', 'indexing', 'represented', 'combination', 'ordinary', '1st', '173rd', '14', '278th', 'printed', 'analogously', 'identify', 'instruct', '173', ']', 'awaken', 'converse', 'sublists', 'manageable', 'piece', 'slicing', '16715', '16735', 'u86', 'thats', 'gamefly', 'actually', 'game', 'buying', '1600', '1625', 'anarcho', 'syndicalist', 'commune', 'sort', 'executive', 'officer', 'week', 'subtlety', 'word1', 'word2', 'word3', 'word4', 'word5', 'word6', 'word7', 'word8', 'word9', 'word10', 'notice', 'zero', '],', 'forward', 'leaf', 'initially', 'typical', 'modern', 'hang', 'mastered', 'century', '19xy', '20th', 'live', 'country', 'floor', 'numbered', 'walking', 'flight', 'stair', 'accidentally', 'traceback', 'recent', 'indexerror', 'syntactically', 'correct', 'brief', 'explanation', 'verify', 'slice', 'omit', '[:', '141525', ':]', 'among', 'merit', 'happiness', 'elinor', 'marianne', 'ranked', 'considerable', 'sister', 'living', 'almost', 'sight', 'disagreement', 'producing', 'coolness', 'husband', 'assigning', 'put', 'left', 'replace', 'consequence', 'generates', 'minute', 'saved', 'lot', '250', '000', 'defining', 'follows', 'misleading', 'moving', 'my_sent', 'xyzzy', 'must', 'letter', 'underscore', 'bravely', 'sir', 'rode', 'camelot', 'noun_phrase', 'multiple', 'happens', '\"...\"', 'matter', 'continuation', 'easier', 'choose', 'remind', 'anyone', 'meant', 'blindly', 'restriction', 'cannot', 'reserved', 'hold', 'especially', 'vocab', 'vocab_size', '19317', 'choice', 'identifier', 'optionally', 'digit', 'abc23', 'fine', '23abc', 'cause', 'sensitive', 'myvar', 'my_var', 'insert', 'hyphen', 'var', 'wrong', 'interprets', '\"-\"', 'minus', 'assign', 'mont', 'montymonty', \"'!'\", \"!'\", 'join', \"(['\", \"'])\", 'statistic', 'bring', 'bear', 'began', 'compile', 'automatic', 'characteristic', 'predicting', 'whether', 'got', 'sure', 'saying', 'said', '[-', 'expect', 'informative', 'imagine', '50', 'frequent', 'tally', 'thousand', 'laborious', 'appearing', 'observable', 'event', 'freqdist', 'fdist1', '260819', 'outcome', 'most_common', \"[(',',\", '18713', '13721', \"('.\", '6862', '6536', '6024', '4569', '4542', \"(';',\", '4072', '3916', '2982', '(\"\\'\",', '2684', \"('-',\", '2552', '2459', '2209', '2124', '1739', '1695', '1661', '1659', '1632', '1620', '(\\'\"\\',', '1478', '1462', '1414', '1280', \"('!\", '1269', '1231', '1137', '1113', '1103', \"('--',\", '1070', '1058', '1052', '1030', '1005', '918', '906', '889', '841', '767', '760', '715', '705', '697', '680', '646', '640', \"('?\", '637', '627', '624', 'invoke', 'counted', '260', '819', 'frequently', 'occurring', 'uppercase', 'nameerror', '900', 'plumbing', 'proportion', 'cumulative', 'account', 'half', 'occur', 'hapaxes', 'lexicographer', 'cetological', 'contraband', 'expostulation', 'others', 'seems', 'rare', 'seeing', 'neither', 'infrequent', 'grained', 'selection', 'perhaps', 'adapt', 'property', 'w', 'express', 'interest', '1a', 'v', '\".', '{', '|', '∈', ')}', 'b', '1b', 'executable', 'long_words', 'circumnavigation', 'physiognomically', 'apprehensiveness', 'cannibalistically', 'characteristically', 'circumnavigating', 'comprehensiveness', 'hermaphroditical', 'indiscriminately', 'indispensableness', 'irresistibleness', 'preternaturalness', 'responsibility', 'simultaneousness', 'subterraneousness', 'supernaturalness', 'superstitiousness', 'uncomfortableness', 'uncompromisedness', 'undiscriminating', 'uninterpenetratingly', 'greater', 'ignored', 'discus', 'changing', 'condition', 'difference', '...]?', 'characterize', 'reflect', 'constitutionally', 'transcontinental', 'informal', 'boooooooooooglyyyyyy', 'yuuuuuuuuuuuummmmmmmmmmmm', 'succeeded', 'typify', 'better', 'promising', 'eliminates', 'antiphilosophists', 'longer', 'seven', 'fdist5', \"['#\", '19teens', \"'#\", 'talkcity_adults', \"'((((((((((',\", \"'........',\", 'cute', '.-', 'everyone', 'football', 'innocent', 'listening', 'seriously', 'tomorrow', 'watching', 'bearing', 'modest', 'milestone', 'bigram', 'unusually', 'red', 'wine', 'resistant', 'substitution', 'sens', 'maroon', 'sound', 'definitely', 'odd', 'accomplished', \"']))\", \"[('\", \"')]\", 'omitted', '...]),', 'generator', '0x10fb8b3a8', '>.', 'essentially', 'except', 'pay', 'attention', 'ago', 'federal', 'government', 'american', 'vice', 'almighty', 'magistrate', 'justice', 'bless', 'indian', 'tribe', 'public', 'political', 'party', 'drinker', 'quiet', 'night', 'smoker', 'age', 'financially', 'secure', 'weekend', 'po', 'rship', 'married', 'mum', 'permanent', 'relationship', 'slim', 'emerge', 'larger', 'creating', 'fdist', '19', '({', '50223', '47933', '42345', '38513', '26597', '17111', '14399', '9966', '6428', '3528', '...})', 'deriving', 'quarter', '20', 'twenty', 'none', 'wonder', 'etc', '[(', '1873', '12', '1053', '13', '567', '177', '70', '17', 'max', 'freq', '19255882431878046', 'roughly', '%)', 'pursue', 'summarizes', 'description', '+=', 'increment', 'iterate', 'greatest', 'tabulate', '|=', 'fdist2', 'update', 'taking', 'far', 'ability', 'potential', 'automation', 'behalf', 'executing', 'met', 'repeatedly', 'looping', 'satisfied', '>=,', 'relational', '<=', '==', '\"=\"', '!=', '>=', 'comparison', 'news', 'changed', 'sent7', 'undefined', 'pierre', 'vinken', '61', 'board', 'nonexecutive', 'nov', '29', \"[',',\", \".']\", 'yield', 'false', 'listed', 'startswith', 'substring', 'islower', 'cased', 'isupper', 'isalpha', 'alphabetic', 'isalnum', 'alphanumeric', 'isdigit', 'istitle', 'titlecased', 'initial', 'capital', 'ableness', 'gnt', 'entirely', \"'))\", 'comfortableness', 'honourableness', 'immutableness', 'sovereignty', '())', 'aaaaaaaaah', 'aaaaaaaah', 'aaaaaah', 'aaaah', 'aaaaugh', 'aaagh', 'c1', 'c2', 'conjunction', 'disjunction', 'explain', \"'-'\", 'wd', 'cie', 'cei', 'operating', 'upper', \"['[',\", \"']',\", 'etymology', 'f', '...],', 'operates', 'described', 'fixed', 'habitually', 'bothering', 'mastering', 'fluent', '17231', 'double', 'capitalization', 'wiped', 'eliminate', 'filtering', '()))', '16948', 'complicated', 'purely', 'simpler', '?).', 'confident', 'nested', 'execute', 'conditional', '].', 'cat', 'invoked', 'executed', 'indent', 'recognized', 'print_function', \"']:\", 'executes', 'circular', 'fashion', 'relates', 'action', 'elif', 'titlecase', 'small', 'multiline', 'invaluable', 'comfortable', \"='\", 'newline', 'tricky', 'ancient', 'ceiling', 'conceit', 'conceited', 'conceive', 'conscience', 'conscientious', 'conscientiously', 'deceitful', 'deceive', 'bottom', 'exploiting', 'opportunity', 'nitty', 'gritty', 'paint', 'bigger', 'navigate', 'universe', 'crucial', 'popularity', 'shortcoming', 'luck', 'tourist', 'site', 'philadelphia', 'pittsburgh', 'limited', 'budget', 'expert', 'digital', 'slr', 'camera', 'prediction', 'steel', 'market', 'credible', 'commentator', 'summarization', 'carried', 'robustness', 'beyond', 'capability', 'philosophical', 'standing', 'intelligent', 'major', 'behaviour', 'become', 'mature', 'unrestricted', 'prospect', 'emerged', 'plausible', 'disambiguation', 'consider', 'ambiguous', 'serve', 'dish', 'food', 'drink', 'office', 'ball', 'plate', 'meal', 'device', 'served', 'unlikely', 'shifted', 'sport', 'crockery', 'invent', 'bizarre', 'image', 'tennis', 'pro', 'frustration', 'china', 'tea', 'laid', 'beside', 'court', 'disambiguate', 'nearby', 'related', 'contextual', 'effect', 'agentive', 'cup', 'stove', 'locative', 'submit', 'friday', 'temporal', 'submitting', '3c', 'italicized', 'interpret', 'lost', 'searcher', 'mountain', 'afternoon', 'pronoun', 'resolution', 'deeper', 'verb', 'harder', 'thief', 'stole', 'painting', 'stealing', '4c', 'sold', 'caught', 'subsequently', 'antecedent', 'tackling', 'anaphora', 'noun', 'refers', 'labeling', 'agent', 'patient', 'instrument', 'relating', 'demonstrates', 'translate', 'accurately', 'conveying', 'original', 'translating', 'french', 'forced', 'gender', 'il', 'masculine', 'elles', 'feminine', 'depends', 'voleurs', 'ont', 'volé', 'peintures', 'été', 'trouvés', 'tard', 'trouvées', 'establishing', 'mt', 'ultimately', 'seeking', 'high', 'root', 'cold', 'war', 'promise', 'sponsorship', 'today', 'integrated', 'serious', 'starkly', 'revealed', 'equilibrium', 'reached', 'alice', 'spring', 'wie', 'lang', 'vor', 'dem', 'folgenden', 'flug', 'zu', 'springen', 'sie', 'bevor', 'der', 'folgende', 'tun', 'tut', 'sprung', 'leap', 'translates', 'german', '>),', 'preposition', 'translated', 'phrasing', 'indicated', 'proper', 'misinterpreted', 'grammatical', 'translationparty', 'target', 'faced', 'collecting', 'massive', 'parallel', 'publish', 'possibly', 'bilingual', 'dictionary', 'alignment', 'dialog', 'history', 'namely', 'turing', 'responding', 'naturally', 'distinguish', 'generated', 'commercial', 'narrowly', 'saving', 'private', 'ryan', 'playing', 'theater', 'paramount', 'madison', '00', 'driving', 'restaurant', 'incorporated', 'asks', 'movie', 'showing', 'determines', 'endowed', 'interact', 'asked', '?,', 'unhelpfully', 'respond', 'yes', 'assumption', 'ensure', 'request', 'handled', 'screening', 'service', 'pipeline', 'parsed', 'planned', 'realized', 'suitably', 'inflected', 'inform', 'stage', 'commonly', 'assumed', 'diagram', 'map', 'via', 'middle', 'reverse', 'converting', 'static', 'repository', 'draw', 'primitive', 'conversation', 'chatbot', 'textual', 'entailment', 'brought', 'recognizing', 'rte', 'scenario', 'suppose', 'evidence', 'hypothesis', 'sandra', 'goudie', 'defeated', 'purnell', 'elected', 'parliament', 'election', 'winning', 'seat', 'coromandel', 'defeating', 'labour', 'candidate', 'pushing', 'incumbent', 'green', 'mp', 'jeanette', 'fitzsimons', 'accept', 'conclusion', 'allow', 'competitor', 'brute', 'chap', 'intensive', 'consequently', 'person', 'david', 'golinkin', 'editor', 'eighteen', '150', 'responsa', 'article', 'sermon', 'supported', 'someone', 'ii', 'conclude', 'limitation', 'despite', 'advance', 'reasoning', 'manner', 'wait', 'solved', 'meantime', 'necessary', 'severe', 'accordingly', 'progress', ',\"', 'superficial', 'indeed', 'equip', 'contribute', 'aspiration', 'summary', 'appearance', ')).', 'operate', 'x', 'derive', 'collapsing', 'distinction', 'ignoring', '()).', ':.', 'assigned', 'reused', 'mult', 'mixed', 'consolidated', '/),', 'link', 'wikipedia', 'acquaint', 'beginner', 'guide', 'moin', 'beginnersguide', 'miscellaneous', 'answered', 'faq', 'delve', 'subscribe', 'mailing', 'announced', 'covered', 'fred', 'damerau', 'ed', '2010', 'handbook', 'chapman', 'crc', 'dale', 'moisl', 'somers', 'daniel', 'mitkov', 'ruslan', '2003', 'oxford', 'international', 'acl', 'www', 'aclweb', '/)', 'host', 'regional', 'conference', 'workshop', 'anthology', 'literature', 'indexed', 'finegan2007', '_', 'grady', 'et', 'al', '2004', 'osu', 'languagelog', 'popular', 'occasional', 'post', 'alphabet', '26', '**', '141167095653376', 'applied', 'score', 'lexically', 'protagonist', 'willoughby', 'played', 'male', 'female', 'novel', 'couple', 'my_string', 'pressing', 'multiplying', 'joined', 'fix', '[\"', '\"]', 'favorite', 'phrase1', 'phrase2', 'whole', ')?', 'typically', '\"[', '\"][', '][', 'alphabetical', 'sunset', 'trial', 'sent8', '()?', 'decreasing', 'script', 'meet', '...].', 'ise', 'z', 'pt', 'sell', 'sea', 'shore', 'sh', 'percent', 'calculates', 'docutils', 'ch01', 'rst2', '1889', 'backlink', 'unknown', 'helpful', 'repeating', 'unfamiliar', 'substituting', 'hows', 'mentioned', 'examined', 'convenience', 'glued', 'treated', 'accessed', 'examines', 'gutenberg', 'electronic', 'archive', '25', 'hosted', 'fileids', 'emma', 'persuasion', 'bible', 'kjv', 'blake', 'poem', 'bryant', 'burgess', 'busterbrown', 'carroll', 'edgeworth', 'parent', 'moby_dick', 'milton', 'paradise', 'shakespeare', 'caesar', 'hamlet', 'macbeth', 'whitman', '192427', 'showed', 'carry', 'concordancing', 'nine', 'obtained', 'surprize', 'cumbersome', 'fileid', 'compact', 'nearest', 'num_chars', 'num_words', 'num_sents', 'num_vocab', ')))', '28', '34', '79', '23', '21', '52', 'recurrent', 'really', 'divide', 'macbeth_sentences', \"[['[',\", 'tragedie', 'william', '1603', \"']'],\", 'actus', 'primus', \"'],\", '1116', 'toile', 'trouble', 'fire', 'burne', 'cauldron', 'bubble', 'longest_len', \"[['\", 'doubtfull', 'stood', 'swimmer', 'cling', 'choake', 'mercilesse', 'macdonwald', '...]]', 'richer', 'firefox', 'forum', 'overheard', 'york', 'pirate', 'carribean', 'advertisement', 'webtext', ')[:', '65', \"'...')\", 'cookie', 'cooky', 'se', 'wind', 'clop', 'king', 'arthur', 'whoa', 'white', 'guy', 'evening', 'asian', 'girl', 'chest', 'ted', 'elliott', 'terr', 'sexy', 'seek', 'attrac', 'lady', 'discreet', 'encoun', 'lovely', 'delicate', 'fragrant', 'rhone', 'polished', 'leather', 'strawb', 'instant', 'messaging', 'collected', 'naval', 'detection', 'internet', 'predator', 'anonymized', 'replacing', 'usernames', 'generic', 'usernnn', 'manually', 'edited', 'remove', 'date', 'chatroom', 'teen', 'adult', '20s_706posts', 'xml', '706', 'gathered', 'room', '2006', 'nps_chat', '123', 'hot', 'pic', 'mirror', '1961', '500', 'categorized', 'editorial', 'icame', 'uib', 'bcm', 'los', 'id', 'a16', 'ca16', 'chicago', 'tribune', 'b02', 'cb02', 'monitor', 'c17', 'cc17', 'magazine', 'd12', 'cd12', 'underwood', 'probing', 'ethic', 'realtor', 'e36', 'ce36', 'norling', 'renting', 'car', 'europe', 'f25', 'cf25', 'lore', 'boroff', 'jewish', 'teenage', 'culture', 'g22', 'cg22', 'belles_lettres', 'reiner', 'coping', 'runaway', 'h15', 'ch15', 'civil', 'defence', 'mobilization', 'fallout', 'shelter', 'j17', 'cj19', 'learned', 'mosteller', 'statistical', 'k04', 'ck04', 'du', 'bois', 'color', 'l13', 'cl13', 'mystery', 'hitchens', 'footstep', 'm01', 'cm01', 'science_fiction', 'heinlein', 'stranger', 'strange', 'n14', 'cn15', 'adventure', 'rattlesnake', 'ridge', 'p12', 'cp12', 'callaghan', 'passion', 'rome', 'r06', 'cr06', 'thurber', 'comedy', 'category', 'fulton', 'county', 'grand', 'jury', \"=['\", \"'...],\", 'convenient', 'studying', 'inquiry', 'stylistics', 'modal', 'news_text', '94', '87', '93', '38', '53', '389', 'wh', 'unpick', 'ignore', 'concentrate', 'cfd', 'conditionalfreqdist', '86', '66', '82', '59', '78', '54', '71', '268', '58', '131', '83', '264', '49', '74', '193', '51', '45', '43', 'predicted', 'reuters', '788', 'totaling', 'classified', '90', 'training', '\";', '14826', 'drawn', '14828', '14829', '14832', 'acq', 'alum', 'barley', 'bop', 'carcass', 'castor', 'oil', 'cocoa', 'coconut', 'coffee', 'copper', 'copra', 'cake', 'corn', 'cotton', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'overlap', '9865', 'grain', 'wheat', '9880', 'money', 'fx', '15618', '15649', '15676', '15728', '15871', '14858', '15033', '15043', '15106', '15287', '15341', '15648', 'handful', \"')[:\", 'cereal', 'export', 'bid', 'requested', 'licence', 'thai', 'trade', 'deficit', 'widens', 'looked', 'fig', 'offset', 'ax', 'dimension', 'washington', '1793', '1797', '()]', '1801', '1805', '1809', '1813', '1817', '1821', 'extracted', '])', 'kept', 'plotted', 'trend', 'observed', 'normalized', 'annotation', 'howto', 'compiler', 'francis', 'kucera', '15m', 'tagged', 'ce', 'treebanks', 'clic', 'ub', '1m', 'catalan', 'pereira', 'warren', 'geographic', 'cmu', 'pronouncing', '127k', 'entry', 'conll', '270k', 'chunked', '700k', 'dutch', 'sel', '150k', 'basque', 'treebank', 'narad', 'penn', 'framenet', 'fillmore', 'baker', '10k', '170k', 'floresta', 'diana', 'santos', '9k', 'gazetteer', 'city', 'misc', '200k', 'hart', 'newby', '2m', 'cspan', 'kumaran', '60k', 'bangla', 'marathi', 'telugu', 'macmorpho', 'nilc', 'usp', 'brazilian', 'pang', 'lee', '2k', 'polarity', 'kantrowitz', 'ross', '8k', 'nist', '1999', 'info', 'extr', 'garofolo', '63k', 'newswire', 'sgml', 'markup', 'nombank', 'meyers', '115k', 'proposition', '1400', 'frame', 'forsyth', 'martell', 'bond', 'aligned', 'pp', 'attachment', 'ratnaparkhi', '28k', 'prepositional', 'modifier', 'bank', 'palmer', '113k', '3300', 'li', 'roth', '6k', '3m', 'roget', 'thesaurus', 'dagan', 'semcor', 'ru', 'mihalcea', '880k', 'senseval', 'pedersen', '600k', 'sentiwordnet', 'esuli', 'sebastiani', '145k', 'synonym', 'bosak', 'union', '485k', 'stopwords', 'porter', '400', 'swadesh', 'wiktionary', 'comparative', 'wordlists', '24', 'switchboard', 'ldc', 'phonecalls', 'transcribed', 'univ', 'decl', '480k', '300', '40k', 'timit', 'audio', 'transcript', 'speaker', 'verbnet', '5k', 'hierarchically', 'wordlist', 'openoffice', '960k', '20k', 'affix', 'miller', 'fellbaum', 'encoding', 'cess_esp', 'el', 'grupo', 'estatal', 'electricit', '\\\\', 'xe9_de_france', 'um', 'revivalismo', 'refrescante', '7_e_meio', 'प', 'ू', 'र', '्', 'ण', 'रत', 'ि', 'ब', 'ं', 'ध', 'हट', 'ा', 'ओ', 'इर', 'क', 'स', 'य', 'ु', 'त', 'udhr', 'abkhaz', 'cyrillic', 'abkh', 'utf8', 'achehnese', 'latin1', 'achuar', 'shiwiar', 'adja', 'afaan_oromo_oromiffa', 'afrikaans', 'aguaruna', 'akuapem_twi', 'albanian_shqip', 'amahuaca', 'javanese', \"')[\", 'saben', 'umat', 'manungsa', 'lair', 'kanthi', 'hak', 'universal', 'declaration', 'boolean', 'chickasaw', 'german_deutsch', 'greenlandic_inuktikut', 'hungarian_magyar', 'ibibio_efik', \"'-\", 'six', 'fewer', 'ibibio', '60', 'inuktitut', 'raw_text', 'unfortunately', 'insufficient', 'industrial', 'developing', 'piecemeal', 'endangered', 'summarized', 'lack', 'correspond', 'notably', 'topical', 'occasionally', 'isolated', 'categorization', '([', '=[', 'f1', 'f2', 'f3', 'abspath', 'stream', 'path', 'locally', 'readme', 'illustrate', 'buster', 'thornton', '1920', 'fishing', 'yawned', 'lay', 'bed', 'watched', 'morning', 'sunbeam', 'creeping', 'plaintextcorpusreader', 'directory', 'usr', 'dict', 'whatever', 'corpus_root', 'initializer', \"'[\", 'abc', ']/.', '*\\\\.', \"'/\", \"*')\", 'connective', 'propernames', 'web2', 'web2a', 'local', ':\\\\', 'bracketparsecorpusreader', 'file_pattern', 'contained', 'subfolders', 'penntreebank', 'mrg', 'wsj', '\".*/', 'wsj_', 'ptb', 'wsj_0001', 'wsj_0002', 'wsj_0003', 'wsj_0004', '49208', 'wsj_2013', 'mr', 'noriega', 'smooth', 'shah', 'iran', 'nicaragua', '\"\\'', 'anastasio', 'somoza', 'ferdinand', 'marcos', 'philippine', 'bloody', 'haiti', 'baby', 'duvalier', 'mylist', 'generalize', 'maintain', 'achieved', 'depicts', '161', '192', 'genre_word', '170576', '#', '_start', '\"\\'\\'\"),', \"'.')]\", '_end', 'usual', '_conditions', 'satisfy', '3899', '3736', '2758', '1776', '1502', '1335', '1186', \"('``',\", '1045', '(\"\\'\\'\",', '1044', '993', '951', '875', '702', '692', '690', '651', '583', '573', '559', '496', \"']['\", 'initialize', 'tabulation', 'reproduced', 'occured', 'exploit', '1865', 'lincoln', 'lowercased', 'derived', 'limit', 'selected', 'cell', '638', '185', '525', '883', '997', '1166', '1283', '1440', '1558', '1638', '171', '263', '614', '717', '894', '1013', '1110', '1213', '1275', 'newsworthy', 'romantic', 'monday', 'noticed', 'multi', ']),', 'permitted', 'introducted', 'consecutive', 'cryptic', 'generate_model', 'reset', 'likely', '());', 'inspecting', 'tends', 'stuck', 'randomly', 'cfdist', 'num', \"({'\", 'creature', \"':\", 'substance', \"',':\", 'soul', '})', 'code_random_text', 'obtains', 'record', 'seed', 'cfdist1', 'cfdist2', 'counter', 'reusing', 'retyped', 'mess', 'reuse', 'compose', 'menu', 'shortly', '================================', 'restart', 'revising', 'paste', 'descriptive', 'separating', 'monty_python', 'inline', 'interacting', 'plural', 'singular', 'reliable', 'localize', 'needed', 'behave', 'equivalent', 'my_text_data', 'arbitrary', 'word_count', 'diversity_score', '[:-', 'sx', 'ch', 'en', 'fairy', 'woman', 'code_plural', 'delete', 'latest', 'collect', 'previously', 'text_proc', 'importing', 'wish', 'fan', 'fen', 'obviously', 'edit', 'existing', 'confusion', 'imported', 'folder', 'secondary', 'enriched', 'my_text', 'word_freq', 'preparation', 'terminology', 'headword', 'lemma', 'homonym', 'gloss', 'spell', 'checker', 'mi', 'spelt', 'unusual_words', 'text_vocab', 'english_vocab', 'abbeyland', 'abhorred', 'abounded', 'abridgement', 'abused', 'abuse', 'accent', 'accepting', 'accommodation', 'accompanied', 'accounted', 'accustomary', 'ache', 'acknowledging', 'aaaaaaaaaaaaaaaaa', 'aaahhhh', 'abortion', 'abou', 'abourted', 'ab', 'ack', 'acros', 'actualy', 'adduser', 'adjusts', 'adoted', 'adreniline', 'ad', 'afe', 'affair', 'affari', 'affect', 'afk', 'agaibn', 'code_unusual', 'computes', 'leaving', 'filter', 'presence', 'fails', 'content_fraction', '7364374824583169', 'puzzle', 'grid', 'chosen', 'solving', 'iterates', 'obligatory', 'constraint', 'trickier', 'solution', 'twice', 'puzzle_letters', 'egivrvonl', 'glover', 'gorlin', 'govern', 'grovel', 'involver', 'lienor', 'linger', 'lovering', 'noiler', 'overling', 'region', 'renvoi', 'revolving', 'ringle', 'roving', 'violer', 'virole', 'male_names', 'female_names', 'abbey', 'abbie', 'addie', 'adrian', 'adrien', 'ajay', 'alex', 'alexis', 'alfie', 'ali', 'alix', 'allie', 'allyn', 'andie', 'andrea', 'andy', 'angel', 'angie', 'ariel', 'ashley', 'aubrey', 'augustine', 'austin', 'averil', 'h', 'equally', 'spreadsheet', 'synthesizer', 'cmudict', '133737', '42371', '42379', ']:', 'fir', 'er1', 'ay1', 'er0', 'firearm', 'aa2', 'fireball', 'ao2', 'phonetic', 'label', 'contrastive', 'pronunciation', 'syllable', 'arpabet', 'individually', ':,', 'pron', 'ph1', 'ph2', 'ph3', 'pait', 'ey1', 'pat', 'ae1', 'pate', 'patt', 'peart', 'peat', 'iy1', 'peet', 'peete', 'pert', 'pet', 'eh1', 'pete', 'pett', 'piet', 'piette', 'pit', 'ih1', 'pitt', 'pot', 'aa1', 'pote', 'ow1', 'pott', 'pout', 'aw1', 'puett', 'uw1', 'purt', 'uh1', 'putt', 'ah1', 'scan', 'looking', 'assigns', 'sounding', 'nick', 'rhyming', 'ih0', 'atlantic', 'audiotronics', 'avionics', 'beatnik', 'calisthenics', 'centronics', 'chamonix', 'chetniks', 'clinic', 'conic', 'cryogenics', 'cynic', 'diasonics', 'dominic', 'ebonics', 'electronics', '\\'\",', 'nics', 'niks', 'nix', 'ntic', 'silent', 'mismatch', 'summarize', 'autumn', 'column', 'condemn', 'damn', 'goddamn', 'hymn', 'solemn', 'gn', 'kn', 'mn', 'pn', 'stress', 'char', \"']]\", 'abbreviated', 'abbreviating', 'accelerated', 'accelerating', 'accelerator', 'accentuated', 'accentuating', 'accommodating', 'accommodative', 'accumulated', 'accumulating', 'accumulative', 'abbreviation', 'abomination', 'abortifacient', 'academician', 'accreditation', 'accumulation', 'acetylcholine', 'adjudication', 'doubly', 'minimally', 'p3', \"]+'-'+\", 'template', '()):', 'wordstring', '\"...\")', 'patch', 'pautsch', 'peach', 'perch', 'petsch', 'petsche', 'piche', 'piech', 'pietsch', 'pitch', 'pac', 'pack', 'paek', 'paik', 'pak', 'pake', 'paque', 'peak', 'peake', 'pech', 'peck', 'peek', 'perc', 'perk', 'pahl', 'pail', 'paille', 'pal', 'pale', 'pall', 'paul', 'paule', 'paull', 'peal', 'peale', 'pearl', 'paign', 'pain', 'paine', 'pan', 'pane', 'pawn', 'payne', 'peine', 'pen', 'penh', 'pine', 'pinn', 'paap', 'paape', 'pap', 'pape', 'papp', 'paup', 'peep', 'pep', 'pip', 'pipe', 'pipp', 'poop', 'pop', 'pope', 'paar', 'par', 'pare', 'parr', 'pear', 'peer', 'pier', 'poor', 'poore', 'por', 'pore', 'porr', 'pour', 'peace', 'pearse', 'pea', 'perce', 'pers', 'perse', 'pesce', 'piss', 'piett', 'peru', 'peugh', 'pew', 'plew', 'plue', 'prew', 'pru', 'prue', 'prugh', 'pshew', 'pugh', 'iterating', 'prondict', 'keyerror', 'existent', 'missing', 'tweak', 'absent', 'mapping', 'ph', ']]', 'ah0', 'ng', 'jh', 'eh0', 'tabular', '200', 'identified', 'iso', '639', 'bg', 'ca', 'cu', 'fr', 'hr', 'la', 'mk', 'nl', 'pl', 'ro', 'sk', 'sl', 'sr', 'sw', 'uk', 'thou', 'big', 'cognate', 'fr2en', 'je', 'tu', 'vous', 'chien', 'dog', 'jeter', 'throw', 'translator', 'de2en', 'es2en', 'hund', 'perro', 'germanic', '139', '140', '141', '142', ')[', 'sagen', 'zeggen', 'decir', 'dire', 'dizer', 'dicere', 'sing', 'singen', 'zingen', 'cantar', 'chanter', 'canere', 'spielen', 'spelen', 'jugar', 'jouer', 'jogar', 'brincar', 'ludere', 'float', 'schweben', 'zweven', 'flotar', 'flotter', 'flutuar', 'boiar', 'fluctuare', 'shoebox', 'replaces', 'traditional', 'card', 'repeatable', 'rotokas', 'kaa', 'gag', '\":', 'dic', 'ge', 'tkp', 'nek', 'pa', 'dcsv', 'vx', 'sc', \"'???\", 'dt', '2005', 'ex', 'apoka', 'ira', 'kaaroi', 'aioa', 'ia', 'reoreopaoro', 'xp', 'kaikai', 'bilong', 'bikos', 'na', 'toktok', 'xe', 'gagging', \"')]),\", 'consist', 'tok', 'pisin', 'loose', 'island', 'bougainville', 'papua', 'guinea', 'contributed', 'notable', 'inventory', 'phoneme', 'rotokas_language', 'semantically', '155', '287', '117', '659', 'motorcar', 'automobile', 'stay', 'pretty', 'benz', 'credited', 'invention', 'remained', 'unchanged', 'wn', 'synset', 'synonymous', 'lemma_names', 'auto', 'signify', 'train', 'carriage', 'gondola', 'elevator', 'prose', 'motor', 'vehicle', 'wheel', 'propelled', 'internal', 'combustion', 'ambiguity', 'pairing', 'unambiguous', '02', '03', '04', 'cable_car', 'railcar', 'railway_car', 'railroad_car', 'elevator_car', 'involving', 'hierarchy', 'abstract', 'gas', 'guzzler', 'hatchback', 'hypernym', 'hyponym', 'relation', 'superordinate', 'subordinate', 'immediate', 'types_of_motorcar', 'ambulance', 'model_t', 'suv', 'stanley_steamer', 'beach_waggon', 'beach_wagon', 'bus', 'cab', 'compact_car', 'convertible', 'coupe', 'cruiser', 'electric', 'electric_automobile', 'electric_car', 'estate_car', 'gas_guzzler', 'hack', 'hardtop', 'heap', 'horseless_carriage', 'rod', 'hot_rod', 'jalopy', 'jeep', 'landrover', 'limo', 'limousine', 'loaner', 'minicar', 'minivan', 'pace_car', 'patrol_car', 'phaeton', 'police_car', 'police_cruiser', 'prowl_car', 'race_car', 'racer', 'racing_car', 'roadster', 'runabout', 'saloon', 'secondhand_car', 'sedan', 'sport_car', 'sport_utility', 'sport_utility_vehicle', 'sports_car', 'squad_car', 'station_waggon', 'station_wagon', 'stock_car', 'subcompact', 'subcompact_car', 'taxi', 'taxicab', 'tourer', 'touring_car', 'seater', 'waggon', 'wagon', 'visiting', 'wheeled_vehicle', 'container', 'motor_vehicle', 'hypernym_paths', 'physical_entity', 'artifact', 'instrumentality', 'self', 'propelled_vehicle', 'conveyance', 'root_hypernyms', 'relate', 'meronym', 'holonym', 'trunk', 'crown', 'part_meronyms', 'heartwood', 'sapwood', 'substance_meronyms', 'forest', 'member_holonyms', 'burl', '07', 'limb', 'stump', 'intricate', 'mint', '05', '`', 'north', 'temperate', 'plant', 'genus', 'mentha', 'aromatic', 'mauve', 'flower', 'fresh', 'candied', 'candy', 'flavored', '06', 'coined', 'authority', 'part_holonyms', 'substance_holonyms', 'stepping', 'entail', 'walk', 'eat', 'chew', 'swallow', 'tease', 'arouse', 'disappoint', 'antonymy', 'supply', 'antonym', 'demand', 'rush', 'horizontal', 'inclined', 'vertical', 'staccato', 'legato', 'dir', 'harmony', \"')).\", 'similarity', 'traverse', 'knowing', 'cf', 'low', 'right_whale', 'orca', 'minke', 'minke_whale', 'tortoise', 'lowest_common_hypernyms', 'baleen_whale', 'vertebrate', 'baleen', 'quantify', 'generality', 'min_depth', 'incorporate', 'insight', 'path_similarity', 'shortest', 'connects', '(-', 'returned', 'comparing', 'decrease', 'inanimate', '16666666666666666', '07692307692307693', '043478260869565216', 'hierarhical', 'entered', 'funct', 'documented', 'extensively', 'published', 'agency', 'elra', 'higher', 'fee', 'brat', 'nlplab', 'olac', 'metadata', 'homepage', 'posting', 'ethnologue', 'touched', 'biber', 'conrad', 'reppen', '1998', 'mcenery', 'meyer', 'sampson', 'mccarthy', 'scott', 'tribble', 'quantitative', 'baayen', 'gries', 'wood', 'fletcher', 'hughes', '1986', 'psycholinguistics', 'retrieval', 'globalwordnet', 'budanitsky', 'hirst', 'phonetics', 'sorting', 'state_union', 'men', 'happened', 'member_meronyms', 'arise', 'suggest', 'strunk', 'nevertheless', 'advise', 'best', 'bartleby', 'strunk3', 'considering', 'fossilized', 'prejudice', '\\'\"', 'itre', 'ci', 'upenn', '/~', 'myl', '001913', 'bbc', 'vicky', 'pollard', 'behind', 'co', 'hi', '6173441', 'stm', 'yeah', '003993', 'impressionistic', 'closed', 'exhibit', 'all_synsets', 'supergloss', 'ratio', 'lowest', 'adjacent', 'omitting', 'absence', 'hedge', 'zipf', 'law', 'inversely', 'proportional', 'rank', '×', '50th', '150th', 'pylab', 'confirm', 'hint', 'logarithmic', 'abcdefg', '\"),', 'accumulate', 'light', 'intelligible', 'strength', 'weakness', 'hybrid', 'observation', 'find_language', 'latin', 'branching', 'factor', 'polysemy', 'adjective', 'adverb', 'predefined', 'ranking', 'experimentally', 'charles', 'gem', 'jewel', 'journey', 'voyage', 'boy', 'lad', 'coast', 'asylum', 'madhouse', 'magician', 'wizard', 'midday', 'noon', 'furnace', 'fruit', 'cock', 'crane', 'implement', 'monk', 'oracle', 'cemetery', 'woodland', 'rooster', 'hill', 'graveyard', 'slave', 'chord', 'smile', 'glass', 'ch03', 'undoubtedly', 'unlimited', 'tokenization', 'stemming', 'consolidate', 'dispense', 'onwards', 'assume', 'pprint', 'word_tokenize', 'catalog', 'ascii', 'finnish', 'italian', '2554', 'crime', 'punishment', 'urllib', 'urlopen', 'decode', 'str', \"'>\", '1176893', '75', 'ebook', 'fyodor', 'dostoevsky', 'downloads', 'proxy', 'detected', \"{'\", 'someproxy', '3128', \"'}\", 'proxyhandler', '176', '893', ').)', 'feed', '254354', '1024', '1062', 'exceptionally', 'young', 'garret', 'lodged', 'walked', 'slowly', 'hesitation', 'bridge', 'katerina', 'ivanovna', 'pyotr', 'petrovitch', 'pulcheria', 'alexandrovna', 'avdotya', 'romanovna', 'rodion', 'romanovitch', 'marfa', 'petrovna', 'sofya', 'semyonovna', 'tm', 'porfiry', 'amalia', 'nikodim', 'fomitch', 'ilya', 'dmitri', 'prokofitch', 'andrey', 'semyonovitch', 'hay', 'header', 'scanned', 'corrected', 'footer', 'reliably', 'resort', 'manual', 'inspection', 'trimming', '5338', 'rfind', '1157743', 'overwrite', 'brush', 'reality', 'unwanted', 'dealing', 'easiest', 'blonde', 'die', 'urban', 'legend', 'passed', 'health', '2284783', \"'<!\", 'doctype', '\"-//', 'w3c', '//', 'dtd', 'transitional', 'glory', 'meta', 'javascript', 'beautifulsoup', 'crummy', '/:', 'bs4', 'get_text', \"'|',\", 'concerning', 'navigation', '110', '390', 'gene', 'hey', 'hair', 'caused', 'recessive', 'blond', 'disadvantage', 'chance', 'disappear', 'thin', 'thought', 'unannotated', 'advantage', 'furthermore', 'smaller', 'reasonable', 'hit', 'adore', 'prefer', 'absolutely', '289', '905', '644', '460', '158', '62', '600', '198', '97', 'allowable', 'severely', 'restricted', 'arbitrarily', 'wildcards', 'inconsistent', 'geographical', 'duplicated', 'boosted', 'unpredictably', 'breaking', 'locating', 'ameliorated', 'apis', 'blogosphere', 'register', 'pypi', 'feedparser', 'llog', 'nll', '/?', 'atom', 'log', 'bf', \"'<\", 'chatting', 'prc', 'thinking', 'au', 'courant', 'dui4xiang4', \"'\\\\\", 'u5c0d', 'u8c61', '\\'(\"\\',', \"'/',\", 'friend', '\\'\"\\',', 'plain', 'offer', 'box', 'gone', 'pyshell', 'toplevel', 'ioerror', 'errno', 'listdir', \"('.')\", 'controlling', 'opened', 'marking', 'newlines', '.\\\\', 'nfruit', 'banana', 'keyboard', 'strip', 'demonstrated', 'pdf', 'msword', 'binary', 'specialized', 'pypdf', 'pywin32', 'particularly', 'challenging', 'conversion', 'drive', 'capturing', 'capture', 'normalization', 'discussed', 'tokenized', 'converted', 'properly', 'int', 'normalizing', 'attributeerror', 'query', \"?'\", 'beatles', 'john', 'george', 'ringo', 'typeerror', 'studiously', 'avoiding', 'focused', 'backslash', 'escape', 'literal', 'otherwise', 'report', 'circus', 'flying', \"\\\\'\", 'couplet', 'thee', '\"\\\\', ':\"', 'rough', 'shake', 'darling', 'bud', 'lease', 'hath', ':\")', 'sonnet', 'triple', '\"\"\"', ':\"\"\"', \"'''\", \":'''\", 'pasted', 'multiply', 'veryveryvery', \"'',\", 'subtraction', 'unsupported', 'operand', '-:', 'muddle', 'told', '-)', 'quotation', 'pythonholy', 'negative', 'colorless', 'sleep', 'furiously', '117092', '87996', '77916', '69326', '65617', 'q', 'j', 'sb', 'tuple', 'unpacking', 'somewhere', 'visualize', 'relative', 'continuous', 'pull', 'pyth', 'stopping', 'applies', 'stop', ':-', '\"\\')', \"...'.\", 'valueerror', 'rindex', 'glue', 'wherever', 'splitlines', 'uppercased', 'trailing', 'concatenating', 'brian', 'granularity', 'correspondingly', 'downstream', 'conversely', 'terminal', 'lennon', 'del', '0th', 'immutable', 'mutable', 'modified', 'speaking', 'realizing', 'extended', 'ø', 'danish', 'norwegian', 'ő', 'hungarian', 'ñ', 'breton', 'ň', 'czech', 'slovak', 'overview', 'uxxxx', 'xxxx', 'hexadecimal', 'normal', 'encoded', 'byte', 'subset', 'utf', 'mechanism', 'decoding', 'perspective', 'glyph', 'paper', 'font', 'polish', 'lat2', 'suggests', 'snippet', 'biblioteka_pruska', '8859', 'locates', 'unicode_samples', 'latin2', 'pruska', 'biblioteka', 'państwowa', 'jej', 'dawne', 'zbiory', 'znane', 'pod', 'nazwą', 'berlinka', 'skarb', 'kultury', 'sztuki', 'niemieckiej', 'przewiezione', 'przez', 'niemców', 'koniec', 'wojny', 'światowej', 'dolny', 'śląsk', 'zostały', 'odnalezione', '1945', 'terytorium', 'polski', 'trafiły', 'biblioteki', 'jagiellońskiej', 'krakowie', 'obejmują', 'ponad', 'tys', 'zabytkowych', 'archiwaliów', 'manuskrypty', 'goethego', 'mozarta', 'beethovena', 'bacha', 'underlying', 'codepoints', 'xxx', 'encode', 'unicode_escape', '\\\\\\\\', 'u0144stwowa', 'nazw', 'u0105', 'niemc', 'xf3w', 'u015bwiatowej', 'u015al', 'u0105sk', 'zosta', 'u0142y', 'trafi', 'jagiello', 'u0144skiej', 'obejmuj', 'archiwali', \".'\", 'preceded', 'u0144', 'dislayed', 'ń', 'xf3', 'corresponds', '128', '255', 'ordinal', 'ord', '324', '0144', 'hex', 'appropriate', 'nacute', 'determining', 'rendered', 'failing', 'configure', 'locale', 'render', 'xc5', 'x84', 'unicodedata', 'prefixing', '+),', 'readlines', '127', \"('{}\", '+{:', '04x', '}', \"{}'.\", 'xc3', 'xb3', '00f3', 'acute', 'x9b', '015b', 'x9a', '015a', 'xc4', 'x85', '0105', 'ogonek', 'x82', '0142', 'stroke', 'ś', 'ą', 'ł', 'alternatively', 'u015bl', \"('\\\\\", 'u015b', \"*',\", 'inputting', '-*-', 'coding', \"-*-'\", 'big5', 'requires', 'preference', 'courier', 'detecting', 'matching', 'describing', 'adopt', 'chevron', '«', '».', 'preprocess', '$».', 'dollar', \"$',\", 'abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', 'wildcard', 'crossword', 'sixth', \"('^..\", \"..$',\", 'abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', 'caret', '$', '«..', '..»?', 'specifies', '«^', '-?', 'mail', '$»', 'email', \"('^\", 'closure', 't9', 'mobile', 'keystroke', 'textonyms', 'hole', 'golf', '4653', '«^[', 'ghi', 'mno', 'jlk', ']$»:', \"('^[\", \"]$',\", 'gold', ']»,', '«[', 'constrains', 'fourth', 'constrained', 'hig', 'nom', 'ljk', 'fed', ']$»', 'matched', 'finger', 'twister', 'pad', 'ghijklmno', ']+$»,', 'concisely', 'center', 'fj', ']+$»', 'corner', 'chat_words', \"+$',\", 'miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee', 'miiiiiinnnnnnnnnneeeeeeee', 'mine', 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee', \"]+$',\", 'ah', 'ahah', 'ahahah', 'ahh', 'ahhahahaha', 'ahhh', 'ahhhh', 'ahhhhhh', 'ahhhhhhhhhhhhhh', 'haaa', 'hah', 'haha', 'hahaaa', 'hahah', 'hahaha', 'hahahaa', 'hahahah', 'hahahaha', '*,', '*$»', '+$»,', 'min', 'mmmmm', 'referred', 'kleene', '«[^', 'aeiouaeiou', ']»', 'vowel', '«^[^', ':):):),', 'grrr', 'cyb3r', 'zzzzzzzz', 'illustrating', '\\\\,', '{},', '|:', ']+\\\\.', '0085', '56', '84', '95', '99', '125', '1650', \"]+\\\\$$',\", \"$']\", ']{', \"}$',\", '1614', '1637', '1787', '1901', '1903', '1917', '1925', '1929', '1933', ']+-[', 'lap', ',}-[', '}-[', ']{,', 'black', 'butter', 'gun', 'toting', 'loan', \"('(\", \")$',\", '%-', 'owned', 'absorbed', 'adopting', 'advancing', 'deprived', '\\\\.', 'braced', '},', 'ai', 'oo', '»,', 'wit', 'wet', 'woot', 'instructive', 'z0', ']*', '+,', ']+', ']?', ',}', '{,', ')+', 'specially', 'backspace', 'band', 'boundary', 'habit', \"'...'\", 'complication', 'regexp', 'findall', 'overlapping', 'supercalifragilisticexpialidocious', 'aeiou', \"]',\", 'fd', \",}',\", 'io', '549', 'ea', '476', 'ie', '331', 'ou', '329', '261', '253', 'ee', '217', '174', 'ua', '109', '106', 'ue', '105', 'ui', '31', '(?,', 'noted', 'redundant', 'becomes', 'dclrtn', 'inalienable', 'inlnble', 'retaining', 'consonant', \"''.\", \"'^[\", ']+|[', ']+$|[^', \"]'\", 'compress', 'english_udhr', 'tokenwrap', ']))', 'unvrsl', 'hmn', 'rghts', 'prmble', 'whrs', 'rcgntn', 'inhrnt', 'dgnty', 'eql', 'mmbrs', 'fmly', 'fndtn', 'frdm', 'jstce', 'pce', 'wrld', 'dsrgrd', 'cntmpt', 'hve', 'rsltd', 'brbrs', 'whch', 'outrgd', 'cnscnce', 'mnknd', 'advnt', 'bngs', 'shll', 'enjy', 'spch', 'ka', 'si', 'rotokas_words', 'cv', 'ptksvr', '418', '148', '420', '187', '63', '89', '47', '27', 'partial', 'complementary', 'conceivably', 'drop', 'pronounced', 'su', 'kasuari', 'cassowary', 'borrowed', 'allowing', 'cv_index', 'cv_word_pairs', 'kaapo', 'kaapopato', 'kaipori', 'kaiporipie', 'kaiporivira', 'kapo', 'kapoa', 'kapokao', 'kapokapo', 'kapokapoa', 'kapokapora', ']».', 'ri', 'therefore', 'laptop', 'versa', 'minded', 'suffix', 'ly', 'iou', 'ive', 'ment', \"'^.\", '*(', 'gave', 'arcane', \"'^.*(?\", 'parenthesize', \"'^(.\", '*)(', 'processe', 'incorrectly', 'star', 'greedy', 'consume', '*?,', \"'^(.*?\", ')(', \")?$',\", \"'')]\", 'spot', '?)', \")?$'\", 'dennis', 'listen', 'lying', 'pond', 'sword', 'supreme', 'derives', 'mandate', 'mass', 'farcical', 'aquatic', 'ceremony', '.\"\"\"', 'distribut', 'basi', 'execut', 'deriv', 'acceptable', '>\"', 'angle', '<.', '*>', 'monied', 'bro', '(<.', '*>)', '>\")', 'nervous', 'dangerous', 'pious', 'queer', 'cape', 'butterless', 'fiendish', 'furious', 'dismasted', 'younger', 'brave', '\"<.', 'twizted', '*>{', ',}\")', 'lmao', 're_show', 'annotates', 'nemo', 'phenomenon', 'tied', 'hobbies_learned', '\"<\\\\', '<\\\\', 'speed', 'water', 'liquid', 'tomb', 'landmark', 'statue', 'monument', 'road', 'military', 'compilation', 'iron', 'metal', 'taxonomy', 'labor', 'exclude', 'ontology', 'correcting', 'suffers', 'risky', 'resulting', 'lemmatization', 'shelf', 'crafting', 'irregular', 'lancaster', 'stripping', 'lie', 'porterstemmer', 'lancasterstemmer', 'denni', 'strang', 'suprem', 'mandat', 'farcic', 'aquat', 'ceremoni', 'den', 'wom', 'ba', 'pow', 'mand', 'som', 'farc', 'aqu', 'suit', 'formatting', 'enumerate', 'indexedtext', '__init__', '_text', '_stemmer', '_index', '((', '_stem', 'wc', 'lcontext', 'rcontext', 'ldisplay', \"'{:>{\", \"}}'.\", ':],', 'rdisplay', \"'{:{\", 'beat', 'retreat', 'minstrel', 'singing', 'bravest', 'nay', 'oh', 'wounded', 'doctor', 'immediately', 'clap', 'piglet', 'danger', 'cave', 'gorge', 'eternal', 'peril', 'tim', 'caerbannog', 'fifty', 'strewn', 'fight', 'til', 'code_stemmer_indexing', 'lemmatizer', 'slower', 'wnl', 'wordnetlemmatizer', 'lemmatize', 'valid', 'decimal', 'mapped', 'acronym', 'aaa', 'improves', 'accuracy', 'tokenizing', 'cutting', 'identifiable', 'unit', 'constitute', 'delay', 'wonderland', '\"\"\"\\'', 'duchess', \",'\", 'hopeful', 'tone', 'pepper', 'kitchen', 'soup', 'maybe', 'tempered', ',\\'...\"\"\"', '[\"\\'', ',\\'\",', \",',\", \"'(\", 'nthough', \"),',\", 'nwell', ',\\'...\"]', \"]+',\", ']+»', '(\\\\', 'rewritten', \"+',\", 'prefix', 'instructs', 'backslashed', 'splitting', \",'.\", 'za', '9_', 'complement', \"['',\", \"'']\", 'xx', 'extend', 'wider', '«\\\\', '+|\\\\', '*»', 'apostrophe', \"+([-']\\\\\", '+)*».', \"[-']\\\\\", '+;', \"+(?:[-']\\\\\", \"+)*|'|[-.\", '(]+|\\\\', '*\",', '[\"\\'\",', \"'--',\", \"'...']\", '«[-.', '(]+»', 'ellipsis', 'separately', '[^', 'tokenizer', 'regexp_tokenize', 'avoids', 'treatment', 'readability', '(?', 'verbose', 'embedded', 'poster', 'cost', \"...'\", \"'''(?\", 'regexps', '(?:[', ']\\\\.', '+(?', ':-\\\\', '+)*', '\\\\$?\\\\', '+(?:\\\\.\\\\', '+)?%?', 'currency', '\\\\.\\\\.\\\\.', '[][.,;\"\\'?', '():-', '`]', \"'$\", 'gap', 'reporting', 'decide', 'treebank_raw', 'contraction', 'normalize', 'lookup', 'segmentation', 'radically', 'presupposes', '250994070456922', 'segment', 'punkt', 'segmenter', 'kiss', 'segmenting', 'sent_tokenize', '[\\'\"', 'nonsense', '\"\\',', 'gregory', 'rational', 'nattempted', 'paradox', 'clerk', 'navvy', 'railway', 'sad', 'tired', 'ntell', 'ticket', 'sloane', 'station', 'victoria', 'wild', 'rapture', ',\\\\', 'eye', 'eden', 'unaccountably', 'unpoetical', 'replied', 'poet', 'syme', 'lucian', 'simultaneously', 'terminate', 'visual', '爱国人', 'ai4', 'guo2', 'ren2', '爱国', '人', '爱', '国人', 'arises', 'hearer', 'learner', 'hearing', 'doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy', 'annotating', '.).', 'pause', 'doyouseethekittyseethedoggydoyoulikethekittylikethedoggy', 'seg1', '0000000000000001000000000010000000000000000100000000000', 'seg2', '0100100100100001001001000010100100010010000100010010000', 'shorter', 'broken', 'segmented', 'segs', '[]', ')):', ':])', 'kitty', 'doggy', 'code_segment', 'reconstruct', 'hypothetical', 'reproduce', 'acquiring', 'brent', '1995', 'objective', 'scoring', 'optimize', 'delimiter', 'derivation', 'reconstructed', 'marker', 'text_size', 'lexicon_size', 'seg3', '0000100100000011001000000110000100010000001100010000001', 'doyou', 'thekitt', 'thedogg', '64', 'code_evaluate', 'reconstructing', 'minimizes', 'thekitty', 'randint', 'flip', 'flip_n', ')-', 'anneal', 'iteration', 'cooling_rate', 'temperature', 'best_segs', '5000', 'doyouseetheki', 'tty', 'thedoggy', 'doyouliketh', 'ekittylike', 'doy', 'ouseetheki', 'ttysee', 'ulikethekittylike', '57', 'seetheki', 'liketh', 'seethekit', 'tysee', 'likethekittylike', 'seethekittysee', '0000100100000001001000000010000100010000000100010000000', 'code_anneal', 'deterministic', 'simulated', 'annealing', 'perturb', 'lowered', 'perturbation', 'reduced', 'degree', 'criterion', 'reformatting', 'silly', \"';'.\", \";.'\", 'wecalledhimtortoisebecausehetaughtus', 'spacer', 'calling', 'enjoys', 'privilege', 'hello', 'nworld', 'naming', 'recreate', 'benefit', 'clue', 'snake', \"'->',\", \"=';\", '->', 'alternating', \"('{}->{};'.\", \"'{}->{};'.\", \";'\", 'curly', \"'{}'\", 'replacement', 'embed', 'unpack', \"'{}->'.\", \"->'\", \"'{}'.\", '{}', \"'{}\", 'sandwich', 'lunch', 'consumed', 'superfluous', '2262', 'unexpected', \"{}'\", \"}',\", \"}'.\", 'spam', 'fritter', 'pancake', 'snack', 'lining', 'padding', \"':'\", '{:', 'padded', 'justified', 'specifier', \"'<'\", 'option', \"'{:\", '41', \"'{:<\", \"}'\", \"'>'\", '2310', \"'{:>\", '{:.', '4f', 'math', \"'{:.\", 'pi', '1416', 'smart', \"'%'\", 'specification', '3205', '9375', '%}\".', '1867', \"%'\", 'tabulated', 'exercising', 'heading', 'separation', \"('{:\", \"('{:>\", 'code_modal_tabulate', \"}}'\", 'bound', 'customize', 'accommodate', 'output_file', 'identical', 'wrap', 'conveniently', 'overflow', \"'('\", \"'),',\", 'textwrap', 'clarity', 'onto', 'fill', '[\"{}', '{}\".', 'wrapped', 'linebreak', 'redefine', \"'%\", 's_', '(%', 'onty', \"'/'.\", 'inadequate', 'bundle', 'appeared', 'canonical', 'citation', 'lexeme', '\\\\.,', '\\\\|,', '\\\\$,', 'lose', 'arg_tuple', 'howtos', 'morphology', 'mertz', 'kuchling', 'amk', 'regex', 'friedl', 'facility', 'ned', 'batchelder', 'nedbatchelder', 'unipain', 'beazley', 'pyvideo', 'video', 'pycon', 'spolsky', 'absolute', 'minimum', 'positively', 'excuse', 'joelonsoftware', 'sighan', 'acquisition', 'niyogi', 'multiword', 'alone', 'baldwin', 'kim', 'heuristic', 'approximation', 'optimum', 'discrete', 'analogy', 'metallurgy', 'discovering', 'hearst', '1992', 'colourless', 'morphological', \"'[:-\", 'inserted', 'ning', 'ality', 'un', 'heat', 'direction', '[::-', '+(\\\\.\\\\', '+)?', '([^', '][^', '])*', '+|[^\\\\', 'determiner', 'arithmetic', \"/').\", 'sole', 'tokenizes', 'monetary', 'rewrite', 'newspaper', 'word_len', 'choosing', 'versus', \"')?\", 'prog', 'clause', 'exclamation', 'fuzzy', 'webpage', 'weather', 'forecast', 'town', 'categorize', 'preserved', 'improve', '|\\\\', '+».', 'hack3r', '|,', '5w33t', '!,', 'ate', 'pig', 'transformation', 'ay', 'ingstray', 'idleay', 'pig_latin', 'preserve', 'qu', 'ietquay', 'yellow', 'chooses', 'aehh', 'uncontrolled', 'sneezing', 'maniacal', 'laughter', 'heheeh', 'eha', 'numeric', 'medline', 'cortisol', 'serum', '+/-', '%,', 'respectively', 'compound', 'fifteen', '\"?', 'possibility', 'motivate', 'μw', 'μs', 'ari', \"'.'].\", '[].', 'newly', 'formed', 'bland', 'inexpressible', 'infuriating', 'legitimate', 'chomsky', 'famous', 'eoldrnnnna', 'nationality', 'canadian', 'australian', 'canada', 'australia', 'list_of_adjectival_forms_of_place_names', '002733', 'lolcat', 'lolspeak', 'lolcatbible', 'php', 'how_to_speak_lolcat', 'sub', 'consulting', '-\\\\', 'nterm', 'identifies', 'hyphenated', 'remain', 'encyclo', 'npedia', 'soundex', 'respective', 'rural', 'confabulation', 'elocution', 'sequoia', 'tenacious', 'unidirectional', 'vsequences', \"(''.\", 'aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa', 'ancestor', 'correlation', 'spearman_correlation', 'unseen', 'discovers', 'ch04', 'wrestling', 'pitfall', 'concise', 'introduce', 'dictated', 'revert', 'seem', 'deserving', 'surprising', 'foo', 'behaves', 'affected', 'bodkin', 'updating', '3133', 'holding', 'copied', 'extends', '[[],', '[],', '[]]', '[[]]', 'propagate', 'overwrote', 'referenced', 'overwriting', '[:].', 'deepcopy', 'equality', 'identity', '==,', 'snake_nest', 'pairwise', 'interloper', '4557855488', '4557854763', 'reveals', 'nonempty', 'evaluated', 'evaluates', 'opposed', 'situation', 'animal', 'bare', 'fish', 'anywhere', 'porpoise', 'tuples', 'enclosed', 'sliced', 'fem', 'grouping', 'snark', '\\',\".', 'turned', 'spectroroute', 'ute', 'computed', 'implicitly', 'aggregating', 'reversed', '))).', 'randomize', 'shuffle', \"':'.\", 'lorry', ',:', 'arrange', 'precedence', 'temporary', 'tmp', 'rearrange', 'handy', 'zip', 'prep', 'det', '...>', 'lazy', '0x10d005448', 'putting', 'cut', '%.', 'training_data', 'test_data', 'wordlens', 'discard', 'commonality', 'di', \":',\", \"@']),\", 'qf', 'predetermined', 'orthographic', 'sampa', 'phon', 'ucl', 'ac', 'home', \"');\", 'venetian', 'blind', \"'];\", 'vbd', 't3', 'heavy', '\\'\\'\\'\"', 'humpty', 'dumpty', 'scornful', '.\"\\'\\'\\'', \"['``',\", '\"\\'\\'\",', ')])', 'notational', 'storage', 'allocated', 'slow', 'streamed', 'lexicographic', 'undisputed', 'volume', 'donald', 'knuth', 'literate', 'ramification', 'layout', 'procedural', 'declarative', 'subtle', 'spacing', 'needle', 'designer', '0008', 'maximizing', 'recommendation', 'messed', 'brace', ']):', 'chore', 'highlight', 'aware', 'pythoneditors', 'implication', 'efficiency', 'influencing', '401545438271973', 'track', 'meaningless', 'dictating', '401', 'understood', 'constitutes', 'word_list', 'instantly', 'recognizable', ']).', 'most_common_words', '(\"%', '3d', '2f', '%%', '42', '67', 'tempting', 'longest', \"''\", 'unextinguishable', 'maxlen', 'transubstantiate', 'inextinguishable', 'incomprehensible', 'overhead', 'instantaneous', 'concern', 'successive', 'trigram', '[[', '()],', \"'},\", '()]]', 'incorrect', '[[{', '}],', '[{', '}]]', 'elegant', \"'<.\", \"*?>',\", 'code_get_text', 'cleaned', 'whenever', 'clutter', 'docstring', 'loaded', '__main__', 'reusable', 'tested', 'risk', 'forget', 'bug', 'increased', 'reliability', 'transparently', 'mechanic', 'parenthesized', 'msg', 'communicates', 'procedure', 'my_sort1', '()),', 'my_sort2', 'my_sort3', 'touch', 'bad', 'set_up', 'reflected', 'visible', 'concerned', 'collision', 'resolve', 'respect', 'global', 'succeed', 'lgb', 'enable', 'introduces', 'portability', 'reusability', 'declare', 'iterator', 'defensive', 'knight', '([\"\\'', 'ti', 'scratch', 'sensible', 'complain', 'clearly', 'slight', 'improvement', 'diagnostic', 'propagated', 'unpredictable', 'assert', 'basestring', 'generalizes', 'isinstance', 'halt', 'execution', 'additionally', 'assertion', 'logical', 'docstrings', 'functional', 'decomposition', 'grows', 'analogous', 'essay', 'expressing', 'abstraction', 'fetch', 'load_corpus', 'maintainable', 'reimplement', 'freq_words', 'gov', 'charter', 'constitution_transcript', \"'=',\", 'congress', 'code_freq_words1', 'poorly', 'elsewhere', 'populated', 'refactor', 'simplify', 'dropping', 'code_freq_words2', 'usability', 'improved', 'signal', 'decomposing', 'adequate', '0257', 'sphinx', 'richly', 'param', ']}.', 'adj', 'ordered', 'rtype', 'num_correct', 'code_sphinx', 'exception', 'last_letter', 'extract_property', 'prop', 'invoking', 'treating', 'supposing', 'latter', 'cmp', 'initializing', 'returning', 'aggregated', 'search1', 'search2', 'zz', '=\"', 'grizzly', 'fizzled', 'rizzuto', 'huzzahs', 'dazzler', 'jazz', 'pezza', 'code_search_examples', 'continued', 'stopped', 'encounter', 'allocate', 'permutation', 'seq', 'perm', ':]):', 'police', 'buffalo', 'recursion', 'haskell', 'alongside', 'is_content_word', 'retains', \"')))\", '75081116158339', 'favored', 'throughout', 'confused', \"='<\", \">',\", '><', \">'\", 'alicealicealicealicealice', 'unnamed', 'args', 'kwargs', '(*', 'african', 'song', 'hen', 'turtle', 'dove', 'shorthand', 'fw', 'rst', 'optionality', 'happy', '(\".', '=\"\")', 'debugging', 'acquired', 'manifestation', 'shooting', 'describes', 'organize', 'logically', 'facilitate', 'separator', 'extn', 'inf', 'emulate', '__file__', 'distance', 'lib', 'python2', 'pyc', 'compiled', 'edloper', 'gmail', 'stevenbird1', 'tom', 'lippincott', 'columbia', '/>', 'demo', '_helper', 'hide', 'externally', '__all__', 'edit_distance', 'jaccard_distance', 'stable', 'depicted', 'my_program', 'localized', 'dividing', 'growing', 'designing', 'mastery', 'placed', 'damage', 'creep', 'unnoticed', 'fixing', 'impression', 'reassurance', 'spontaneous', 'fault', 'flippancy', 'aside', 'faulty', 'decomposed', \"('.').\", 'broke', 'rsplit', 'rightmost', 'intact', 'released', '297', 'misunderstanding', '98', '\"%', '.%', '02d', '.\",', 'find_words', 'wordlength', '=[]):', 'omg', 'teh', 'sitted', 'mat', 'progressed', 'stack', 'trace', 'pinpointing', 'reduce', 'smallest', 'offending', 'purport', 'debugger', 'breakpoints', 'pdb', 'mymodule', 'myfunction', \"()')\", 'breakpoint', 'arose', ')\")', '>(', ')<', '>()', 'prompting', 'assertionerror', 'notification', 'bugfix', 'trap', 'magically', 'articulate', 'undo', 'resolved', 'suite', 'regression', 'regress', 'unintended', 'ensuring', 'sync', 'strategy', 'algorithmic', 'adapting', 'elaborate', 'prevalent', 'conquer', 'attack', 'pile', 'merge', 'recursively', 'transforming', 'simplifies', 'factorial', 'factorial1', '*=', 'recursive', 'ordering', 'base', 'factorial2', 'deeply', 'rooted', 'size1', 'iterative', 'layer', 'maintains', 'size2', 'procedurally', 'happening', 'abbreviate', '190', 'trie', 'held', 'hien', 'chair', 'flesh', 'chic', 'stylish', 'nicer', \"'}},\", \"'}}},\", \"'}}}\", \"'}}}}}\", 'code_trie', 'nesting', 'pushed', 'tradeoff', 'significantly', 'auxiliary', 'faster', \"'*\", '...\")', 'movie_reviews', 'abspaths', 'idx', 'quit', 'raw_input', 'code_search_documents', 'invert', 'preprocessed', 'tagged_corpus', 'wm', '[[(', 'code_strings_to_ints', 'maintaining', 'membership', 'timeit', 'timer', 'setup', 'simulate', '100000', 'setup_list', ')\"', 'setup_set', '))\"', '1000', '78092288971', '0037260055542', 'mere', '0037', 'magnitude', 'planning', 'scheduling', 'remainder', 'pingala', '5th', 'wrote', 'treatise', 'sanskrit', 'prosody', 'chandas', 'shastra', 'virahanka', '6th', 'meter', 'marked', 'v4', 'ssl', 'sss', '}.', 'prefixed', 'v2', 'v3', 'ss', 'virahanka1', '[\"\"]', 'virahanka2', '[[\"\"],', '\"]]', 'virahanka3', '={', ':[\"\"],', ':[\"', '\"]}):', 'memoize', 'virahanka4', 'code_virahanka', 'iv', 'memoization', 'v1', 'wasteful', 'v20', '181', 'v40', '245', '986', 'filling', 'crucially', 'wasted', 'wastage', 'decorator', 'housekeeping', 'cluttering', 'recalculating', 'realize', 'matlab', 'sourceforge', 'net', 'lined', 'arange', 'pyplot', 'rgbcmyk', 'blue', 'cyan', 'magenta', 'bar_chart', 'ind', 'bar_groups', ']],', 'xticks', 'loc', 'ylabel', 'code_modal_plot', 'visitor', 'agg', 'backend', 'raster', 'pixel', 'savefig', 'directs', 'png', \"('<\", \">')\", 'img', 'src', '\"/>\\')', \"('</\", '></', 'lanl', 'initializes', 'traversal', 'nx', 'shortest_path_distance', 'add_edge', 'hyponym_graph', 'graph_draw', 'draw_graphviz', 'node_size', 'node_color', 'with_labels', 'code_networkx', 'darkest', 'csv', 'sli', 'wo', 'intr', 'lifting', 'setting', 'foot', 'wake', 'weik', 'intrans', 'cease', 'input_file', 'rb', \"...']\", 'dimensional', 'cube', 'transpose', '([[', ']])', 'matrix', 'latent', 'implicit', 'linalg', ',-', 'vt', 'svd', '([[-', '4472136', '89442719', '32455532', '16227766', '70710678', 'gaussian', 'agglomerative', 'dendrogram', 'mysql', 'pylucene', 'etree', 'imaplib', 'incremented', 'unnecessary', 'enumerated', 'essential', 'namespace', 'declared', 'recomputation', 'scratched', 'surface', 'stdtypes', 'importance', 'itertools', 'multimedia', 'guzdial', 'sys', 'rich', 'harel', 'levitin', 'guidance', 'hunt', 'thomas', 'mcconnell', 'flanked', '__getitem__', 'winded', \"'?'].\", 'transform', \"'!'].\", 'sliding', 'limiting', 'pointed', 'inequality', 'montague', 'lexicographical', 'normalizes', 'helper', 'cmp_len', '[:],', 'word_table', \"[['']\", 'word_vowels', 'novel10', 'gematria', 'hidden', 'essene', 'gemcal', 'htm', 'letter_vals', '800', '666', 'shorten', 'proximity', 'itemgetter', 'uniquely', 'vanguard', 'vang', '-,', 'linkage', 'levenshtein', 'norvig', 'combinatorial', 'mathematics', 'c0', 'cn', 'σ0', 'cicn', 'nth', 'increase', 'zhao', 'zobel', 'authorship', 'identification', 'clintoneast', 'alphabetically', '---', 'ic', 'uniqueness', 'discarding', 'compression', 'justify', 'approximately', 'evenly', 'extractive', 'highest', 'orientation', 'p97', '1023', 'statistically', 'improbable', 'amazon', 'gp', 'sipshelp', '002679', 'grammarian', 'classifying', 'tagset', 'attache', 'pos_tag', 'cc', 'nn', 'jj', 'coordinating', 'queried', 'upenn_tagset', \".*').\", '.??', '?.', 'refuse', 'prp', 'vbp', 'vb', 'tense', 'deny', 'trash', 'homophone', 'pronounce', 'ski', 'race', 'commonplace', 'obscure', 'justification', 'bought', 'w1w', 'w2', 'heard', 'felt', 'mostly', 'worth', 'clothes', 'scrobbling', 'scrobble', 'str2tuple', 'tagged_token', 'commented', 'ap', 'nns', ',/,', 'ppo', 'atlanta', 'tl', 'purchasing', 'vbg', 'wdt', 'pps', '``/``', 'ber', 'ql', 'operated', 'vbn', 'accepted', 'inure', 'jjt', 'abx', \"''/''\", './.', 'nr', 'investigation', 'dti', 'irregularity', 'tagged_words', 'conll2000', 'confidence', 'nnp', \"(',',\", \"','),\", 'tagsets', 'sinica_treebank', 'ä', 'neu', 'åæ', 'nad', 'åç', 'nba', 'মহ', 'ি', 'ষ', 'ে', 'র', 'সন', '্', 'ত', 'া', 'ন', \"(':',\", 'sym', 'mac_morpho', 'jersei', 'atinge', 'xe9dia', 'conll2002', 'sao', 'paulo', 'vmi', \"('(',\", 'fpa', 'cess_cat', 'da0ms0', 'tribunal_suprem', 'np0000o', 'tagged_sents', 'presenting', 'trained', 'simplified', 'adp', 'adposition', 'adv', 'conj', 'africa', 'numeral', '1991', 'prt', 'particle', 'ersatz', 'esprit', 'dunno', 'gr8', 'univeristy', 'brown_news_tagged', 'tag_fd', '30640', '12355', '11928', '11389', '6706', '3349', '2717', '2535', '2264', '2166', 'vd', 'vn', 'scotland', 'yesterday', 'sat', 'recount', 'colonization', 'mary', 'impressed', 'teacher', \"(('\", 'word_tag_pairs', 'noun_preceders', 'confirms', 'referent', 'adjunct', 'fell', 'dot', 'stock', 'suddenly', 'stone', 'mouse', 'cheese', 'pizza', 'gusto', 'word_tag_fd', 'wt', 'rose', 'reported', 'paired', 'cfd1', 'cfd2', 'compared', 'priced', 'fined', 'paid', 'traded', 'clarify', 'participle', 'surrounding', 'accused', 'idx1', 'kicked', 'swiftly', 'idx2', 'head', 'vbz', \"']),\", 'predicate', 'falling', 'classifies', 'uncertain', 'watch', 'schoolhouse', 'rock', 'youtube', 'unsimplified', 'variant', 'possessive', 'hl', 'headline', 'findtags', 'tag_prefix', 'tagged_text', 'tagdict', '137', '88', '85', '72', '[(\"', 'company', '$-', 'navy', 'army', 'gallery', 'league', 'sp', 'salary', 'eva', 'aya', 'ovum', '68', 'fort', 'dr', 'oak', 'basin', '101', '69', '46', 'janitor', 'taxpayer', 'dealer', 'idol', 'giant', 'bros', '.\\'\",', 'writer', 'offense', 'sacrifice', 'fund', 'master', 'communist', 'code_findtags', 'constructing', 'brown_learned_text', 'analytically', 'apt', 'became', 'brown_lrnd_tagged', '>\").', 'w1', 't1', 't2', 'w3', \"')):\", 'tagged_sent', 'protect', 'allowed', 'code_three_word_phrase', 'grant', 'cnj', 'near', 'correlate', 'associative', 'hash', 'hashmap', 'phonebook', 'ip', 'morph', 'analyzer', 'morpheme', 'specifically', 'legal', 'evaluating', 'inherently', '\":\",', 'val', 'overwritten', 'normally', \"{['\", 'unhashable', 'defaultdict', '\")).', '_automatically', 'struggle', 'guarantee', 'unk', 'v1000', 'alice2', 'rabbit', 'sitting', '\",\\'\",', '\"?\\'\"', '1001', 'incrementally', 'emulating', 'tallying', 'code_dictionary', '8336', 'schematic', 'my_dictionary', 'item_key', 'last_letters', 'abactinally', 'abandonedly', 'abasedly', 'abashedly', 'abashlessly', 'abbreviately', 'abdominally', 'abhorrently', 'abidingly', 'abiogenetically', 'abiologically', 'zy', 'blazy', 'bleezy', 'blowzy', 'boozy', 'breezy', 'bronzy', 'buzzy', 'chazy', 'anagram', 'aeilnrt', 'entrail', 'latrine', 'ratline', 'reliant', 'retinal', 'trenail', \"((''.\", 'initialization', ')][', '(<', \"'>,\", 'iterated', 'inverting', '32', 'virtue', 'thine', 'mortal', 'pos2', 'realistic', 'peacefully', \"'})\", 'inverted', '([(', 'k1', 'k2', '...])', 'd1', 'd2', 'brown_tagged_sents', 'brown_sents', 'banal', 'establishes', 'baseline', 'egg', 'ham', 'sam', 'default_tagger', 'defaulttagger', 'unsurprisingly', 'eighth', '13089484257215028', 'gerund', '3rd', 'ould', 'md', \"*\\\\'\", \"$'),\", \"'^-?[\", ']+(\\\\.', \"]+)?$',\", 'cardinal', 'fifth', 'regexp_tagger', 'regexptagger', \"[('``',\", 'received', '20326391789486245', '«.', 'partially', 'automate', 'unigramtagger', 'most_freq_words', 'likely_tags', 'baseline_tagger', '45578495136941344', 'untagged', \"'``'),\", 'bedz', 'voter', 'lt', 'word_freqs', 'words_by_freq', 'perfs', 'bo', 'varying', 'xlabel', 'code_baseline_tagger', 'rapidly', 'eventually', 'reaching', 'plateau', 'theme', 'sd', 'greatly', 'multiplied', 'impartial', 'judge', 'assessed', 'regarded', 'mistake', 'lead', 'guideline', 'undertaking', 'inevitably', 'depend', 'creator', 'neutral', 'maximize', 'usefulness', 'unigram', 'cafe', 'unigram_tagger', 'apartment', 'terrace', 'beg', 'entrance', 'bez', 'direct', '9349006503968017', 'memorized', 'perfect', 'useless', '%:', '4160', 'train_sents', 'test_sents', '811721', 'unigrams', 'isolation', 'priori', 'generalization', 'tn', 'circled', 'shaded', 'grey', 'ngramtagger', 'bigram_tagger', 'bigramtagger', 'unseen_sent', '4203', 'population', 'congo', 'innumerable', 'dialect', 'manages', 'badly', 'overall', '102063', 'specificity', 'sparse', 'cross', 'accurate', 't0', '844513', 'trigramtagger', 'initialized', 'retain', 'cutoff', 'pkl', 'pickle', 'dump', 'wb', 'enterprise', 'maze', 'regulatory', 'rp', 'empirically', 'ambiguous_contexts', '049297702068029296', 'legitimately', 'test_tags', 'gold_tags', 'confusionmatrix', 'dropped', 'annotator', 'collapse', 'finer', 'deciding', 'church', 'bloothooft', '1996', 'imperfection', 'rise', '1990s', 'employed', 'strike', 'considers', 'impractical', 'conditioned', 'inductive', 'inventor', 'successively', 'transforms', 'compiles', 'transformational', 'correction', 'bough', 'branch', 'twig', 'sky', 'canvas', 'broad', 'vocational', 'rehabilitation', 'phase', 'scored', 'corrects', 'interpretable', 'brill_demo', '6555', '------------------+-------------------------------------------------------', \"-'\", \"'*-\", '--------------------------+------------------------+--------------------------', 'guest', 'honor', 'speedway', 'hauled', 'dway', 'crew', 'official', 'indianapolis', 'fortun', 'ter', 'fortune', 'dro', 'drooled', 'schoolboy', 'olboys', 'code_brill_demo', 'instantiated', 'residual', 'belongs', 'ness', 'ill', 'illness', 'establish', 'morphologically', 'ongoing', 'incomplete', 'eating', 'suspicion', 'mainly', 'formalize', 'underpin', 'intuition', 'verjaardag', 'birthday', 'zij', 'vandaag', 'jarig', 'exact', 'acquire', 'cyberslacker', 'fatoush', 'blamestorm', 'sars', 'cantopop', 'bupkis', 'noughties', 'muggle', 'robata', 'belonging', 'gradually', 'morpho', 'receive', 'excursion', 'ungrammatical', 'processor', 'morphosyntactic', 'bem', 'ben', 'carrying', 'finely', 'variation', 'unavoidable', \"'}.\", 'repair', 'petrov', 'da', 'mcdonald', '2012', 'contiguous', 'brown_tagset', 'directive', 'indicating', 'emphasized', 'synthesis', 'ssml', '</', 'wf', 'wnsn', '\">', 'measured', 'shape', 'configuration', 'contour', 'conformation', 'turin', 'italiano', 'progetto', 'realizzazione', 'primo', 'ordin', 'porto', 'turistico', 'dell', 'albania', 'communicative', 'user117', 'dude', 'ynquestion', 'user120', 'bye', 'gonna', 'user122', 'user2', 'slap', 'trout', 'user121', 'pm', 'tryin', 'spoof', 'british', 'waffle', 'falkland', 'juvenile', 'defendant', 'contest', 'opponent', 'clock', 'chase', 'mode', 'xyz', \"']?\", 'deleting', 'deleted', 'interchanged', 'affixtagger', 'substituted', 'typesseq', 'removing', 'minimize', 'regard', \"')],\", 'eg', 'lab', 'qualifier', 'estimate', '102', 'varied', 'fail', 'circumstance', 'happen', 'untouched', 'contribution', 'axis', 'semilogx', 'gradient', 'wordi', 'tagi', 'discriminate', 'epistemic', 'deontic', 'altogether', '$)', 'ignores', 'abney', 'impossibility', 'lidstone', 'laplace', 'anti', 'ngram', 'prevent', 'estimating', 'fold', 'validation', 'inherits', 'encapsulates', 'tend', 'indicative', 'salient', 'focusing', 'politics', 'river', 'financial', 'institution', 'tilting', 'depositing', 'jointly', 'extractor', 'distinctive', 'precisely', 'gender_features', ']}', 'shrek', 'booleans', 'prepare', 'labeled_names', \"')])\", 'featuresets', 'train_set', 'test_set', 'naivebayesclassifier', 'neo', 'trinity', '2199', 'conforms', 'expectation', '77', 'distinguishing', 'show_most_informative_features', '33', 'likelihood', 'retrain', 'apply_features', 'enormous', 'impact', 'decent', 'thorough', 'guided', 'sink', 'gender_features2', 'first_letter', 'abcdefghijklmnopqrstuvwxyz', '({})\".', \")':\", '...}', 'code_gender_features_overfitting', 'overfits', 'overfitting', 'relatively', 'relying', 'idiosyncrasy', 'problematic', 'overfit', '768', 'productive', 'refining', 'subdivided', 'train_names', '1500', 'devtest_names', 'test_names', 'devtest_set', 'tricking', 'adjusted', '={:<', 'abigail', 'cindelyn', 'katheryn', 'kathryn', 'aldrich', 'mitch', 'yn', 'predominantly', 'adjust', 'suffix1', 'suffix2', ':]}', 'rebuilding', 'dataset', '76', '%):', '782', 'repeated', 'trust', 'unused', 'categorizes', 'all_words', 'word_features', 'document_features', 'document_words', \"({})'.\", 'cv957_8737', 'waste', 'code_document_classify_fd', '81', 'outstanding', 'neg', 'seagal', 'wonderfully', 'damon', 'code_document_classify_use', 'apparently', 'crafted', 'suffix_fdist', ':]]', 'common_suffixes', 'er', \"'``',\", 'ion', 'pos_features', 'tinted', 'highlighting', 'impossible', 'rely', 'exclusively', 'highlighted', 'decisiontreeclassifier', '62705121829935351', 'pseudocode', '(,)', \"','\", '(.)', \"'.'\", '\",\".', 'certainly', '\".\").', 'augmenting', 'leverage', 'functioning', 'revise', 'dependent', '{\"', ')\":', '][-', 'prev', '()[', 'untagged_sent', 'untag', '78915962207856782', 'code_suffix_pos_tag', 'detector', 'learns', 'gubernatorial', 'independent', 'joint', 'augment', 'proceed', 'consecutivepostagger', 'taggeri', 'featureset', '79796012981', 'code_consecutive_pos_tagger', 'commit', 'iteratively', 'inconsistency', 'markov', 'trillion', '3010', 'efficiently', 'chain', 'viewed', 'terminates', 'merged', 'punct_features', 'punct', \"'.?!']\", '936026936026936', 'segment_sentences', \"'.?!'\", 'code_classification_based_segmenter', 'straightforward', 'performative', 'forgive', 'bet', 'climb', 'greeting', 'clarification', 'emotion', 'continuer', 'xml_posts', '()[:', '10000', 'dialogue_act_features', '())]', 'parviz', 'davudi', 'meeting', 'shanghai', 'organisation', 'sco', 'fledgling', 'bind', 'russia', 'soviet', 'republic', 'asia', 'terrorism', 'llc', 'nelson', 'beaver', 'chester', 'jennie', 'stewart', 'holder', 'carolina', 'analytical', 'laboratory', 'successful', 'reasonably', 'ideal', 'captured', 'hyp_extra', 'motivates', 'ne', 'filtered', 'intro', 'rtefeatureextractor', '??', 'rte_features', 'rtepair', 'word_overlap', 'word_hyp_extra', 'ne_overlap', 'ne_hyp_extra', 'code_rte_features', 'bag', 'throwing', 'rte3_dev', \"'])[\", 'text_words', 'hyp_words', 'rte_classify', 'impressive', 'scaling', 'numerically', 'pure', 'unreasonable', 'recommend', 'interfacing', 'trustworthy', 'guiding', 'misleadingly', 'balanced', 'skew', 'err', 'safety', 'consideration', 'reflects', 'somewhat', 'file_ids', 'stringent', 'predicts', 'interpreting', 'hardly', 'measuring', 'inter', 'attempting', 'irrelevant', 'outweighs', 'tp', '/(', 'fp', 'fn', 'harmonic', 'subdivide', 'diagonal', '|)', 'tag_list', 'apply_tagger', 'cm', 'pretty_format', 'sort_by_count', 'show_percents', 'truncate', '----+----------------------------------------------------------------+', '%>', '%>|', 'col', 'reserve', 'varies', 'vary', 'skeptical', 'flowchart', 'selects', 'arrive', 'conventionally', 'upside', 'picking', 'decides', 'achieves', 'picked', 'sufficient', 'grow', 'leftmost', 'disorganized', '−', 'σl', 'labelsp', 'log2p', 'probs', '811', 'code_entropy', 'calculated', 'weighted', 'coherent', 'suited', 'hierarchical', 'categorical', 'phylogeny', 'prune', 'checked', 'automotive', 'murder', 'hasword', 'exponentially', 'repetition', 'weak', 'predictor', 'incremental', 'descended', 'overcomes', 'dark', 'indicator', 'strong', 'closest', 'voting', 'contributes', 'unrealistic', 'simplifying', 'independence', 'bayesian', 'generative', 'maximizes', ')/', 'suffices', 'prodf', 'featuresp', ')`', 'equation', 'smoothing', 'toward', 'fit', 'basically', 'heldout', 'valued', 'orange', 'binning', 'height', 'bell', 'variance', 'naivete', 'dependence', 'correlated', 'weight', 'deserves', 'precise', 'σx', ')|', 'σlabel', 'refine', 'optimal', 'refinement', 'generalized', 'gi', 'considerably', 'conjugate', 'cg', 'bfgs', 'applicable', 'receives', 'unlabeled', 'product', 'prodjoint', 'unwarranted', 'principle', 'dominates', 'vi', 'fewest', 'coming', 'vii', '%;', 'strictly', 'price', 'topographical', 'skyline', 'largely', 'neural', 'opaque', 'ass', 'deemed', 'sufficiently', 'explanatory', 'interchangeable', 'polar', 'detest', 'postulate', 'causal', 'construction', 'worrying', 'helping', 'tune', 'unrealistically', 'optimistic', 'weka', 'mallet', 'tadm', 'megam', 'alpaydin', 'mathematically', 'intense', 'hastie', 'tibshirani', 'friedman', 'daelemans', 'bosch', 'feldman', 'sanger', 'segaran', 'wei', 'manning', 'schutze', 'raghavan', 'coded', 'naively', 'increasing', 'kiusalaas', 'agirre', 'edmonds', 'melamed', 'croft', 'metzler', 'strohman', 'driven', 'sponsored', 'competition', 'ace', 'aquaint', '6900', 'chip', 'yanswer', 'indication', 'infrequently', 'ppattachment', 'ppattach', 'noun1', 'noun2', 'chairman', 'inst', \".')\", 'nattach', 'connect', 'researcher', 'jar', 'cupboard', 'campus', 'letterman', 'ch06', '1264', 'truly', 'staggering', 'complexity', 'located', 'predictable', 'orgname', 'locationname', 'omnicom', 'ddb', 'needham', 'kaplan', 'thaler', 'bbdo', 'south', 'georgia', 'pacific', '?\"', 'locs', 'e1', 'rel', 'e2', \"=='\", 'ieer', 'nyt19980315', 'packaged', 'corp', 'arrived', 'hertz', 'channel', 'worldwide', 'corporate', 'advertising', 'brand', 'soft', 'toilet', 'tissue', 'sparkle', 'towel', 'ken', 'haldin', 'spokesman', 'glean', 'reap', 'sql', 'resume', 'harvesting', 'patent', 'scanning', 'electronically', 'biology', 'medicine', '.:', 'prove', \"']).\", 'ie_preprocess', 'participate', 'definite', 'ni', 'indefinite', 'recording', 'omits', 'chunker', 'chunkers', 'hardware', 'fragmented', 'nnps', 'nominal', 'graphically', 'barked', '\")]', '{<', '>?<', '>*<', '>}\"', 'cp', 'regexpparser', 'code_chunkex', 'delimited', 'sharp', 'dive', 'policy', 'jjr', 'panamanian', 'dictator', 'manuel', '.*>*<', '.*>+.', 'mansion', '%/', 'fastest', 'jjs', 'chunkparser', 'flat', '\\\\$>?<', '>}', '>+}', 'rapunzel', '$\"),', 'golden', 'code_chunker1', 'escaped', '$.', 'permissive', '>+}.', 'tracing', 'interrogate', \"*>}')\", 'subtree', 'subtrees', 'overtake', 'encapsulate', 'find_chunks', '*>}\"', ',}}\"', 'chinking', 'chink', 'periphery', 'remains', 'excise', '{<.', '*>+}', '}<', '>+{', 'code_chinker', 'chinker', 'befits', 'status', 'iob', 'suffixed', 'constituent', 'manipulated', 'expanding', 'vp', 'conllstr2tree', 'moreover', 'carlyle', 'merchant', 'banking', 'chunk_types', '100th', 'chunked_sents', './', './.)', 'delivered', '(\"\")', 'chunkparse', '{<[', 'cdjnp', '*>+}\"', 'unigramchunker', 'chunkparseri', 'constructor', 'train_data', 'tree2conlltags', 'pos_tags', 'tagged_pos_tags', 'chunktags', 'chunktag', 'conlltags', 'conlltags2tree', 'code_unigram_chunker', 'unigram_chunker', '92', 'achieving', 'postags', \"[('#',\", \"('$',\", \"(')',\", 'pdt', 'rbr', 'uh', 'wp', 'wrb', 'discovered', '$,', 'bigramchunker', 'bigram_chunker', 'joey', 'farmer', 'rice', 'maxentclassifier', 'wrapper', 'consecutivenpchunktagger', 'npchunk_features', 'consecutivenpchunker', '[[((', 'code_classifier_chunker', 'prevword', 'prevpos', 'hypothesized', 'reduction', 'rate', 'lookahead', 'nextword', 'nextpos', '+%', 'tags_since_dt', \"'+'.\", '96', '91', 'cascaded', '*><', '>+$}', 'sit', ')))))', 'code_cascaded_chunker', 'headed', '_saw', ')))))))', 'cascading', 'deep', 'cascade', 'connected', 'reachable', 'distinguished', 'standardly', 'metaphor', 'talk', 'sibling', 'chased', '))))', 'homogeneous', 'discourse', 'tree1', 'tree2', 'tree3', 'tree4', 'zoom', 'postscript', 'inclusion', \")))')\", 'code_traverse', 'duck', 'gpe', 'geo', 'province', 'eddy', 'bonte', 'obama', 'murray', 'mount', 'everest', 'june', '175', 'gbp', 'pct', 'stonehenge', 'east', 'midlothian', 'ner', 'prelude', 'qa', 'recovering', 'isolate', 'minimal', 'retrieved', 'passage', 'prominent', 'attraction', 'alexandria', 'getty', 'prone', 'sanchez', 'dominican', 'vietnam', 'existence', 'contemporary', 'dior', 'yankee', 'infielder', 'posed', 'cecil', 'escondido', 'village', 'woordvoerder', 'van', 'diezelfde', 'hogeschool', 'punc', 'ne_chunk', 'brooke', 'mossman', '...)', 'approaching', 'α', 'intervenes', '!\\\\', '.+', 'disregard', 'supervising', 'transition', \"'.*\\\\\", 'bin', \")')\", 'parsed_docs', 'nyt_19980315', 'extract_rels', 'rtuple', 'whyy', 'mcglashan', 'amp', 'sarrail', 'firm', 'san', 'mateo', 'arlington', 'brookings', 'idealab', 'incubator', 'angeles', 'waterloo', 'wgbh', 'boston', 'bastille', 'opera', 'paris', 'transportation', 'committee', 'secured', '];', 'excluding', 'filler', 'devise', 'clausal', 'relsym', 'vnv', 'zijn', 'werd', 'wordt', 'worden', '\"))', 'cornet_d', 'elzius', 'buitenlandse_handel', 'johan_rottiers', 'kardinaal_van_roey_instituut', 'annie_lennox', 'eurythmics', 'lcon', 'rcon', 'intervene', 'populate', 'intervening', 'due', 'pioneering', 'vinartus', 'spa', '97a', '1975', 'tukey', 'bio', 'ramshaw', 'marcus', 'ananiadou', 'mcnaught', 'generalizing', 'solely', 'receiving', 'assistant', 'devising', 'coordinated', 'august', 'supervisor', 'adjudicator', 'chunkscore', 'missed', 'regexpchunk', 'merging', 'erroneous', \"')...\", 'treebank_chunk', 'tree2conllstr', 'chunk2brackets', 'chunk2iob', 'bracket2iob', 'iob2bracket', 'resp', 'speculate', 'cope', 'finite', 'dilemma', 'stressed', 'daily', 'gigantic', 'uttered', 'telegraph', '2387900', 'argue', 'imaginary', 'judgement', 'reject', 'agree', 'perfectly', 'usain', 'bolt', '100m', 'jamaica', 'observer', 'andre', 'ingenuity', 'winnie', 'pooh', 'milne', 'joy', 'ship', '.]', 'liked', 'terrible', 'hour', 'imprisonment', 'owl', 'flown', 'comfort', 'aunt', 'seagull', 'quietly', 'slipping', 'hanging', 'toe', 'luckily', 'sudden', 'loud', 'squawk', 'woke', 'jerk', 'brain', 'captain', 'mate', 'rescue', 'indefinitely', 'concoct', 'intertwined', 'competent', 'ubiquitous', 'groucho', 'marx', 'cracker', '1930', 'hunting', 'shot', 'elephant', 'pajama', 'groucho_grammar', '(\"\"\"', '\"\"\")', 'chartparser', '))))))', 'depict', '3b', 'fighting', 'tiresome', 'blame', 'investigating', 'formedness', 'degenerate', 'childrens', '22816', 'roared', 'slip', 'worst', 'clumsy', 'whoever', 'intuitively', 'salad', 'coordinate', 'coordination', 'conjoined', 'aps', 'conjoin', 'substitutability', 'rendering', 'fat', 'brook', 'substitute', 'grammaticality', 'reproduces', 'topmost', 'bounded', 'admitted', 'grammar1', 'bob', 'telescope', 'park', 'rd_parser', 'recursivedescentparser', 'code_cfg1', 'righthand', 'descent', 'rdparser', 'autostep', 'button', 'structurally', 'attached', 'balcony', 'overlooking', 'cfgs', 'mygrammar', 'currently', 'disallowed', 'new_york', 'indirect', 'grammar2', 'propn', 'chatterer', 'joe', 'squirrel', 'angry', 'frightened', 'tall', 'code_cfg2', '10a', '10b', 'beware', 'conform', 'licensed', 'fringe', 'psycholinguistic', 'submitted', 'undergo', 'shift', 'subgoals', 'expansion', 'expands', 'hence', 'consults', 'enlarge', 'backtracks', 'dangling', 'backtrack', 'send', 'infinite', 'backtracking', 'rebuilt', 'proceeds', 'push', 'popped', 'reducing', 'srparser', 'shifting', 'succeeds', 'shiftreduceparser', 'guaranteed', 'exists', 'verbosely', 'sr_parser', 'sr_parse', 'undone', 'resolving', 'conflict', 'favoring', 'lr', '12c', 'pointless', '12a', '12b', '⇒*', 'trapped', 'preprocesses', 'suffer', 'completeness', 'remedy', 'subconstituent', 'wfst', 'reminiscent', 'triangular', 'denote', 'a0a1', '`+', 'ch08', 'rh', 'init_wfst', 'numtokens', 'lh', 'complete_wfst', 'mid', 'nt1', 'nt2', '(\"[%', '[%', '==>', ']\"', ')],', 'nwfst', '((\"%-', '4d', '(\"%-', 'wfst0', 'wfst1', 'code_wfst', 'acceptor', 'nonterminal', 'propose', 'structural', 'slighly', 'asymmetric', 'tensed', 'arc', 'sbj', 'nmod', 'groucho_dep_grammar', 'dependencygrammar', 'projective', 'crossing', 'descendent', 'pdp', 'projectivedependencyparser', 'proposed', 'appealing', 'embody', 'formalism', 'valency', '15d', '16d', '.*', 'imagination', 'tradition', 'constraining', 'subcategories', 'transitive', 'subcategorized', 'tv', 'excluded', 'intransitive', 'datv', 'dative', 'sv', 'sentential', 'contrasted', 'modifer', '17d', 'toy', 'scaled', 'succinct', 'modularize', 'distribute', 'collaborative', 'lfg', 'pargram', 'hpsg', 'lingo', 'lexicalized', 'adjoining', 'xtag', 'parsed_sents', '(,', ',)', 'adjp', ',))', 'clr', '(.', '.))', 'child_nodes', 'code_sentential_complement', \"]['\", 'amongst', 'rejected', 'pe08', 'prepared', 'large_grammars', 'sinica', '3450', 'pernicious', 'astronomical', 'sbar', '132', '429', '430', '862', '796', '786', '208', '012', '1012', 'effortlessly', 'patil', '1982', 'spelled', 'overwhelmed', 'gibberish', 'klavans', 'resnik', 'hundredth', 'hectare', 'sq', 'designating', 'drawing', 'paddock', 'anticipated', 'explains', 'horrendous', 'inefficiency', 'seemingly', 'innocuous', 'sheer', 'notion', 'recipient', '19a', '19b', 'heebie', 'jeebies', 'dtv', \"')\\\\\", 'print_node', '%\\\\', 'chef', 'ovation', 'advertiser', 'discount', 'politician', 'consumer', 'straight', 'scoop', 'crisis', 'mitsui', 'tech', 'medical', 'mitsubishi', 'foster', 'gift', 'suspend', 'trading', 'futu', 'quick', 'approval', 'billion', 'supplemental', 'appr', 'obligation', 'cal', '``', 'qualified', 'rating', '``...', 'veto', 'code_give', 'tendency', 'animacy', 'contributing', 'surveyed', 'bresnan', 'jack', 'code_pcfg1', 'impose', 'obeys', 'viterbi_parser', 'viterbiparser', \"']):\", '064', 'characterization', 'α1', 'αn', 'grammatically', 'inefficient', 'substructure', 'globally', 'radford', '1988', 'gentle', 'unbounded', '1965', 'jacob', 'rosenbaum', '1970', 'explored', 'jackendoff', '1977', 'prime', 'typographically', 'demanding', 'burton', 'robert', '1997', 'practically', 'constituency', 'exemplification', 'huddleston', 'pullum', 'levin', 'www2', 'parc', 'istl', 'nltt', 'delph', 'partner', 'prohibition', 'dana', 'cheered', 'underlining', 'lx', 'invocation', 'boost', 'cleverer', 'buffalo_buffalo_buffalo_buffalo_buffalo_buffalo_buffalo_buffalo', 'listener', 'list_of_homophonous_phrases', 'ran', 'garden', 'garden_path_sentence', 'draw_trees', 'influence', 'claimed', 'regularity', 'youngish', 'nikolay', 'parfenovich', 'sincere', 'liking', 'discriminated', 'procurator', 'karamazov', '[[[', ',]', 'n3', 'ascent', 'complementation', 'dumped', 'filled', 'inheriting', 'parsei', 'flexibility', 'atomic', 'decompose', 'computationally', 'subcategorization', 'orth', 'ref', 'orthography', 'exhaustive', 'obj', 'agt', 'linking', 'lex2fs', 'subj', '=>', 'experiencer', 'exp', 'surprised', 'hoc', 'elegantly', 'offered', 'expressiveness', 'demonstrative', '2b', '2nd', 'paradigm', 'agrees', 'sg', 'ch09', '252', '257', 'blocking', 'np_sg', 'vp_sg', 'np_pl', 'vp_pl', 'det_sg', 'n_sg', 'det_pl', 'n_pl', 'v_sg', 'v_pl', 'vps', 'counterpart', 'aesthetically', 'unappealing', 'doubling', 'unattractive', 'blowing', 'spoke', 'informally', '=?', 'admit', '11a', '11b', '13a', '13b', 'prohibited', 'incompatibility', 'choosy', 'underspecified', 'letting', 'economical', 'explicitly', 'show_cfg', 'book_grammars', 'feat0', 'fcfg', '###################', ']->', 'jody', 'pres', 'disappears', 'disappeared', 'code_feat0cfg', 'experimentation', 'load_parser', '|.', 'chil', '.|', 'init', '|[----]', '[----]', '[----]|', '|[---->', '{?', '[---->', '[---->|', '[---------]|', '|[==============]|', 'code_featurecharttrace', 'admissible', 'flow', 'upwards', 'binding', 'assembles', 'instantiate', 'subpart', 'aux', '=+]', 'abbreviates', '=+', '=-,', 'attaching', 'radical', 'agr', 'avm', 'gnd', 'avms', 'athough', 'visually', 'pleasing', 'stick', 'significance', 'bundled', 'cop', 'featstruct', 'fs1', 'acc', 'fs2', '(\"[', '\\']]\"))', 'telno', 'divert', 'groundwork', 'acyclic', 'dag', 'rue', 'pascal', 'spouse', 'sharing', 'reentrancy', '->(', '(\"\"\"[', '=(', ')]]\"\"\"))', 'coindex', ')]\"))', 'subsumption', '23a', '23b', '23c', 'fs0', 'subsumes', '⊑', 'reentrancies', 'incommensurable', 'subsumed', '25b', '25a', '25c', 'unify', 'formally', '⊔', 'symmetric', 'unifying', 'π', 'interacts', '\\']]]\"\"\")', ']]]\")', 'unified', ')]]\"\"\")', 'stated', 'address1', '\\']]\")', 'address2', ']\")', 'augmented', 'gpsg', 'subcat', 'adopts', 'mnemonic', 'trans', 'correspondence', 'belong', 'comp', 'categorial', 'patr', 'encodes', '=<', '>]', 'comprises', 'discharged', 'traditionally', 'verbal', 'revisited', 'factoring', 'abstracting', 'phrasal', '34a', '34b', 'projection', 'maximal', '35', 'inversion', 'switched', 'interrogative', 'rarely', '39', 'positioned', '[+', 'inv', 'slot', 'ungrammaticality', 'music', '45a', 'preposed', '45b', '__', 'hip', 'hop', 'occurence', 'termed', 'nature', 'constellation', 'percolated', 'dominated', 'fortunately', 'reducible', 'feat1', ']/?', 'code_slashcfg', 'percolate', 'slashed', ']/', '[]/', 'admits', 'masc', 'neut', 'gen', 'dat', 'nominative', 'accusative', 'helfen', '1693', '1698', '1702', '1707', 'comprising', 'objcase', 'hunde', 'hunden', 'katze', 'katzen', 'ich', 'mich', 'mir', 'wir', 'ihr', 'komme', 'kommst', 'kommt', 'kommen', 'sehe', 'mag', 'siehst', 'magst', 'sieht', 'folge', 'helfe', 'folgst', 'hilfst', 'folgt', 'hilft', 'sehen', 'moegen', 'moegt', 'folgen', 'helft', 'code_germancfg', 'governs', 'failure', 'fol', 'kat', '|[---]', '[---]', '[---]|', '|[--->', '[--->', 'scanner', 'admitting', ']].', 'realization', 'constrain', '[+/-', 'entrant', 'earliest', 'phonological', 'labial', 'voice', 'chomskyan', 'advocated', 'gazdar', '1985', 'dahl', 'saint', 'dizier', 'elaborated', 'grosz', 'stickel', '1983', 'primarily', 'shieber', 'algebraic', 'attempted', 'negation', 'pioneered', 'kasper', 'johnson', 'argues', 'integral', 'huang', 'chen', '1989', 'sag', 'wasow', 'bibliography', 'cl', 'uni', 'bremen', 'bib', 'permissible', 'anomalous', 'deficiency', 'stipulate', 'subtype', 'emele', 'zajac', 'examination', 'carpenter', 'copestake', 'implementing', 'copious', 'nerbonne', 'netter', '1994', '{\\\\\"', 'ller', 'integration', 'sings', 'precious', '2028', '2033', '2038', '2043', 'earleychartparser', ']]\")', 'fs3', 'fs4', ')]\")', 'fs5', 'fs6', 'fs7', 'fs8', 'fs9', 'fs10', 'code_featstructures', 'subsume', 'heute', 'cart', 'sand', 'conjugation', 'redundantly', 'whereby', '-->', 'harness', 'machinery', 'interrogating', 'querying', 'athens', 'greece', '1980s', '1368', 'bangkok', 'thailand', '1178', 'barcelona', 'spain', 'berlin', 'east_germany', '3481', 'birmingham', 'united_kingdom', '1112', 'city_table', 'retrieving', 'w3schools', 'sql0', 'assemble', 'tandem', 'supplemented', 'recipe', 'splice', '=(?', '\"\\']', \"='']\", \"()['\", 'db', 'chat80', 'sql_query', 'city_database', 'canton', 'chungking', 'dairen', 'harbin', 'kowloon', 'mukden', 'peking', 'sian', 'tientsin', 'understands', 'native', 'margrietje', 'houdt', 'brunoke', 'olga', 'convince', 'earley', 'instrumental', 'criticism', 'wired', 'embarrassing', 'sharpen', '4a', '4b', 'sql1', 'cond1', 'cond2', 'classical', 'methodology', 'begged', 'favourite', 'doll', 'depiction', 'truth', 'sylvania', 'freedonia', 'fictional', 'featured', 'emphasize', 'broadly', 'judgment', 'symbolic', 'logician', 'klaus', 'evi', 'denotes', 'falsity', 'raised', 'alan', 'famously', '1950', 'successfully', 'imitated', 'imitation', 'popularly', 'stepped', 'somehow', 'proposal', 'capacity', 'propositional', 'φ', 'ψ', '....', 'formalization', 'boolean_ops', 'equivalence', '<->', 'iff', 'departs', 'moon', 'nltks', 'subclass', 'read_expr', \"('-(\", 'negatedexpression', '-(', ')>', 'andexpression', 'orexpression', '))>', 'iffexpression', 'premise', 'validity', 'asymmetry', 'snf', 'fns', '-.', 'a1', 'proof', 'lp', 'notfns', \"('-\", '\"|\"', 'inductively', 'valuation', \"([('\", 'dom', 'confined', 'expressive', 'differing', 'angus', 'formalized', 'bertie', 'unary', 'intrinsic', 'houden_van', 'substantive', 'predication', 'aj', 'σ', 'τ', '〈', '〉', 'expr', 'type_check', 'constantexpression', ',?>', 'infer', 'intending', 'signature', 'sig', \">'}\", '〉〉.', 'cyril', 'denotation', 'pointing', 'uttering', '15a', 'coreferential', '15b', '16a', 'coreference', '16b', '17a', '17b', 'exposition', '∧', 'existential', 'quantifier', '∃', '18a', '18b', 'idiomatically', '18c', '.∃', '∀', '.∀', '20a', '20c', 'presuppose', 'presupposition', 'astring', 'bark', 'unbound', \")').\", \"')}\", \"('((\", \"))').\", 'proving', 'doubt', 'formalizing', 'north_of', '⊢', \"))')\", 'happily', 'summarizing', 'restate', 'arity', '〉,', 'β', 'argued', 'clearer', '≥', 'dn', 'theoretic', 'olive', '{(', \"{('\", \"',)},\", \"')},\", \"',),\", \"',)}}\", 'singleton', 'τ1', 'τn', \"',)\", 'examplle', 'satisfies', 'purge', 'composed', 'quantification', 'satisfaction', 'quantified', \"))',\", 'satisfier', 'fmla1', 'fmla2', 'fmla3', 'disjuncts', 'universally', 'pencil', 'everybody', 'admires', 'admire', '27b', 'stronger', '27a', 'bruce', 'admired', '∃,', 'claiming', 'associating', 'elspeth', 'julia', 'matthew', 'val2', 'visualized', 'vain', 'dom2', 'm2', 'g2', 'fmla4', 'fmla5', 'fmla6', 'mace4', 'builder', 'mace', 'build_model', 'unspecified', 'a3', 'socrates', 'mb', 'derivable', 's1', 's2', 'sn', 'counterexample', 'provable', 'unsuccessfully', 'countermodel', 'eve', 'macecommand', 'a4', 'a5', 'a6', 'mc', 'puzzling', 'skolem', 'knew', 'decided', 'denoting', 'disjoint', 'dramatically', 'accord', 'a7', 'reflection', 'compositional', 'compositionality', 'frege', 'gleitman', 'formulation', 'granted', 'entailed', 'integrate', 'smoothly', 'composition', '=<?', ')>]', '=<\\\\', 'launching', 'kit', 'λ', 'glossed', '){', 'λw', 'alonzo', 'computable', '33a', '33b', '33c', 'chew_gum', 'λx', '\\\\),', 'lambdaexpression', \"))'))\", 'gum', 'chewing', 'subjectless', '〉.', 'predicated', 'gerald', ')\\\\', '))[', '))(', 'λs', ')(\\\\', 'stipulated', '〉!', '〈〈', '39a', '39b', '40a', '41a', '41b', 'forbid', '∀,', 'relabeling', 'variablebinderexpressions', '==),', 'expr1', 'expr2', 'alpha_convert', 'subterms', 'subterm', 'z1', 'expr3', '))(\\\\', 'z14', 'excursus', 'forgiven', 'surely', '42a', '42b', 'raising', 'instantiating', '))>]', '))>].', 'sticking', 'colloquially', 'inverse', 'tvp', \"'(\\\\\", 'applicationexpression', ')))(\\\\', 'z2', 'interpret_sents', 'synrep', 'semrep', 'irene', 'bite', 'ankle', 'grammar_file', ')>,', 'z3', '))>,', 'evaluate_sents', 'resembles', 'syntree', 'z4', 'coupled', '53a', '53b', 'numerous', 'scoped', 'scopings', 'plug', 'instantiation', 'core', 'swapped', 'compositionally', '=(/)]]', '=<@', '=(<', '),@', ')>)]]', 'metavariable', 'b1', '+?', 'b2', ')]]', '<?', ')>.', 'uniformly', 'yielding', '))]]', '=()]]', 'chaser', 'chasee', 'cooper_storage', 'turning', 'cooperstore', 'parse_with_bindops', 'cs_semrep', 's_retrieve', 'anaphoric', 'referring', 'owns', 'drt', 'drs', '54a', 'integrating', 'trigger', 'inquire', 'drss', 'read_dexpr', 'drtexpression', 'drs1', \"('([\", \")])')\", '],[', 'screenshot', 'clash', 'drs2', '(([', ')]))', 'drs3', \"('([],\", '[(([', \")]))])')\", 'drt_resolve_anaphora', 'conservative', 'resolve_anaphora', '[...],', '[...]', 'drs4', 'drs5', 'drs6', 'swapping', 'indefinites', 'ease', '],[])', '((([', '([],[', ')])>]', '));', 'simplification', 'drtparser', 'logic_parser', 'glaringly', 'omission', 'redressed', 'thread', 'rj', 'discoursetester', 'dance', 's0', 'r0', 'consistchk', 'retracting', 'add_sentence', 'd0', 'retract_sentence', 'informchk', \"))':\", 'invokes', 'configured', 'malt', \"[('^(\", \"('^(\", 'ex_quant', 'univ_quant', 'rc', 'drtgluereadingcommand', 'depparser', 'maltparser', '([],[(([', ')]))])', 'r1', 'quantfier', 'scoping', ')`.', 'show_thread_readings', 'anaphoraresolutionexception', 'z6', 'z10', ')])),', 'inadmissible', 'relettered', 'z24', 'z20', 'z12', 'z15', 'z17', 'provers', 'believed', 'ary', 'q1', 'q2', 'outermost', 'mccune', 'blackburn', 'bos', 'dalrymple', 'intensional', 'dealt', 'androutsopoulos', 'ritchie', 'thanisch', 'hodges', 'insightful', 'ranging', 'gamut', 'kamp', 'reyle', 'definitive', 'modality', 'chierchia', 'ginet', 'agnostic', 'heim', 'kratzer', 'larson', 'segal', 'underspecification', 'lappin', 'benthem', 'meulen', 'sulk', 'snow', 'rain', 'tofu', 'cough', 'sneeze', 'hate', 'taller', 'fourlegged', 'nobody', 'somebody', 'coughed', 'sneezed', 'asleep', 'capuccino', 'loved', 'detested', 'e3', 'fido', 'x0', 'x1', \"('\\\\\\\\\", 'face', 'obstacle', 'workflow', 'lifecycle', 'crawling', 'texas', 'mit', 'acoustic', 'eight', 'educational', 'greasy', 'wash', 'oily', 'rag', 'phonetically', 'diphones', 'comparability', '160', 'recorded', 'transcription', 'customary', 'dr1', 'fvmh0', 'sa1', \"#',\", 'iy', 'hv', 'ae', 'dcl', 'aa', 'kcl', 'ux', 'tcl', 'gcl', 'epi', 'dx', 'ao', 'ih', \"#']\", 'word_times', '7812', '10610', '14496', '15791', '20720', '25647', '26906', '32668', '37890', '38531', '42417', '43091', '46052', '50522', 'timitdict', 'transcription_dict', 'ao1', 'axr', 'england', 'demographic', 'permitting', 'vocal', 'spkrinfo', 'speakerinfo', 'vmh0', 'sex', 'trn', 'recdate', 'birthdate', '08', 'ht', 'wht', 'envisaged', 'sociolinguistics', 'revision', 'subdirectory', 'aks0', 'wav', 'schematically', 'subfields', 'amid', 'narrative', 'biased', 'incorporates', 'mini', 'cycle', 'converter', 'creation', 'unfolds', 'elicitation', 'archival', 'computerization', 'boon', 'decade', 'routinely', 'experimental', 'norm', 'funded', 'curation', 'gather', 'anc', 'bnc', 'reliance', 'mundane', 'regularly', 'devised', 'validated', 'specialist', 'uncertainty', 'consistently', 'reveal', 'adjudicated', 'exercised', 'exceptional', 'kappa', '333', 'rectangle', 's3', 'windowdiff', 'awarding', 'scorer', '00000010000000001000000', '00000001000000010000000', '00010000000000000001000', '571', 'slide', 'summed', 'shrink', 'sensitivity', 'evolution', 'disfluency', 'intonation', 'recycling', 'desire', 'replication', 'naturalistic', 'renaming', 'retokenizing', 'enrich', 'onerous', 'enriching', 'confronts', 'aligning', 'chaotic', 'centrally', 'curated', 'periodic', 'interval', 'submission', 'publishing', 'publication', 'standoff', 'rearranged', 'silently', 'obtaining', 'sigwac', 'reproducible', 'desired', 'gnu', 'wget', 'crawler', 'heritrix', 'mime', 'cyclic', 'latency', 'overloading', 'banned', 'validate', 'maximized', 'authoring', 'macro', 'capable', 'verifying', 'proprietary', '...\".', 'msonormal', 'mso', 'spacerun', '[<', 'spelle', '0pt', '.</', '...<', 'legal_pos', 'used_pos', 'illegal', \"'>([\", ']+)<\")', '1252', 'illegal_pos', 'iceberg', 'maintainer', 'lexical_data', 'html_file', '_entry', 'dict1', 'writerows', 'code_html2csv', 'microsoft', 'gzip', '+\".', 'gz', '\",\"', 'f_out', 'rosettaproject', 'fledged', 'declaring', 'schema', 'dominant', 'structuring', 'exploratory', 'unworkable', '...\"', 'defn', 'defns', 'defn_words', \"['...',\", 'arrives', 'isomorphic', 'transliterate', 'digested', 'discarded', 'defn_word', 'idx_file', 'idx_words', 'idx_line', '\"{}:', 'vexing', 'unavoidably', 'loosing', 'inject', 'tripping', 'unambiguously', 'rhetorical', '\\'/>\",', 'forge', 'rootedness', 'connectedness', 'acyclicity', 'invalidate', 'obliterate', 'litter', 'filespaces', 'founded', 'focussing', 'viz', 'innovation', 'filesystem', 'disturbing', 'insulates', 'dissemination', 'expedient', 'treasure', 'embodied', '~', 'oral', 'nuance', 'threatened', 'remnant', 'subspecies', 'therapeutic', 'breathtaking', 'colorful', 'tapestry', 'stretching', 'extinction', 'facet', 'heritage', 'recognizers', 'priority', 'voiced', 'curating', 'vexed', 'literary', 'continually', 'overriding', 'confusible', 'kw', \"('[\", \"'(.\", 'repl', \"('[^\", '))[:', 'illefent', 'lfnt', 'ebsekwieous', 'bskws', 'nuculerr', 'nclr', 'anicular', 'inocular', 'nucellar', 'nuclear', 'unicolor', 'uniocular', 'unocular', 'fuzzy_spell', 'olefiant', 'oliphant', 'elephanta', 'obsequious', 'nucular', 'gleaning', 'thanks', 'cetacean', 'mammal', 'streamlined', 'breathing', 'blowhole', 'closing', '>;', 'nicely', 'past_tense', 'akin', 'xref', 'modeled', 'abstracted', 'panacea', 'elementtree', 'merchant_file', '163', '\"?>', 'stylesheet', 'cs', 'href', '<!--', '<!', 'venice', 'stagedir', 'antonio', 'salarino', 'salanio', 'sooth', ':</', 'getchildren', '0x10ac43d18', '_element', '0x10ac43c28', 'persona', '0x10ac43bd8', 'scndescr', '0x10b067f98', 'playsubt', '0x10af37048', '0x10af37098', '0x10b936368', '0x10b934b88', '0x10cfd8188', '0x10cfadb38', 'subtitle', '0x10cfd8228', '0x10cfb02c8', '0x10cfb0318', 'portia', '0x10cfb0368', 'mercy', 'strain', 'romeo', 'juliet', 'doth', 'fading', 'air', 'merry', 'hear', 'ear', 'trusted', 'madam', 'musician', 'wren', 'navigating', 'speaker_seq', 'speaker_freq', 'top5', 'shylock', 'bassanio', '73', 'gratiano', 'lorenzo', 'oth', 'speaker_seq2', 'anto', 'bass', 'grat', 'shyl', '153', 'exchange', 'largest', '0x10b2f6958', 'kaakaaro', 'kaakaaviko', 'kaakaavo', 'kaakaoko', 'kaakasi', 'kaakau', 'kaakauko', 'kaakito', 'kaakuupato', 'kuvuto', 'stdout', 'util', 'elementtree_indent', 'isi', 'cooking', 'kukim', 'itoo', 'sf', 'flora', 'aug', 'taeavi', 'iria', 'kovopaueva', 'kaparapasia', 'planim', 'gaden', 'tasol', 'paia', 'planted', 'cook', 'tr', 'td', '>\\\\', 'findtext', '>%', '\"</', 'kakae', '>??', '?</', 'kakaevira', 'kakapikoa', 'kakapikoto', 'newborn', 'kakapu', 'sling', 'kakapua', 'kakara', 'arm', 'kakarapaia', 'kakarau', 'frog', 'trivially', '635', 'documentary', 'cvcvcvv', 'subelement', \"'[^\", 'v_', 'add_cv_field', 'cv_field', 'to_sfm_string', 'kaeviro', 'lift', 'antap', 'motion', 'nt', 'plane', 'jun', 'pita', 'kaeviroroe', 'kepa', 'kekesia', 'oa', 'vuripierevo', 'kiuvu', 'lukim', 'haus', 'win', 'bagarapim', 'destroyed', 'cvvcvcv', 'code_add_cv_field', 'safer', 'validating', 'practicable', 'field_sequences', \"(':'.\", 'rt', 'conformance', \"'+'\", \"('''\", 'sem_field', 'eng', 'ex_pidgin', 'ex_english', 'cmt', \"''')\", 'validate_lexicon', 'ignored_tags', 'marker_list', '(\"+\",', '(\"-\",', 'arg', 'code_toolbox_validation', 'lexfunc', 'lf', '>(<', 'lv', 'ln', '>*)*}', 'rf', 'xv', 'xn', '>*}', 'gv', 'dv', 'rn', 'hm', '>+<', 'toolboxdata', 'iu_mien_samp', 'code_chunk_toolbox', 'iu', 'mien', 'aggregation', 'physical', 'dublin', 'initiative', 'interdisciplinary', 'consensus', 'oai', 'scholarly', 'surrogate', 'server', 'offering', 'archived', 'protocol', 'harvest', 'partnership', 'virtual', 'archiving', 'interoperating', 'housing', 'ensured', 'controlled', 'descriptor', 'xmlns', '/\"', 'purl', 'dc', 'dcterms', 'xsi', 'xmlschema', 'schemalocation', 'xsd', 'kayardild', 'tangkic', 'evans', 'nicholas', 'gyd', '3110127954', ')</', 'mouton', 'gruyter', 'hardcover', '837', '0646119966', 'language_description', '\"/>', 'dcmitype', 'participating', 'harvested', 'consultation', 'rec', 'bpr', 'callhome', 'ldc97l18', 'multilex', 'icp', 'inpg', 'm0001', 'slelex', 'siemens', 's0048', 'korean', 'interlinear', 'uri', 'registered', 'icann', 'local_id', 'disseminating', 'upload', 'variability', 'interchange', 'shortcut', 'ide', 'suderman', 'lingua', 'graecae', 'tlg', 'childes', 'macwhinney', 'lamel', 'proceeding', 'promotes', 'cleaneval', 'sigann', 'encouraging', 'interoperability', 'buseman', 'ddp', 'tamanji', 'hirotani', 'simon', 'latech', 'zvon', 'olif', 'survey', 'thompson', 'mckelvie', 'farghaly', 'artstein', 'poesio', 'pevzner', 'deletes', 'sanitize', 'reduplication', '(..+)\\\\', 'syl', 'gl', \"'][\", 'xrf', 'referencing', '>,</', 'equipped', 'exciting', 'endeavor', 'spite', 'attest', 'hiding', 'sun', 'faulkner', 'dying', '1935', 'toaster', 'exhaust', 'dormitory', 'amiodarone', 'weakly', 'inhibited', 'cyp2c9', 'cyp2d6', 'cyp3a4', 'mediated', 'ki', '271', 'μm', 'pmid', '10718780', 'iraqi', 'earnest', 'prayer', 'righteous', 'wonderful', 'twas', 'brillig', 'slithy', 'toves', 'gyre', 'gimble', 'wabe', 'lewis', 'jabberwocky', '1872', 'afaik', 'discipline', 'philosophy', 'anthropology', 'psychology', 'hermeneutics', 'forensics', 'telephony', 'pedagogy', 'archaeology', 'cryptanalysis', 'pathology', 'deepen', 'intellect', 'manifested', 'barely', 'remark', 'rectify', 'grew', 'dating', '1900s', 'russell', 'wittgenstein', 'tarski', 'lambek', 'carnap', 'amenable', 'automaton', 'pushdown', 'intepret', 'outlined', 'relies', 'practitioner', 'prolog', 'eclipsed', 'emulated', 'typified', 'halle', '1968', 'hopelessly', 'hugely', 'assisted', 'quantitatively', 'embraced', 'metaphysical', 'debate', 'rationalism', 'empiricism', 'realism', 'idealism', 'enlightenment', 'western', 'backdrop', 'orthodox', 'divine', 'revelation', 'seventeenth', 'eighteenth', 'philosopher', 'sensory', 'descartes', 'leibniz', 'rationalist', 'asserting', 'origin', 'innate', 'implanted', 'birth', 'euclidean', 'geometry', 'supernatural', 'locke', 'empiricist', 'faculty', 'reflecting', 'galileo', 'planet', 'solar', 'heliocentric', 'geocentric', 'introspection', 'enshrined', 'kant', 'realist', 'perception', 'idealist', 'intrinsically', 'unobservable', 'betrays', 'occupy', 'territory', 'lean', 'shed', 'alive', 'nuanced', 'polarized', 'balancing', 'innately', 'analogical', 'circle', 'roadmap', 'suppletion', 'concatenative', 'duplication', 'expense', 'mapreduce', 'vibrant', 'encompassing', 'inheritance', 'nlg', 'heterogeneous', 'evolving', 'curate', 'liberating', 'spend', 'contrib', 'housed', 'nltk_contrib', 'imperfect', 'notify', 'mainstream', 'tackled', 'envoi', 'speak', 'profound', 'elusive', 'scientist', 'striving', 'fluency', 'unfortunate', 'concluded', 'pathway', 'hacking']\n"
     ]
    }
   ],
   "source": [
    "import scipy.spatial.distance as distance\n",
    "\n",
    "# BoW\n",
    "\n",
    "# 文書集合からターム素性集合（コードブック）を作る\n",
    "def collect_words_eng(docs):\n",
    "    '''英文書集合から単語コードブック作成。\n",
    "    シンプルに文書集合を予め決めうちした方式で処理する。\n",
    "    必要に応じて指定できるようにしていた方が使い易いかも。\n",
    "\n",
    "    :param docs(list): 1文書1文字列で保存。複数文書をリストとして並べたもの。\n",
    "    :return (list): 文分割、単語分割、基本形、ストップワード除去した、ユニークな単語一覧。\n",
    "    '''\n",
    "    codebook = []\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.append('.')   # ピリオドを追加。\n",
    "    stopwords.append(',')   # カンマを追加。\n",
    "    stopwords.append('')    # 空文字を追加。\n",
    "    stopwords.append('!')\n",
    "    stopwords.append('?')\n",
    "    wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    for doc in docs:\n",
    "        for sent in sent_tokenize(doc):\n",
    "            for word in wordpunct_tokenize(sent):\n",
    "                this_word = wnl.lemmatize(word.lower())\n",
    "                if this_word not in codebook and this_word not in stopwords:\n",
    "                    codebook.append(this_word)\n",
    "    return codebook\n",
    "\n",
    "codebook = collect_words_eng(docs3)\n",
    "print('codebook = ',codebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs[0] = Preface\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Preface\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This is a book about Natural Language Processing. By \"natural\n",
      "language\" we mean a language that is used for everyday\n",
      "communication by humans; languages like English, Hindi or\n",
      "Portuguese. In contrast to artificial languages such as\n",
      "programming languages and mathematical notations, natural languages have\n",
      "evolved as they pass from generation to generation, and are hard to\n",
      "pin down with explicit rules. We will take Natural Language Processing — or NLP\n",
      "for short — in a wide sense to cover any kind of computer manipulation\n",
      "of natural language. At one extreme, it could be as simple as counting\n",
      "word frequencies to compare different writing styles.\n",
      "At the other extreme, NLP involves \"understanding\" complete human\n",
      "utterances, at least to the extent of being able to give useful\n",
      "responses to them.\n",
      "Technologies based on NLP\n",
      "are becoming increasingly widespread. For example, phones and handheld computers\n",
      "support predictive text and handwriting recognition;  web\n",
      "search engines give access to information locked up in unstructured\n",
      "text; machine translation allows us to retrieve texts written in\n",
      "Chinese and read them in Spanish; text analysis enables us to\n",
      "detect sentiment in tweets and blogs.  By providing more natural human-machine interfaces, and more\n",
      "sophisticated access to stored information, language processing has\n",
      "come to play a central role in the multilingual information society.\n",
      "This book provides a highly accessible introduction to the field of NLP.\n",
      "It can be used for individual study or as the textbook for a course\n",
      "on natural language processing or computational linguistics,\n",
      "or as a supplement to courses in artificial intelligence,\n",
      "text mining, or corpus linguistics.\n",
      "The book is intensely practical, containing\n",
      "hundreds of fully-worked examples and graded exercises.\n",
      "The book is based on the Python programming language together with an open source\n",
      "library called the Natural Language Toolkit (NLTK).\n",
      "NLTK includes extensive software, data, and documentation, all freely downloadable from http://nltk.org/.\n",
      "Distributions are provided for Windows, Macintosh and Unix platforms.\n",
      "We strongly encourage you to download Python and NLTK, and try out the\n",
      "examples and exercises along the way.\n",
      "\n",
      "Audience\n",
      "NLP is important for scientific, economic, social, and cultural reasons.\n",
      "NLP is experiencing rapid growth as its theories and methods are deployed in\n",
      "a variety of new language technologies.  For this reason it is\n",
      "important for a wide range of people to have a working knowledge of NLP.\n",
      "Within industry, this includes people in\n",
      "human-computer interaction, business information analysis,\n",
      "and web software development.\n",
      "Within academia, it includes people in areas from\n",
      "humanities computing and corpus linguistics\n",
      "through to computer science and artificial intelligence.\n",
      "(To many people in academia, NLP is known by the name of\n",
      "\"Computational Linguistics.\")\n",
      "This book is intended for a diverse range of people who want to\n",
      "learn how to write programs that analyze written language,\n",
      "regardless of previous programming experience:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "New to programming?:\n",
      " The early chapters of the book are suitable\n",
      "for readers with no prior knowledge of programming, so long as you\n",
      "aren't afraid to tackle new concepts and develop new computing skills.\n",
      "The book is full of examples that you can copy and\n",
      "try for yourself, together with hundreds of graded exercises.\n",
      "If you need a more general introduction to Python, see the\n",
      "list of Python resources at http://docs.python.org/.\n",
      "\n",
      "New to Python?:Experienced programmers can quickly learn enough\n",
      "Python using this book to get immersed in natural language processing.\n",
      "All relevant Python features are carefully explained and exemplified,\n",
      "and you will quickly come to appreciate Python's suitability for this\n",
      "application area.  The language index will help you locate relevant\n",
      "discussions in the book.\n",
      "\n",
      "Already dreaming in Python?:\n",
      " Skim the Python examples\n",
      "and dig into the interesting language analysis\n",
      "material that starts in 1..\n",
      "You'll soon be applying your skills to this fascinating domain.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Emphasis\n",
      "This book is a practical introduction to NLP.  You will learn by\n",
      "example, write real programs, and grasp the value of being able to\n",
      "test an idea through implementation.  If you haven't learnt already,\n",
      "this book will teach you programming.  Unlike other programming\n",
      "books, we provide extensive illustrations and exercises from NLP.  The\n",
      "approach we have taken is also principled, in that we cover the\n",
      "theoretical underpinnings and don't shy away from careful linguistic\n",
      "and computational analysis.  We have tried to be pragmatic in\n",
      "striking a balance between theory and application, identifying the\n",
      "connections and the tensions.  Finally, we recognize that you\n",
      "won't get through this unless it is also pleasurable, so we have\n",
      "tried to include many applications and examples that are interesting\n",
      "and entertaining, sometimes whimsical.\n",
      "Note that this book is not a reference work.  Its coverage of Python\n",
      "and NLP is selective, and presented in a tutorial style.  For\n",
      "reference material, please consult the substantial quantity of\n",
      "searchable resources available at http://python.org/ and http://nltk.org/.\n",
      "This book is not an advanced computer science text.\n",
      "The content ranges from introductory to intermediate, and is\n",
      "directed at readers who want to learn how to analyze\n",
      "text using Python and the Natural Language Toolkit.\n",
      "To learn about advanced algorithms implemented in NLTK,\n",
      "you can examine the Python code linked from http://nltk.org/,\n",
      "and consult the other materials cited in this book.\n",
      "\n",
      "\n",
      "What You Will Learn\n",
      "By digging into the material presented here, you will learn:\n",
      "\n",
      "How simple programs can help you manipulate and analyze\n",
      "language data, and how to write these programs\n",
      "How key concepts from NLP and linguistics are used to describe and\n",
      "analyse language\n",
      "How data structures and algorithms are used in NLP\n",
      "How language data is stored in standard formats, and how data can\n",
      "be used to evaluate the performance of NLP techniques\n",
      "\n",
      "Depending on your background, and your motivation for being interested in NLP,\n",
      "you will gain different kinds of skills and knowledge from this book, as set out\n",
      "in III.1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Goals\n",
      "Background in arts and humanities\n",
      "Background in science and engineering\n",
      "\n",
      "\n",
      "\n",
      "Language analysis\n",
      "Manipulating large corpora,\n",
      "exploring linguistic models,\n",
      "and testing empirical claims.\n",
      "Using techniques in data modeling,\n",
      "data mining, and knowledge discovery\n",
      "to analyze natural language.\n",
      "\n",
      "Language technology\n",
      "Building robust systems to\n",
      "perform linguistic tasks\n",
      "with technological applications.\n",
      "Using linguistic algorithms and\n",
      "data structures in robust\n",
      "language processing software.\n",
      "\n",
      "\n",
      "Table III.1: Skills and knowledge to be gained from reading this book, depending on readers' goals and background\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Organization\n",
      "The early chapters are organized in order of conceptual difficulty,\n",
      "starting with a practical introduction to language processing\n",
      "that shows how to explore interesting bodies of text using\n",
      "tiny Python programs (Chapters 1-3).  This is followed by\n",
      "a chapter on structured programming (Chapter 4) that consolidates the\n",
      "programming topics scattered across the preceding chapters.\n",
      "After this, the pace picks up, and we move\n",
      "on to a series of chapters covering fundamental topics in\n",
      "language processing:\n",
      "tagging, classification, and information extraction (Chapters 5-7).\n",
      "The next three chapters look at ways to parse a sentence, recognize\n",
      "its syntactic structure, and construct representations of meaning (Chapters 8-10).\n",
      "The final chapter is devoted to linguistic data and how it can\n",
      "be managed effectively (Chapter 11).\n",
      "The book concludes with an Afterword, briefly discussing the past and future of the field.\n",
      "Within each chapter, we switch between different styles of presentation.\n",
      "In one style, natural language is the driver.  We analyze language,\n",
      "explore linguistic concepts, and use programming examples to support\n",
      "the discussion.  We often employ Python constructs that have\n",
      "not been introduced systematically, so you can see their purpose\n",
      "before delving into the details of how and why they work.\n",
      "This is just like learning idiomatic expressions in a foreign language:\n",
      "you're able to buy a nice pastry without first having learnt\n",
      "the intricacies of question formation.\n",
      "In the other style of presentation, the programming language will be the driver.\n",
      "We'll analyze programs, explore algorithms, and the linguistic examples\n",
      "will play a supporting role.\n",
      "Each chapter ends with a series of graded exercises,\n",
      "which are useful for consolidating the material.\n",
      "The exercises are graded according to the following scheme:\n",
      "☼ is for easy exercises that involve minor modifications\n",
      "to supplied code samples or other simple activities;\n",
      "◑ is for intermediate exercises that explore an aspect\n",
      "of the material in more depth, requiring careful analysis and design;\n",
      "★ is for difficult, open-ended tasks that will challenge your\n",
      "understanding of the material and force you to think independently\n",
      "(readers new to programming should skip these).\n",
      "Each chapter has a further reading section and an online \"extras\"\n",
      "section at http://nltk.org/, with pointers to more advanced materials and\n",
      "online resources.  Online versions of all the code examples are also\n",
      "available there.\n",
      "\n",
      "\n",
      "Why Python?\n",
      "Python is a simple yet powerful programming language with excellent\n",
      "functionality for processing linguistic data.  Python can be\n",
      "downloaded for free from http://python.org/.\n",
      "Installers are available for all platforms.\n",
      "Here is a five-line Python program that processes file.txt\n",
      "and prints all the words ending in ing:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> for line in open(\"file.txt\"):\n",
      "...     for word in line.split():\n",
      "...         if word.endswith('ing'):\n",
      "...             print(word)\n",
      "\n",
      "\n",
      "\n",
      "This program illustrates some of the main features of Python.  First,\n",
      "whitespace is used to nest lines of code, thus the line starting\n",
      "with if falls inside the scope of the previous line starting with\n",
      "for; this ensures that the ing test is performed for each\n",
      "word.  Second, Python is object-oriented; each variable is an entity\n",
      "that has certain defined attributes and methods.  For example, the\n",
      "value of the variable line is more than a sequence of characters.\n",
      "It is a string object that has a \"method\" (or operation) called\n",
      "split() that we can use to break a line into its words.  To apply\n",
      "a method to an object, we write the object name, followed by a period,\n",
      "followed by the method name, i.e. line.split().  Third, methods\n",
      "have arguments expressed inside parentheses.  For instance, in the\n",
      "example, word.endswith('ing') had the argument 'ing' to\n",
      "indicate that we wanted words ending with ing and not something else.\n",
      "Finally — and most importantly —\n",
      "Python is highly readable, so much so that it is fairly easy to guess\n",
      "what the program does even if you have never written a program\n",
      "before.\n",
      "We chose Python because it has a shallow learning curve,\n",
      "its syntax and semantics are transparent,\n",
      "and it has good string-handling functionality.  As an interpreted\n",
      "language, Python facilitates interactive exploration.  As an\n",
      "object-oriented language, Python permits data and methods to be\n",
      "encapsulated and re-used easily.  As a dynamic language, Python\n",
      "permits attributes to be added to objects on the fly, and permits\n",
      "variables to be typed dynamically, facilitating rapid development.\n",
      "Python comes with an extensive\n",
      "standard library, including components for graphical programming,\n",
      "numerical processing, and web connectivity.\n",
      "Python is heavily used in industry, scientific research, and education\n",
      "around the world.  Python is often praised for the way it facilitates\n",
      "productivity, quality, and maintainability of software.  A collection of\n",
      "Python success stories is posted at http://python.org/about/success/.\n",
      "NLTK defines an infrastructure that can be used to build NLP\n",
      "programs in Python.  It provides\n",
      "basic classes for representing data relevant to natural language processing;\n",
      "standard interfaces for performing tasks such as\n",
      "part-of-speech tagging, syntactic parsing, and text classification;\n",
      "and standard implementations for each task which can be combined to solve complex problems.\n",
      "NLTK comes with extensive documentation. In addition to this\n",
      "book, the website at http://nltk.org/ provides API documentation\n",
      "that covers every module, class and function in the toolkit,\n",
      "specifying parameters and giving examples of usage.\n",
      "\n",
      "\n",
      "Python 3 and NLTK 3\n",
      "This version of the book has been updated to support Python 3 and NLTK\n",
      "3. Python 3 includes some significant changes:\n",
      "\n",
      "the print statement is now a function requiring parentheses;\n",
      "many functions now return iterators instead of lists (to save memory usage);\n",
      "integer division returns a floating point number\n",
      "all text is now Unicode\n",
      "strings are formatted using the format method\n",
      "\n",
      "For a more detailed list of changes, please see\n",
      "https://docs.python.org/dev/whatsnew/3.0.html.\n",
      "There is a utility called 2to3.py which can convert your Python 2\n",
      "code to Python 3; for details please see\n",
      "https://docs.python.org/2/library/2to3.html.\n",
      "NLTK also includes some pervasive changes:\n",
      "\n",
      "many types are initialised from strings using a fromstring() method\n",
      "many functions now return iterators instead of lists\n",
      "ContextFreeGrammar is now called CFG and WeightedGrammar is now called PCFG\n",
      "batch_tokenize() is now called tokenize_sents(); there are corresponding changes for batch taggers, parsers, and classifiers\n",
      "some implementations have been removed in favour of external packages, or because they could not be maintained adequately\n",
      "\n",
      "For a more detailed list of changes, please see\n",
      "https://github.com/nltk/nltk/wiki/Porting-your-code-to-NLTK-3.0.\n",
      "\n",
      "\n",
      "Software Requirements\n",
      "To get the most out of this book, you should install several free software packages.\n",
      "Current download pointers and instructions are available at http://nltk.org/.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Python:The material presented in this book assumes that you are using Python version 3.2 or later.\n",
      "(Note that NLTK 3.0 also works with Python 2.6 and 2.7.)\n",
      "\n",
      "NLTK:The code examples in this book use NLTK version 3.0.  Subsequent releases of NLTK\n",
      "will be backward-compatible with NLTK 3.0.\n",
      "\n",
      "NLTK-Data:This contains the linguistic corpora that are analyzed and processed in the book.\n",
      "\n",
      "NumPy:(recommended)\n",
      "This is a scientific computing library with support for multidimensional arrays and\n",
      "linear algebra, required for certain probability, tagging, clustering, and classification\n",
      "tasks.\n",
      "\n",
      "Matplotlib:(recommended)\n",
      "This is a 2D plotting library for data visualization,\n",
      "and is used in some of the book's code samples that produce line graphs and bar charts.\n",
      "\n",
      "Stanford NLP Tools:\n",
      " (recommended)\n",
      "NLTK includes interfaces to the Stanford NLP Tools which are useful for large scale\n",
      "language processing (see http://nlp.stanford.edu/software/).\n",
      "\n",
      "NetworkX:(optional)\n",
      "This is a library for storing and manipulating network structures consisting of\n",
      "nodes and edges.  For visualizing semantic networks, also install the Graphviz library.\n",
      "\n",
      "Prover9:(optional)\n",
      "This is an automated theorem prover for first-order and equational logic, used\n",
      "to support inference in language processing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Natural Language Toolkit (NLTK)\n",
      "NLTK was originally created in 2001 as part of a computational linguistics\n",
      "course in the Department of Computer and Information Science at the\n",
      "University of Pennsylvania.  Since then it has been developed\n",
      "and expanded with the help of dozens of contributors.  It has now been\n",
      "adopted in courses in dozens of universities, and serves as the basis\n",
      "of many research projects.  See VIII.1 for a list of the most\n",
      "important NLTK modules.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Language processing task\n",
      "NLTK modules\n",
      "Functionality\n",
      "\n",
      "\n",
      "\n",
      "Accessing corpora\n",
      "corpus\n",
      "standardized interfaces to corpora and lexicons\n",
      "\n",
      "String processing\n",
      "tokenize, stem\n",
      "tokenizers, sentence tokenizers, stemmers\n",
      "\n",
      "Collocation discovery\n",
      "collocations\n",
      "t-test, chi-squared, point-wise mutual information\n",
      "\n",
      "Part-of-speech tagging\n",
      "tag\n",
      "n-gram, backoff, Brill, HMM, TnT\n",
      "\n",
      "Machine learning\n",
      "classify, cluster, tbl\n",
      "decision tree, maximum entropy, naive Bayes, EM, k-means\n",
      "\n",
      "Chunking\n",
      "chunk\n",
      "regular expression, n-gram, named-entity\n",
      "\n",
      "Parsing\n",
      "parse, ccg\n",
      "chart, feature-based, unification, probabilistic, dependency\n",
      "\n",
      "Semantic interpretation\n",
      "sem, inference\n",
      "lambda calculus, first-order logic, model checking\n",
      "\n",
      "Evaluation metrics\n",
      "metrics\n",
      "precision, recall, agreement coefficients\n",
      "\n",
      "Probability and estimation\n",
      "probability\n",
      "frequency distributions, smoothed probability distributions\n",
      "\n",
      "Applications\n",
      "app, chat\n",
      "graphical concordancer, parsers, WordNet browser, chatbots\n",
      "\n",
      "Linguistic fieldwork\n",
      "toolbox\n",
      "manipulate data in SIL Toolbox format\n",
      "\n",
      "\n",
      "Table VIII.1: Language processing tasks and corresponding NLTK modules with examples of functionality\n",
      "\n",
      "\n",
      "NLTK was designed with four primary goals in mind:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Simplicity:To provide an intuitive framework along with\n",
      "substantial building blocks, giving users a practical\n",
      "knowledge of NLP without getting bogged down in the tedious\n",
      "house-keeping usually associated with processing annotated\n",
      "language data\n",
      "\n",
      "Consistency:To provide a uniform framework with consistent\n",
      "interfaces and data structures, and easily-guessable method names\n",
      "\n",
      "Extensibility:To provide a structure into which new software\n",
      "modules can be easily accommodated, including alternative\n",
      "implementations and competing approaches to the same task\n",
      "\n",
      "Modularity:To provide components that can be used\n",
      "independently without needing to understand the rest of\n",
      "the toolkit\n",
      "\n",
      "\n",
      "\n",
      "Contrasting with these goals are three non-requirements —\n",
      "potentially useful qualities that we have deliberately avoided.  First,\n",
      "while the toolkit provides a wide range of functions, it is not\n",
      "encyclopedic; it is a toolkit, not a system, and it will\n",
      "continue to evolve with the field of NLP.\n",
      "Second, while the toolkit is efficient enough to support\n",
      "meaningful tasks, it is not highly optimized for runtime performance;\n",
      "such optimizations often involve more complex algorithms,\n",
      "or implementations in lower-level programming languages such as C or C++.\n",
      "This would make the software less readable and more difficult to install.\n",
      "Third, we have tried to avoid clever programming tricks,\n",
      "since we believe that clear implementations are preferable\n",
      "to ingenious yet indecipherable ones.\n",
      "\n",
      "\n",
      "For Instructors\n",
      "Natural Language Processing is often taught within the\n",
      "confines of a single-semester course at advanced undergraduate level\n",
      "or postgraduate level.  Many instructors have found that it is\n",
      "difficult to cover both the theoretical and practical sides of the\n",
      "subject in such a short span of time.  Some courses focus on theory to\n",
      "the exclusion of practical exercises, and deprive students of the\n",
      "challenge and excitement of writing programs to automatically process\n",
      "language.  Other courses are simply designed to teach programming for\n",
      "linguists, and do not manage to cover any significant NLP content.\n",
      "NLTK was originally developed to address this problem,\n",
      "making it feasible to cover a substantial amount of theory and\n",
      "practice within a single-semester course, even if students have no\n",
      "prior programming experience.\n",
      "A significant fraction of any NLP syllabus deals with\n",
      "algorithms and data structures.  On their own these can be rather\n",
      "dry, but NLTK brings them to life with the help of\n",
      "interactive graphical user interfaces that make it possible\n",
      "to view algorithms step-by-step.  Most NLTK components include\n",
      "a demonstration that performs an interesting task without\n",
      "requiring any special input from the user.\n",
      "An effective way to deliver the materials is through interactive\n",
      "presentation of the examples in this book, entering them in a Python session,\n",
      "observing what they do, and modifying them to explore some empirical\n",
      "or theoretical issue.\n",
      "This book contains hundreds of exercises that can be used\n",
      "as the basis for student assignments.  The simplest exercises involve\n",
      "modifying a supplied program fragment in a specified way in order to\n",
      "answer a concrete question.  At the other end of the spectrum, NLTK\n",
      "provides a flexible framework for graduate-level research projects,\n",
      "with standard implementations of all the basic data structures\n",
      "and algorithms, interfaces to dozens of widely used datasets (corpora),\n",
      "and a flexible and extensible architecture.  Additional support for\n",
      "teaching using NLTK is available on the NLTK website.\n",
      "\n",
      "We believe this book is unique in providing a comprehensive\n",
      "framework for students to learn about NLP in the context of learning\n",
      "to program.  What sets these\n",
      "materials apart is the tight coupling of the chapters\n",
      "and exercises with NLTK, giving students — even those with\n",
      "no prior programming experience — a practical introduction to\n",
      "NLP.  After completing these materials, students will be ready to\n",
      "attempt one of the more advanced textbooks, such as Speech and\n",
      "Language Processing, by Jurafsky and Martin (Prentice Hall, 2008).\n",
      "This book presents programming concepts in an unusual order, beginning\n",
      "with a non-trivial data type — lists of strings — then introducing\n",
      "non-trivial control structures such as comprehensions and conditionals.\n",
      "These idioms permit us to do useful language processing from the start.\n",
      "Once this motivation is in place, we return to a systematic presentation\n",
      "of fundamental concepts such as strings, loops, files, and so forth.\n",
      "In this way, we cover the same ground as more conventional approaches,\n",
      "without expecting readers to be interested in the programming\n",
      "language for its own sake.\n",
      "Two possible course plans are illustrated in IX.1.  The first\n",
      "one presumes an arts/humanities audience, whereas the second one presumes\n",
      "a science/engineering audience.  Other course plans could cover the first\n",
      "five chapters, then devote the remaining amount of time to a single area,\n",
      "such as text classification (Chapters 6-7), syntax (Chapters 8-9),\n",
      "semantics (Chapter 10), or linguistic data management (Chapter 11).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter\n",
      "Arts and Humanities\n",
      "Science and Engineering\n",
      "\n",
      "\n",
      "\n",
      "1  Language Processing and Python\n",
      "2-4\n",
      "2\n",
      "\n",
      "2  Accessing Text Corpora and Lexical Resources\n",
      "2-4\n",
      "2\n",
      "\n",
      "3  Processing Raw Text\n",
      "2-4\n",
      "2\n",
      "\n",
      "4  Writing Structured Programs\n",
      "2-4\n",
      "1-2\n",
      "\n",
      "5  Categorizing and Tagging Words\n",
      "2-4\n",
      "2-4\n",
      "\n",
      "6  Learning to Classify Text\n",
      "0-2\n",
      "2-4\n",
      "\n",
      "7  Extracting Information from Text\n",
      "2\n",
      "2-4\n",
      "\n",
      "8  Analyzing Sentence Structure\n",
      "2-4\n",
      "2-4\n",
      "\n",
      "9  Building Feature Based Grammars\n",
      "2-4\n",
      "1-4\n",
      "\n",
      "10 Analyzing the Meaning of Sentences\n",
      "1-2\n",
      "1-4\n",
      "\n",
      "11 Managing Linguistic Data\n",
      "1-2\n",
      "1-4\n",
      "\n",
      "Total\n",
      "18-36\n",
      "18-36\n",
      "\n",
      "\n",
      "Table IX.1: Suggested course plans; approximate number of lectures per chapter\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Conventions Used in This Book\n",
      "The following typographical conventions are used in this book:\n",
      "Bold -- Indicates new terms.\n",
      "Italic -- Used within paragraphs to refer to linguistic examples,\n",
      "the names of texts, and URLs; also used for filenames and file extensions.\n",
      "Constant width -- Used for program listings,\n",
      "as well as within paragraphs to refer to program elements\n",
      "such as variable or function names, statements, and keywords;\n",
      "also used for program names.\n",
      "Constant width bold -- Shows commands or other text that should\n",
      "be typed literally by the user.\n",
      "Constant width italic -- Shows text that should be\n",
      "replaced with user-supplied values or by values\n",
      "determined by context; also used for metavariables within program code examples.\n",
      "\n",
      "Note\n",
      "This icon signifies a tip, suggestion, or general note.\n",
      "\n",
      "\n",
      "Caution!\n",
      "This icon indicates a warning or caution.\n",
      "\n",
      "\n",
      "\n",
      "Using Code Examples\n",
      "This book is here to help you get your job done. In general, you may use the code in\n",
      "this book in your programs and documentation. You do not need to contact us for\n",
      "permission unless youÕre reproducing a significant portion of the code. For example,\n",
      "writing a program that uses several chunks of code from this book does not require\n",
      "permission. Selling or distributing a CD-ROM of examples from OÕReilly books does\n",
      "require permission. Answering a question by citing this book and quoting example\n",
      "code does not require permission. Incorporating a significant amount of example code\n",
      "from this book into your productÕs documentation does require permission.\n",
      "We appreciate, but do not require, attribution. An attribution usually includes the title,\n",
      "author, publisher, and ISBN. For example: ÒNatural Language Processing with Python,\n",
      "by Steven Bird, Ewan Klein, and Edward Loper.  O'Reilly Media, 978-0-596-51649-9.Ó\n",
      "If you feel your use of code examples falls outside fair use or the permission given above,\n",
      "feel free to contact us at permissions@oreilly.com.\n",
      "\n",
      "\n",
      "Acknowledgments\n",
      "The authors are indebted to the following people for feedback on\n",
      "earlier drafts of this book:\n",
      "Doug Arnold,\n",
      "Michaela Atterer,\n",
      "Greg Aumann,\n",
      "Kenneth Beesley,\n",
      "Steven Bethard,\n",
      "Ondrej Bojar,\n",
      "Chris Cieri,\n",
      "Robin Cooper,\n",
      "Grev Corbett,\n",
      "James Curran,\n",
      "Dan Garrette,\n",
      "Jean Mark Gawron,\n",
      "Doug Hellmann,\n",
      "Nitin Indurkhya,\n",
      "Mark Liberman,\n",
      "Peter Ljunglöf,\n",
      "Stefan Müller,\n",
      "Robin Munn,\n",
      "Joel Nothman,\n",
      "Adam Przepiorkowski,\n",
      "Brandon Rhodes,\n",
      "Stuart Robinson,\n",
      "Jussi Salmela,\n",
      "Kyle Schlansker,\n",
      "Rob Speer,\n",
      "and\n",
      "Richard Sproat.\n",
      "We are thankful to many students and colleagues for their comments on\n",
      "the class materials that evolved into these chapters,\n",
      "including participants at NLP and linguistics summer schools\n",
      "in Brazil, India, and the USA.\n",
      "This book would not exist without the members of the nltk-dev\n",
      "developer community, named on the NLTK website,\n",
      "who have given so freely of their time and expertise in building and extending NLTK.\n",
      "We are grateful to the U.S. National Science Foundation, the Linguistic Data Consortium,\n",
      "an Edward Clarence Dyason Fellowship,\n",
      "and the Universities of Pennsylvania, Edinburgh, and Melbourne for supporting our work\n",
      "on this book.\n",
      "We thank Julie Steele, Abby Fox, Loranah Dimant, and the rest of the O'Reilly team, for\n",
      "organizing comprehensive reviews of our drafts from people across the NLP\n",
      "and Python communities, for cheerfully customizing O'Reilly's production tools,\n",
      "and for meticulous copy-editing work.\n",
      "In preparing the revised edition for Python 3, we are grateful to\n",
      "Michael Korobov for leading the effort to port NLTK to Python 3, and to\n",
      "Antoine Trux for his meticulous feedback on the first edition.\n",
      "Finally, we owe a huge debt of gratitude to Mimo and Jee\n",
      "for their love, patience, and support over the many years that we worked on this book.\n",
      "We hope that our children — Andrew, Alison, Kirsten, Leonie, and Maaike —\n",
      "catch our enthusiasm for language and computation from these pages.\n",
      "\n",
      "\n",
      "\n",
      "About the Authors\n",
      "Steven Bird is Associate Professor in the\n",
      "Department of Computer Science and Software Engineering\n",
      "at the University of Melbourne, and Senior Research Associate in the\n",
      "Linguistic Data Consortium at the University of Pennsylvania.\n",
      "He completed a PhD on computational phonology at the University of\n",
      "Edinburgh in 1990, supervised by Ewan Klein.\n",
      "He later moved to Cameroon to conduct linguistic fieldwork on the\n",
      "Grassfields Bantu languages under the auspices of the Summer Institute\n",
      "of Linguistics.  More recently, he spent\n",
      "several years as Associate Director of the Linguistic Data Consortium\n",
      "where he led an R&D team to create models and tools for large\n",
      "databases of annotated text.  At Melbourne University,\n",
      "he established a language technology research group and has\n",
      "taught at all levels of the undergraduate computer science curriculum.\n",
      "In 2009, Steven is President of the Association for Computational Linguistics.\n",
      "Ewan Klein is Professor of Language Technology in the School of\n",
      "Informatics at the University of Edinburgh. He completed a PhD on\n",
      "formal semantics at the University of Cambridge in 1978. After some\n",
      "years working at the Universities of Sussex and Newcastle upon Tyne,\n",
      "Ewan took up a teaching position at Edinburgh. He was involved in the\n",
      "establishment of Edinburgh's Language Technology Group in 1993, and has\n",
      "been closely associated with it ever since.  From 2000–2002,\n",
      "he took leave from the University to act as Research Manager for the\n",
      "Edinburgh-based Natural Language Research Group of Edify Corporation,\n",
      "Santa Clara, and was responsible for spoken dialogue processing.  Ewan\n",
      "is a past President of the European Chapter of the Association for\n",
      "Computational Linguistics and was a founding member and Coordinator of\n",
      "the European Network of Excellence in Human Language Technologies\n",
      "(ELSNET).\n",
      "Edward Loper has recently completed a PhD\n",
      "on machine learning for natural language processing\n",
      "at the the University of Pennsylvania.\n",
      "Edward was a student in Steven's graduate course on\n",
      "computational linguistics in the fall of 2000, and\n",
      "went on to be a TA and share in the development of\n",
      "NLTK.  In addition to NLTK, he has\n",
      "helped develop two packages for documenting and\n",
      "testing Python software, epydoc and doctest.\n",
      "\n",
      "\n",
      "Royalties\n",
      "Royalties from the sale of this book are being used to support\n",
      "the development of the Natural Language Toolkit.\n",
      "\n",
      "\n",
      "Figure XIV.1: Edward Loper, Ewan Klein, and Steven Bird, Stanford, July 2007\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "About this document...\n",
      "UPDATED FOR NLTK 3.0.\n",
      "This is a chapter from Natural Language Processing with Python,\n",
      "by Steven Bird, Ewan Klein and Edward Loper,\n",
      "Copyright © 2019 the authors.\n",
      "It is distributed with the Natural Language Toolkit [http://nltk.org/],\n",
      "Version 3.0, under the terms of the\n",
      "Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\n",
      "[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\n",
      "This document was built on\n",
      "Wed  4 Sep 2019 11:40:48 ACST\n",
      "vectors[0] = [2, 42, 20, 62, 26, 9, 2, 22, 1, 1, 5, 18, 2, 1, 1, 1, 1, 3, 21, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 11, 27, 2, 3, 1, 8, 2, 8, 1, 6, 2, 3, 4, 1, 10, 2, 1, 3, 4, 5, 1, 2, 1, 1, 1, 1, 3, 2, 5, 1, 7, 5, 1, 1, 1, 25, 1, 1, 9, 1, 19, 1, 1, 3, 1, 1, 2, 8, 1, 1, 4, 1, 1, 7, 1, 3, 1, 1, 1, 6, 1, 1, 1, 1, 1, 2, 79, 7, 1, 2, 13, 4, 2, 1, 2, 1, 1, 5, 3, 1, 5, 3, 1, 1, 2, 12, 8, 11, 1, 2, 2, 9, 1, 7, 1, 3, 1, 2, 4, 12, 53, 2, 3, 1, 7, 6, 10, 22, 46, 9, 7, 4, 11, 25, 5, 2, 1, 16, 16, 14, 6, 3, 1, 1, 1, 1, 2, 1, 1, 2, 2, 2, 6, 3, 3, 3, 1, 1, 1, 2, 1, 2, 1, 4, 10, 1, 1, 8, 4, 7, 2, 6, 8, 2, 1, 1, 4, 2, 3, 4, 3, 9, 9, 1, 7, 1, 1, 1, 2, 8, 4, 21, 6, 1, 2, 3, 29, 2, 26, 1, 5, 3, 1, 18, 1, 1, 5, 2, 4, 1, 2, 2, 3, 7, 7, 4, 3, 1, 1, 2, 2, 10, 4, 1, 3, 4, 1, 1, 1, 2, 1, 5, 1, 5, 1, 2, 2, 1, 1, 1, 4, 13, 2, 16, 1, 1, 1, 1, 1, 1, 1, 1, 4, 3, 1, 7, 2, 2, 1, 5, 1, 3, 1, 9, 1, 3, 1, 1, 1, 2, 17, 3, 1, 1, 1, 1, 1, 1, 3, 2, 2, 1, 2, 1, 1, 1, 4, 2, 6, 1, 1, 3, 1, 4, 2, 3, 1, 1, 5, 21, 5, 2, 1, 2, 1, 8, 1, 1, 16, 1, 2, 1, 1, 2, 1, 1, 1, 10, 5, 3, 1, 2, 2, 2, 4, 2, 2, 1, 2, 2, 4, 3, 4, 2, 3, 1, 3, 2, 2, 1, 1, 2, 4, 2, 2, 1, 10, 1, 3, 1, 2, 1, 1, 5, 1, 1, 3, 3, 5, 1, 1, 20, 3, 2, 17, 9, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 2, 5, 4, 1, 2, 4, 1, 2, 1, 2, 4, 2, 2, 1, 2, 3, 3, 1, 1, 1, 1, 4, 1, 1, 1, 1, 2, 1, 1, 4, 2, 6, 4, 1, 1, 1, 1, 1, 2, 6, 1, 2, 1, 1, 1, 1, 6, 8, 1, 3, 1, 2, 2, 1, 1, 3, 1, 1, 2, 3, 1, 1, 3, 2, 1, 1, 1, 1, 3, 1, 1, 3, 1, 2, 1, 1, 2, 1, 2, 3, 1, 2, 5, 2, 1, 1, 4, 1, 3, 1, 2, 10, 2, 4, 2, 3, 2, 6, 1, 1, 1, 4, 3, 1, 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 3, 6, 2, 4, 2, 2, 1, 2, 1, 1, 7, 1, 3, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 5, 3, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 2, 3, 1, 4, 1, 3, 1, 1, 1, 2, 1, 1, 3, 3, 3, 1, 1, 1, 7, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3, 1, 1, 3, 3, 2, 1, 1, 2, 2, 2, 3, 1, 1, 5, 6, 1, 1, 3, 2, 2, 5, 5, 2, 4, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 11, 2, 1, 2, 1, 1, 25, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 2, 3, 3, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 4, 3, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 2, 4, 4, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 8, 2, 1, 1, 2, 12, 4, 3, 2, 1, 3, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 4, 1, 5, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 3, 2, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 4, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 2, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 5, 2, 2, 2, 2, 2, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 7, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 4, 1, 1, 1, 7, 4, 7, 5, 6, 4, 3, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 3, 1, 1, 1, 6, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "docs[1] = 1. Language Processing and Python\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1. Language Processing and Python\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "It is easy to get our hands on millions of words of text.\n",
      "What can we do with it, assuming we can write some simple programs?\n",
      "In this chapter we'll address the following questions:\n",
      "\n",
      "What can we achieve by combining simple programming techniques with large quantities of text?\n",
      "How can we automatically extract key words and phrases that sum up the style and content of a text?\n",
      "What tools and techniques does the Python programming language provide for such work?\n",
      "What are some of the interesting challenges of natural language processing?\n",
      "\n",
      "This chapter is divided into sections that skip between two quite\n",
      "different styles.  In the \"computing with language\" sections we will\n",
      "take on some linguistically motivated programming tasks without necessarily\n",
      "explaining how they work.  In the \"closer look at Python\" sections we\n",
      "will systematically review key programming concepts.  We'll flag the two styles in the section titles,\n",
      "but later chapters will mix both styles without being so up-front about it.\n",
      "We hope this style of introduction gives you an\n",
      "authentic taste of what will come later, while covering a range of\n",
      "elementary concepts in linguistics and computer science.\n",
      "If you have basic familiarity with both areas, you can skip to\n",
      "5;\n",
      "we will repeat any important points in later chapters, and if you miss anything\n",
      "you can easily consult the online reference material at http://nltk.org/.\n",
      "If the material is completely new to you, this chapter will raise\n",
      "more questions than it answers, questions that are addressed in\n",
      "the rest of this book.\n",
      "\n",
      "1   Computing with Language: Texts and Words\n",
      "We're all very familiar with text, since we read and write it every day.\n",
      "Here we will treat text as raw data for the programs we write,\n",
      "programs that manipulate and analyze it in a variety of interesting ways.\n",
      "But before we can do this, we have to get started with the Python interpreter.\n",
      "\n",
      "1.1   Getting Started with Python\n",
      "One of the friendly things about Python is that it allows you\n",
      "to type directly into the interactive interpreter —\n",
      "the program that will be running your Python programs.\n",
      "You can access the Python interpreter using a simple graphical interface\n",
      "called the Interactive DeveLopment Environment (IDLE).\n",
      "On a Mac you can find this under Applications→MacPython,\n",
      "and on Windows under All Programs→Python.\n",
      "Under Unix you can run Python from the shell by typing idle\n",
      "(if this is not installed, try typing python).\n",
      "The interpreter will print a blurb about your Python version;\n",
      "simply check that you are running Python 3.2 or later\n",
      "(here it is for 3.4.2):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Python 3.4.2 (default, Oct 15 2014, 22:01:37)\n",
      "[GCC 4.2.1 Compatible Apple LLVM 5.1 (clang-503.0.40)] on darwin\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "If you are unable to run the Python interpreter, you probably don't\n",
      "have Python installed correctly.  Please visit http://python.org/ for\n",
      "detailed instructions. NLTK 3.0 works for Python 2.6 and\n",
      "2.7. If you are using one of these older versions, note that\n",
      "the / operator rounds\n",
      "fractional results downwards (so 1/3 will give you 0).\n",
      "In order to get the expected behavior of division\n",
      "you need to type: from __future__ import division\n",
      "\n",
      "The >>> prompt indicates that the Python interpreter is now waiting\n",
      "for input.  When copying examples from this book, don't type\n",
      "the \">>>\" yourself.  Now, let's begin by using Python as a calculator:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> 1 + 5 * 2 - 3\n",
      "8\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "Once the interpreter has finished calculating the answer and displaying it, the\n",
      "prompt reappears. This means the Python interpreter is waiting for another instruction.\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Enter a few more expressions of your own. You can use asterisk (*)\n",
      "for multiplication and slash (/) for division, and parentheses for\n",
      "bracketing expressions.\n",
      "\n",
      "\n",
      "The preceding examples demonstrate how you can work interactively with the\n",
      "Python interpreter, experimenting with various expressions in the language\n",
      "to see what they do.\n",
      "Now let's try a nonsensical expression to see how the interpreter handles it:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> 1 +\n",
      "  File \"<stdin>\", line 1\n",
      "    1 +\n",
      "      ^\n",
      "SyntaxError: invalid syntax\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "This produced a syntax error.  In Python, it doesn't make sense\n",
      "to end an instruction with a plus sign. The Python interpreter\n",
      "indicates the line where the problem occurred (line 1 of <stdin>,\n",
      "which stands for \"standard input\").\n",
      "Now that we can use the Python interpreter, we're ready to start working\n",
      "with language data.\n",
      "\n",
      "\n",
      "1.2   Getting Started with NLTK\n",
      "Before going further you should install NLTK 3.0, downloadable for free from http://nltk.org/.\n",
      "Follow the instructions there to download the version required for your platform.\n",
      "Once you've installed NLTK, start up the Python interpreter as\n",
      "before, and install the data required for the book by\n",
      "typing the following two commands at the Python prompt, then selecting\n",
      "the book collection as shown in 1.1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> import nltk\n",
      ">>> nltk.download()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figure 1.1: Downloading the NLTK Book Collection: browse the available packages\n",
      "using nltk.download().  The Collections tab on the downloader\n",
      "shows how the packages are grouped into sets, and you should select the line labeled\n",
      "book to obtain all\n",
      "data required for the examples and exercises in this book.  It consists\n",
      "of about 30 compressed files requiring about 100Mb disk space.\n",
      "The full collection of data (i.e., all in the downloader) is\n",
      "nearly ten times this size (at the time of writing) and continues to expand.\n",
      "\n",
      "Once the data is downloaded to your machine, you can load some of it\n",
      "using the Python interpreter.\n",
      "The first step is to type a special command at the\n",
      "Python prompt which tells the interpreter to load some texts for us to\n",
      "explore: from nltk.book import *.\n",
      "This says \"from NLTK's book module, load\n",
      "all items.\"  The book module contains all the data you will need\n",
      "as you read this chapter.  After printing a welcome message, it loads\n",
      "the text of several books (this will take a few seconds).  Here's the\n",
      "command again, together with the output that\n",
      "you will see.  Take care to get spelling and punctuation right, and\n",
      "remember that you don't type the >>>.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.book import *\n",
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "Any time we want to find out about these texts, we just have\n",
      "to enter their names at the Python prompt:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text1\n",
      "<Text: Moby Dick by Herman Melville 1851>\n",
      ">>> text2\n",
      "<Text: Sense and Sensibility by Jane Austen 1811>\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "Now that we can use the Python interpreter, and have some data to work with,\n",
      "we're ready to get started.\n",
      "\n",
      "\n",
      "1.3   Searching Text\n",
      "There are many ways to examine the context of a text apart from\n",
      "simply reading it.  A concordance view shows us every occurrence of a given word, together\n",
      "with some context.  Here we look up the word monstrous in Moby\n",
      "Dick by entering text1 followed by a period, then the term\n",
      "concordance, and then placing \"monstrous\" in parentheses:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text1.concordance(\"monstrous\")\n",
      "Displaying 11 of 11 matches:\n",
      "ong the former , one was of a most monstrous size . ... This came towards us ,\n",
      "ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\n",
      "ll over with a heathenish array of monstrous clubs and spears . Some were thick\n",
      "d as you gazed , and wondered what monstrous cannibal and savage could ever hav\n",
      "that has survived the flood ; most monstrous and most mountainous ! That Himmal\n",
      "they might scout at Moby Dick as a monstrous fable , or still worse and more de\n",
      "th of Radney .'\" CHAPTER 55 Of the monstrous Pictures of Whales . I shall ere l\n",
      "ing Scenes . In connexion with the monstrous pictures of whales , I am strongly\n",
      "ere to enter upon those still more monstrous stories of them which are to be fo\n",
      "ght have been rummaged out of this monstrous cabinet there is no telling . But\n",
      "of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "The first time you use a concordance on a particular text, it takes a\n",
      "few extra seconds to build an index so that subsequent searches are fast.\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Try searching for other words; to save re-typing, you might be able to\n",
      "use up-arrow, Ctrl-up-arrow or Alt-p to access the previous command and modify the word being searched.\n",
      "You can also try searches on some of the other texts we have included.\n",
      "For example, search Sense and Sensibility for the word\n",
      "affection, using text2.concordance(\"affection\").  Search the book of Genesis\n",
      "to find out how long some people lived, using\n",
      "text3.concordance(\"lived\").  You could look at text4, the\n",
      "Inaugural Address Corpus, to see examples of English going\n",
      "back to 1789, and search for words like nation, terror, god\n",
      "to see how these words have been used differently over time.\n",
      "We've also included text5, the NPS Chat Corpus: search this for\n",
      "unconventional words like im, ur, lol.\n",
      "(Note that this corpus is uncensored!)\n",
      "\n",
      "Once you've spent a little while examining these texts, we hope you have a new\n",
      "sense of the richness and diversity of language.  In the next chapter\n",
      "you will learn how to access a broader range of text, including text in\n",
      "languages other than English.\n",
      "A concordance permits us to see words in context.  For example, we saw that\n",
      "monstrous occurred in contexts such as the ___ pictures\n",
      "and a ___ size .  What other words appear in a similar range\n",
      "of contexts?  We can find out\n",
      "by appending the term similar to the name of the text in\n",
      "question, then inserting the relevant word in parentheses:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text1.similar(\"monstrous\")\n",
      "mean part maddens doleful gamesome subtly uncommon careful untoward\n",
      "exasperate loving passing mouldy christian few true mystifying\n",
      "imperial modifies contemptible\n",
      ">>> text2.similar(\"monstrous\")\n",
      "very heartily so exceedingly remarkably as vast a great amazingly\n",
      "extremely good sweet\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "Observe that we get different results for different texts.\n",
      "Austen uses this word quite differently from Melville; for her, monstrous has\n",
      "positive connotations, and sometimes functions as an intensifier like the word\n",
      "very.\n",
      "The term common_contexts allows us to examine just the\n",
      "contexts that are shared by two or more words, such as monstrous\n",
      "and very. We have to enclose these words by square brackets as\n",
      "well as parentheses, and separate them with a comma:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text2.common_contexts([\"monstrous\", \"very\"])\n",
      "a_pretty is_pretty am_glad be_glad a_lucky\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Pick another pair of words and compare their usage in two different texts, using\n",
      "the similar() and common_contexts() functions.\n",
      "\n",
      "It is one thing to automatically detect that a particular word occurs in a text,\n",
      "and to display some words that appear in the same context.  However, we can also determine\n",
      "the location of a word in the text: how many words from the beginning it appears.\n",
      "This positional information can be displayed using a dispersion plot.\n",
      "Each stripe represents an instance\n",
      "of a word, and each row represents the entire text.  In 1.2 we\n",
      "see some striking patterns of word usage over the last 220 years\n",
      "(in an artificial text constructed by joining\n",
      "the texts of the Inaugural Address Corpus end-to-end).\n",
      "You can produce this plot as shown below.\n",
      "You might like to try more words (e.g., liberty, constitution),\n",
      "and different texts.  Can you predict the\n",
      "dispersion of a word before you view it?  As before, take\n",
      "care to get the quotes, commas, brackets and parentheses exactly right.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figure 1.2: Lexical Dispersion Plot for Words in U.S. Presidential Inaugural Addresses:\n",
      "This can be used to investigate changes in language use over time.\n",
      "\n",
      "\n",
      "Note\n",
      "Important:\n",
      "You need to have Python's NumPy and Matplotlib packages installed\n",
      "in order to produce the graphical plots used in this book.\n",
      "Please see http://nltk.org/ for installation instructions.\n",
      "\n",
      "\n",
      "Note\n",
      "You can also plot the frequency of word usage through time using\n",
      "https://books.google.com/ngrams\n",
      "\n",
      "Now, just for fun, let's try generating some random text in the various\n",
      "styles we have just seen.  To do this, we type the name of the text\n",
      "followed by the term generate. (We need to include the\n",
      "parentheses, but there's nothing that goes between them.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text3.generate()\n",
      "In the beginning of his brother is a hairy man , whose top may reach\n",
      "unto heaven ; and ye shall sow the land of Egypt there was no bread in\n",
      "all that he was taken out of the month , upon the earth . So shall thy\n",
      "wages be ? And they made their father ; and Isaac was old , and kissed\n",
      "him : and Laban with his cattle in the midst of the hands of Esau thy\n",
      "first born , and Phichol the chief butler unto his son Isaac , she\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "The generate() method is not available in NLTK 3.0 but will be\n",
      "reinstated in a subsequent version.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.4   Counting Vocabulary\n",
      "The most obvious fact about texts that emerges from the preceding examples is that\n",
      "they differ in the vocabulary they use.  In this section we will see how to use the\n",
      "computer to count the words in a text in a variety of useful ways.\n",
      "As before, you will jump right in and experiment with\n",
      "the Python interpreter, even though you may not have studied Python systematically\n",
      "yet.  Test your understanding by modifying the examples, and trying the\n",
      "exercises at the end of the chapter.\n",
      "Let's begin by finding out the length of a text from start to finish,\n",
      "in terms of the words and punctuation symbols that appear.  We use the\n",
      "term len to get the length of something, which we'll apply here to the\n",
      "book of Genesis:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> len(text3)\n",
      "44764\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "So Genesis has 44,764 words and punctuation symbols, or \"tokens.\"\n",
      "A token is the technical name for a sequence of characters\n",
      "— such as hairy, his, or :) — that we want to treat as a\n",
      "group. When we count the number of tokens in a text, say, the phrase\n",
      "to be or not to be, we are counting occurrences of these\n",
      "sequences. Thus, in our example phrase there are two occurrences of to,\n",
      "two of be, and one each of or and not. But there are\n",
      "only four distinct vocabulary items in this phrase.\n",
      "How many distinct words does the book of Genesis contain?\n",
      "To work this out in Python, we have to pose the question slightly\n",
      "differently.  The vocabulary of a text is just the set of tokens\n",
      "that it uses, since in a set, all duplicates are collapsed\n",
      "together. In Python we can obtain the vocabulary items of text3 with the\n",
      "command: set(text3).  When you do this, many screens of words will\n",
      "fly past.  Now try the following:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sorted(set(text3)) \n",
      "['!', \"'\", '(', ')', ',', ',)', '.', '.)', ':', ';', ';)', '?', '?)',\n",
      "'A', 'Abel', 'Abelmizraim', 'Abidah', 'Abide', 'Abimael', 'Abimelech',\n",
      "'Abr', 'Abrah', 'Abraham', 'Abram', 'Accad', 'Achbor', 'Adah', ...]\n",
      ">>> len(set(text3)) \n",
      "2789\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "By wrapping sorted() around the Python expression set(text3)\n",
      ",  we obtain a sorted list of vocabulary items, beginning\n",
      "with various punctuation symbols and continuing with words starting with A.  All\n",
      "capitalized words precede lowercase words.\n",
      "We discover the size of the vocabulary indirectly, by asking\n",
      "for the number of items in the set, and again we can use len to\n",
      "obtain this number .  Although it has 44,764 tokens, this book\n",
      "has only 2,789 distinct words, or \"word types.\"\n",
      "A word type is the form or spelling of the word independently of its\n",
      "specific occurrences in a text — that is, the\n",
      "word considered as a unique item of vocabulary.  Our count of 2,789 items\n",
      "will include punctuation symbols, so we will generally call these\n",
      "unique items types instead of word types.\n",
      "Now, let's calculate a measure of the lexical\n",
      "richness of the text.  The next example shows us that the number of\n",
      "distinct words is just 6% of the total number of words, or equivalently\n",
      "that each word is used 16 times on average\n",
      "(remember if you're using Python 2, to start with from __future__ import division).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> len(set(text3)) / len(text3)\n",
      "0.06230453042623537\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "Next, let's focus on particular words.  We can count how often a word occurs\n",
      "in a text, and compute what percentage of the text is taken up by a specific word:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text3.count(\"smote\")\n",
      "5\n",
      ">>> 100 * text4.count('a') / len(text4)\n",
      "1.4643016433938312\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "How many times does the word lol appear in text5?\n",
      "How much is this as a percentage of the total number of words\n",
      "in this text?\n",
      "\n",
      "You may want to repeat such calculations on several texts,\n",
      "but it is tedious to keep retyping the formula.  Instead,\n",
      "you can come up with your own name for a task, like\n",
      "\"lexical_diversity\" or \"percentage\", and associate it with a block of code.\n",
      "Now you only have to type a short\n",
      "name instead of one or more complete lines of Python code, and\n",
      "you can re-use it as often as you like. The block of code that does a\n",
      "task for us is called a function, and\n",
      "we define a short name for our function with the keyword def. The\n",
      "next example shows how to define two new functions,\n",
      "lexical_diversity() and   percentage():\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def lexical_diversity(text): \n",
      "...     return len(set(text)) / len(text) \n",
      "...\n",
      ">>> def percentage(count, total): \n",
      "...     return 100 * count / total\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Caution!\n",
      "The Python interpreter changes the prompt from\n",
      ">>> to ... after encountering the colon at the\n",
      "end of the first line.  The ... prompt indicates\n",
      "that Python expects an indented code block to appear next.\n",
      "It is up to you to do the indentation, by typing four\n",
      "spaces or hitting the tab key.  To finish the indented block just\n",
      "enter a blank line.\n",
      "\n",
      "In the definition of lexical_diversity() , we\n",
      "specify a parameter named text . This parameter is\n",
      "a \"placeholder\" for the actual text whose lexical diversity we want to\n",
      "compute, and reoccurs in the block of code that will run when the\n",
      "function is used . Similarly, percentage() is defined to\n",
      "take two parameters, named count and total .\n",
      "Once Python knows that lexical_diversity() and percentage()\n",
      "are the names for specific blocks\n",
      "of code, we can go ahead and use these functions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> lexical_diversity(text3)\n",
      "0.06230453042623537\n",
      ">>> lexical_diversity(text5)\n",
      "0.13477005109975562\n",
      ">>> percentage(4, 5)\n",
      "80.0\n",
      ">>> percentage(text4.count('a'), len(text4))\n",
      "1.4643016433938312\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "To recap, we use or call a function such as lexical_diversity() by typing its name, followed\n",
      "by an open parenthesis, the name of the text, and then a close\n",
      "parenthesis. These parentheses will show up often; their role is to separate\n",
      "the name of a task — such as lexical_diversity() — from the data\n",
      "that the task is to be performed on — such as text3.\n",
      "The data value that we place in the parentheses when we call a\n",
      "function is an argument to the function.\n",
      "You have already encountered several functions in this chapter, such\n",
      "as len(), set(), and sorted(). By convention, we will\n",
      "always add an empty pair of parentheses after a function name, as in\n",
      "len(), just to make clear that what we are talking about is a\n",
      "function rather than some other kind of Python expression.\n",
      "Functions are an important concept in programming, and we only\n",
      "mention them at the outset to give newcomers a sense of the\n",
      "power and creativity of programming.  Don't worry if you find it a bit\n",
      "confusing right now.\n",
      "Later we'll see how to use functions when tabulating data, as in 1.1.\n",
      "Each row of the table will involve the same computation but\n",
      "with different data, and we'll do this repetitive work using a function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Genre\n",
      "Tokens\n",
      "Types\n",
      "Lexical diversity\n",
      "\n",
      "\n",
      "\n",
      "skill and hobbies\n",
      "82345\n",
      "11935\n",
      "0.145\n",
      "\n",
      "humor\n",
      "21695\n",
      "5017\n",
      "0.231\n",
      "\n",
      "fiction: science\n",
      "14470\n",
      "3233\n",
      "0.223\n",
      "\n",
      "press: reportage\n",
      "100554\n",
      "14394\n",
      "0.143\n",
      "\n",
      "fiction: romance\n",
      "70022\n",
      "8452\n",
      "0.121\n",
      "\n",
      "religion\n",
      "39399\n",
      "6373\n",
      "0.162\n",
      "\n",
      "\n",
      "Table 1.1: Lexical Diversity of Various Genres in the Brown Corpus\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2   A Closer Look at Python: Texts as Lists of Words\n",
      "\n",
      "You've seen some important elements of the Python programming language.\n",
      "Let's take a few moments to review them systematically.\n",
      "\n",
      "2.1   Lists\n",
      "\n",
      "What is a text?  At one level, it is a sequence of symbols on a page such\n",
      "as this one.  At another level, it is a sequence of chapters, made up\n",
      "of a sequence of sections, where each section is a sequence of paragraphs,\n",
      "and so on.  However, for our purposes, we will think of a text as nothing\n",
      "more than a sequence of words and punctuation.  Here's how we represent\n",
      "text in Python, in this case the opening sentence of Moby Dick:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent1 = ['Call', 'me', 'Ishmael', '.']\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "After the prompt we've given a name we made up, sent1, followed\n",
      "by the equals sign, and then some quoted words, separated with\n",
      "commas, and surrounded with brackets.  This bracketed material\n",
      "is known as a list in Python: it is how we store a text.\n",
      "We can inspect it by typing the name . We can ask for its length .\n",
      "We can even apply our own lexical_diversity() function to it .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent1 \n",
      "['Call', 'me', 'Ishmael', '.']\n",
      ">>> len(sent1) \n",
      "4\n",
      ">>> lexical_diversity(sent1) \n",
      "1.0\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "Some more lists have been defined for you,\n",
      "one for the opening sentence of each of our texts,\n",
      "sent2 … sent9.  We inspect two of them\n",
      "here; you can see the rest for yourself using the Python interpreter\n",
      "(if you get an error which says that sent2 is not defined, you\n",
      "need to first type from nltk.book import *).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent2\n",
      "['The', 'family', 'of', 'Dashwood', 'had', 'long',\n",
      "'been', 'settled', 'in', 'Sussex', '.']\n",
      ">>> sent3\n",
      "['In', 'the', 'beginning', 'God', 'created', 'the',\n",
      "'heaven', 'and', 'the', 'earth', '.']\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Make up a few sentences of your own, by typing a name, equals\n",
      "sign, and a list of words, like this:\n",
      "ex1 = ['Monty', 'Python', 'and', 'the', 'Holy', 'Grail'].\n",
      "Repeat some of the other Python operations we saw earlier in\n",
      "1,\n",
      "e.g., sorted(ex1), len(set(ex1)), ex1.count('the').\n",
      "\n",
      "A pleasant surprise is that we can use Python's addition operator on lists.\n",
      "Adding two lists  creates a new list\n",
      "with everything from the first list, followed\n",
      "by everything from the second list:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> ['Monty', 'Python'] + ['and', 'the', 'Holy', 'Grail'] \n",
      "['Monty', 'Python', 'and', 'the', 'Holy', 'Grail']\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "This special use of the addition operation is called concatenation;\n",
      "it combines the lists together into a single list.  We can concatenate\n",
      "sentences to build up a text.\n",
      "\n",
      "We don't have to literally type the lists either; we can use short\n",
      "names that refer to pre-defined lists.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent4 + sent1\n",
      "['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the',\n",
      "'House', 'of', 'Representatives', ':', 'Call', 'me', 'Ishmael', '.']\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "What if we want to add a single item to a list? This is known as appending.\n",
      "When we append() to a list, the list itself is updated as a result\n",
      "of the operation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent1.append(\"Some\")\n",
      ">>> sent1\n",
      "['Call', 'me', 'Ishmael', '.', 'Some']\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.2   Indexing Lists\n",
      "\n",
      "\n",
      "As we have seen, a text in Python is a list of words, represented\n",
      "using a combination of brackets and quotes.  Just as with an ordinary\n",
      "page of text, we can count up the total number of words in text1\n",
      "with len(text1), and count the occurrences in a text of a\n",
      "particular word — say, 'heaven' — using text1.count('heaven').\n",
      "With some patience, we can pick out the 1st, 173rd, or even 14,278th\n",
      "word in a printed text. Analogously, we can identify the elements of a\n",
      "Python list by their order of occurrence in the list. The number that\n",
      "represents this position is the item's index.  We instruct Python\n",
      "to show us the item that occurs at an index such as 173 in a text\n",
      "by writing the name of the text followed by the index inside square brackets:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text4[173]\n",
      "'awaken'\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "We can do the converse; given a word, find the index of when it first\n",
      "occurs:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text4.index('awaken')\n",
      "173\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "Indexes are a common way to access the words of a text,\n",
      "or, more generally, the elements of any list.\n",
      "Python permits us to access sublists as well, extracting\n",
      "manageable pieces of language from large texts, a technique\n",
      "known as slicing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text5[16715:16735]\n",
      "['U86', 'thats', 'why', 'something', 'like', 'gamefly', 'is', 'so', 'good',\n",
      "'because', 'you', 'can', 'actually', 'play', 'a', 'full', 'game', 'without',\n",
      "'buying', 'it']\n",
      ">>> text6[1600:1625]\n",
      "['We', \"'\", 're', 'an', 'anarcho', '-', 'syndicalist', 'commune', '.', 'We',\n",
      "'take', 'it', 'in', 'turns', 'to', 'act', 'as', 'a', 'sort', 'of', 'executive',\n",
      "'officer', 'for', 'the', 'week']\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "Indexes have some subtleties, and we'll explore these with\n",
      "the help of an artificial sentence:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent = ['word1', 'word2', 'word3', 'word4', 'word5',\n",
      "...         'word6', 'word7', 'word8', 'word9', 'word10']\n",
      ">>> sent[0]\n",
      "'word1'\n",
      ">>> sent[9]\n",
      "'word10'\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "Notice that our indexes start from zero: sent element zero, written sent[0],\n",
      "is the first word, 'word1', whereas sent element 9 is 'word10'.\n",
      "The reason is simple: the moment Python accesses the content of a list from\n",
      "the computer's memory, it is already at the first element;\n",
      "we have to tell it how many elements forward to go.\n",
      "Thus, zero steps forward leaves it at the first element.\n",
      "\n",
      "Note\n",
      "This practice of counting from zero is initially confusing,\n",
      "but typical of modern programming languages.\n",
      "You'll quickly get the hang of it if\n",
      "you've mastered the system of counting centuries where 19XY is a year\n",
      "in the 20th century, or if you live in a country where the floors of\n",
      "a building are numbered from 1, and so walking up n-1 flights of\n",
      "stairs takes you to level n.\n",
      "\n",
      "Now, if we accidentally use an index that is too large, we get an error:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent[10]\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in ?\n",
      "IndexError: list index out of range\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "This time it is not a syntax error, because the program fragment is syntactically correct.\n",
      "Instead, it is a runtime error, and it produces a Traceback message that\n",
      "shows the context of the error, followed by the name of the error,\n",
      "IndexError, and a brief explanation.\n",
      "Let's take a closer look at slicing, using our artificial sentence again.\n",
      "Here we verify that the slice 5:8 includes sent elements at\n",
      "indexes 5, 6, and 7:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent[5:8]\n",
      "['word6', 'word7', 'word8']\n",
      ">>> sent[5]\n",
      "'word6'\n",
      ">>> sent[6]\n",
      "'word7'\n",
      ">>> sent[7]\n",
      "'word8'\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "By convention, m:n means elements m…n-1.\n",
      "As the next example shows,\n",
      "we can omit the first number if the slice begins at the start of the\n",
      "list , and we can omit the second number if the slice goes to the end :\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent[:3] \n",
      "['word1', 'word2', 'word3']\n",
      ">>> text2[141525:] \n",
      "['among', 'the', 'merits', 'and', 'the', 'happiness', 'of', 'Elinor', 'and', 'Marianne',\n",
      "',', 'let', 'it', 'not', 'be', 'ranked', 'as', 'the', 'least', 'considerable', ',',\n",
      "'that', 'though', 'sisters', ',', 'and', 'living', 'almost', 'within', 'sight', 'of',\n",
      "'each', 'other', ',', 'they', 'could', 'live', 'without', 'disagreement', 'between',\n",
      "'themselves', ',', 'or', 'producing', 'coolness', 'between', 'their', 'husbands', '.',\n",
      "'THE', 'END']\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "We can modify an element of a list by assigning to one of its index values.\n",
      "In the next example, we put sent[0] on the left of the equals sign .  We can also\n",
      "replace an entire slice with new material .  A consequence of this\n",
      "last change is that the list only has four elements, and accessing a later value\n",
      "generates an error .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent[0] = 'First' \n",
      ">>> sent[9] = 'Last'\n",
      ">>> len(sent)\n",
      "10\n",
      ">>> sent[1:9] = ['Second', 'Third'] \n",
      ">>> sent\n",
      "['First', 'Second', 'Third', 'Last']\n",
      ">>> sent[9] \n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in ?\n",
      "IndexError: list index out of range\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Take a few minutes to define a sentence of your own and modify individual words and\n",
      "groups of words (slices) using the same methods used earlier.  Check your understanding\n",
      "by trying the exercises on lists at the end of this chapter.\n",
      "\n",
      "\n",
      "\n",
      "2.3   Variables\n",
      "From the start of 1, you have had\n",
      "access to texts called text1, text2, and so on.  It saved a lot\n",
      "of typing to be able to refer to a 250,000-word book with a short name\n",
      "like this!  In general, we can make up names for anything we care\n",
      "to calculate.  We did this ourselves in the previous sections, e.g.,\n",
      "defining a variable sent1, as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent1 = ['Call', 'me', 'Ishmael', '.']\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "Such lines have the form: variable = expression.  Python will evaluate\n",
      "the expression, and save its result to the variable.  This process is\n",
      "called assignment.  It does not generate any output;\n",
      "you have to type the variable on a line of its\n",
      "own to inspect its contents.  The equals sign is slightly misleading,\n",
      "since information is moving from the right side to the left.\n",
      "It might help to think of it as a left-arrow.\n",
      "The name of the variable can be anything you like, e.g., my_sent, sentence, xyzzy.\n",
      "It must start with a letter, and can include numbers and underscores.\n",
      "Here are some examples of variables and assignments:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> my_sent = ['Bravely', 'bold', 'Sir', 'Robin', ',', 'rode',\n",
      "... 'forth', 'from', 'Camelot', '.']\n",
      ">>> noun_phrase = my_sent[1:4]\n",
      ">>> noun_phrase\n",
      "['bold', 'Sir', 'Robin']\n",
      ">>> wOrDs = sorted(noun_phrase)\n",
      ">>> wOrDs\n",
      "['Robin', 'Sir', 'bold']\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "Remember that capitalized words appear before lowercase words in sorted lists.\n",
      "\n",
      "Note\n",
      "Notice in the previous example that we split the definition\n",
      "of my_sent over two lines.  Python expressions can be split across\n",
      "multiple lines, so long as this happens within any kind of brackets.\n",
      "Python uses the \"...\" prompt to indicate that more input is\n",
      "expected.  It doesn't matter how much indentation is used in these\n",
      "continuation lines, but some indentation usually makes them easier to read.\n",
      "\n",
      "It is good to choose meaningful variable names to remind you — and to help anyone\n",
      "else who reads your Python code — what your code is meant to do.\n",
      "Python does not try to make sense of the names; it blindly follows your instructions,\n",
      "and does not object if you do something confusing, such as one = 'two' or two = 3.\n",
      "The only restriction is that\n",
      "a variable name cannot be any of Python's reserved words, such as\n",
      "def, if, not,\n",
      "and import.  If you use a reserved word, Python will produce a syntax error:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> not = 'Camelot'           \n",
      "File \"<stdin>\", line 1\n",
      "    not = 'Camelot'\n",
      "        ^\n",
      "SyntaxError: invalid syntax\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "We will often use variables to hold intermediate steps of a computation, especially\n",
      "when this makes the code easier to follow.  Thus len(set(text1)) could also be written:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> vocab = set(text1)\n",
      ">>> vocab_size = len(vocab)\n",
      ">>> vocab_size\n",
      "19317\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Caution!\n",
      "Take care with your choice of names (or identifiers) for Python\n",
      "variables.  First, you should start the name with a letter, optionally\n",
      "followed by digits (0 to 9) or letters. Thus, abc23 is fine, but\n",
      "23abc will cause a syntax error.\n",
      "Names are case-sensitive, which means that myVar and myvar\n",
      "are distinct variables.  Variable names cannot contain whitespace,\n",
      "but you can separate words using an underscore, e.g.,\n",
      "my_var. Be careful not to insert a hyphen instead of an\n",
      "underscore: my-var is wrong, since Python interprets the\n",
      "\"-\" as a minus sign.\n",
      "\n",
      "\n",
      "\n",
      "2.4   Strings\n",
      "Some of the methods we used to access the elements of a list also work with individual words,\n",
      "or strings.  For example, we can assign a string to a variable ,\n",
      "index a string , and slice a string :\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> name = 'Monty' \n",
      ">>> name[0] \n",
      "'M'\n",
      ">>> name[:4] \n",
      "'Mont'\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "We can also perform multiplication and addition with strings:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> name * 2\n",
      "'MontyMonty'\n",
      ">>> name + '!'\n",
      "'Monty!'\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "We can join the words of a list to make a single string, or split a string into a list, as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> ' '.join(['Monty', 'Python'])\n",
      "'Monty Python'\n",
      ">>> 'Monty Python'.split()\n",
      "['Monty', 'Python']\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "We will come back to the topic of strings in 3.\n",
      "For the time being, we have two important building blocks\n",
      "— lists and strings —\n",
      "and are ready to get back to some language analysis.\n",
      "\n",
      "\n",
      "\n",
      "3   Computing with Language: Simple Statistics\n",
      "\n",
      "Let's return to our exploration of the ways we can bring our computational\n",
      "resources to bear on large quantities of text.  We began this discussion in\n",
      "1, and saw how to search for words\n",
      "in context, how to compile the vocabulary of a text, how to generate random\n",
      "text in the same style, and so on.\n",
      "In this section we pick up the question of what makes a text distinct,\n",
      "and use automatic methods to find characteristic words and expressions\n",
      "of a text.  As in 1, you can try\n",
      "new features of the Python language by copying them into the interpreter,\n",
      "and you'll learn about these features systematically in the following section.\n",
      "Before continuing further, you might like to check your understanding of the\n",
      "last section by predicting the output of the following code.  You can use\n",
      "the interpreter to check whether you got it right.  If you're not sure how\n",
      "to do this task, it would be a good idea to review the previous section\n",
      "before continuing further.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> saying = ['After', 'all', 'is', 'said', 'and', 'done',\n",
      "...           'more', 'is', 'said', 'than', 'done']\n",
      ">>> tokens = set(saying)\n",
      ">>> tokens = sorted(tokens)\n",
      ">>> tokens[-2:]\n",
      "what output do you expect here?\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.1   Frequency Distributions\n",
      "How can we automatically identify the words of a text that are most\n",
      "informative about the topic and genre of the text?  Imagine how you might\n",
      "go about finding the 50 most frequent words of a book.  One method\n",
      "would be to keep a tally for each vocabulary item, like that shown in 3.1.\n",
      "The tally would need thousands of rows, and it would be an exceedingly\n",
      "laborious process — so laborious that we would rather assign the task to a machine.\n",
      "\n",
      "\n",
      "Figure 3.1: Counting Words Appearing in a Text (a frequency distribution)\n",
      "\n",
      "The table in 3.1 is known as a frequency distribution,\n",
      "and it tells us the frequency of each vocabulary item in the text.\n",
      "(In general, it could count any kind of observable event.)\n",
      "It is a \"distribution\"\n",
      "because it tells us how the total number of word tokens in the text\n",
      "are distributed across the vocabulary items.\n",
      "Since we often need frequency distributions in language processing, NLTK\n",
      "provides built-in support for them.  Let's use a FreqDist to find the\n",
      "50 most frequent words of Moby Dick:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fdist1 = FreqDist(text1) \n",
      ">>> print(fdist1) \n",
      "<FreqDist with 19317 samples and 260819 outcomes>\n",
      ">>> fdist1.most_common(50) \n",
      "[(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024),\n",
      "('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982),\n",
      "(\"'\", 2684), ('-', 2552), ('his', 2459), ('it', 2209), ('I', 2124),\n",
      "('s', 1739), ('is', 1695), ('he', 1661), ('with', 1659), ('was', 1632),\n",
      "('as', 1620), ('\"', 1478), ('all', 1462), ('for', 1414), ('this', 1280),\n",
      "('!', 1269), ('at', 1231), ('by', 1137), ('but', 1113), ('not', 1103),\n",
      "('--', 1070), ('him', 1058), ('from', 1052), ('be', 1030), ('on', 1005),\n",
      "('so', 918), ('whale', 906), ('one', 889), ('you', 841), ('had', 767),\n",
      "('have', 760), ('there', 715), ('But', 705), ('or', 697), ('were', 680),\n",
      "('now', 646), ('which', 640), ('?', 637), ('me', 627), ('like', 624)]\n",
      ">>> fdist1['whale']\n",
      "906\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "When we first invoke FreqDist, we pass the name of the text as an\n",
      "argument . We can inspect the total number of words (\"outcomes\")\n",
      "that have been counted up  — 260,819 in the\n",
      "case of Moby Dick. The expression most_common(50) gives us a list of\n",
      "the 50 most frequently occurring types in the text .\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Try the preceding frequency distribution example for yourself, for\n",
      "text2.  Be careful to use the correct parentheses and uppercase letters.\n",
      "If you get an error message NameError: name 'FreqDist' is not defined,\n",
      "you need to start your work with from nltk.book import *\n",
      "\n",
      "\n",
      "Do any words produced in the last example help us grasp the topic or genre of this text?\n",
      "Only one word, whale, is slightly informative!  It occurs over 900 times.\n",
      "The rest of the words tell us nothing about the text; they're just English \"plumbing.\"\n",
      "What proportion of the text is taken up with such words?\n",
      "We can generate a cumulative frequency plot for these words,\n",
      "using fdist1.plot(50, cumulative=True), to produce the graph in 3.2.\n",
      "These 50 words account for nearly half the book!\n",
      "\n",
      "\n",
      "Figure 3.2: Cumulative Frequency Plot for 50 Most Frequently Words in Moby Dick:\n",
      "these account for nearly half of the tokens.\n",
      "\n",
      "If the frequent words don't help us, how about the words that occur once\n",
      "only, the so-called hapaxes?  View them by typing fdist1.hapaxes().\n",
      "This list contains lexicographer, cetological,\n",
      "contraband, expostulations, and about 9,000 others.\n",
      "It seems that there are too many rare words, and without seeing the\n",
      "context we probably can't guess what half of the hapaxes mean in any case!\n",
      "Since neither frequent nor infrequent words help, we need to try\n",
      "something else.\n",
      "\n",
      "\n",
      "3.2   Fine-grained Selection of Words\n",
      "Next, let's look at the long words of a text; perhaps these will be\n",
      "more characteristic and informative.  For this we adapt some notation\n",
      "from set theory.  We would like to find the words from the vocabulary\n",
      "of the text that are more than 15 characters long.  Let's call\n",
      "this property P, so that P(w) is true\n",
      "if and only if w is more than 15 characters long.\n",
      "Now we can express the words of interest using mathematical\n",
      "set notation as shown in (1a).\n",
      "This means \"the set of all w such that w is an\n",
      "element of V (the vocabulary) and w has property P\".\n",
      "\n",
      "  (1)\n",
      "  a.{w | w ∈ V & P(w)}\n",
      "\n",
      "  b.[w for w in V if p(w)]\n",
      "\n",
      "The corresponding Python expression is given in (1b).\n",
      "(Note that it produces a list, not a set, which means that duplicates are possible.)\n",
      "Observe how similar the two notations are.  Let's go one more step and\n",
      "write executable Python code:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> V = set(text1)\n",
      ">>> long_words = [w for w in V if len(w) > 15]\n",
      ">>> sorted(long_words)\n",
      "['CIRCUMNAVIGATION', 'Physiognomically', 'apprehensiveness', 'cannibalistically',\n",
      "'characteristically', 'circumnavigating', 'circumnavigation', 'circumnavigations',\n",
      "'comprehensiveness', 'hermaphroditical', 'indiscriminately', 'indispensableness',\n",
      "'irresistibleness', 'physiognomically', 'preternaturalness', 'responsibilities',\n",
      "'simultaneousness', 'subterraneousness', 'supernaturalness', 'superstitiousness',\n",
      "'uncomfortableness', 'uncompromisedness', 'undiscriminating', 'uninterpenetratingly']\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "For each word w in the vocabulary V, we check whether\n",
      "len(w) is greater than 15; all other words will\n",
      "be ignored.  We will discuss this syntax more carefully later.\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Try out the previous statements in the Python interpreter,\n",
      "and experiment with changing the text and changing the length condition.\n",
      "Does it make a difference to your results if you change the\n",
      "variable names, e.g., using [word for word in vocab if ...]?\n",
      "\n",
      "Let's return to our task of finding words that characterize a text.\n",
      "Notice that the long words in text4 reflect its national focus\n",
      "— constitutionally, transcontinental —\n",
      "whereas those in text5 reflect its informal content:\n",
      "boooooooooooglyyyyyy and yuuuuuuuuuuuummmmmmmmmmmm.\n",
      "Have we succeeded in automatically extracting words that typify\n",
      "a text?  Well, these very long words are often hapaxes (i.e., unique)\n",
      "and perhaps it would be better to find frequently occurring\n",
      "long words.  This seems promising since it eliminates\n",
      "frequent short words (e.g., the) and infrequent long words\n",
      "(e.g. antiphilosophists).\n",
      "Here are all words from the chat corpus\n",
      "that are longer than seven characters, that occur more than seven times:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fdist5 = FreqDist(text5)\n",
      ">>> sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7)\n",
      "['#14-19teens', '#talkcity_adults', '((((((((((', '........', 'Question',\n",
      "'actually', 'anything', 'computer', 'cute.-ass', 'everyone', 'football',\n",
      "'innocent', 'listening', 'remember', 'seriously', 'something', 'together',\n",
      "'tomorrow', 'watching']\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "Notice how we have used two conditions: len(w) > 7 ensures that the\n",
      "words are longer than seven letters, and fdist5[w] > 7 ensures that\n",
      "these words occur more than seven times.  At last we have managed to\n",
      "automatically identify the frequently-occurring content-bearing\n",
      "words of the text.  It is a modest but important milestone: a tiny piece of code,\n",
      "processing tens of thousands of words, produces some informative output.\n",
      "\n",
      "\n",
      "3.3   Collocations and Bigrams\n",
      "A collocation is a sequence of words that occur together\n",
      "unusually often. Thus red wine is a collocation, whereas the\n",
      "wine is not. A characteristic of collocations is that they are\n",
      "resistant to substitution with words that have similar senses;\n",
      "for example, maroon wine sounds definitely odd.\n",
      "To get a handle on collocations, we start off by extracting from a text\n",
      "a list of word pairs, also known as bigrams. This is easily\n",
      "accomplished with the function bigrams():\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> list(bigrams(['more', 'is', 'said', 'than', 'done']))\n",
      "[('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')]\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "If you omitted list() above, and just typed bigrams(['more', ...]),\n",
      "you would have seen output of the form <generator object bigrams at 0x10fb8b3a8>.\n",
      "This is Python's way of saying that it is ready to compute\n",
      "a sequence of items, in this case, bigrams. For now, you just need\n",
      "to know to tell Python to convert it into a list, using list().\n",
      "\n",
      "Here we see that the pair of words than-done is a bigram, and we write\n",
      "it in Python as ('than', 'done').  Now, collocations are essentially\n",
      "just frequent bigrams, except that we want to pay more attention to the\n",
      "cases that involve rare words.  In particular, we want to find\n",
      "bigrams that occur more often than we would expect based on\n",
      "the frequency of the individual words.  The collocations() function\n",
      "does this for us. We will see how it works later.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text4.collocations()\n",
      "United States; fellow citizens; four years; years ago; Federal\n",
      "Government; General Government; American people; Vice President; Old\n",
      "World; Almighty God; Fellow citizens; Chief Magistrate; Chief Justice;\n",
      "God bless; every citizen; Indian tribes; public debt; one another;\n",
      "foreign nations; political parties\n",
      ">>> text8.collocations()\n",
      "would like; medium build; social drinker; quiet nights; non smoker;\n",
      "long term; age open; Would like; easy going; financially secure; fun\n",
      "times; similar interests; Age open; weekends away; poss rship; well\n",
      "presented; never married; single mum; permanent relationship; slim\n",
      "build\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "The collocations that emerge are very specific to the genre of the\n",
      "texts. In order to find  red wine as a collocation, we would\n",
      "need to process a much larger body of text.\n",
      "\n",
      "\n",
      "3.4   Counting Other Things\n",
      "Counting words is useful, but we can count other things too.  For example, we can\n",
      "look at the distribution of word lengths in a text, by creating a FreqDist\n",
      "out of a long list of numbers, where each number is the length of the corresponding\n",
      "word in the text:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> [len(w) for w in text1] \n",
      "[1, 4, 4, 2, 6, 8, 4, 1, 9, 1, 1, 8, 2, 1, 4, 11, 5, 2, 1, 7, 6, 1, 3, 4, 5, 2, ...]\n",
      ">>> fdist = FreqDist(len(w) for w in text1)  \n",
      ">>> print(fdist)  \n",
      "<FreqDist with 19 samples and 260819 outcomes>\n",
      ">>> fdist\n",
      "FreqDist({3: 50223, 1: 47933, 4: 42345, 2: 38513, 5: 26597, 6: 17111, 7: 14399,\n",
      "  8: 9966, 9: 6428, 10: 3528, ...})\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "We start by deriving a list of the lengths of words in text1\n",
      ",\n",
      "and the FreqDist then counts the number of times each of these\n",
      "occurs . The result  is a distribution containing\n",
      "a quarter of a million items, each of which is a number corresponding to a\n",
      "word token in the text.  But there are at most only 20 distinct\n",
      "items being counted, the numbers 1 through 20, because there are only 20\n",
      "different word lengths.  I.e., there are words consisting of just one character,\n",
      "two characters, ..., twenty characters, but none with twenty one or more\n",
      "characters.  One might wonder how frequent the different lengths of word are\n",
      "(e.g., how many words of length four appear in the text, are there more words of length five\n",
      "than length four, etc). We can do this as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fdist.most_common()\n",
      "[(3, 50223), (1, 47933), (4, 42345), (2, 38513), (5, 26597), (6, 17111), (7, 14399),\n",
      "(8, 9966), (9, 6428), (10, 3528), (11, 1873), (12, 1053), (13, 567), (14, 177),\n",
      "(15, 70), (16, 22), (17, 12), (18, 1), (20, 1)]\n",
      ">>> fdist.max()\n",
      "3\n",
      ">>> fdist[3]\n",
      "50223\n",
      ">>> fdist.freq(3)\n",
      "0.19255882431878046\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "From this we see that the most frequent word length is 3, and that\n",
      "words of length 3 account for roughly 50,000 (or 20%) of the words making up the\n",
      "book.  Although we will not pursue it here, further analysis of word\n",
      "length might help us understand differences between authors, genres, or\n",
      "languages.\n",
      "3.1 summarizes the functions defined in frequency distributions.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Example\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "fdist = FreqDist(samples)\n",
      "create a frequency distribution containing the given samples\n",
      "\n",
      "fdist[sample] += 1\n",
      "increment the count for this sample\n",
      "\n",
      "fdist['monstrous']\n",
      "count of the number of times a given sample occurred\n",
      "\n",
      "fdist.freq('monstrous')\n",
      "frequency of a given sample\n",
      "\n",
      "fdist.N()\n",
      "total number of samples\n",
      "\n",
      "fdist.most_common(n)\n",
      "the n most common samples and their frequencies\n",
      "\n",
      "for sample in fdist:\n",
      "iterate over the samples\n",
      "\n",
      "fdist.max()\n",
      "sample with the greatest count\n",
      "\n",
      "fdist.tabulate()\n",
      "tabulate the frequency distribution\n",
      "\n",
      "fdist.plot()\n",
      "graphical plot of the frequency distribution\n",
      "\n",
      "fdist.plot(cumulative=True)\n",
      "cumulative plot of the frequency distribution\n",
      "\n",
      "fdist1 |= fdist2\n",
      "update fdist1 with counts from fdist2\n",
      "\n",
      "fdist1 < fdist2\n",
      "test if samples in fdist1 occur less frequently than in fdist2\n",
      "\n",
      "\n",
      "Table 3.1: Functions Defined for NLTK's Frequency Distributions\n",
      "\n",
      "\n",
      "Our discussion of frequency distributions has introduced some important Python concepts,\n",
      "and we will look at them systematically in 4.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4   Back to Python: Making Decisions and Taking Control\n",
      "\n",
      "So far, our little programs have had some interesting qualities:\n",
      "the ability to work with language, and\n",
      "the potential to save human effort through automation.\n",
      "A key feature of programming is the ability of machines to\n",
      "make decisions on our behalf, executing instructions when\n",
      "certain conditions are met, or repeatedly looping through\n",
      "text data until some condition is satisfied.  This feature\n",
      "is known as control, and is the focus of this section.\n",
      "\n",
      "4.1   Conditionals\n",
      "Python supports a wide range of operators, such as < and >=, for\n",
      "testing the relationship between values. The full set of these relational\n",
      "operators is shown in 4.1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operator\n",
      "Relationship\n",
      "\n",
      "\n",
      "\n",
      "<\n",
      "less than\n",
      "\n",
      "<=\n",
      "less than or equal to\n",
      "\n",
      "==\n",
      "equal to (note this is two \"=\" signs, not one)\n",
      "\n",
      "!=\n",
      "not equal to\n",
      "\n",
      ">\n",
      "greater than\n",
      "\n",
      ">=\n",
      "greater than or equal to\n",
      "\n",
      "\n",
      "Table 4.1: Numerical Comparison Operators\n",
      "\n",
      "\n",
      "We can use these to select different words from a sentence of news text.\n",
      "Here are some examples — only the operator is changed from one\n",
      "line to the next.  They all use sent7, the first sentence from text7\n",
      "(Wall Street Journal).  As before, if you get an error saying that sent7\n",
      "is undefined, you need to first type: from nltk.book import *\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent7\n",
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the',\n",
      "'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']\n",
      ">>> [w for w in sent7 if len(w) < 4]\n",
      "[',', '61', 'old', ',', 'the', 'as', 'a', '29', '.']\n",
      ">>> [w for w in sent7 if len(w) <= 4]\n",
      "[',', '61', 'old', ',', 'will', 'join', 'the', 'as', 'a', 'Nov.', '29', '.']\n",
      ">>> [w for w in sent7 if len(w) == 4]\n",
      "['will', 'join', 'Nov.']\n",
      ">>> [w for w in sent7 if len(w) != 4]\n",
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'the', 'board',\n",
      "'as', 'a', 'nonexecutive', 'director', '29', '.']\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "There is a common pattern to all of these examples:\n",
      "[w for w in text if condition ], where condition is a\n",
      "Python \"test\" that yields either true or false.\n",
      "In the cases shown in the previous code example, the condition is always a numerical comparison.\n",
      "However, we can also test various properties of words,\n",
      "using the functions listed in 4.2.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Function\n",
      "Meaning\n",
      "\n",
      "\n",
      "\n",
      "s.startswith(t)\n",
      "test if s starts with t\n",
      "\n",
      "s.endswith(t)\n",
      "test if s ends with t\n",
      "\n",
      "t in s\n",
      "test if t is a substring of s\n",
      "\n",
      "s.islower()\n",
      "test if s contains cased characters and all are lowercase\n",
      "\n",
      "s.isupper()\n",
      "test if s contains cased characters and all are uppercase\n",
      "\n",
      "s.isalpha()\n",
      "test if s is non-empty and all characters in s are alphabetic\n",
      "\n",
      "s.isalnum()\n",
      "test if s is non-empty and all characters in s are alphanumeric\n",
      "\n",
      "s.isdigit()\n",
      "test if s is non-empty and all characters in s are digits\n",
      "\n",
      "s.istitle()\n",
      "test if s contains cased characters and is titlecased\n",
      "(i.e. all words in s have initial capitals)\n",
      "\n",
      "\n",
      "Table 4.2: Some Word Comparison Operators\n",
      "\n",
      "\n",
      "Here are some examples of these operators being used to\n",
      "select words from our texts:\n",
      "words ending with -ableness;\n",
      "words containing gnt;\n",
      "words having an initial capital;\n",
      "and words consisting entirely of digits.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sorted(w for w in set(text1) if w.endswith('ableness'))\n",
      "['comfortableness', 'honourableness', 'immutableness', 'indispensableness', ...]\n",
      ">>> sorted(term for term in set(text4) if 'gnt' in term)\n",
      "['Sovereignty', 'sovereignties', 'sovereignty']\n",
      ">>> sorted(item for item in set(text6) if item.istitle())\n",
      "['A', 'Aaaaaaaaah', 'Aaaaaaaah', 'Aaaaaah', 'Aaaah', 'Aaaaugh', 'Aaagh', ...]\n",
      ">>> sorted(item for item in set(sent7) if item.isdigit())\n",
      "['29', '61']\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "We can also create more complex conditions.  If c is a\n",
      "condition, then not c is also a condition.\n",
      "If we have two conditions c1 and c2,\n",
      "then we can combine them to form a new condition using conjunction and disjunction:\n",
      "c1 and c2,\n",
      "c1 or c2.\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Run the following examples and try to explain what is going on in each one.\n",
      "Next, try to make up some conditions of your own.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sorted(w for w in set(text7) if '-' in w and 'index' in w)\n",
      ">>> sorted(wd for wd in set(text3) if wd.istitle() and len(wd) > 10)\n",
      ">>> sorted(w for w in set(sent7) if not w.islower())\n",
      ">>> sorted(t for t in set(text2) if 'cie' in t or 'cei' in t)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4.2   Operating on Every Element\n",
      "In 3, we saw some examples of\n",
      "counting items other than words.  Let's take a closer look at the notation we used:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> [len(w) for w in text1]\n",
      "[1, 4, 4, 2, 6, 8, 4, 1, 9, 1, 1, 8, 2, 1, 4, 11, 5, 2, 1, 7, 6, 1, 3, 4, 5, 2, ...]\n",
      ">>> [w.upper() for w in text1]\n",
      "['[', 'MOBY', 'DICK', 'BY', 'HERMAN', 'MELVILLE', '1851', ']', 'ETYMOLOGY', '.', ...]\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "These expressions have the form [f(w) for ...] or [w.f() for ...], where\n",
      "f is a function that operates on a word to compute its length, or to\n",
      "convert it to uppercase.\n",
      "For now, you don't need to understand the difference between the notations f(w) and\n",
      "w.f().  Instead, simply learn this Python idiom which performs the\n",
      "same operation on every element of a list.  In the preceding examples, it goes through\n",
      "each word in text1, assigning each one in turn to the variable w and\n",
      "performing the specified operation on the variable.\n",
      "\n",
      "Note\n",
      "The notation just described is called a \"list comprehension.\"  This is our first example\n",
      "of a Python idiom, a fixed notation that we use habitually without bothering to\n",
      "analyze each time.  Mastering such idioms is an important part of becoming a\n",
      "fluent Python programmer.\n",
      "\n",
      "Let's return to the question of vocabulary size, and apply the same idiom here:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> len(text1)\n",
      "260819\n",
      ">>> len(set(text1))\n",
      "19317\n",
      ">>> len(set(word.lower() for word in text1))\n",
      "17231\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "Now that we are not double-counting words like This and this, which differ only\n",
      "in capitalization, we've wiped 2,000 off the vocabulary count!  We can go a step further\n",
      "and eliminate numbers and punctuation from the vocabulary count by filtering out any\n",
      "non-alphabetic items:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> len(set(word.lower() for word in text1 if word.isalpha()))\n",
      "16948\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "This example is slightly complicated: it lowercases all the purely alphabetic items.\n",
      "Perhaps it would have been simpler just to count the lowercase-only items, but this\n",
      "gives the wrong answer (why?).\n",
      "Don't worry if you don't feel confident with list comprehensions yet,\n",
      "since you'll see many more examples along with explanations in the following chapters.\n",
      "\n",
      "\n",
      "4.3   Nested Code Blocks\n",
      "Most programming languages permit us to execute a block of code when a\n",
      "conditional expression, or if statement, is satisfied.  We\n",
      "already saw examples of conditional tests in code like [w for w in\n",
      "sent7 if len(w) < 4]. In the following program, we have created a\n",
      "variable called word containing the string value 'cat'. The\n",
      "if statement checks whether the test len(word) < 5 is true.\n",
      "It is, so the body of the if statement is invoked and the\n",
      "print statement is executed, displaying a message to the user.\n",
      "Remember to indent the print statement by typing four spaces.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> word = 'cat'\n",
      ">>> if len(word) < 5:\n",
      "...     print('word length is less than 5')\n",
      "...   \n",
      "word length is less than 5\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "When we use the Python interpreter we have to add an extra blank line \n",
      "in order for it to detect that the nested block is complete.\n",
      "\n",
      "Note\n",
      "If you are using Python 2.6 or 2.7, you need to include the following\n",
      "line in order for the above print function to be recognized:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from __future__ import print_function\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If we change the conditional test to len(word) >= 5,\n",
      "to check that the length of word is greater than or equal to 5,\n",
      "then the test will no longer be true.\n",
      "This time, the body of the if statement will not be executed,\n",
      "and no message is shown to the user:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> if len(word) >= 5:\n",
      "...   print('word length is greater than or equal to 5')\n",
      "...\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "An if statement is known as a control structure\n",
      "because it controls whether the code in the indented block will be run.\n",
      "Another control structure is the for loop.\n",
      "Try the following, and remember to include the colon and the four spaces:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> for word in ['Call', 'me', 'Ishmael', '.']:\n",
      "...     print(word)\n",
      "...\n",
      "Call\n",
      "me\n",
      "Ishmael\n",
      ".\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "This is called a loop because Python executes the code in\n",
      "circular fashion.  It starts by performing the\n",
      "assignment word = 'Call',\n",
      "effectively using the word variable to name the first\n",
      "item of the list.  Then, it displays the value of word\n",
      "to the user.  Next, it goes back to the for statement,\n",
      "and performs the assignment word = 'me', before displaying this new value\n",
      "to the user, and so on.  It continues in this fashion until\n",
      "every item of the list has been processed.\n",
      "\n",
      "\n",
      "4.4   Looping with Conditions\n",
      "Now we can combine the if and for statements.\n",
      "We will loop over every item of the list, and print\n",
      "the item only if it ends with the letter l.  We'll pick another\n",
      "name for the variable to demonstrate that Python doesn't\n",
      "try to make sense of variable names.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent1 = ['Call', 'me', 'Ishmael', '.']\n",
      ">>> for xyzzy in sent1:\n",
      "...     if xyzzy.endswith('l'):\n",
      "...         print(xyzzy)\n",
      "...\n",
      "Call\n",
      "Ishmael\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "You will notice that if and for statements\n",
      "have a colon at the end of the line,\n",
      "before the indentation begins. In fact, all Python\n",
      "control structures end with a colon.  The colon\n",
      "indicates that the current statement relates to the\n",
      "indented block that follows.\n",
      "We can also specify an action to be taken if\n",
      "the condition of the if statement is not met.\n",
      "Here we see the elif (else if) statement, and\n",
      "the else statement.  Notice that these also have\n",
      "colons before the indented code.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> for token in sent1:\n",
      "...     if token.islower():\n",
      "...         print(token, 'is a lowercase word')\n",
      "...     elif token.istitle():\n",
      "...         print(token, 'is a titlecase word')\n",
      "...     else:\n",
      "...         print(token, 'is punctuation')\n",
      "...\n",
      "Call is a titlecase word\n",
      "me is a lowercase word\n",
      "Ishmael is a titlecase word\n",
      ". is punctuation\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "As you can see, even with this small amount of Python knowledge,\n",
      "you can start to build multiline Python programs.\n",
      "It's important to develop such programs in pieces,\n",
      "testing that each piece does what you expect before\n",
      "combining them into a program.  This is why the Python\n",
      "interactive interpreter is so invaluable, and why you should get\n",
      "comfortable using it.\n",
      "Finally, let's combine the idioms we've been exploring.\n",
      "First, we create a list of cie and cei words,\n",
      "then we loop over each item and print it.  Notice the\n",
      "extra information given in the print statement: end=' '.\n",
      "This tells Python to print a space (not the default newline) after each word.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tricky = sorted(w for w in set(text2) if 'cie' in w or 'cei' in w)\n",
      ">>> for word in tricky:\n",
      "...     print(word, end=' ')\n",
      "ancient ceiling conceit conceited conceive conscience\n",
      "conscientious conscientiously deceitful deceive ...\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5   Automatic Natural Language Understanding\n",
      "\n",
      "We have been exploring language bottom-up, with the help of texts and\n",
      "the Python programming\n",
      "language.  However, we're also interested in exploiting our knowledge of language and computation\n",
      "by building useful language technologies. We'll take the opportunity\n",
      "now to step back from the nitty-gritty of code in order to paint a\n",
      "bigger picture of natural language processing.\n",
      "At a purely practical level, we all need help to navigate the universe of information\n",
      "locked up in text on the Web.  Search engines have been crucial to the\n",
      "growth and popularity of the Web, but have some shortcomings.\n",
      "It takes skill, knowledge, and some luck,\n",
      "to extract answers to such questions as: What tourist sites can I\n",
      "visit between Philadelphia and Pittsburgh on a limited budget?\n",
      "What do experts say about digital SLR cameras? What\n",
      "predictions about the steel market were made by credible commentators\n",
      "in the past week? Getting a computer to answer them automatically\n",
      "involves a range of language processing tasks, including information extraction,\n",
      "inference, and summarization, and would need to be carried out on a scale\n",
      "and with a level of robustness that is still beyond our current capabilities.\n",
      "On a more philosophical level, a long-standing challenge within artificial intelligence\n",
      "has been to build intelligent machines, and a major part of intelligent behaviour is understanding\n",
      "language.  For many years this goal has been seen as too difficult.\n",
      "However, as NLP technologies become more mature, and robust methods for\n",
      "analyzing unrestricted text become more widespread, the prospect of\n",
      "natural language understanding has re-emerged as a plausible goal.\n",
      "In this section we describe some language understanding technologies,\n",
      "to give you a sense of the interesting challenges that are waiting for you.\n",
      "\n",
      "5.1   Word Sense Disambiguation\n",
      "In word sense disambiguation we want to work out\n",
      "which sense of a word was intended in a given context.  Consider the\n",
      "ambiguous words serve and dish:\n",
      "\n",
      "  (2)\n",
      "  a.serve: help with food or drink; hold an office; put ball into play\n",
      "\n",
      "  b.dish: plate; course of a meal; communications device\n",
      "\n",
      "In a sentence containing the phrase: he served the dish, you\n",
      "can detect that both serve and dish are being used with\n",
      "their food meanings.  It's unlikely that the topic of discussion\n",
      "shifted from sports to crockery in the space of three words.\n",
      "This would force you to invent bizarre images, like a tennis pro\n",
      "taking out his or her frustrations on a china tea-set laid out beside the court.\n",
      "In other words, we automatically disambiguate words using context, exploiting\n",
      "the simple fact that nearby words have closely related meanings.\n",
      "As another example of this contextual effect, consider the word\n",
      "by, which has several meanings, e.g.: the book by\n",
      "Chesterton (agentive — Chesterton was the author of the book);\n",
      "the cup by the stove (locative — the stove is where the\n",
      "cup is); and submit by Friday (temporal — Friday is the\n",
      "time of the submitting).\n",
      "Observe in (3c) that the meaning of the italicized word helps us\n",
      "interpret the meaning of by.\n",
      "\n",
      "  (3)\n",
      "  a.The lost children were found by the searchers  (agentive)\n",
      "\n",
      "  b.The lost children were found by the mountain   (locative)\n",
      "\n",
      "  c.The lost children were found by the afternoon  (temporal)\n",
      "\n",
      "\n",
      "\n",
      "5.2   Pronoun Resolution\n",
      "A deeper kind of language understanding is to work out \"who did what to whom\" —\n",
      "i.e., to detect the subjects and objects of verbs.  You learnt to do this in\n",
      "elementary school, but it's harder than you might think.\n",
      "In the sentence the thieves stole the paintings\n",
      "it is easy to tell who performed the stealing action.\n",
      "Consider three possible following sentences in (4c), and try to determine\n",
      "what was sold, caught, and found (one case is ambiguous).\n",
      "\n",
      "  (4)\n",
      "  a.The thieves stole the paintings.  They were subsequently sold.\n",
      "\n",
      "  b.The thieves stole the paintings.  They were subsequently caught.\n",
      "\n",
      "  c.The thieves stole the paintings.  They were subsequently found.\n",
      "\n",
      "Answering this question involves finding the antecedent of the pronoun they,\n",
      "either thieves or paintings.  Computational techniques for tackling this problem\n",
      "include anaphora resolution — identifying what a pronoun or noun phrase\n",
      "refers to — and semantic role labeling — identifying how a noun phrase\n",
      "relates to the verb (as agent, patient, instrument, and so on).\n",
      "\n",
      "\n",
      "5.3   Generating Language Output\n",
      "If we can automatically solve such problems of language understanding, we will\n",
      "be able to move on to tasks that involve generating language output, such as\n",
      "question answering and machine translation.  In the first case,\n",
      "a machine should be able to answer a user's questions relating to collection of texts:\n",
      "\n",
      "  (5)\n",
      "  a.Text: ... The thieves stole the paintings.  They were subsequently sold. ...\n",
      "\n",
      "  b.Human: Who or what was sold?\n",
      "\n",
      "  c.Machine: The paintings.\n",
      "\n",
      "The machine's answer demonstrates that it has correctly worked out that they\n",
      "refers to paintings and not to thieves.  In the second case, the machine should\n",
      "be able to translate the text into another language, accurately\n",
      "conveying the meaning of the original text.  In translating the example text into French,\n",
      "we are forced to choose the gender of the pronoun in the second sentence:\n",
      "ils (masculine) if the thieves are found, and elles (feminine) if\n",
      "the paintings are found.  Correct translation actually depends on correct understanding of\n",
      "the pronoun.\n",
      "\n",
      "  (6)\n",
      "  a.The thieves stole the paintings.  They were subsequently found.\n",
      "\n",
      "  b.Les voleurs ont volé les peintures. Ils ont été trouvés plus tard.  (the thieves)\n",
      "\n",
      "  c.Les voleurs ont volé les peintures. Elles ont été trouvées plus tard.  (the paintings)\n",
      "\n",
      "In all of these examples, working out the sense of a word, the subject of a verb, and the\n",
      "antecedent of a pronoun are steps in establishing the meaning of a sentence, things\n",
      "we would expect a language understanding system to be able to do.\n",
      "\n",
      "\n",
      "5.4   Machine Translation\n",
      "For a long time now, machine translation (MT) has\n",
      "been the holy grail of language understanding,\n",
      "ultimately seeking to provide high-quality,\n",
      "idiomatic translation between any pair of languages.\n",
      "Its roots go back to the early days of the Cold War, when the promise\n",
      "of automatic translation led to substantial government sponsorship,\n",
      "and with it, the genesis of NLP itself.\n",
      "Today, practical translation systems exist for particular pairs\n",
      "of languages, and some are integrated into web search engines.\n",
      "However, these systems have some serious shortcomings, which\n",
      "are starkly revealed by translating a sentence back and forth\n",
      "between a pair of languages until equilibrium is reached, e.g.:\n",
      "\n",
      "0> how long before the next flight to Alice Springs?\n",
      "1> wie lang vor dem folgenden Flug zu Alice Springs?\n",
      "2> how long before the following flight to Alice jump?\n",
      "3> wie lang vor dem folgenden Flug zu Alice springen Sie?\n",
      "4> how long before the following flight to Alice do you jump?\n",
      "5> wie lang, bevor der folgende Flug zu Alice tun, Sie springen?\n",
      "6> how long, before the following flight to Alice does, do you jump?\n",
      "7> wie lang bevor der folgende Flug zu Alice tut, tun Sie springen?\n",
      "8> how long before the following flight to Alice does, do you jump?\n",
      "9> wie lang, bevor der folgende Flug zu Alice tut, tun Sie springen?\n",
      "10> how long, before the following flight does to Alice, do do you jump?\n",
      "11> wie lang bevor der folgende Flug zu Alice tut, Sie tun Sprung?\n",
      "12> how long before the following flight does leap to Alice, does you?\n",
      "\n",
      "Observe that the system correctly translates Alice Springs from English\n",
      "to German (in the line starting 1>), but on the way back to English, this ends up as Alice jump\n",
      "(line 2).  The preposition before is initially translated into the corresponding\n",
      "German preposition vor, but later into the conjunction bevor (line 5).\n",
      "After line 5 the sentences become nonsensical (but notice the various phrasings\n",
      "indicated by the commas, and the change from jump to leap).\n",
      "The translation system did not recognize when a word was part of a proper name,\n",
      "and it misinterpreted the grammatical structure.\n",
      "\n",
      "Note\n",
      "Your Turn: Try this yourself using http://translationparty.com/\n",
      "\n",
      "Machine translation is difficult because a given word could have several possible\n",
      "translations (depending on its meaning), and because word order must be changed\n",
      "in keeping with the grammatical structure of the target language.\n",
      "Today these difficulties are being faced by collecting massive quantities of\n",
      "parallel texts from news and government websites that publish documents\n",
      "in two or more languages.  Given a document in German and English, and possibly\n",
      "a bilingual dictionary, we can automatically pair up the sentences,\n",
      "a process called text alignment.  Once we have a million or more sentence\n",
      "pairs, we can detect corresponding words and phrases, and build a model\n",
      "that can be used for translating new text.\n",
      "\n",
      "\n",
      "5.5   Spoken Dialog Systems\n",
      "In the history of artificial intelligence, the chief measure of intelligence\n",
      "has been a linguistic one, namely the Turing Test: can a dialogue system,\n",
      "responding to a user's text input, perform so naturally that we cannot distinguish\n",
      "it from a human-generated response?  In contrast, today's commercial dialogue systems\n",
      "are very limited, but still perform useful functions in narrowly-defined domains,\n",
      "as we see here:\n",
      "\n",
      "S: How may I help you?\n",
      "U: When is Saving Private Ryan playing?\n",
      "S: For what theater?\n",
      "U: The Paramount theater.\n",
      "S: Saving Private Ryan is not playing at the Paramount theater, but\n",
      "it's playing at the Madison theater at 3:00, 5:30, 8:00, and 10:30.\n",
      "\n",
      "You could not ask this system to provide driving instructions or\n",
      "details of nearby restaurants unless the required information\n",
      "had already been stored and suitable question-answer pairs\n",
      "had been incorporated into the language processing system.\n",
      "Observe that this system seems to understand the user's goals:\n",
      "the user asks when a movie is showing and the system\n",
      "correctly determines from this that the user wants to see\n",
      "the movie. This inference seems so obvious that you probably\n",
      "didn't notice it was made, yet a natural language system\n",
      "needs to be endowed with this capability in order to interact\n",
      "naturally.  Without it, when asked Do you know when Saving Private\n",
      "Ryan is playing?, a system might unhelpfully respond with a cold Yes.\n",
      "However, the developers of commercial dialogue systems use\n",
      "contextual assumptions and business logic to ensure that the different ways in which a user might\n",
      "express requests or provide information are handled in a way that\n",
      "makes sense for the particular application.  So, if you type\n",
      "When is ..., or I want to know when ..., or Can you tell me\n",
      "when ..., simple rules will always yield screening times.  This is\n",
      "enough for the system to provide a useful service.\n",
      "\n",
      "\n",
      "Figure 5.1: Simple Pipeline Architecture for a Spoken Dialogue System:\n",
      "Spoken input (top left) is analyzed, words are recognized, sentences are parsed and\n",
      "interpreted in context, application-specific actions take place (top right);\n",
      "a response is planned, realized as a syntactic structure, then to suitably\n",
      "inflected words, and finally to spoken output; different types of\n",
      "linguistic knowledge inform each stage of the process.\n",
      "\n",
      "Dialogue systems give us an opportunity to mention the\n",
      "commonly assumed pipeline for NLP.\n",
      "5.1 shows the architecture of a simple dialogue system.\n",
      "Along the top of the diagram, moving from left to right, is a\n",
      "\"pipeline\" of some language understanding components.\n",
      "These map from speech input via syntactic parsing\n",
      "to some kind of meaning representation.  Along the middle, moving from\n",
      "right to left, is the reverse pipeline of components for converting\n",
      "concepts to speech.  These components make up the dynamic aspects of the system.\n",
      "At the bottom of the diagram are some representative bodies of\n",
      "static information: the repositories of language-related data that\n",
      "the processing components draw on to do their work.\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "For an example of a primitive dialogue system, try having\n",
      "a conversation with an NLTK chatbot.  To see the available chatbots,\n",
      "run nltk.chat.chatbots().\n",
      "(Remember to import nltk first.)\n",
      "\n",
      "\n",
      "\n",
      "5.6   Textual Entailment\n",
      "The challenge of language understanding has been brought into focus in recent years by a public\n",
      "\"shared task\" called Recognizing Textual Entailment (RTE). The basic\n",
      "scenario is simple.  Suppose you want to find evidence to support\n",
      "the hypothesis: Sandra Goudie was defeated by Max Purnell, and\n",
      "that you have another short text that seems to be relevant, for example,\n",
      "Sandra Goudie was first elected to Parliament in the 2002 elections,\n",
      "narrowly winning the seat of Coromandel by defeating Labour candidate\n",
      "Max Purnell and pushing incumbent Green MP Jeanette Fitzsimons into\n",
      "third place.  Does the text provide enough evidence for you to\n",
      "accept the hypothesis?  In this particular case, the answer will be \"No.\"\n",
      "You can draw this conclusion easily, but it is very hard to come up with\n",
      "automated methods for making the right decision. The RTE\n",
      "Challenges provide data that allow competitors to develop their\n",
      "systems, but not enough data for \"brute force\" machine learning techniques (a topic\n",
      "we will cover in chap-data-intensive).  Consequently, some\n",
      "linguistic analysis is crucial. In the previous example, it is important\n",
      "for the system to note that Sandra Goudie names the person being\n",
      "defeated in the hypothesis, not the person doing the defeating in the\n",
      "text. As another illustration of the difficulty of the task, consider\n",
      "the following text-hypothesis pair:\n",
      "\n",
      "  (7)\n",
      "  a.Text: David Golinkin is the editor or author of eighteen books, and over 150 responsa, articles, sermons and books\n",
      "\n",
      "  b.Hypothesis: Golinkin has written eighteen books\n",
      "\n",
      "In order to determine whether the hypothesis is supported by the\n",
      "text, the system needs the following background knowledge:\n",
      "(i) if someone is an author of a book, then he/she has written that\n",
      "book; (ii) if someone is an editor of a book, then he/she has not\n",
      "written (all of) that book; (iii) if someone is editor or author of eighteen\n",
      "books, then one cannot conclude that he/she is author of eighteen books.\n",
      "\n",
      "\n",
      "5.7   Limitations of NLP\n",
      "Despite the research-led advances in tasks like RTE, natural language\n",
      "systems that have been deployed for real-world applications still cannot perform\n",
      "common-sense reasoning or draw on world knowledge in a general and\n",
      "robust manner.  We can wait for these difficult artificial\n",
      "intelligence problems to be solved, but in the meantime it is\n",
      "necessary to live with some severe limitations on the reasoning and\n",
      "knowledge capabilities of natural language systems. Accordingly, right\n",
      "from the beginning, an important goal of NLP research has been to\n",
      "make progress on the difficult task of building technologies that\n",
      "\"understand language,\" using superficial yet powerful techniques instead of\n",
      "unrestricted knowledge and reasoning capabilities.\n",
      "Indeed, this is one of the goals of this book, and we hope to equip you with\n",
      "the knowledge and skills to build useful NLP systems, and to\n",
      "contribute to the long-term aspiration of building intelligent machines.\n",
      "\n",
      "\n",
      "\n",
      "6   Summary\n",
      "\n",
      "Texts are represented in Python using lists:\n",
      "['Monty', 'Python'].  We can use indexing, slicing,\n",
      "and the len() function on lists.\n",
      "A word \"token\" is a particular appearance of a given word in a text;\n",
      "a word \"type\" is the unique form of the word as a particular sequence\n",
      "of letters.  We count word tokens using len(text) and word types using\n",
      "len(set(text)).\n",
      "We obtain the vocabulary of a text t using sorted(set(t)).\n",
      "We operate on each item of a text using [f(x) for x in text].\n",
      "To derive the vocabulary, collapsing case distinctions and ignoring punctuation,\n",
      "we can write set(w.lower() for w in text if w.isalpha()).\n",
      "We process each word in a text using a for statement, such\n",
      "as for w in t: or for word in text:.  This must be followed by the colon character\n",
      "and an indented block of code, to be executed each time through the loop.\n",
      "We test a condition using an if statement: if len(word) < 5:.\n",
      "This must be followed by the colon character and an indented block of\n",
      "code, to be executed only if the condition is true.\n",
      "A frequency distribution is a collection of items along with their frequency counts\n",
      "(e.g., the words of a text and their frequency of appearance).\n",
      "A function is a block of code that has been assigned a name and can\n",
      "be reused. Functions are defined using the def keyword, as in\n",
      "def mult(x, y); x and y are parameters of the function,\n",
      "and act as placeholders for actual data values.\n",
      "A function is called by specifying its name followed by zero or more\n",
      "arguments inside parentheses, like this: texts(), mult(3, 4), len(text1).\n",
      "\n",
      "\n",
      "\n",
      "7   Further Reading\n",
      "This chapter has introduced new concepts in programming, natural language processing,\n",
      "and linguistics, all mixed in together.\n",
      "Many of them are consolidated in the following chapters.  However, you may also want to\n",
      "consult the online materials provided with this chapter (at http://nltk.org/), including links\n",
      "to additional background materials, and links to online NLP systems.\n",
      "You may also like to read up on\n",
      "some linguistics and NLP-related concepts in Wikipedia (e.g., collocations,\n",
      "the Turing Test, the type-token distinction).\n",
      "You should acquaint yourself with the Python documentation available at http://docs.python.org/,\n",
      "including the many tutorials and comprehensive reference materials linked there.\n",
      "A Beginner's Guide to Python is available at http://wiki.python.org/moin/BeginnersGuide.\n",
      "Miscellaneous questions about Python might be answered in the FAQ at\n",
      "http://python.org/doc/faq/general/.\n",
      "As you delve into NLTK, you might want to subscribe to the mailing list where new\n",
      "releases of the toolkit are announced.  There is also an NLTK-Users mailing list,\n",
      "where users help each other as they learn how to use Python and NLTK for\n",
      "language analysis work.  Details of these lists are available at http://nltk.org/.\n",
      "For more information on the topics covered in 5,\n",
      "and on NLP more generally, you might like to consult one of the following excellent\n",
      "books:\n",
      "\n",
      "Indurkhya, Nitin and Fred Damerau (eds, 2010) Handbook of Natural Language Processing\n",
      "(Second Edition) Chapman & Hall/CRC. 2010.  (Indurkhya & Damerau, 2010) (Dale, Moisl, & Somers, 2000)\n",
      "Jurafsky, Daniel and James Martin (2008) Speech and Language Processing (Second Edition).  Prentice Hall.\n",
      "(Jurafsky & Martin, 2008)\n",
      "Mitkov, Ruslan (ed, 2003) The Oxford Handbook of Computational Linguistics.  Oxford University Press.\n",
      "(second edition expected in 2010).  (Mitkov, 2002)\n",
      "\n",
      "The Association for Computational Linguistics is the international organization that\n",
      "represents the field of NLP.  The ACL website (http://www.aclweb.org/) hosts many useful resources, including:\n",
      "information about international and regional conferences and workshops;\n",
      "the ACL Wiki with links to hundreds of useful resources;\n",
      "and the ACL Anthology, which contains most of the NLP research literature\n",
      "from the past 50+ years, fully indexed and freely downloadable.\n",
      "Some excellent introductory Linguistics textbooks are:\n",
      "[Finegan2007]_, (O'Grady et al, 2004), (OSU, 2007).  You might like to consult\n",
      "LanguageLog, a popular linguistics blog with occasional posts that\n",
      "use the techniques described in this book.\n",
      "\n",
      "\n",
      "8   Exercises\n",
      "\n",
      "\n",
      "☼ Try using the Python interpreter as a calculator, and\n",
      "typing expressions like 12 / (4 + 1).\n",
      "\n",
      "☼ Given an alphabet of 26 letters, there are 26 to the power\n",
      "10, or 26 ** 10, ten-letter strings we can form.  That works out\n",
      "to 141167095653376.  How many hundred-letter strings are possible?\n",
      "\n",
      "☼ The Python multiplication operation can be applied to lists.\n",
      "What happens when you type ['Monty', 'Python'] * 20,\n",
      "or 3 * sent1?\n",
      "\n",
      "☼ Review 1 on\n",
      "computing with language.  How many words are there in text2?\n",
      "How many distinct words are there?\n",
      "\n",
      "☼ Compare the lexical diversity scores for humor\n",
      "and romance fiction in 1.1.  Which genre is\n",
      "more lexically diverse?\n",
      "\n",
      "☼ Produce a dispersion plot of the four main protagonists in\n",
      "Sense and Sensibility: Elinor, Marianne, Edward, and Willoughby.\n",
      "What can you observe about the different roles played by the males\n",
      "and females in this novel?  Can you identify the couples?\n",
      "\n",
      "☼ Find the collocations in text5.\n",
      "\n",
      "☼ Consider the following Python expression: len(set(text4)).\n",
      "State the purpose of this expression.  Describe the two steps\n",
      "involved in performing this computation.\n",
      "\n",
      "☼ Review 2\n",
      "on lists and strings.\n",
      "\n",
      "Define a string and assign it to a variable, e.g.,\n",
      "my_string = 'My String' (but put something more interesting in the string).\n",
      "Print the contents of this variable in two ways, first\n",
      "by simply typing the variable name and pressing enter, then\n",
      "by using the print statement.\n",
      "Try adding the string to itself using my_string + my_string, or multiplying\n",
      "it by a number, e.g., my_string * 3.  Notice that the strings\n",
      "are joined together without any spaces.  How could you fix this?\n",
      "\n",
      "\n",
      "☼ Define a variable my_sent to be a list of words, using\n",
      "the syntax my_sent = [\"My\", \"sent\"] (but with your own words,\n",
      "or a favorite saying).\n",
      "\n",
      "Use ' '.join(my_sent) to convert this into a string.\n",
      "Use split() to split the string back into the list form\n",
      "you had to start with.\n",
      "\n",
      "\n",
      "☼ Define several variables containing lists of words, e.g., phrase1,\n",
      "phrase2, and so on.  Join them together in various combinations (using the plus operator)\n",
      "to form whole sentences.  What is the relationship between\n",
      "len(phrase1 + phrase2) and len(phrase1) + len(phrase2)?\n",
      "\n",
      "☼ Consider the following two expressions, which have the same\n",
      "value.  Which one will typically be more relevant in NLP?  Why?\n",
      "\n",
      "\"Monty Python\"[6:12]\n",
      "[\"Monty\", \"Python\"][1]\n",
      "\n",
      "\n",
      "☼ We have seen how to represent a sentence as a list of words, where\n",
      "each word is a sequence of characters.  What does sent1[2][2] do?\n",
      "Why?  Experiment with other index values.\n",
      "\n",
      "☼ The first sentence of text3 is provided to you in the\n",
      "variable sent3.  The index of the in sent3 is 1, because sent3[1]\n",
      "gives us 'the'.  What are the indexes of the two other occurrences\n",
      "of this word in sent3?\n",
      "\n",
      "☼ Review the discussion of conditionals in 4.\n",
      "Find all words in the Chat Corpus (text5)\n",
      "starting with the letter b.  Show them in alphabetical order.\n",
      "\n",
      "☼ Type the expression list(range(10)) at the interpreter prompt.\n",
      "Now try list(range(10, 20)), list(range(10, 20, 2)), and list(range(20, 10, -2)).\n",
      "We will see a variety of uses for this built-in function in later chapters.\n",
      "\n",
      "◑ Use text9.index() to find the index of the word sunset.\n",
      "You'll need to insert this word as an argument between the parentheses.\n",
      "By a process of trial and error, find the slice for the complete sentence that\n",
      "contains this word.\n",
      "\n",
      "◑ Using list addition, and the set and sorted operations, compute the\n",
      "vocabulary of the sentences sent1 ... sent8.\n",
      "\n",
      "◑ What is the difference between the following two lines?\n",
      "Which one will give a larger value?  Will this be the case for other texts?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sorted(set(w.lower() for w in text1))\n",
      ">>> sorted(w.lower() for w in set(text1))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "◑ What is the difference between the following two tests:\n",
      "w.isupper() and not w.islower()?\n",
      "\n",
      "◑ Write the slice expression that extracts the last two words of text2.\n",
      "\n",
      "◑ Find all the four-letter words in the Chat Corpus (text5).\n",
      "With the help of a frequency distribution (FreqDist), show these\n",
      "words in decreasing order of frequency.\n",
      "\n",
      "◑ Review the discussion of looping with conditions in 4.\n",
      "Use a combination of for and if statements to loop over the words of\n",
      "the movie script for Monty Python and the Holy Grail (text6)\n",
      "and print all the uppercase words, one per line.\n",
      "\n",
      "◑ Write expressions for finding all words in text6 that\n",
      "meet the conditions listed below.  The result should be in the form of\n",
      "a list of words: ['word1', 'word2', ...].\n",
      "\n",
      "Ending in ise\n",
      "Containing the letter z\n",
      "Containing the sequence of letters pt\n",
      "Having all lowercase letters except for an initial capital (i.e., titlecase)\n",
      "\n",
      "\n",
      "◑ Define sent to be the list of words\n",
      "['she', 'sells', 'sea', 'shells', 'by', 'the', 'sea', 'shore'].\n",
      "Now write code to perform the following tasks:\n",
      "\n",
      "Print all words beginning with sh\n",
      "Print all words longer than four characters\n",
      "\n",
      "\n",
      "◑ What does the following Python code do?  sum(len(w) for w in text1)\n",
      "Can you use it to work out the average word length of a text?\n",
      "\n",
      "◑ Define a function called vocab_size(text) that has a single\n",
      "parameter for the text, and which returns the vocabulary size of the text.\n",
      "\n",
      "◑ Define a function percent(word, text) that calculates\n",
      "how often a given word occurs in a text, and expresses the result\n",
      "as a percentage.\n",
      "\n",
      "◑ We have been using sets to store vocabularies.  Try the following\n",
      "Python expression: set(sent3) < set(text1).  Experiment with this using\n",
      "different arguments to set().  What does it do?\n",
      "Can you think of a practical application for this?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "About this document...\n",
      "UPDATED FOR NLTK 3.0.\n",
      "This is a chapter from Natural Language Processing with Python,\n",
      "by Steven Bird, Ewan Klein and Edward Loper,\n",
      "Copyright © 2019 the authors.\n",
      "It is distributed with the Natural Language Toolkit [http://nltk.org/],\n",
      "Version 3.0, under the terms of the\n",
      "Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\n",
      "[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\n",
      "This document was built on\n",
      "Wed  4 Sep 2019 11:40:48 ACST\n",
      "\n",
      "\n",
      "\n",
      "Docutils System Messages\n",
      "\n",
      "System Message: ERROR/3 (ch01.rst2, line 1889); backlink\n",
      "Unknown target name: \"finegan2007\".\n",
      "vectors[1] = [0, 41, 11, 58, 13, 52, 7, 13, 0, 1, 3, 71, 25, 6, 0, 0, 1, 6, 12, 1, 7, 0, 1, 0, 1, 0, 0, 1, 16, 25, 12, 6, 1, 16, 1, 5, 5, 0, 31, 0, 8, 10, 9, 228, 24, 2, 13, 2, 7, 2, 14, 3, 0, 1, 0, 6, 9, 8, 2, 4, 1, 1, 0, 1, 39, 0, 0, 3, 0, 133, 0, 0, 3, 9, 2, 8, 11, 1, 0, 14, 10, 2, 26, 0, 5, 0, 5, 0, 4, 0, 5, 0, 0, 1, 0, 63, 1, 0, 1, 25, 4, 2, 0, 3, 0, 0, 1, 0, 0, 1, 1, 3, 0, 1, 1, 4, 7, 0, 4, 0, 11, 0, 3, 8, 2, 1, 1, 0, 4, 116, 9, 3, 0, 0, 13, 2, 260, 31, 31, 1, 0, 0, 18, 1, 1, 2, 14, 14, 12, 4, 17, 2, 1, 0, 1, 1, 1, 0, 3, 23, 4, 10, 0, 11, 0, 0, 1, 0, 1, 0, 0, 1, 1, 7, 1, 3, 11, 11, 2, 2, 9, 3, 0, 0, 1, 1, 0, 1, 0, 4, 2, 16, 7, 46, 0, 1, 1, 13, 4, 9, 12, 2, 0, 7, 0, 183, 1, 18, 1, 0, 0, 22, 461, 0, 0, 7, 2, 3, 3, 0, 19, 5, 21, 71, 3, 2, 0, 1, 1, 3, 46, 16, 0, 3, 4, 1, 0, 0, 0, 0, 5, 21, 15, 0, 5, 4, 0, 0, 0, 5, 8, 16, 84, 0, 0, 0, 0, 1, 0, 1, 1, 11, 21, 1, 0, 1, 0, 0, 7, 1, 0, 4, 18, 0, 0, 0, 0, 1, 3, 3, 0, 0, 1, 0, 2, 0, 0, 2, 1, 1, 0, 6, 0, 1, 0, 26, 2, 18, 0, 0, 1, 1, 2, 4, 1, 3, 0, 6, 25, 0, 6, 2, 1, 0, 0, 0, 2, 25, 1, 1, 0, 0, 1, 4, 2, 0, 6, 1, 0, 1, 0, 7, 1, 2, 0, 1, 0, 46, 1, 5, 0, 0, 0, 4, 2, 1, 2, 0, 0, 0, 0, 5, 2, 31, 5, 15, 0, 6, 0, 2, 1, 0, 12, 0, 2, 3, 11, 2, 4, 1, 49, 11, 0, 46, 128, 0, 6, 0, 2, 4, 0, 4, 1, 0, 1, 0, 0, 0, 1, 41, 16, 12, 2, 9, 0, 26, 2, 0, 1, 10, 12, 13, 0, 0, 1, 1, 7, 0, 0, 0, 0, 3, 0, 0, 0, 0, 37, 9, 0, 2, 5, 2, 0, 2, 1, 1, 22, 1, 0, 0, 0, 8, 25, 0, 14, 0, 0, 15, 0, 0, 28, 0, 16, 3, 3, 0, 0, 0, 14, 0, 13, 1, 0, 1, 0, 0, 4, 0, 5, 2, 4, 1, 2, 14, 3, 3, 0, 5, 4, 1, 2, 0, 1, 1, 0, 1, 26, 7, 5, 0, 23, 2, 1, 184, 8, 0, 32, 6, 4, 3, 55, 1, 0, 1, 1, 0, 5, 0, 2, 0, 2, 2, 11, 3, 0, 27, 0, 1, 9, 0, 12, 18, 21, 7, 44, 0, 3, 1, 22, 7, 3, 5, 0, 14, 1, 9, 1, 0, 6, 5, 0, 0, 3, 0, 1, 19, 4, 1, 0, 0, 0, 8, 0, 0, 4, 0, 1, 0, 3, 1, 3, 0, 3, 1, 0, 1, 1, 0, 0, 5, 4, 3, 2, 0, 0, 3, 0, 1, 3, 0, 0, 2, 0, 6, 0, 1, 0, 0, 0, 8, 2, 0, 0, 3, 4, 3, 1, 0, 1, 1, 4, 4, 2, 0, 7, 2, 34, 1, 5, 0, 3, 2, 0, 6, 20, 6, 0, 7, 3, 1, 5, 0, 4, 0, 1, 22, 0, 0, 1, 0, 0, 28, 0, 0, 0, 0, 3, 49, 0, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 2, 0, 0, 2, 6, 2, 8, 0, 10, 16, 4, 2, 1, 0, 1, 7, 1, 1, 1, 0, 0, 0, 1, 0, 0, 4, 0, 0, 1, 0, 0, 0, 8, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2, 15, 0, 2, 0, 0, 1, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 2, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 15, 12, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 5, 6, 6, 0, 16, 16, 9, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 8, 1, 2, 0, 23, 4, 0, 0, 0, 0, 9, 4, 0, 0, 5, 3, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 4, 4, 8, 0, 2, 2, 6, 0, 0, 1, 0, 0, 1, 0, 4, 0, 1, 1, 9, 0, 0, 0, 0, 0, 0, 76, 0, 2, 1, 0, 4, 1, 13, 1, 0, 0, 0, 4, 0, 2, 2, 1, 2, 2, 0, 0, 6, 0, 0, 6, 2, 2, 5, 3, 0, 6, 2, 0, 0, 0, 0, 28, 0, 0, 0, 0, 3, 0, 0, 12, 0, 6, 1, 0, 3, 1, 0, 0, 9, 1, 0, 0, 0, 0, 1, 2, 0, 3, 0, 4, 12, 0, 1, 2, 0, 0, 0, 0, 0, 0, 4, 16, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 6, 6, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 7, 0, 0, 0, 1, 1, 1, 1, 2, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 14, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 9, 3, 3, 0, 0, 0, 0, 0, 0, 0, 4, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 5, 3, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 4, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 4, 2, 1, 2, 2, 46, 1, 1, 5, 1, 1, 2, 3, 3, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 2, 3, 1, 1, 2, 3, 8, 2, 1, 2, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 2, 1, 3, 1, 4, 1, 1, 1, 1, 2, 2, 4, 26, 1, 5, 1, 2, 1, 2, 1, 18, 2, 1, 6, 2, 13, 4, 1, 7, 5, 2, 1, 6, 1, 2, 1, 1, 1, 1, 1, 1, 1, 4, 1, 10, 1, 1, 3, 4, 2, 1, 10, 1, 1, 8, 1, 3, 1, 3, 11, 10, 3, 2, 1, 18, 4, 2, 11, 10, 1, 1, 4, 1, 10, 13, 5, 1, 1, 3, 1, 1, 1, 2, 1, 1, 7, 2, 2, 4, 5, 4, 2, 2, 2, 2, 14, 4, 7, 3, 15, 1, 1, 3, 4, 2, 1, 7, 1, 1, 2, 2, 1, 3, 1, 5, 1, 3, 1, 1, 1, 7, 18, 3, 3, 7, 2, 1, 4, 9, 1, 5, 34, 6, 1, 1, 7, 9, 4, 2, 10, 11, 7, 1, 2, 1, 30, 6, 3, 17, 2, 2, 23, 9, 9, 3, 4, 3, 12, 4, 2, 3, 2, 16, 6, 13, 4, 11, 5, 15, 6, 6, 3, 2, 2, 2, 2, 1, 2, 1, 17, 3, 1, 24, 2, 7, 7, 22, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 15, 1, 1, 5, 1, 1, 1, 1, 1, 1, 4, 3, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 1, 3, 1, 1, 6, 3, 1, 2, 2, 2, 10, 1, 2, 1, 4, 3, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 5, 1, 5, 2, 7, 8, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 3, 2, 1, 2, 6, 3, 4, 2, 2, 1, 1, 1, 1, 1, 11, 7, 2, 8, 3, 1, 1, 1, 1, 4, 13, 1, 4, 3, 2, 2, 10, 1, 1, 1, 1, 1, 1, 2, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 6, 6, 3, 10, 1, 2, 2, 4, 1, 2, 4, 1, 1, 1, 1, 1, 1, 2, 2, 1, 5, 1, 2, 6, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 23, 2, 3, 1, 2, 26, 8, 4, 2, 1, 2, 5, 21, 2, 5, 49, 1, 2, 2, 22, 1, 1, 8, 2, 1, 4, 2, 1, 1, 23, 11, 1, 384, 2, 1, 1, 13, 1, 14, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 3, 2, 1, 8, 1, 1, 1, 2, 2, 10, 5, 1, 3, 17, 2, 2, 1, 1, 2, 2, 2, 5, 10, 1, 2, 2, 1, 2, 1, 1, 11, 9, 2, 6, 1, 8, 1, 7, 4, 1, 2, 2, 2, 2, 2, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1, 4, 3, 3, 4, 1, 2, 1, 1, 2, 1, 2, 1, 3, 1, 1, 7, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 13, 2, 36, 40, 10, 12, 10, 1, 1, 1, 1, 2, 4, 2, 3, 2, 1, 1, 1, 1, 6, 4, 3, 3, 3, 1, 1, 2, 1, 2, 23, 1, 4, 1, 3, 1, 1, 3, 2, 1, 2, 2, 2, 2, 3, 1, 1, 1, 3, 1, 1, 1, 4, 1, 3, 36, 2, 1, 1, 1, 4, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 5, 3, 2, 1, 1, 3, 3, 3, 1, 3, 10, 5, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 8, 1, 1, 3, 3, 3, 1, 4, 1, 2, 1, 8, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 6, 1, 1, 1, 1, 1, 1, 1, 4, 1, 5, 1, 3, 7, 4, 4, 15, 3, 1, 3, 1, 3, 3, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 5, 2, 2, 1, 3, 3, 3, 1, 1, 1, 3, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 7, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 5, 1, 1, 5, 5, 1, 4, 4, 1, 10, 8, 2, 2, 2, 1, 1, 1, 13, 10, 3, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 5, 3, 4, 1, 1, 1, 1, 5, 3, 3, 6, 4, 1, 1, 1, 1, 1, 5, 2, 1, 1, 2, 1, 1, 3, 1, 3, 78, 3, 2, 1, 6, 2, 1, 1, 1, 1, 8, 1, 1, 2, 3, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 2, 19, 5, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 1, 2, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 1, 18, 1, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 9, 2, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 4, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 3, 2, 1, 1, 2, 2, 1, 2, 3, 3, 2, 2, 10, 1, 2, 2, 5, 2, 2, 3, 5, 2, 1, 2, 1, 2, 1, 1, 4, 3, 2, 3, 3, 1, 1, 2, 4, 1, 3, 3, 2, 2, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 3, 3, 2, 1, 1, 1, 4, 3, 3, 1, 1, 1, 1, 1, 6, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 3, 2, 2, 1, 4, 1, 2, 1, 1, 1, 1, 2, 2, 3, 2, 4, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 3, 1, 1, 3, 1, 2, 1, 1, 1, 2, 6, 2, 3, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 3, 1, 1, 1, 6, 2, 1, 3, 1, 10, 6, 11, 1, 1, 4, 2, 5, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 2, 1, 1, 2, 4, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 15, 3, 6, 6, 3, 2, 2, 6, 6, 4, 5, 5, 4, 4, 4, 3, 1, 2, 1, 3, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 3, 3, 3, 4, 4, 2, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 3, 1, 1, 2, 6, 3, 3, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 3, 4, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 4, 1, 4, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 4, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 2, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "docs[2] = 2. Accessing Text Corpora and Lexical Resources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2. Accessing Text Corpora and Lexical Resources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Practical work in Natural Language Processing typically uses\n",
      "large bodies of linguistic data, or corpora.\n",
      "The goal of this chapter is to answer the following questions:\n",
      "\n",
      "What are some useful text corpora and lexical resources, and how can we access them with Python?\n",
      "Which Python constructs are most helpful for this work?\n",
      "How do we avoid repeating ourselves when writing Python code?\n",
      "\n",
      "This chapter continues to present programming concepts by example, in the\n",
      "context of a linguistic processing task.  We will wait until later before\n",
      "exploring each Python construct systematically.  Don't worry if you see\n",
      "an example that contains something unfamiliar; simply try it out and see\n",
      "what it does, and — if you're game — modify it by substituting\n",
      "some part of the code with a different text or word.  This way you will\n",
      "associate a task with a programming idiom, and learn the hows and whys later.\n",
      "\n",
      "1   Accessing Text Corpora\n",
      "As just mentioned, a text corpus is a large body of text. Many\n",
      "corpora are designed to contain a careful balance of material\n",
      "in one or more genres.  We examined some small text collections in\n",
      "1., such as the speeches known as the US Presidential\n",
      "Inaugural Addresses.  This particular corpus actually contains dozens\n",
      "of individual texts — one per address — but for convenience\n",
      "we glued them end-to-end and treated them as a single text.\n",
      "1. also used various pre-defined texts that\n",
      "we accessed by typing from nltk.book import *.  However, since we want\n",
      "to be able to work with other texts, this section examines a\n",
      "variety of text corpora. We'll see how\n",
      "to select individual texts, and how to work with them.\n",
      "\n",
      "1.1   Gutenberg Corpus\n",
      "NLTK includes a small selection of texts from the Project Gutenberg\n",
      "electronic text archive, which contains\n",
      "some 25,000 free electronic books, hosted at http://www.gutenberg.org/.  We begin\n",
      "by getting the Python interpreter to load the NLTK package,\n",
      "then ask to see nltk.corpus.gutenberg.fileids(), the file identifiers in\n",
      "this corpus:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> import nltk\n",
      ">>> nltk.corpus.gutenberg.fileids()\n",
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',\n",
      "'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt',\n",
      "'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt',\n",
      "'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt',\n",
      "'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt',\n",
      "'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
      "\n",
      "\n",
      "\n",
      "Let's pick out the first of these texts — Emma by Jane Austen — and\n",
      "give it a short name, emma, then find out how many words it contains:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
      ">>> len(emma)\n",
      "192427\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "In 1, we showed how you\n",
      "could carry out concordancing of a text such as text1 with the\n",
      "command text1.concordance(). However, this assumes that you are\n",
      "using one of the nine texts obtained as a result of doing from\n",
      "nltk.book import *. Now that you have started examining data from\n",
      "nltk.corpus, as in the previous example, you have to employ the\n",
      "following pair of statements to perform concordancing and other\n",
      "tasks from 1:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
      ">>> emma.concordance(\"surprize\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "When we defined emma, we invoked the words() function of the gutenberg\n",
      "object in NLTK's corpus package.\n",
      "But since it is cumbersome to type such long names all the time, Python provides\n",
      "another version of the import statement, as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import gutenberg\n",
      ">>> gutenberg.fileids()\n",
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', ...]\n",
      ">>> emma = gutenberg.words('austen-emma.txt')\n",
      "\n",
      "\n",
      "\n",
      "Let's write a short program to display other information about each\n",
      "text, by looping over all the values of fileid corresponding to\n",
      "the gutenberg file identifiers listed earlier and then computing\n",
      "statistics for each text.  For a compact output display, we will round\n",
      "each number to the nearest integer, using round().\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> for fileid in gutenberg.fileids():\n",
      "...     num_chars = len(gutenberg.raw(fileid)) \n",
      "...     num_words = len(gutenberg.words(fileid))\n",
      "...     num_sents = len(gutenberg.sents(fileid))\n",
      "...     num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\n",
      "...     print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)\n",
      "...\n",
      "5 25 26 austen-emma.txt\n",
      "5 26 17 austen-persuasion.txt\n",
      "5 28 22 austen-sense.txt\n",
      "4 34 79 bible-kjv.txt\n",
      "5 19 5 blake-poems.txt\n",
      "4 19 14 bryant-stories.txt\n",
      "4 18 12 burgess-busterbrown.txt\n",
      "4 20 13 carroll-alice.txt\n",
      "5 20 12 chesterton-ball.txt\n",
      "5 23 11 chesterton-brown.txt\n",
      "5 18 11 chesterton-thursday.txt\n",
      "4 21 25 edgeworth-parents.txt\n",
      "5 26 15 melville-moby_dick.txt\n",
      "5 52 11 milton-paradise.txt\n",
      "4 12 9 shakespeare-caesar.txt\n",
      "4 12 8 shakespeare-hamlet.txt\n",
      "4 12 7 shakespeare-macbeth.txt\n",
      "5 36 12 whitman-leaves.txt\n",
      "\n",
      "\n",
      "\n",
      "This program displays three statistics for each text:\n",
      "average word length, average sentence length, and the number of times each vocabulary\n",
      "item appears in the text on average (our lexical diversity score).\n",
      "Observe that average word length appears to be a general property of English, since\n",
      "it has a recurrent value of 4.  (In fact, the average word length is really\n",
      "3 not 4, since the num_chars variable counts space characters.)\n",
      "By contrast average sentence length and lexical diversity\n",
      "appear to be characteristics of particular authors.\n",
      "The previous example also showed how we can access the \"raw\" text of the book ,\n",
      "not split up into tokens.  The raw() function gives us the contents of the file\n",
      "without any linguistic processing.  So, for example, len(gutenberg.raw('blake-poems.txt'))\n",
      "tells us how many letters occur in the text, including the spaces between words.\n",
      "The sents() function divides the text up into its sentences, where each sentence is\n",
      "a list of words:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
      ">>> macbeth_sentences\n",
      "[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare',\n",
      "'1603', ']'], ['Actus', 'Primus', '.'], ...]\n",
      ">>> macbeth_sentences[1116]\n",
      "['Double', ',', 'double', ',', 'toile', 'and', 'trouble', ';',\n",
      "'Fire', 'burne', ',', 'and', 'Cauldron', 'bubble']\n",
      ">>> longest_len = max(len(s) for s in macbeth_sentences)\n",
      ">>> [s for s in macbeth_sentences if len(s) == longest_len]\n",
      "[['Doubtfull', 'it', 'stood', ',', 'As', 'two', 'spent', 'Swimmers', ',', 'that',\n",
      "'doe', 'cling', 'together', ',', 'And', 'choake', 'their', 'Art', ':', 'The',\n",
      "'mercilesse', 'Macdonwald', ...]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Most NLTK corpus readers include a variety of access methods\n",
      "apart from words(), raw(), and sents().  Richer\n",
      "linguistic content is available from some corpora, such as part-of-speech\n",
      "tags, dialogue tags, syntactic trees, and so forth; we will see these\n",
      "in later chapters.\n",
      "\n",
      "\n",
      "\n",
      "1.2   Web and Chat Text\n",
      "Although Project Gutenberg contains thousands of books, it represents established\n",
      "literature.  It is important to consider less formal language as well.  NLTK's\n",
      "small collection of web text includes content from a Firefox discussion forum,\n",
      "conversations overheard in New York, the movie script of Pirates of the Carribean,\n",
      "personal advertisements, and wine reviews:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import webtext\n",
      ">>> for fileid in webtext.fileids():\n",
      "...     print(fileid, webtext.raw(fileid)[:65], '...')\n",
      "...\n",
      "firefox.txt Cookie Manager: \"Don't allow sites that set removed cookies to se...\n",
      "grail.txt SCENE 1: [wind] [clop clop clop] KING ARTHUR: Whoa there!  [clop...\n",
      "overheard.txt White guy: So, do you have any plans for this evening? Asian girl...\n",
      "pirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terr...\n",
      "singles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encoun...\n",
      "wine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawb...\n",
      "\n",
      "\n",
      "\n",
      "There is also a corpus of instant messaging chat sessions, originally collected\n",
      "by the Naval Postgraduate School for research on automatic detection of Internet predators.\n",
      "The corpus contains over 10,000 posts, anonymized by replacing usernames with generic\n",
      "names of the form \"UserNNN\", and manually edited to remove any other identifying information.\n",
      "The corpus is organized into 15 files, where each file contains several hundred posts\n",
      "collected on a given date, for an age-specific chatroom (teens, 20s, 30s, 40s, plus a\n",
      "generic adults chatroom).  The filename contains the date, chatroom,\n",
      "and number of posts; e.g., 10-19-20s_706posts.xml contains 706 posts gathered from\n",
      "the 20s chat room on 10/19/2006.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import nps_chat\n",
      ">>> chatroom = nps_chat.posts('10-19-20s_706posts.xml')\n",
      ">>> chatroom[123]\n",
      "['i', 'do', \"n't\", 'want', 'hot', 'pics', 'of', 'a', 'female', ',',\n",
      "'I', 'can', 'look', 'in', 'a', 'mirror', '.']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.3   Brown Corpus\n",
      "The Brown Corpus was the first million-word electronic\n",
      "corpus of English, created in 1961 at Brown University.\n",
      "This corpus contains text from 500 sources, and the sources\n",
      "have been categorized by genre, such as news, editorial, and so on.\n",
      "1.1 gives an example of each genre\n",
      "(for a complete list, see http://icame.uib.no/brown/bcm-los.html).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ID\n",
      "File\n",
      "Genre\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "A16\n",
      "ca16\n",
      "news\n",
      "Chicago Tribune: Society Reportage\n",
      "\n",
      "B02\n",
      "cb02\n",
      "editorial\n",
      "Christian Science Monitor: Editorials\n",
      "\n",
      "C17\n",
      "cc17\n",
      "reviews\n",
      "Time Magazine: Reviews\n",
      "\n",
      "D12\n",
      "cd12\n",
      "religion\n",
      "Underwood: Probing the Ethics of Realtors\n",
      "\n",
      "E36\n",
      "ce36\n",
      "hobbies\n",
      "Norling: Renting a Car in Europe\n",
      "\n",
      "F25\n",
      "cf25\n",
      "lore\n",
      "Boroff: Jewish Teenage Culture\n",
      "\n",
      "G22\n",
      "cg22\n",
      "belles_lettres\n",
      "Reiner: Coping with Runaway Technology\n",
      "\n",
      "H15\n",
      "ch15\n",
      "government\n",
      "US Office of Civil and Defence Mobilization: The Family Fallout Shelter\n",
      "\n",
      "J17\n",
      "cj19\n",
      "learned\n",
      "Mosteller: Probability with Statistical Applications\n",
      "\n",
      "K04\n",
      "ck04\n",
      "fiction\n",
      "W.E.B. Du Bois: Worlds of Color\n",
      "\n",
      "L13\n",
      "cl13\n",
      "mystery\n",
      "Hitchens: Footsteps in the Night\n",
      "\n",
      "M01\n",
      "cm01\n",
      "science_fiction\n",
      "Heinlein: Stranger in a Strange Land\n",
      "\n",
      "N14\n",
      "cn15\n",
      "adventure\n",
      "Field: Rattlesnake Ridge\n",
      "\n",
      "P12\n",
      "cp12\n",
      "romance\n",
      "Callaghan: A Passion in Rome\n",
      "\n",
      "R06\n",
      "cr06\n",
      "humor\n",
      "Thurber: The Future, If Any, of Comedy\n",
      "\n",
      "\n",
      "Table 1.1: Example Document for Each Section of the Brown Corpus\n",
      "\n",
      "\n",
      "We can access the corpus as a list of words, or a list of sentences (where each sentence\n",
      "is itself just a list of words).  We can optionally specify particular categories or files to read:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import brown\n",
      ">>> brown.categories()\n",
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
      "'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance',\n",
      "'science_fiction']\n",
      ">>> brown.words(categories='news')\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
      ">>> brown.words(fileids=['cg22'])\n",
      "['Does', 'our', 'society', 'have', 'a', 'runaway', ',', ...]\n",
      ">>> brown.sents(categories=['news', 'editorial', 'reviews'])\n",
      "[['The', 'Fulton', 'County'...], ['The', 'jury', 'further'...], ...]\n",
      "\n",
      "\n",
      "\n",
      "The Brown Corpus is a convenient resource for studying systematic differences between\n",
      "genres, a kind of linguistic inquiry known as stylistics.\n",
      "Let's compare genres in their usage of modal verbs.  The first step\n",
      "is to produce the counts for a particular genre.  Remember to\n",
      "import nltk before doing the following:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import brown\n",
      ">>> news_text = brown.words(categories='news')\n",
      ">>> fdist = nltk.FreqDist(w.lower() for w in news_text)\n",
      ">>> modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
      ">>> for m in modals:\n",
      "...     print(m + ':', fdist[m], end=' ')\n",
      "...\n",
      "can: 94 could: 87 may: 93 might: 38 must: 53 will: 389\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "We need to include end=' ' in order for the print function to\n",
      "put its output on a single line.\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Choose a different section of the Brown Corpus, and adapt the previous\n",
      "example to count a selection of wh words, such as what,\n",
      "when, where, who, and why.\n",
      "\n",
      "Next, we need to obtain counts for each genre of interest.  We'll use\n",
      "NLTK's support for conditional frequency distributions. These are\n",
      "presented systematically in 2,\n",
      "where we also unpick the following code line by line. For the moment,\n",
      "you can ignore the details and just concentrate on the output.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (genre, word)\n",
      "...           for genre in brown.categories()\n",
      "...           for word in brown.words(categories=genre))\n",
      ">>> genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
      ">>> modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
      ">>> cfd.tabulate(conditions=genres, samples=modals)\n",
      "                 can could  may might must will\n",
      "           news   93   86   66   38   50  389\n",
      "       religion   82   59   78   12   54   71\n",
      "        hobbies  268   58  131   22   83  264\n",
      "science_fiction   16   49    4   12    8   16\n",
      "        romance   74  193   11   51   45   43\n",
      "          humor   16   30    8    8    9   13\n",
      "\n",
      "\n",
      "\n",
      "Observe that the most frequent modal in the news genre is will,\n",
      "while the most frequent modal in the romance genre is could.\n",
      "Would you have predicted this?  The idea that word counts\n",
      "might distinguish genres will be taken up again in chap-data-intensive.\n",
      "\n",
      "\n",
      "\n",
      "1.4   Reuters Corpus\n",
      "The Reuters Corpus contains 10,788 news documents totaling 1.3 million words.\n",
      "The documents have been classified into 90 topics, and grouped\n",
      "into two sets, called \"training\" and \"test\"; thus, the text with\n",
      "fileid 'test/14826' is a document drawn from the test set. This split is for\n",
      "training and testing algorithms that automatically detect the topic of a document,\n",
      "as we will see in chap-data-intensive.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import reuters\n",
      ">>> reuters.fileids()\n",
      "['test/14826', 'test/14828', 'test/14829', 'test/14832', ...]\n",
      ">>> reuters.categories()\n",
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa',\n",
      "'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn',\n",
      "'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', ...]\n",
      "\n",
      "\n",
      "\n",
      "Unlike the Brown Corpus, categories in the Reuters corpus overlap with\n",
      "each other, simply because a news story often covers multiple topics.\n",
      "We can ask for the topics covered by one or more documents, or for the\n",
      "documents included in one or more categories. For convenience, the\n",
      "corpus methods accept a single fileid or a list of fileids.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> reuters.categories('training/9865')\n",
      "['barley', 'corn', 'grain', 'wheat']\n",
      ">>> reuters.categories(['training/9865', 'training/9880'])\n",
      "['barley', 'corn', 'grain', 'money-fx', 'wheat']\n",
      ">>> reuters.fileids('barley')\n",
      "['test/15618', 'test/15649', 'test/15676', 'test/15728', 'test/15871', ...]\n",
      ">>> reuters.fileids(['barley', 'corn'])\n",
      "['test/14832', 'test/14858', 'test/15033', 'test/15043', 'test/15106',\n",
      "'test/15287', 'test/15341', 'test/15618', 'test/15648', 'test/15649', ...]\n",
      "\n",
      "\n",
      "\n",
      "Similarly, we can specify the words or sentences we want in terms of\n",
      "files or categories. The first handful of words in each of these texts are the\n",
      "titles, which by convention are stored as upper case.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> reuters.words('training/9865')[:14]\n",
      "['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', 'BIDS',\n",
      "'DETAILED', 'French', 'operators', 'have', 'requested', 'licences', 'to', 'export']\n",
      ">>> reuters.words(['training/9865', 'training/9880'])\n",
      "['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]\n",
      ">>> reuters.words(categories='barley')\n",
      "['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]\n",
      ">>> reuters.words(categories=['barley', 'corn'])\n",
      "['THAI', 'TRADE', 'DEFICIT', 'WIDENS', 'IN', 'FIRST', ...]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.5   Inaugural Address Corpus\n",
      "\n",
      "In 1, we looked at\n",
      "the Inaugural Address Corpus,\n",
      "but treated it as a single text.  The graph in fig-inaugural\n",
      "used \"word offset\" as one of the axes; this is the numerical index of the\n",
      "word in the corpus, counting from the first word of the first address.\n",
      "However, the corpus is actually a collection of 55 texts, one for each presidential address.\n",
      "An interesting property of this collection is its time dimension:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import inaugural\n",
      ">>> inaugural.fileids()\n",
      "['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', ...]\n",
      ">>> [fileid[:4] for fileid in inaugural.fileids()]\n",
      "['1789', '1793', '1797', '1801', '1805', '1809', '1813', '1817', '1821', ...]\n",
      "\n",
      "\n",
      "\n",
      "Notice that the year of each text appears in its filename.  To get the year\n",
      "out of the filename, we extracted the first four characters, using fileid[:4].\n",
      "Let's look at how the words America and citizen are used over time.\n",
      "The following code\n",
      "converts the words in the Inaugural corpus\n",
      "to lowercase using w.lower() ,\n",
      "then checks if they start with either of the \"targets\"\n",
      "america or citizen using startswith() .\n",
      "Thus it will count words like American's and Citizens.\n",
      "We'll learn about conditional frequency distributions in\n",
      "2; for now just consider\n",
      "the output, shown in 1.1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (target, fileid[:4])\n",
      "...           for fileid in inaugural.fileids()\n",
      "...           for w in inaugural.words(fileid)\n",
      "...           for target in ['america', 'citizen']\n",
      "...           if w.lower().startswith(target)) \n",
      ">>> cfd.plot()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figure 1.1: Plot of a Conditional Frequency Distribution: all words in the Inaugural Address\n",
      "Corpus that begin with america or citizen are counted; separate counts\n",
      "are kept for each address; these are plotted so that trends in usage over time can\n",
      "be observed; counts are not normalized for document length.\n",
      "\n",
      "\n",
      "\n",
      "1.6   Annotated Text Corpora\n",
      "Many text corpora contain linguistic annotations, representing POS tags,\n",
      "named entities, syntactic structures, semantic roles, and so forth.  NLTK provides\n",
      "convenient ways to access several of these corpora, and has data packages containing corpora\n",
      "and corpus samples, freely downloadable for use in teaching and research.\n",
      "1.2 lists some of the corpora.  For information about\n",
      "downloading them, see http://nltk.org/data.\n",
      "For more examples of how to access NLTK corpora,\n",
      "please consult the Corpus HOWTO at http://nltk.org/howto.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Corpus\n",
      "Compiler\n",
      "Contents\n",
      "\n",
      "\n",
      "\n",
      "Brown Corpus\n",
      "Francis, Kucera\n",
      "15 genres, 1.15M words, tagged, categorized\n",
      "\n",
      "CESS Treebanks\n",
      "CLiC-UB\n",
      "1M words, tagged and parsed (Catalan, Spanish)\n",
      "\n",
      "Chat-80 Data Files\n",
      "Pereira & Warren\n",
      "World Geographic Database\n",
      "\n",
      "CMU Pronouncing Dictionary\n",
      "CMU\n",
      "127k entries\n",
      "\n",
      "CoNLL 2000 Chunking Data\n",
      "CoNLL\n",
      "270k words, tagged and chunked\n",
      "\n",
      "CoNLL 2002 Named Entity\n",
      "CoNLL\n",
      "700k words, pos- and named-entity-tagged (Dutch, Spanish)\n",
      "\n",
      "CoNLL 2007 Dependency Treebanks (sel)\n",
      "CoNLL\n",
      "150k words, dependency parsed (Basque, Catalan)\n",
      "\n",
      "Dependency Treebank\n",
      "Narad\n",
      "Dependency parsed version of Penn Treebank sample\n",
      "\n",
      "FrameNet\n",
      "Fillmore, Baker et al\n",
      "10k word senses, 170k manually annotated sentences\n",
      "\n",
      "Floresta Treebank\n",
      "Diana Santos et al\n",
      "9k sentences, tagged and parsed (Portuguese)\n",
      "\n",
      "Gazetteer Lists\n",
      "Various\n",
      "Lists of cities and countries\n",
      "\n",
      "Genesis Corpus\n",
      "Misc web sources\n",
      "6 texts, 200k words, 6 languages\n",
      "\n",
      "Gutenberg (selections)\n",
      "Hart, Newby, et al\n",
      "18 texts, 2M words\n",
      "\n",
      "Inaugural Address Corpus\n",
      "CSpan\n",
      "US Presidential Inaugural Addresses (1789-present)\n",
      "\n",
      "Indian POS-Tagged Corpus\n",
      "Kumaran et al\n",
      "60k words, tagged (Bangla, Hindi, Marathi, Telugu)\n",
      "\n",
      "MacMorpho Corpus\n",
      "NILC, USP, Brazil\n",
      "1M words, tagged (Brazilian Portuguese)\n",
      "\n",
      "Movie Reviews\n",
      "Pang, Lee\n",
      "2k movie reviews with sentiment polarity classification\n",
      "\n",
      "Names Corpus\n",
      "Kantrowitz, Ross\n",
      "8k male and female names\n",
      "\n",
      "NIST 1999 Info Extr (selections)\n",
      "Garofolo\n",
      "63k words, newswire and named-entity SGML markup\n",
      "\n",
      "Nombank\n",
      "Meyers\n",
      "115k propositions, 1400 noun frames\n",
      "\n",
      "NPS Chat Corpus\n",
      "Forsyth, Martell\n",
      "10k IM chat posts, POS-tagged and dialogue-act tagged\n",
      "\n",
      "Open Multilingual WordNet\n",
      "Bond et al\n",
      "15 languages, aligned to English WordNet\n",
      "\n",
      "PP Attachment Corpus\n",
      "Ratnaparkhi\n",
      "28k prepositional phrases, tagged as noun or verb modifiers\n",
      "\n",
      "Proposition Bank\n",
      "Palmer\n",
      "113k propositions, 3300 verb frames\n",
      "\n",
      "Question Classification\n",
      "Li, Roth\n",
      "6k questions, categorized\n",
      "\n",
      "Reuters Corpus\n",
      "Reuters\n",
      "1.3M words, 10k news documents, categorized\n",
      "\n",
      "Roget's Thesaurus\n",
      "Project Gutenberg\n",
      "200k words, formatted text\n",
      "\n",
      "RTE Textual Entailment\n",
      "Dagan et al\n",
      "8k sentence pairs, categorized\n",
      "\n",
      "SEMCOR\n",
      "Rus, Mihalcea\n",
      "880k words, part-of-speech and sense tagged\n",
      "\n",
      "Senseval 2 Corpus\n",
      "Pedersen\n",
      "600k words, part-of-speech and sense tagged\n",
      "\n",
      "SentiWordNet\n",
      "Esuli, Sebastiani\n",
      "sentiment scores for 145k WordNet synonym sets\n",
      "\n",
      "Shakespeare texts (selections)\n",
      "Bosak\n",
      "8 books in XML format\n",
      "\n",
      "State of the Union Corpus\n",
      "CSPAN\n",
      "485k words, formatted text\n",
      "\n",
      "Stopwords Corpus\n",
      "Porter et al\n",
      "2,400 stopwords for 11 languages\n",
      "\n",
      "Swadesh Corpus\n",
      "Wiktionary\n",
      "comparative wordlists in 24 languages\n",
      "\n",
      "Switchboard Corpus (selections)\n",
      "LDC\n",
      "36 phonecalls, transcribed, parsed\n",
      "\n",
      "Univ Decl of Human Rights\n",
      "United Nations\n",
      "480k words, 300+ languages\n",
      "\n",
      "Penn Treebank (selections)\n",
      "LDC\n",
      "40k words, tagged and parsed\n",
      "\n",
      "TIMIT Corpus (selections)\n",
      "NIST/LDC\n",
      "audio files and transcripts for 16 speakers\n",
      "\n",
      "VerbNet 2.1\n",
      "Palmer et al\n",
      "5k verbs, hierarchically organized, linked to WordNet\n",
      "\n",
      "Wordlist Corpus\n",
      "OpenOffice.org et al\n",
      "960k words and 20k affixes for 8 languages\n",
      "\n",
      "WordNet 3.0 (English)\n",
      "Miller, Fellbaum\n",
      "145k synonym sets\n",
      "\n",
      "\n",
      "Table 1.2: Some of the Corpora and Corpus Samples Distributed with NLTK: For information about downloading\n",
      "and using them, please consult the NLTK website.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.7   Corpora in Other Languages\n",
      "NLTK comes with corpora for many languages, though in some cases\n",
      "you will need to learn how to manipulate character encodings in Python\n",
      "before using these corpora (see 3.3).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nltk.corpus.cess_esp.words()\n",
      "['El', 'grupo', 'estatal', 'Electricit\\xe9_de_France', ...]\n",
      ">>> nltk.corpus.floresta.words()\n",
      "['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', ...]\n",
      ">>> nltk.corpus.indian.words('hindi.pos')\n",
      "['पूर्ण', 'प्रतिबंध', 'हटाओ', ':', 'इराक', 'संयुक्त', ...]\n",
      ">>> nltk.corpus.udhr.fileids()\n",
      "['Abkhaz-Cyrillic+Abkh', 'Abkhaz-UTF8', 'Achehnese-Latin1', 'Achuar-Shiwiar-Latin1',\n",
      "'Adja-UTF8', 'Afaan_Oromo_Oromiffa-Latin1', 'Afrikaans-Latin1', 'Aguaruna-Latin1',\n",
      "'Akuapem_Twi-UTF8', 'Albanian_Shqip-Latin1', 'Amahuaca', 'Amahuaca-Latin1', ...]\n",
      ">>> nltk.corpus.udhr.words('Javanese-Latin1')[11:]\n",
      "['Saben', 'umat', 'manungsa', 'lair', 'kanthi', 'hak', ...]\n",
      "\n",
      "\n",
      "\n",
      "The last of these corpora, udhr, contains the Universal Declaration of Human Rights\n",
      "in over 300 languages.  The fileids for this corpus include\n",
      "information about the character encoding used in the file,\n",
      "such as UTF8 or Latin1.\n",
      "Let's use a conditional frequency distribution to examine the differences in word lengths\n",
      "for a selection of languages included in the udhr corpus.\n",
      "The output is shown in 1.2 (run the program yourself to see a color plot).\n",
      "Note that True and False are Python's built-in boolean values.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import udhr\n",
      ">>> languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
      "...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (lang, len(word))\n",
      "...           for lang in languages\n",
      "...           for word in udhr.words(lang + '-Latin1'))\n",
      ">>> cfd.plot(cumulative=True)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figure 1.2: Cumulative Word Length Distributions:\n",
      "Six translations of the Universal Declaration of Human Rights are processed;\n",
      "this graph shows that words having 5 or fewer letters account for about\n",
      "80% of Ibibio text, 60% of German text, and 25% of Inuktitut text.\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Pick a language of interest in udhr.fileids(), and define a variable\n",
      "raw_text = udhr.raw(Language-Latin1).  Now plot a frequency\n",
      "distribution of the letters of the text using nltk.FreqDist(raw_text).plot().\n",
      "\n",
      "Unfortunately, for many languages, substantial corpora are not yet available.  Often there is\n",
      "insufficient government or industrial support for developing language resources, and individual\n",
      "efforts are piecemeal and hard to discover or re-use.  Some languages have no\n",
      "established writing system, or are endangered.  (See 7\n",
      "for suggestions on how to locate language resources.)\n",
      "\n",
      "\n",
      "1.8   Text Corpus Structure\n",
      "We have seen a variety of corpus structures so far; these are\n",
      "summarized in 1.3.\n",
      "The simplest kind lacks any structure: it is just a collection of texts.\n",
      "Often, texts are grouped into categories that might correspond to genre, source, author, language, etc.\n",
      "Sometimes these categories overlap, notably in the case of topical categories as a text can be\n",
      "relevant to more than one topic.  Occasionally, text collections have temporal structure,\n",
      "news collections being the most common example.\n",
      "\n",
      "\n",
      "Figure 1.3: Common Structures for Text Corpora: The simplest kind of corpus is a collection\n",
      "of isolated texts with no particular organization; some corpora are structured\n",
      "into categories like genre (Brown Corpus); some categorizations overlap, such as\n",
      "topic categories (Reuters Corpus); other corpora represent language use over time\n",
      "(Inaugural Address Corpus).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Example\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "fileids()\n",
      "the files of the corpus\n",
      "\n",
      "fileids([categories])\n",
      "the files of the corpus corresponding to these categories\n",
      "\n",
      "categories()\n",
      "the categories of the corpus\n",
      "\n",
      "categories([fileids])\n",
      "the categories of the corpus corresponding to these files\n",
      "\n",
      "raw()\n",
      "the raw content of the corpus\n",
      "\n",
      "raw(fileids=[f1,f2,f3])\n",
      "the raw content of the specified files\n",
      "\n",
      "raw(categories=[c1,c2])\n",
      "the raw content of the specified categories\n",
      "\n",
      "words()\n",
      "the words of the whole corpus\n",
      "\n",
      "words(fileids=[f1,f2,f3])\n",
      "the words of the specified fileids\n",
      "\n",
      "words(categories=[c1,c2])\n",
      "the words of the specified categories\n",
      "\n",
      "sents()\n",
      "the sentences of the whole corpus\n",
      "\n",
      "sents(fileids=[f1,f2,f3])\n",
      "the sentences of the specified fileids\n",
      "\n",
      "sents(categories=[c1,c2])\n",
      "the sentences of the specified categories\n",
      "\n",
      "abspath(fileid)\n",
      "the location of the given file on disk\n",
      "\n",
      "encoding(fileid)\n",
      "the encoding of the file (if known)\n",
      "\n",
      "open(fileid)\n",
      "open a stream for reading the given corpus file\n",
      "\n",
      "root\n",
      "if the path to the root of locally installed corpus\n",
      "\n",
      "readme()\n",
      "the contents of the README file of the corpus\n",
      "\n",
      "\n",
      "Table 1.3: Basic Corpus Functionality defined in NLTK: more documentation can be found using\n",
      "help(nltk.corpus.reader) and by reading the online Corpus HOWTO at http://nltk.org/howto.\n",
      "\n",
      "\n",
      "NLTK's corpus readers support efficient access to a variety of corpora, and can\n",
      "be used to work with new corpora.  1.3 lists functionality\n",
      "provided by the corpus readers.  We illustrate the difference between some\n",
      "of the corpus access methods below:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> raw = gutenberg.raw(\"burgess-busterbrown.txt\")\n",
      ">>> raw[1:20]\n",
      "'The Adventures of B'\n",
      ">>> words = gutenberg.words(\"burgess-busterbrown.txt\")\n",
      ">>> words[1:20]\n",
      "['The', 'Adventures', 'of', 'Buster', 'Bear', 'by', 'Thornton', 'W', '.',\n",
      "'Burgess', '1920', ']', 'I', 'BUSTER', 'BEAR', 'GOES', 'FISHING', 'Buster',\n",
      "'Bear']\n",
      ">>> sents = gutenberg.sents(\"burgess-busterbrown.txt\")\n",
      ">>> sents[1:20]\n",
      "[['I'], ['BUSTER', 'BEAR', 'GOES', 'FISHING'], ['Buster', 'Bear', 'yawned', 'as',\n",
      "'he', 'lay', 'on', 'his', 'comfortable', 'bed', 'of', 'leaves', 'and', 'watched',\n",
      "'the', 'first', 'early', 'morning', 'sunbeams', 'creeping', 'through', ...], ...]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.9   Loading your own Corpus\n",
      "If you have your own collection of text files that you would like to access using\n",
      "the above methods, you can easily load them with the help of NLTK's\n",
      "PlaintextCorpusReader. Check the location of your files on your file system; in\n",
      "the following example, we have taken this to be the directory\n",
      "/usr/share/dict. Whatever the location, set this to be the value of\n",
      "corpus_root .\n",
      "The second parameter of the PlaintextCorpusReader initializer \n",
      "can be a list of fileids, like ['a.txt', 'test/b.txt'],\n",
      "or a pattern that matches all fileids, like '[abc]/.*\\.txt'\n",
      "(see 3.4 for information\n",
      "about regular expressions).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import PlaintextCorpusReader\n",
      ">>> corpus_root = '/usr/share/dict' \n",
      ">>> wordlists = PlaintextCorpusReader(corpus_root, '.*') \n",
      ">>> wordlists.fileids()\n",
      "['README', 'connectives', 'propernames', 'web2', 'web2a', 'words']\n",
      ">>> wordlists.words('connectives')\n",
      "['the', 'of', 'and', 'to', 'a', 'in', 'that', 'is', ...]\n",
      "\n",
      "\n",
      "\n",
      "As another example, suppose you have your own local copy of Penn Treebank (release 3),\n",
      "in C:\\corpora.  We can use the BracketParseCorpusReader to access this\n",
      "corpus.  We specify the corpus_root to be the location of the parsed Wall Street\n",
      "Journal component of the corpus , and give a file_pattern\n",
      "that matches the files contained within its subfolders  (using forward slashes).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import BracketParseCorpusReader\n",
      ">>> corpus_root = r\"C:\\corpora\\penntreebank\\parsed\\mrg\\wsj\" \n",
      ">>> file_pattern = r\".*/wsj_.*\\.mrg\" \n",
      ">>> ptb = BracketParseCorpusReader(corpus_root, file_pattern)\n",
      ">>> ptb.fileids()\n",
      "['00/wsj_0001.mrg', '00/wsj_0002.mrg', '00/wsj_0003.mrg', '00/wsj_0004.mrg', ...]\n",
      ">>> len(ptb.sents())\n",
      "49208\n",
      ">>> ptb.sents(fileids='20/wsj_2013.mrg')[19]\n",
      "['The', '55-year-old', 'Mr.', 'Noriega', 'is', \"n't\", 'as', 'smooth', 'as', 'the',\n",
      "'shah', 'of', 'Iran', ',', 'as', 'well-born', 'as', 'Nicaragua', \"'s\", 'Anastasio',\n",
      "'Somoza', ',', 'as', 'imperial', 'as', 'Ferdinand', 'Marcos', 'of', 'the', 'Philippines',\n",
      "'or', 'as', 'bloody', 'as', 'Haiti', \"'s\", 'Baby', Doc', 'Duvalier', '.']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2   Conditional Frequency Distributions\n",
      "We introduced frequency distributions in 3.\n",
      "We saw that given some list mylist of words or other items,\n",
      "FreqDist(mylist) would compute the number of occurrences of each\n",
      "item in the list.  Here we will generalize this idea.\n",
      "When the texts of a corpus are divided into several\n",
      "categories, by genre, topic, author, etc, we can maintain separate\n",
      "frequency distributions for each category.  This will allow us to\n",
      "study systematic differences between the categories.  In the previous\n",
      "section we achieved this using NLTK's ConditionalFreqDist data\n",
      "type.  A conditional frequency distribution is a collection of\n",
      "frequency distributions, each one for a different \"condition\".  The\n",
      "condition will often be the category of the text.  2.1\n",
      "depicts a fragment of a conditional frequency distribution having just\n",
      "two conditions, one for news text and one for romance text.\n",
      "\n",
      "\n",
      "Figure 2.1: Counting Words Appearing in a Text Collection (a conditional frequency distribution)\n",
      "\n",
      "\n",
      "2.1   Conditions and Events\n",
      "A frequency distribution counts observable events,\n",
      "such as the appearance of words in a text.  A conditional\n",
      "frequency distribution needs to pair each event with a condition.\n",
      "So instead of processing a sequence of words ,\n",
      "we have to process a sequence of pairs :\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...] \n",
      ">>> pairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ...] \n",
      "\n",
      "\n",
      "\n",
      "Each pair has the form (condition, event).  If we were processing the\n",
      "entire Brown Corpus by genre there would be 15 conditions (one per genre),\n",
      "and 1,161,192 events (one per word).\n",
      "\n",
      "\n",
      "2.2   Counting Words by Genre\n",
      "In 1 we saw a conditional\n",
      "frequency distribution where the condition was the section of the\n",
      "Brown Corpus, and for each condition we counted words. Whereas\n",
      "FreqDist() takes a simple list as input, ConditionalFreqDist()\n",
      "takes a list of pairs.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import brown\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (genre, word)\n",
      "...           for genre in brown.categories()\n",
      "...           for word in brown.words(categories=genre))\n",
      "\n",
      "\n",
      "\n",
      "Let's break this down, and look at just two genres, news and romance.\n",
      "For each genre , we loop over every word in the genre ,\n",
      "producing pairs consisting of the genre and the word :\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> genre_word = [(genre, word) \n",
      "...               for genre in ['news', 'romance'] \n",
      "...               for word in brown.words(categories=genre)] \n",
      ">>> len(genre_word)\n",
      "170576\n",
      "\n",
      "\n",
      "\n",
      "So, as we can see below,\n",
      "pairs at the beginning of the list genre_word will be of the form\n",
      "('news', word) , while those at the end will be of the form\n",
      "('romance', word) .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> genre_word[:4]\n",
      "[('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ('news', 'Grand')] # [_start-genre]\n",
      ">>> genre_word[-4:]\n",
      "[('romance', 'afraid'), ('romance', 'not'), ('romance', \"''\"), ('romance', '.')] # [_end-genre]\n",
      "\n",
      "\n",
      "\n",
      "We can now use this list of pairs to create a ConditionalFreqDist, and\n",
      "save it in a variable cfd.  As usual, we can type the name of the\n",
      "variable to inspect it , and verify it has two conditions :\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> cfd = nltk.ConditionalFreqDist(genre_word)\n",
      ">>> cfd \n",
      "<ConditionalFreqDist with 2 conditions>\n",
      ">>> cfd.conditions()\n",
      "['news', 'romance'] # [_conditions-cfd]\n",
      "\n",
      "\n",
      "\n",
      "Let's access the two conditions, and satisfy ourselves that each is just\n",
      "a frequency distribution:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(cfd['news'])\n",
      "<FreqDist with 14394 samples and 100554 outcomes>\n",
      ">>> print(cfd['romance'])\n",
      "<FreqDist with 8452 samples and 70022 outcomes>\n",
      ">>> cfd['romance'].most_common(20)\n",
      "[(',', 3899), ('.', 3736), ('the', 2758), ('and', 1776), ('to', 1502),\n",
      "('a', 1335), ('of', 1186), ('``', 1045), (\"''\", 1044), ('was', 993),\n",
      "('I', 951), ('in', 875), ('he', 702), ('had', 692), ('?', 690),\n",
      "('her', 651), ('that', 583), ('it', 573), ('his', 559), ('she', 496)]\n",
      ">>> cfd['romance']['could']\n",
      "193\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.3   Plotting and Tabulating Distributions\n",
      "Apart from combining two or more frequency distributions, and being easy to initialize,\n",
      "a ConditionalFreqDist provides some useful methods for tabulation and plotting.\n",
      "The plot in 1.1 was based on a conditional frequency distribution\n",
      "reproduced in the code below.\n",
      "The condition is either of the words america or citizen ,\n",
      "and the counts being plotted are the number of times the word occured in a particular speech.\n",
      "It exploits the fact that the filename for each speech, e.g., 1865-Lincoln.txt\n",
      "contains the year as the first four characters .\n",
      "This code generates the pair ('america', '1865') for\n",
      "every instance of a word whose lowercased form starts with america\n",
      "— such as Americans — in the file 1865-Lincoln.txt.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import inaugural\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (target, fileid[:4]) \n",
      "...           for fileid in inaugural.fileids()\n",
      "...           for w in inaugural.words(fileid)\n",
      "...           for target in ['america', 'citizen'] \n",
      "...           if w.lower().startswith(target))\n",
      "\n",
      "\n",
      "\n",
      "The plot in 1.2 was also based on a conditional frequency distribution,\n",
      "reproduced below.  This time, the condition is the name of the language\n",
      "and the counts being plotted are derived from word lengths .\n",
      "It exploits the fact that the filename for each language is the language name followed\n",
      "by '-Latin1' (the character encoding).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import udhr\n",
      ">>> languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
      "...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (lang, len(word)) \n",
      "...           for lang in languages\n",
      "...           for word in udhr.words(lang + '-Latin1'))\n",
      "\n",
      "\n",
      "\n",
      "In the plot() and tabulate() methods, we can\n",
      "optionally specify which conditions to display with a conditions= parameter.\n",
      "When we omit it, we get all the conditions.  Similarly, we can limit the\n",
      "samples to display with a samples= parameter.  This makes it possible to\n",
      "load a large quantity of data into a conditional frequency distribution, and then\n",
      "to explore it by plotting or tabulating selected conditions and samples.  It also\n",
      "gives us full control over the order of conditions and samples in any displays.\n",
      "For example, we can tabulate the cumulative frequency data just for two\n",
      "languages, and for words less than 10 characters long, as shown below.\n",
      "We interpret the last cell on the top row to mean that 1,638 words of the\n",
      "English text have 9 or fewer letters.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> cfd.tabulate(conditions=['English', 'German_Deutsch'],\n",
      "...              samples=range(10), cumulative=True)\n",
      "                  0    1    2    3    4    5    6    7    8    9\n",
      "       English    0  185  525  883  997 1166 1283 1440 1558 1638\n",
      "German_Deutsch    0  171  263  614  717  894 1013 1110 1213 1275\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Working with the news and romance genres from the Brown Corpus,\n",
      "find out which days of the week are most newsworthy, and which are most romantic.\n",
      "Define a variable called days containing a list of days of the week, i.e.\n",
      "['Monday', ...].  Now tabulate the counts for these words using\n",
      "cfd.tabulate(samples=days).  Now try the same thing using plot in place of tabulate.\n",
      "You may control the output order of days with the help of an extra parameter:\n",
      "samples=['Monday', ...].\n",
      "\n",
      "You may have noticed that the multi-line expressions we have been\n",
      "using with conditional frequency distributions look like list\n",
      "comprehensions, but without the brackets.  In general,\n",
      "when we use a list comprehension as a parameter to a function,\n",
      "like set([w.lower() for w in t]), we are permitted to omit\n",
      "the square brackets and just write: set(w.lower() for w in t).\n",
      "(See the discussion of \"generator expressions\" in 4.2\n",
      "for more about this.)\n",
      "\n",
      "\n",
      "2.4   Generating Random Text with Bigrams\n",
      "We can use a conditional frequency distribution to create a table of\n",
      "bigrams (word pairs). (We introducted bigrams in\n",
      "3.)\n",
      "The bigrams() function takes a list of\n",
      "words and builds a list of consecutive word pairs.\n",
      "Remember that, in order to see the result and not a cryptic\n",
      "\"generator object\", we need to use the list() function:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven',\n",
      "...   'and', 'the', 'earth', '.']\n",
      ">>> list(nltk.bigrams(sent))\n",
      "[('In', 'the'), ('the', 'beginning'), ('beginning', 'God'), ('God', 'created'),\n",
      "('created', 'the'), ('the', 'heaven'), ('heaven', 'and'), ('and', 'the'),\n",
      "('the', 'earth'), ('earth', '.')]\n",
      "\n",
      "\n",
      "\n",
      "In 2.2, we treat each word as a condition, and for each one\n",
      "we effectively create a frequency distribution over the following\n",
      "words.  The function generate_model() contains a simple loop to\n",
      "generate text. When we call the function, we choose a word (such as\n",
      "'living') as our initial context, then once inside the loop, we\n",
      "print the current value of the variable word, and reset word\n",
      "to be the most likely token in that context (using max()); next\n",
      "time through the loop, we use that word as our new context.  As you\n",
      "can see by inspecting the output, this simple approach to text\n",
      "generation tends to get stuck in loops; another method would be to\n",
      "randomly choose the next word from among the available words.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def generate_model(cfdist, word, num=15):\n",
      "    for i in range(num):\n",
      "        print(word, end=' ')\n",
      "        word = cfdist[word].max()\n",
      "\n",
      "text = nltk.corpus.genesis.words('english-kjv.txt')\n",
      "bigrams = nltk.bigrams(text)\n",
      "cfd = nltk.ConditionalFreqDist(bigrams) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> cfd['living']\n",
      "FreqDist({'creature': 7, 'thing': 4, 'substance': 2, ',': 1, '.': 1, 'soul': 1})\n",
      ">>> generate_model(cfd, 'living')\n",
      "living creature that he said , and the land of the land of the land\n",
      "\n",
      "\n",
      "Example 2.2 (code_random_text.py): Figure 2.2: Generating Random Text: this program obtains all bigrams\n",
      "from the text of the book of Genesis, then constructs a\n",
      "conditional frequency distribution to record which\n",
      "words are most likely to follow a given word; e.g., after\n",
      "the word living, the most likely word is\n",
      "creature; the generate_model() function uses this\n",
      "data, and a seed word, to generate random text.\n",
      "\n",
      "Conditional frequency distributions are a useful data structure for many NLP tasks.\n",
      "Their commonly-used methods are summarized in 2.1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Example\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "cfdist = ConditionalFreqDist(pairs)\n",
      "create a conditional frequency distribution from a list of pairs\n",
      "\n",
      "cfdist.conditions()\n",
      "the conditions\n",
      "\n",
      "cfdist[condition]\n",
      "the frequency distribution for this condition\n",
      "\n",
      "cfdist[condition][sample]\n",
      "frequency for the given sample for this condition\n",
      "\n",
      "cfdist.tabulate()\n",
      "tabulate the conditional frequency distribution\n",
      "\n",
      "cfdist.tabulate(samples, conditions)\n",
      "tabulation limited to the specified samples and conditions\n",
      "\n",
      "cfdist.plot()\n",
      "graphical plot of the conditional frequency distribution\n",
      "\n",
      "cfdist.plot(samples, conditions)\n",
      "graphical plot limited to the specified samples and conditions\n",
      "\n",
      "cfdist1 < cfdist2\n",
      "test if samples in cfdist1 occur less frequently than in cfdist2\n",
      "\n",
      "\n",
      "Table 2.1: NLTK's Conditional Frequency Distributions: commonly-used methods and idioms for defining,\n",
      "accessing, and visualizing a conditional frequency distribution of counters.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3   More Python: Reusing Code\n",
      "By this time you've probably typed and retyped a lot of code in the Python\n",
      "interactive interpreter.  If you mess up when retyping a complex example you have\n",
      "to enter it again.  Using the arrow keys to access and modify previous commands is helpful but only goes so\n",
      "far.  In this section we see two important ways to reuse code: text editors and Python functions.\n",
      "\n",
      "3.1   Creating Programs with a Text Editor\n",
      "The Python interactive interpreter performs your instructions as soon as you type\n",
      "them.  Often, it is better to compose a multi-line program using a text editor,\n",
      "then ask Python to run the whole program at once.  Using IDLE, you can do\n",
      "this by going to the File menu and opening a new window.  Try this now, and\n",
      "enter the following one-line program:\n",
      "\n",
      "print('Monty Python')\n",
      "\n",
      "Save this program in a file called monty.py, then\n",
      "go to the Run menu, and select the command Run Module.\n",
      "(We'll learn what modules are shortly.)\n",
      "The result in the main IDLE window should look like this:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> ================================ RESTART ================================\n",
      ">>>\n",
      "Monty Python\n",
      ">>>\n",
      "\n",
      "\n",
      "\n",
      "You can also type from monty import * and it will do the same thing.\n",
      "From now on, you have a choice of using the interactive interpreter or a\n",
      "text editor to create your programs.  It is often convenient to test your ideas\n",
      "using the interpreter, revising a line of code until it does what you expect.\n",
      "Once you're ready, you can paste the code\n",
      "(minus any >>> or ... prompts) into the text editor,\n",
      "continue to expand it, and finally save the program\n",
      "in a file so that you don't have to type it in again later.\n",
      "Give the file a short but descriptive name, using all lowercase letters and separating\n",
      "words with underscore, and using the .py filename extension, e.g., monty_python.py.\n",
      "\n",
      "Note\n",
      "Important:\n",
      "Our inline code examples include the >>> and ... prompts\n",
      "as if we are interacting directly with the interpreter.  As they get more complicated,\n",
      "you should instead type them into the editor, without the prompts, and run them\n",
      "from the editor as shown above.  When we provide longer programs in this book,\n",
      "we will leave out the prompts to remind you to type them into a file rather\n",
      "than using the interpreter.  You can see this already in 2.2 above.\n",
      "Note that it still includes a couple of lines with the Python prompt;\n",
      "this is the interactive part of the task where you inspect some data and invoke a function.\n",
      "Remember that all code samples like 2.2 are downloadable\n",
      "from http://nltk.org/.\n",
      "\n",
      "\n",
      "\n",
      "3.2   Functions\n",
      "Suppose that you work on analyzing text that involves different forms\n",
      "of the same word, and that part of your program needs to work out\n",
      "the plural form of a given singular noun.  Suppose it needs to do this\n",
      "work in two places, once when it is processing some texts, and again\n",
      "when it is processing user input.\n",
      "Rather than repeating the same code several times over, it is more\n",
      "efficient and reliable to localize this work inside a function.\n",
      "A function is just a named block of code that performs some well-defined\n",
      "task, as we saw in 1.\n",
      "A function is usually defined to take some inputs, using special variables known as parameters,\n",
      "and it may produce a result, also known as a return value.\n",
      "We define a function using the keyword def followed by the\n",
      "function name and any input parameters, followed by the body of the\n",
      "function.  Here's the function we saw in 1\n",
      "(including the import statement that is needed for Python 2, in order to make division behave as expected):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from __future__ import division\n",
      ">>> def lexical_diversity(text):\n",
      "...     return len(text) / len(set(text))\n",
      "\n",
      "\n",
      "\n",
      "We use the keyword return to indicate the value that is\n",
      "produced as output by the function.  In the above example,\n",
      "all the work of the function is done in the return statement.\n",
      "Here's an equivalent definition which does the same work\n",
      "using multiple lines of code.  We'll change the parameter name\n",
      "from text to my_text_data to remind you that this is an arbitrary choice:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def lexical_diversity(my_text_data):\n",
      "...     word_count = len(my_text_data)\n",
      "...     vocab_size = len(set(my_text_data))\n",
      "...     diversity_score = vocab_size / word_count\n",
      "...     return diversity_score\n",
      "\n",
      "\n",
      "\n",
      "Notice that we've created some new variables inside the body of the function.\n",
      "These are local variables and are not accessible outside the function.\n",
      "So now we have defined a function with the name lexical_diversity. But just\n",
      "defining it won't produce any output!\n",
      "Functions do nothing until they are \"called\" (or \"invoked\"):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import genesis\n",
      ">>> kjv = genesis.words('english-kjv.txt')\n",
      ">>> lexical_diversity(kjv)\n",
      "0.06230453042623537\n",
      "\n",
      "\n",
      "\n",
      "Let's return to our earlier scenario, and actually define a simple\n",
      "function to work out English plurals.  The function plural() in 3.1\n",
      "takes a singular noun and generates a plural form, though it is not always\n",
      "correct.  (We'll discuss functions at greater length in 4.4.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def plural(word):\n",
      "    if word.endswith('y'):\n",
      "        return word[:-1] + 'ies'\n",
      "    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:\n",
      "        return word + 'es'\n",
      "    elif word.endswith('an'):\n",
      "        return word[:-2] + 'en'\n",
      "    else:\n",
      "        return word + 's'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> plural('fairy')\n",
      "'fairies'\n",
      ">>> plural('woman')\n",
      "'women'\n",
      "\n",
      "\n",
      "Example 3.1 (code_plural.py): Figure 3.1: A Python Function: this function tries to work out the\n",
      "plural form of any English noun; the keyword def (define)\n",
      "is followed by the function name, then a parameter inside\n",
      "parentheses, and a colon; the body of the function is the\n",
      "indented block of code; it tries to recognize patterns\n",
      "within the word and process the word accordingly; e.g., if the\n",
      "word ends with y, delete the y and add ies.\n",
      "\n",
      "The endswith() function is always associated with a string object\n",
      "(e.g., word in 3.1).  To call such functions, we give\n",
      "the name of the object, a period, and then the name of the function.\n",
      "These functions are usually known as methods.\n",
      "\n",
      "\n",
      "3.3   Modules\n",
      "Over time you will find that you create a variety of useful little text processing functions,\n",
      "and you end up copying them from old programs to new ones.  Which file contains the\n",
      "latest version of the function you want to use?\n",
      "It makes life a lot easier if you can collect your work into a single place, and\n",
      "access previously defined functions without making copies.\n",
      "To do this, save your function(s) in a file called (say) text_proc.py.\n",
      "Now, you can access your work simply by importing it from the file:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from text_proc import plural\n",
      ">>> plural('wish')\n",
      "wishes\n",
      ">>> plural('fan')\n",
      "fen\n",
      "\n",
      "\n",
      "\n",
      "Our plural function obviously has an error, since the plural of\n",
      "fan is fans.\n",
      "Instead of typing in a new version of the function, we can\n",
      "simply edit the existing one.  Thus, at every\n",
      "stage, there is only one version of our plural function, and no confusion about\n",
      "which one is being used.\n",
      "A collection of variable and function definitions in a file is called a Python\n",
      "module.  A collection of related modules is called a package.\n",
      "NLTK's code for processing the Brown Corpus is an example of a module,\n",
      "and its collection of code for processing all the different corpora is\n",
      "an example of a package.  NLTK itself is a set of packages, sometimes\n",
      "called a library.\n",
      "\n",
      "Caution!\n",
      "If you are creating a file to contain some of your Python\n",
      "code, do not name your file nltk.py: it may get imported in\n",
      "place of the \"real\" NLTK package.  When it imports modules, Python\n",
      "first looks in the current directory (folder).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4   Lexical Resources\n",
      "A lexicon, or lexical resource, is a collection of words and/or phrases along\n",
      "with associated information such as part of speech and sense definitions.\n",
      "Lexical resources are secondary to texts, and are usually created and enriched with the help\n",
      "of texts.  For example, if we have defined a text my_text, then\n",
      "vocab = sorted(set(my_text)) builds the vocabulary of my_text,\n",
      "while word_freq = FreqDist(my_text) counts the frequency of each word in the text.  Both\n",
      "of vocab and word_freq are simple lexical resources.  Similarly, a concordance\n",
      "like the one we saw in 1\n",
      "gives us information about word usage that might help in the preparation of\n",
      "a dictionary.  Standard terminology for lexicons is illustrated in 4.1.\n",
      "A lexical entry consists of a headword (also known as a lemma)\n",
      "along with additional information such as the part of speech and the sense\n",
      "definition.  Two distinct words having the same spelling are called homonyms.\n",
      "\n",
      "\n",
      "Figure 4.1: Lexicon Terminology: lexical entries for two lemmas\n",
      "having the same spelling (homonyms), providing part of speech\n",
      "and gloss information.\n",
      "\n",
      "The simplest kind of lexicon is nothing more than a sorted list of words.\n",
      "Sophisticated lexicons include complex structure within and across\n",
      "the individual entries.  In this section we'll look at some lexical resources\n",
      "included with NLTK.\n",
      "\n",
      "4.1   Wordlist Corpora\n",
      "\n",
      "NLTK includes some corpora that are nothing more than wordlists.\n",
      "The Words Corpus is the /usr/share/dict/words file from Unix, used by\n",
      "some spell checkers.  We can use it to find unusual or mis-spelt\n",
      "words in a text corpus, as shown in 4.2.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def unusual_words(text):\n",
      "    text_vocab = set(w.lower() for w in text if w.isalpha())\n",
      "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
      "    unusual = text_vocab - english_vocab\n",
      "    return sorted(unusual)\n",
      "\n",
      ">>> unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt'))\n",
      "['abbeyland', 'abhorred', 'abilities', 'abounded', 'abridgement', 'abused', 'abuses',\n",
      "'accents', 'accepting', 'accommodations', 'accompanied', 'accounted', 'accounts',\n",
      "'accustomary', 'aches', 'acknowledging', 'acknowledgment', 'acknowledgments', ...]\n",
      ">>> unusual_words(nltk.corpus.nps_chat.words())\n",
      "['aaaaaaaaaaaaaaaaa', 'aaahhhh', 'abortions', 'abou', 'abourted', 'abs', 'ack',\n",
      "'acros', 'actualy', 'adams', 'adds', 'adduser', 'adjusts', 'adoted', 'adreniline',\n",
      "'ads', 'adults', 'afe', 'affairs', 'affari', 'affects', 'afk', 'agaibn', 'ages', ...]\n",
      "\n",
      "\n",
      "Example 4.2 (code_unusual.py): Figure 4.2: Filtering a Text: this program computes the vocabulary of a text,\n",
      "then removes all items that occur in an existing wordlist,\n",
      "leaving just the uncommon or mis-spelt words.\n",
      "\n",
      "There is also a corpus of stopwords, that is, high-frequency\n",
      "words like the, to and also that we sometimes\n",
      "want to filter out of a document before further processing. Stopwords\n",
      "usually have little lexical content, and their presence in a text fails\n",
      "to distinguish it from other texts.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import stopwords\n",
      ">>> stopwords.words('english')\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
      "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
      "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
      "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
      "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
      "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
      "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
      "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
      "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
      "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
      "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
      "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
      "\n",
      "\n",
      "\n",
      "Let's define a function to compute what fraction of words in a text are not in the\n",
      "stopwords list:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def content_fraction(text):\n",
      "...     stopwords = nltk.corpus.stopwords.words('english')\n",
      "...     content = [w for w in text if w.lower() not in stopwords]\n",
      "...     return len(content) / len(text)\n",
      "...\n",
      ">>> content_fraction(nltk.corpus.reuters.words())\n",
      "0.7364374824583169\n",
      "\n",
      "\n",
      "\n",
      "Thus, with the help of stopwords we filter out over a quarter of the words of the text.\n",
      "Notice that we've combined two different kinds of corpus here, using a lexical\n",
      "resource to filter the content of a text corpus.\n",
      "\n",
      "\n",
      "Figure 4.3: A Word Puzzle: a grid of randomly chosen letters with rules for\n",
      "creating words out of the letters; this puzzle is known as \"Target.\"\n",
      "\n",
      "A wordlist is useful for solving word puzzles, such as the one in 4.3.\n",
      "Our program iterates through every word and, for each one, checks whether it meets\n",
      "the conditions.  It is easy to check obligatory letter \n",
      "and length constraints  (and we'll\n",
      "only look for words with six or more letters here).\n",
      "It is trickier to check that candidate solutions only use combinations of the\n",
      "supplied letters, especially since some of the supplied letters\n",
      "appear twice (here, the letter v).\n",
      "The FreqDist comparison method  permits us to check that\n",
      "the frequency of each letter in the candidate word is less than or equal\n",
      "to the frequency of the corresponding letter in the puzzle.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> puzzle_letters = nltk.FreqDist('egivrvonl')\n",
      ">>> obligatory = 'r'\n",
      ">>> wordlist = nltk.corpus.words.words()\n",
      ">>> [w for w in wordlist if len(w) >= 6 \n",
      "...                      and obligatory in w \n",
      "...                      and nltk.FreqDist(w) <= puzzle_letters] \n",
      "['glover', 'gorlin', 'govern', 'grovel', 'ignore', 'involver', 'lienor',\n",
      "'linger', 'longer', 'lovering', 'noiler', 'overling', 'region', 'renvoi',\n",
      "'revolving', 'ringle', 'roving', 'violer', 'virole']\n",
      "\n",
      "\n",
      "\n",
      "One more wordlist corpus is the Names corpus, containing 8,000 first names categorized by gender.\n",
      "The male and female names are stored in separate files.  Let's find names which appear\n",
      "in both files, i.e. names that are ambiguous for gender:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> names = nltk.corpus.names\n",
      ">>> names.fileids()\n",
      "['female.txt', 'male.txt']\n",
      ">>> male_names = names.words('male.txt')\n",
      ">>> female_names = names.words('female.txt')\n",
      ">>> [w for w in male_names if w in female_names]\n",
      "['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',\n",
      "'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',\n",
      "'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ...]\n",
      "\n",
      "\n",
      "\n",
      "It is well known that names ending in the letter a are almost always female.\n",
      "We can see this and some other patterns in the graph in 4.4,\n",
      "produced by the following code.  Remember that name[-1] is the last letter\n",
      "of name.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (fileid, name[-1])\n",
      "...           for fileid in names.fileids()\n",
      "...           for name in names.words(fileid))\n",
      ">>> cfd.plot()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figure 4.4: Conditional Frequency Distribution: this plot shows the number of female and male names\n",
      "ending with each letter of the alphabet; most names ending with a, e or i\n",
      "are female; names ending in h and l are equally likely to be male or female;\n",
      "names ending in k, o, r, s, and t are likely to be male.\n",
      "\n",
      "\n",
      "\n",
      "4.2   A Pronouncing Dictionary\n",
      "A slightly richer kind of lexical resource is a table (or spreadsheet), containing a word\n",
      "plus some properties in each row.  NLTK includes the CMU Pronouncing\n",
      "Dictionary for US English, which was designed for\n",
      "use by speech synthesizers.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> entries = nltk.corpus.cmudict.entries()\n",
      ">>> len(entries)\n",
      "133737\n",
      ">>> for entry in entries[42371:42379]:\n",
      "...     print(entry)\n",
      "...\n",
      "('fir', ['F', 'ER1'])\n",
      "('fire', ['F', 'AY1', 'ER0'])\n",
      "('fire', ['F', 'AY1', 'R'])\n",
      "('firearm', ['F', 'AY1', 'ER0', 'AA2', 'R', 'M'])\n",
      "('firearm', ['F', 'AY1', 'R', 'AA2', 'R', 'M'])\n",
      "('firearms', ['F', 'AY1', 'ER0', 'AA2', 'R', 'M', 'Z'])\n",
      "('firearms', ['F', 'AY1', 'R', 'AA2', 'R', 'M', 'Z'])\n",
      "('fireball', ['F', 'AY1', 'ER0', 'B', 'AO2', 'L'])\n",
      "\n",
      "\n",
      "\n",
      "For each word, this lexicon provides a list of phonetic\n",
      "codes — distinct labels for each contrastive sound —\n",
      "known as phones.  Observe that fire has two pronunciations\n",
      "(in US English):\n",
      "the one-syllable F AY1 R, and the two-syllable F AY1 ER0.\n",
      "The symbols in the CMU Pronouncing Dictionary are from the Arpabet,\n",
      "described in more detail at http://en.wikipedia.org/wiki/Arpabet\n",
      "\n",
      "Each entry consists of two parts, and we can\n",
      "process these individually using a more complex version of the for statement.\n",
      "Instead of writing for entry in entries:, we replace\n",
      "entry with two variable names, word, pron .\n",
      "Now, each time through the loop, word is assigned the first part of the\n",
      "entry, and pron is assigned the second part of the entry:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> for word, pron in entries: \n",
      "...     if len(pron) == 3: \n",
      "...         ph1, ph2, ph3 = pron \n",
      "...         if ph1 == 'P' and ph3 == 'T':\n",
      "...             print(word, ph2, end=' ')\n",
      "...\n",
      "pait EY1 pat AE1 pate EY1 patt AE1 peart ER1 peat IY1 peet IY1 peete IY1 pert ER1\n",
      "pet EH1 pete IY1 pett EH1 piet IY1 piette IY1 pit IH1 pitt IH1 pot AA1 pote OW1\n",
      "pott AA1 pout AW1 puett UW1 purt ER1 put UH1 putt AH1\n",
      "\n",
      "\n",
      "\n",
      "The above program scans the lexicon looking for entries whose pronunciation consists of\n",
      "three phones .  If the condition is true, it assigns the contents\n",
      "of pron to three new variables ph1, ph2 and ph3.  Notice the unusual\n",
      "form of the statement which does that work .\n",
      "Here's another example of the same for statement, this time used inside a list\n",
      "comprehension.  This program finds all words whose pronunciation ends with a syllable\n",
      "sounding like nicks.  You could use this method to find rhyming words.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> syllable = ['N', 'IH0', 'K', 'S']\n",
      ">>> [word for word, pron in entries if pron[-4:] == syllable]\n",
      "[\"atlantic's\", 'audiotronics', 'avionics', 'beatniks', 'calisthenics', 'centronics',\n",
      "'chamonix', 'chetniks', \"clinic's\", 'clinics', 'conics', 'conics', 'cryogenics',\n",
      "'cynics', 'diasonics', \"dominic's\", 'ebonics', 'electronics', \"electronics'\", ...]\n",
      "\n",
      "\n",
      "\n",
      "Notice that the one pronunciation is spelt in several ways: nics, niks, nix,\n",
      "even ntic's with a silent t, for the word atlantic's.  Let's look for some other\n",
      "mismatches between pronunciation and writing.  Can you summarize the purpose of\n",
      "the following examples and explain how they work?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> [w for w, pron in entries if pron[-1] == 'M' and w[-1] == 'n']\n",
      "['autumn', 'column', 'condemn', 'damn', 'goddamn', 'hymn', 'solemn']\n",
      ">>> sorted(set(w[:2] for w, pron in entries if pron[0] == 'N' and w[0] != 'n'))\n",
      "['gn', 'kn', 'mn', 'pn']\n",
      "\n",
      "\n",
      "\n",
      "The phones contain digits to represent\n",
      "primary stress (1), secondary stress (2) and no stress (0).\n",
      "As our final example, we define a function to extract the stress digits\n",
      "and then scan our lexicon to find words having a particular stress pattern.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def stress(pron):\n",
      "...     return [char for phone in pron for char in phone if char.isdigit()]\n",
      ">>> [w for w, pron in entries if stress(pron) == ['0', '1', '0', '2', '0']]\n",
      "['abbreviated', 'abbreviated', 'abbreviating', 'accelerated', 'accelerating',\n",
      "'accelerator', 'accelerators', 'accentuated', 'accentuating', 'accommodated',\n",
      "'accommodating', 'accommodative', 'accumulated', 'accumulating', 'accumulative', ...]\n",
      ">>> [w for w, pron in entries if stress(pron) == ['0', '2', '0', '1', '0']]\n",
      "['abbreviation', 'abbreviations', 'abomination', 'abortifacient', 'abortifacients',\n",
      "'academicians', 'accommodation', 'accommodations', 'accreditation', 'accreditations',\n",
      "'accumulation', 'accumulations', 'acetylcholine', 'acetylcholine', 'adjudication', ...]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "A subtlety of the above program is that our\n",
      "user-defined function stress() is invoked inside the condition of\n",
      "a list comprehension.  There is also a doubly-nested for loop.\n",
      "There's a lot going on here and you might want\n",
      "to return to this once you've had more experience using list comprehensions.\n",
      "\n",
      "We can use a conditional frequency distribution to help us find minimally-contrasting\n",
      "sets of words.  Here we find all the p-words consisting of three sounds ,\n",
      "and group them according to their first and last sounds .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> p3 = [(pron[0]+'-'+pron[2], word) \n",
      "...       for (word, pron) in entries\n",
      "...       if pron[0] == 'P' and len(pron) == 3] \n",
      ">>> cfd = nltk.ConditionalFreqDist(p3)\n",
      ">>> for template in sorted(cfd.conditions()):\n",
      "...     if len(cfd[template]) > 10:\n",
      "...         words = sorted(cfd[template])\n",
      "...         wordstring = ' '.join(words)\n",
      "...         print(template, wordstring[:70] + \"...\")\n",
      "...\n",
      "P-CH patch pautsch peach perch petsch petsche piche piech pietsch pitch pit...\n",
      "P-K pac pack paek paik pak pake paque peak peake pech peck peek perc perk ...\n",
      "P-L pahl pail paille pal pale pall paul paule paull peal peale pearl pearl...\n",
      "P-N paign pain paine pan pane pawn payne peine pen penh penn pin pine pinn...\n",
      "P-P paap paape pap pape papp paup peep pep pip pipe pipp poop pop pope pop...\n",
      "P-R paar pair par pare parr pear peer pier poor poore por pore porr pour...\n",
      "P-S pace pass pasts peace pearse pease perce pers perse pesce piece piss p...\n",
      "P-T pait pat pate patt peart peat peet peete pert pet pete pett piet piett...\n",
      "P-UW1 peru peugh pew plew plue prew pru prue prugh pshew pugh...\n",
      "\n",
      "\n",
      "\n",
      "Rather than iterating over the whole dictionary, we can also access it\n",
      "by looking up particular words.  We will use Python's dictionary data\n",
      "structure, which we will study systematically in 3.\n",
      "We look up a dictionary by giving its name followed by a key\n",
      "(such as the word 'fire') inside square brackets .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> prondict = nltk.corpus.cmudict.dict()\n",
      ">>> prondict['fire'] \n",
      "[['F', 'AY1', 'ER0'], ['F', 'AY1', 'R']]\n",
      ">>> prondict['blog'] \n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "KeyError: 'blog'\n",
      ">>> prondict['blog'] = [['B', 'L', 'AA1', 'G']] \n",
      ">>> prondict['blog']\n",
      "[['B', 'L', 'AA1', 'G']]\n",
      "\n",
      "\n",
      "\n",
      "If we try to look up a non-existent key , we get a KeyError.\n",
      "This is similar to what happens when we index a list with an\n",
      "integer that is too large, producing an IndexError.\n",
      "The word blog is missing from the pronouncing dictionary,\n",
      "so we tweak our version by assigning a value for this key \n",
      "(this has no effect on the NLTK corpus; next time we access it,\n",
      "blog will still be absent).\n",
      "We can use any lexical resource to process a text, e.g., to filter out words having\n",
      "some lexical property (like nouns), or mapping every word of the text.\n",
      "For example, the following text-to-speech function looks up each word\n",
      "of the text in the pronunciation dictionary.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = ['natural', 'language', 'processing']\n",
      ">>> [ph for w in text for ph in prondict[w][0]]\n",
      "['N', 'AE1', 'CH', 'ER0', 'AH0', 'L', 'L', 'AE1', 'NG', 'G', 'W', 'AH0', 'JH',\n",
      "'P', 'R', 'AA1', 'S', 'EH0', 'S', 'IH0', 'NG']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4.3   Comparative Wordlists\n",
      "Another example of a tabular lexicon is the comparative wordlist.\n",
      "NLTK includes so-called Swadesh wordlists, lists of about 200 common words\n",
      "in several languages.  The languages are identified using an ISO 639 two-letter code.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import swadesh\n",
      ">>> swadesh.fileids()\n",
      "['be', 'bg', 'bs', 'ca', 'cs', 'cu', 'de', 'en', 'es', 'fr', 'hr', 'it', 'la', 'mk',\n",
      "'nl', 'pl', 'pt', 'ro', 'ru', 'sk', 'sl', 'sr', 'sw', 'uk']\n",
      ">>> swadesh.words('en')\n",
      "['I', 'you (singular), thou', 'he', 'we', 'you (plural)', 'they', 'this', 'that',\n",
      "'here', 'there', 'who', 'what', 'where', 'when', 'how', 'not', 'all', 'many', 'some',\n",
      "'few', 'other', 'one', 'two', 'three', 'four', 'five', 'big', 'long', 'wide', ...]\n",
      "\n",
      "\n",
      "\n",
      "We can access cognate words from multiple languages using the entries() method,\n",
      "specifying a list of languages.  With one further step we can convert this into\n",
      "a simple dictionary (we'll learn about dict() in 3).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fr2en = swadesh.entries(['fr', 'en'])\n",
      ">>> fr2en\n",
      "[('je', 'I'), ('tu, vous', 'you (singular), thou'), ('il', 'he'), ...]\n",
      ">>> translate = dict(fr2en)\n",
      ">>> translate['chien']\n",
      "'dog'\n",
      ">>> translate['jeter']\n",
      "'throw'\n",
      "\n",
      "\n",
      "\n",
      "We can make our simple translator more useful by adding other source languages.\n",
      "Let's get the German-English and Spanish-English pairs, convert each to a\n",
      "dictionary using dict(), then update our original translate dictionary\n",
      "with these additional mappings:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> de2en = swadesh.entries(['de', 'en'])    # German-English\n",
      ">>> es2en = swadesh.entries(['es', 'en'])    # Spanish-English\n",
      ">>> translate.update(dict(de2en))\n",
      ">>> translate.update(dict(es2en))\n",
      ">>> translate['Hund']\n",
      "'dog'\n",
      ">>> translate['perro']\n",
      "'dog'\n",
      "\n",
      "\n",
      "\n",
      "We can compare words in various Germanic and Romance languages:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> languages = ['en', 'de', 'nl', 'es', 'fr', 'pt', 'la']\n",
      ">>> for i in [139, 140, 141, 142]:\n",
      "...     print(swadesh.entries(languages)[i])\n",
      "...\n",
      "('say', 'sagen', 'zeggen', 'decir', 'dire', 'dizer', 'dicere')\n",
      "('sing', 'singen', 'zingen', 'cantar', 'chanter', 'cantar', 'canere')\n",
      "('play', 'spielen', 'spelen', 'jugar', 'jouer', 'jogar, brincar', 'ludere')\n",
      "('float', 'schweben', 'zweven', 'flotar', 'flotter', 'flutuar, boiar', 'fluctuare')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4.4   Shoebox and Toolbox Lexicons\n",
      "Perhaps the single most popular tool used by linguists for managing data\n",
      "is Toolbox, previously known as Shoebox since it replaces\n",
      "the field linguist's traditional shoebox full of file cards.\n",
      "Toolbox is freely downloadable from http://www.sil.org/computing/toolbox/.\n",
      "A Toolbox file consists of a collection of entries,\n",
      "where each entry is made up of one or more fields.\n",
      "Most fields are optional or repeatable, which means that this kind of\n",
      "lexical resource cannot be treated as a table or spreadsheet.\n",
      "Here is a dictionary for the Rotokas language.  We see just the first entry,\n",
      "for the word kaa meaning \"to gag\":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import toolbox\n",
      ">>> toolbox.entries('rotokas.dic')\n",
      "[('kaa', [('ps', 'V'), ('pt', 'A'), ('ge', 'gag'), ('tkp', 'nek i pas'),\n",
      "('dcsv', 'true'), ('vx', '1'), ('sc', '???'), ('dt', '29/Oct/2005'),\n",
      "('ex', 'Apoka ira kaaroi aioa-ia reoreopaoro.'),\n",
      "('xp', 'Kaikai i pas long nek bilong Apoka bikos em i kaikai na toktok.'),\n",
      "('xe', 'Apoka is gagging from food while talking.')]), ...]\n",
      "\n",
      "\n",
      "\n",
      "Entries consist of a series of attribute-value pairs, like ('ps', 'V')\n",
      "to indicate that the part-of-speech is 'V' (verb), and ('ge', 'gag')\n",
      "to indicate that the gloss-into-English is 'gag'.\n",
      "The last three pairs contain\n",
      "an example sentence in Rotokas and its translations into Tok Pisin and English.\n",
      "The loose structure of Toolbox files makes it hard for us to do much more with them\n",
      "at this stage.  XML provides a powerful way to process this kind of corpus and\n",
      "we will return to this topic in 11..\n",
      "\n",
      "Note\n",
      "The Rotokas language is spoken on the island of Bougainville, Papua New Guinea.\n",
      "This lexicon was contributed to NLTK by Stuart Robinson.\n",
      "Rotokas is notable for having an inventory of just 12 phonemes (contrastive sounds),\n",
      "http://en.wikipedia.org/wiki/Rotokas_language\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5   WordNet\n",
      "\n",
      "WordNet is a semantically-oriented dictionary of English,\n",
      "similar to a traditional thesaurus but with a richer structure.\n",
      "NLTK includes the English WordNet, with 155,287 words\n",
      "and 117,659 synonym sets.  We'll begin by\n",
      "looking at synonyms and how they are accessed in WordNet.\n",
      "\n",
      "5.1   Senses and Synonyms\n",
      "\n",
      "\n",
      "Consider the sentence in (1a).\n",
      "If we replace the word motorcar in (1a) by automobile,\n",
      "to get (1b), the meaning of the sentence stays pretty much the same:\n",
      "\n",
      "  (1)\n",
      "  a.Benz is credited with the invention of the motorcar.\n",
      "\n",
      "  b.Benz is credited with the invention of the automobile.\n",
      "\n",
      "Since everything else in the sentence has remained unchanged, we can\n",
      "conclude that the words motorcar and automobile have the\n",
      "same meaning, i.e. they are synonyms.  We can explore these\n",
      "words with the help of WordNet:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import wordnet as wn\n",
      ">>> wn.synsets('motorcar')\n",
      "[Synset('car.n.01')]\n",
      "\n",
      "\n",
      "\n",
      "Thus, motorcar has just one possible meaning and it is identified as car.n.01,\n",
      "the first noun sense of car.  The entity car.n.01 is called a synset,\n",
      "or \"synonym set\", a collection of synonymous words (or \"lemmas\"):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> wn.synset('car.n.01').lemma_names()\n",
      "['car', 'auto', 'automobile', 'machine', 'motorcar']\n",
      "\n",
      "\n",
      "\n",
      "Each word of a synset can have several meanings, e.g., car can also signify\n",
      "a train carriage, a gondola, or an elevator car.  However, we are only interested\n",
      "in the single meaning that is common to all words of the above synset.  Synsets\n",
      "also come with a prose definition and some example sentences:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> wn.synset('car.n.01').definition()\n",
      "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'\n",
      ">>> wn.synset('car.n.01').examples()\n",
      "['he needs a car to get to work']\n",
      "\n",
      "\n",
      "\n",
      "Although definitions help humans to understand the intended meaning of a synset,\n",
      "the words of the synset are often more useful for our programs.\n",
      "To eliminate ambiguity, we will identify these words as\n",
      "car.n.01.automobile, car.n.01.motorcar, and so on.\n",
      "This pairing of a synset with a word is called a lemma.\n",
      "We can get all the lemmas for a given synset ,\n",
      "look up a particular lemma ,\n",
      "get the synset corresponding to a lemma ,\n",
      "and get the \"name\" of a lemma :\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> wn.synset('car.n.01').lemmas() \n",
      "[Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'),\n",
      "Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')]\n",
      ">>> wn.lemma('car.n.01.automobile') \n",
      "Lemma('car.n.01.automobile')\n",
      ">>> wn.lemma('car.n.01.automobile').synset() \n",
      "Synset('car.n.01')\n",
      ">>> wn.lemma('car.n.01.automobile').name() \n",
      "'automobile'\n",
      "\n",
      "\n",
      "\n",
      "Unlike the word motorcar, which is unambiguous and has one\n",
      "synset, the word car is ambiguous, having five synsets:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> wn.synsets('car')\n",
      "[Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'),\n",
      "Synset('cable_car.n.01')]\n",
      ">>> for synset in wn.synsets('car'):\n",
      "...     print(synset.lemma_names())\n",
      "...\n",
      "['car', 'auto', 'automobile', 'machine', 'motorcar']\n",
      "['car', 'railcar', 'railway_car', 'railroad_car']\n",
      "['car', 'gondola']\n",
      "['car', 'elevator_car']\n",
      "['cable_car', 'car']\n",
      "\n",
      "\n",
      "\n",
      "For convenience, we can access all the lemmas involving the word car\n",
      "as follows.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> wn.lemmas('car')\n",
      "[Lemma('car.n.01.car'), Lemma('car.n.02.car'), Lemma('car.n.03.car'),\n",
      "Lemma('car.n.04.car'), Lemma('cable_car.n.01.car')]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Write down all the senses of the word dish that you can think of.  Now, explore this\n",
      "word with the help of WordNet, using the same operations we used above.\n",
      "\n",
      "\n",
      "\n",
      "5.2   The WordNet Hierarchy\n",
      "WordNet synsets correspond to abstract concepts, and they don't always\n",
      "have corresponding words in English.  These concepts are linked together in a hierarchy.\n",
      "Some concepts are very general, such as Entity, State, Event — these are called\n",
      "unique beginners or root synsets.  Others, such as gas guzzler and\n",
      "hatchback, are much more specific. A small portion of a concept\n",
      "hierarchy is illustrated in 5.1.\n",
      "\n",
      "\n",
      "Figure 5.1: Fragment of WordNet Concept Hierarchy: nodes correspond to synsets;\n",
      "edges indicate the hypernym/hyponym relation, i.e. the relation between\n",
      "superordinate and subordinate concepts.\n",
      "\n",
      "WordNet makes it easy to navigate between concepts.\n",
      "For example, given a concept like motorcar,\n",
      "we can look at the concepts that are more specific;\n",
      "the (immediate) hyponyms.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> motorcar = wn.synset('car.n.01')\n",
      ">>> types_of_motorcar = motorcar.hyponyms()\n",
      ">>> types_of_motorcar[0]\n",
      "Synset('ambulance.n.01')\n",
      ">>> sorted(lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas())\n",
      "['Model_T', 'S.U.V.', 'SUV', 'Stanley_Steamer', 'ambulance', 'beach_waggon',\n",
      "'beach_wagon', 'bus', 'cab', 'compact', 'compact_car', 'convertible',\n",
      "'coupe', 'cruiser', 'electric', 'electric_automobile', 'electric_car',\n",
      "'estate_car', 'gas_guzzler', 'hack', 'hardtop', 'hatchback', 'heap',\n",
      "'horseless_carriage', 'hot-rod', 'hot_rod', 'jalopy', 'jeep', 'landrover',\n",
      "'limo', 'limousine', 'loaner', 'minicar', 'minivan', 'pace_car', 'patrol_car',\n",
      "'phaeton', 'police_car', 'police_cruiser', 'prowl_car', 'race_car', 'racer',\n",
      "'racing_car', 'roadster', 'runabout', 'saloon', 'secondhand_car', 'sedan',\n",
      "'sport_car', 'sport_utility', 'sport_utility_vehicle', 'sports_car', 'squad_car',\n",
      "'station_waggon', 'station_wagon', 'stock_car', 'subcompact', 'subcompact_car',\n",
      "'taxi', 'taxicab', 'tourer', 'touring_car', 'two-seater', 'used-car', 'waggon',\n",
      "'wagon']\n",
      "\n",
      "\n",
      "\n",
      "We can also navigate up the hierarchy by visiting hypernyms.  Some words\n",
      "have multiple paths, because they can be classified in more than one way.\n",
      "There are two paths between car.n.01 and entity.n.01 because\n",
      "wheeled_vehicle.n.01 can be classified as both a vehicle and a container.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> motorcar.hypernyms()\n",
      "[Synset('motor_vehicle.n.01')]\n",
      ">>> paths = motorcar.hypernym_paths()\n",
      ">>> len(paths)\n",
      "2\n",
      ">>> [synset.name() for synset in paths[0]]\n",
      "['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'artifact.n.01',\n",
      "'instrumentality.n.03', 'container.n.01', 'wheeled_vehicle.n.01',\n",
      "'self-propelled_vehicle.n.01', 'motor_vehicle.n.01', 'car.n.01']\n",
      ">>> [synset.name() for synset in paths[1]]\n",
      "['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'artifact.n.01',\n",
      "'instrumentality.n.03', 'conveyance.n.03', 'vehicle.n.01', 'wheeled_vehicle.n.01',\n",
      "'self-propelled_vehicle.n.01', 'motor_vehicle.n.01', 'car.n.01']\n",
      "\n",
      "\n",
      "\n",
      "We can get the most general hypernyms (or root hypernyms) of\n",
      "a synset as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> motorcar.root_hypernyms()\n",
      "[Synset('entity.n.01')]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Try out NLTK's convenient graphical WordNet browser: nltk.app.wordnet().\n",
      "Explore the WordNet hierarchy by following the hypernym and hyponym links.\n",
      "\n",
      "\n",
      "\n",
      "5.3   More Lexical Relations\n",
      "Hypernyms and hyponyms are called lexical relations because they relate one\n",
      "synset to another.  These two relations navigate up and down the \"is-a\" hierarchy.\n",
      "Another important way to navigate the WordNet network is from items to their\n",
      "components (meronyms) or to the things they are contained in (holonyms).\n",
      "For example, the parts of a tree are its trunk, crown, and so on;\n",
      "the part_meronyms().\n",
      "The substance a tree is made of includes heartwood and sapwood;\n",
      "the substance_meronyms().\n",
      "A collection of trees forms a forest; the member_holonyms():\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> wn.synset('tree.n.01').part_meronyms()\n",
      "[Synset('burl.n.02'), Synset('crown.n.07'), Synset('limb.n.02'),\n",
      "Synset('stump.n.01'), Synset('trunk.n.01')]\n",
      ">>> wn.synset('tree.n.01').substance_meronyms()\n",
      "[Synset('heartwood.n.01'), Synset('sapwood.n.01')]\n",
      ">>> wn.synset('tree.n.01').member_holonyms()\n",
      "[Synset('forest.n.01')]\n",
      "\n",
      "\n",
      "\n",
      "To see just how intricate things can get, consider the word mint, which\n",
      "has several closely-related senses.  We can see that mint.n.04 is part of\n",
      "mint.n.02 and the substance from which mint.n.05 is made.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> for synset in wn.synsets('mint', wn.NOUN):\n",
      "...     print(synset.name() + ':', synset.definition())\n",
      "...\n",
      "batch.n.02: (often followed by `of') a large number or amount or extent\n",
      "mint.n.02: any north temperate plant of the genus Mentha with aromatic leaves and\n",
      "           small mauve flowers\n",
      "mint.n.03: any member of the mint family of plants\n",
      "mint.n.04: the leaves of a mint plant used fresh or candied\n",
      "mint.n.05: a candy that is flavored with a mint oil\n",
      "mint.n.06: a plant where money is coined by authority of the government\n",
      ">>> wn.synset('mint.n.04').part_holonyms()\n",
      "[Synset('mint.n.02')]\n",
      ">>> wn.synset('mint.n.04').substance_holonyms()\n",
      "[Synset('mint.n.05')]\n",
      "\n",
      "\n",
      "\n",
      "There are also relationships between verbs.  For example, the act of walking involves the act of stepping,\n",
      "so walking entails stepping.  Some verbs have multiple entailments:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> wn.synset('walk.v.01').entailments()\n",
      "[Synset('step.v.01')]\n",
      ">>> wn.synset('eat.v.01').entailments()\n",
      "[Synset('chew.v.01'), Synset('swallow.v.01')]\n",
      ">>> wn.synset('tease.v.03').entailments()\n",
      "[Synset('arouse.v.07'), Synset('disappoint.v.01')]\n",
      "\n",
      "\n",
      "\n",
      "Some lexical relationships hold between lemmas, e.g., antonymy:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> wn.lemma('supply.n.02.supply').antonyms()\n",
      "[Lemma('demand.n.02.demand')]\n",
      ">>> wn.lemma('rush.v.01.rush').antonyms()\n",
      "[Lemma('linger.v.04.linger')]\n",
      ">>> wn.lemma('horizontal.a.01.horizontal').antonyms()\n",
      "[Lemma('inclined.a.02.inclined'), Lemma('vertical.a.01.vertical')]\n",
      ">>> wn.lemma('staccato.r.01.staccato').antonyms()\n",
      "[Lemma('legato.r.01.legato')]\n",
      "\n",
      "\n",
      "\n",
      "You can see the lexical relations, and the other methods defined\n",
      "on a synset, using dir(), for example: dir(wn.synset('harmony.n.02')).\n",
      "\n",
      "\n",
      "5.4   Semantic Similarity\n",
      "\n",
      "We have seen that synsets are linked by a complex network of\n",
      "lexical relations.  Given a particular synset, we can traverse\n",
      "the WordNet network to find synsets with related meanings.\n",
      "Knowing which words are semantically related\n",
      "is useful for indexing a collection of texts, so\n",
      "that a search for a general term like vehicle will match documents\n",
      "containing specific terms like limousine.\n",
      "Recall that each synset has one or more hypernym paths that link it\n",
      "to a root hypernym such as entity.n.01.\n",
      "Two synsets linked to the same root may have several hypernyms in common\n",
      "(cf 5.1).\n",
      "If two synsets share a very specific hypernym — one that is low\n",
      "down in the hypernym hierarchy — they must be closely related.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> right = wn.synset('right_whale.n.01')\n",
      ">>> orca = wn.synset('orca.n.01')\n",
      ">>> minke = wn.synset('minke_whale.n.01')\n",
      ">>> tortoise = wn.synset('tortoise.n.01')\n",
      ">>> novel = wn.synset('novel.n.01')\n",
      ">>> right.lowest_common_hypernyms(minke)\n",
      "[Synset('baleen_whale.n.01')]\n",
      ">>> right.lowest_common_hypernyms(orca)\n",
      "[Synset('whale.n.02')]\n",
      ">>> right.lowest_common_hypernyms(tortoise)\n",
      "[Synset('vertebrate.n.01')]\n",
      ">>> right.lowest_common_hypernyms(novel)\n",
      "[Synset('entity.n.01')]\n",
      "\n",
      "\n",
      "\n",
      "Of course we know that whale is very specific (and baleen whale even more so),\n",
      "while vertebrate is more general and entity is completely general.\n",
      "We can quantify this concept of generality by looking up the depth of each synset:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> wn.synset('baleen_whale.n.01').min_depth()\n",
      "14\n",
      ">>> wn.synset('whale.n.02').min_depth()\n",
      "13\n",
      ">>> wn.synset('vertebrate.n.01').min_depth()\n",
      "8\n",
      ">>> wn.synset('entity.n.01').min_depth()\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "Similarity measures have been defined over the collection of WordNet synsets\n",
      "which incorporate the above insight.  For example,\n",
      "path_similarity assigns a score in the range 0–1 based on the shortest path that connects the concepts in the hypernym\n",
      "hierarchy (-1 is returned in those cases where a path cannot be\n",
      "found).  Comparing a synset with itself will return 1.\n",
      "Consider the following similarity scores, relating right whale\n",
      "to minke whale, orca, tortoise, and novel.\n",
      "Although the numbers won't mean much, they decrease as\n",
      "we move away from the semantic space of sea creatures to inanimate objects.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> right.path_similarity(minke)\n",
      "0.25\n",
      ">>> right.path_similarity(orca)\n",
      "0.16666666666666666\n",
      ">>> right.path_similarity(tortoise)\n",
      "0.07692307692307693\n",
      ">>> right.path_similarity(novel)\n",
      "0.043478260869565216\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Several other similarity measures are available; you can type help(wn)\n",
      "for more information.  NLTK also includes VerbNet, a hierarhical verb lexicon linked to WordNet.\n",
      "It can be accessed with nltk.corpus.verbnet.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6   Summary\n",
      "\n",
      "A text corpus is a large, structured collection of texts.  NLTK comes with many corpora,\n",
      "e.g., the Brown Corpus, nltk.corpus.brown.\n",
      "Some text corpora are categorized, e.g., by genre or topic; sometimes the\n",
      "categories of a corpus overlap each other.\n",
      "A conditional frequency distribution is a collection of frequency distributions,\n",
      "each one for a different condition.  They can be used for counting word frequencies,\n",
      "given a context or a genre.\n",
      "Python programs more than a few lines long should be entered using a text editor,\n",
      "saved to a file with a .py extension, and accessed using an import statement.\n",
      "Python functions permit you to associate a name with a particular block of code,\n",
      "and re-use that code as often as necessary.\n",
      "Some functions, known as \"methods\", are associated with an object and we give the object\n",
      "name followed by a period followed by the function, like this: x.funct(y),\n",
      "e.g., word.isalpha().\n",
      "To find out about some variable v,\n",
      "type help(v) in the Python interactive interpreter to read the help entry for this kind of object.\n",
      "WordNet is a semantically-oriented dictionary of English, consisting of synonym sets — or synsets —\n",
      "and organized into a network.\n",
      "Some functions are not available by default, but must be accessed using\n",
      "Python's import statement.\n",
      "\n",
      "\n",
      "\n",
      "7   Further Reading\n",
      "Extra materials for this chapter are posted at http://nltk.org/, including links to freely\n",
      "available resources on the web.  The corpus methods are summarized in the\n",
      "Corpus HOWTO, at http://nltk.org/howto, and documented extensively in the online API documentation.\n",
      "Significant sources of published corpora are the Linguistic Data Consortium (LDC) and\n",
      "the European Language Resources Agency (ELRA).  Hundreds of annotated text and speech\n",
      "corpora are available in dozens of languages.  Non-commercial licences permit the data to\n",
      "be used in teaching and research.  For some corpora, commercial licenses are also available\n",
      "(but for a higher fee).\n",
      "A good tool for creating annotated text corpora is called Brat,\n",
      "and available from http://brat.nlplab.org/.\n",
      "These and many other language resources have been documented using OLAC Metadata, and can\n",
      "be searched via the OLAC homepage at http://www.language-archives.org/.  Corpora List is a mailing list\n",
      "for discussions about corpora, and you can find resources by searching the list archives\n",
      "or posting to the list.\n",
      "The most complete inventory of the world's languages is Ethnologue, http://www.ethnologue.com/.\n",
      "Of 7,000 languages, only a few dozen have substantial digital resources suitable for\n",
      "use in NLP.\n",
      "This chapter has touched on the field of Corpus Linguistics.  Other useful books in this\n",
      "area include (Biber, Conrad, & Reppen, 1998), (McEnery, 2006), (Meyer, 2002), (Sampson & McCarthy, 2005), (Scott & Tribble, 2006).\n",
      "Further readings in quantitative data analysis in linguistics are:\n",
      "(Baayen, 2008), (Gries, 2009), (Woods, Fletcher, & Hughes, 1986).\n",
      "The original description of WordNet is (Fellbaum, 1998).\n",
      "Although WordNet was originally developed for research\n",
      "in psycholinguistics, it is now widely used in NLP and Information Retrieval.\n",
      "WordNets are being developed for many other languages, as documented\n",
      "at http://www.globalwordnet.org/.\n",
      "For a study of WordNet similarity measures, see (Budanitsky & Hirst, 2006).\n",
      "Other topics touched on in this chapter were phonetics and lexical semantics,\n",
      "and we refer readers to chapters 7 and 20 of (Jurafsky & Martin, 2008).\n",
      "\n",
      "\n",
      "8   Exercises\n",
      "\n",
      "☼ Create a variable phrase containing a list of words.\n",
      "Review the operations described in the previous chapter, including addition,\n",
      "multiplication, indexing, slicing, and sorting.\n",
      "☼ Use the corpus module to explore austen-persuasion.txt.\n",
      "How many word tokens does this book have?  How many word types?\n",
      "☼ Use the Brown corpus reader nltk.corpus.brown.words() or the Web text corpus\n",
      "reader nltk.corpus.webtext.words() to access some sample text in two different genres.\n",
      "☼ Read in the texts of the State of the Union addresses, using the\n",
      "state_union corpus reader.  Count occurrences of men, women,\n",
      "and people in each document.  What has happened to the usage of these\n",
      "words over time?\n",
      "☼ Investigate the holonym-meronym relations for some nouns.\n",
      "Remember that there are three kinds of holonym-meronym relation,\n",
      "so you need to use:\n",
      "member_meronyms(), part_meronyms(), substance_meronyms(),\n",
      "member_holonyms(), part_holonyms(), and substance_holonyms().\n",
      "☼ In the discussion of comparative wordlists, we created an object\n",
      "called translate which you could look up using words in both German and Spanish\n",
      "in order to get corresponding words in English.\n",
      "What problem might arise with this approach?\n",
      "Can you suggest a way to avoid this problem?\n",
      "☼ According to Strunk and White's Elements of Style,\n",
      "the word however, used at the start of a sentence,\n",
      "means \"in whatever way\" or \"to whatever extent\", and not\n",
      "\"nevertheless\".  They give this example of correct usage:\n",
      "However you advise him, he will probably do as he thinks best.\n",
      "(http://www.bartleby.com/141/strunk3.html)\n",
      "Use the concordance tool to study actual usage of this word\n",
      "in the various texts we have been considering.\n",
      "See also the LanguageLog posting \"Fossilized prejudices about 'however'\"\n",
      "at http://itre.cis.upenn.edu/~myl/languagelog/archives/001913.html\n",
      "◑ Define a conditional frequency distribution over the Names corpus\n",
      "that allows you to see which initial letters are more frequent for males\n",
      "vs. females (cf. 4.4).\n",
      "◑ Pick a pair of texts and study the differences between them,\n",
      "in terms of vocabulary, vocabulary richness, genre, etc.  Can you\n",
      "find pairs of words which have quite different meanings across the\n",
      "two texts, such as monstrous in Moby Dick and in Sense and Sensibility?\n",
      "◑ Read the BBC News article: UK's Vicky Pollards 'left behind' http://news.bbc.co.uk/1/hi/education/6173441.stm.\n",
      "The article gives the following statistic about teen language:\n",
      "\"the top 20 words used, including yeah, no, but and like, account for around a third of all words.\"\n",
      "How many word types account for a third\n",
      "of all word tokens, for a variety of text sources?  What do you conclude about this statistic?\n",
      "Read more about this on LanguageLog, at http://itre.cis.upenn.edu/~myl/languagelog/archives/003993.html.\n",
      "◑ Investigate the table of modal distributions and look for other patterns.\n",
      "Try to explain them in terms of your own impressionistic understanding\n",
      "of the different genres.  Can you find other closed classes of words that\n",
      "exhibit significant differences across different genres?\n",
      "◑ The CMU Pronouncing Dictionary contains multiple pronunciations\n",
      "for certain words.  How many distinct words does it contain?  What fraction\n",
      "of words in this dictionary have more than one possible pronunciation?\n",
      "◑ What percentage of noun synsets have no hyponyms?\n",
      "You can get all noun synsets using wn.all_synsets('n').\n",
      "◑ Define a function supergloss(s) that takes a synset s as its argument\n",
      "and returns a string consisting of the concatenation of the definition of s, and\n",
      "the definitions of all the hypernyms and hyponyms of s.\n",
      "◑ Write a program to find all words that occur at least three times in the Brown Corpus.\n",
      "◑ Write a program to generate a table of lexical diversity scores (i.e. token/type ratios), as we saw in\n",
      "1.1.  Include the full set of Brown Corpus genres (nltk.corpus.brown.categories()).\n",
      "Which genre has the lowest diversity (greatest number of tokens per type)?\n",
      "Is this what you would have expected?\n",
      "◑ Write a function that finds the 50 most frequently occurring words\n",
      "of a text that are not stopwords.\n",
      "◑ Write a program to print the 50 most frequent bigrams\n",
      "(pairs of adjacent words) of a text, omitting bigrams that contain stopwords.\n",
      "◑ Write a program to create a table of word frequencies by genre,\n",
      "like the one given in 1 for modals.\n",
      "Choose your own words and try to find words whose presence\n",
      "(or absence) is typical of a genre.  Discuss your findings.\n",
      "◑ Write a function word_freq() that takes a word and the name of a section\n",
      "of the Brown Corpus as arguments, and computes the frequency of the word\n",
      "in that section of the corpus.\n",
      "◑ Write a program to guess the number of syllables contained in a text,\n",
      "making use of the CMU Pronouncing Dictionary.\n",
      "◑ Define a function hedge(text) which processes a\n",
      "text and produces a new version with the word\n",
      "'like' between every third word.\n",
      "★ Zipf's Law:\n",
      "Let f(w) be the frequency of a word w in free text. Suppose that\n",
      "all the words of a text are ranked according to their frequency,\n",
      "with the most frequent word first. Zipf's law states that the\n",
      "frequency of a word type is inversely proportional to its rank\n",
      "(i.e. f × r = k, for some constant k). For example, the 50th most\n",
      "common word type should occur three times as frequently as the\n",
      "150th most common word type.\n",
      "Write a function to process a large text and plot word\n",
      "frequency against word rank using pylab.plot. Do\n",
      "you confirm Zipf's law? (Hint: it helps to use a logarithmic scale).\n",
      "What is going on at the extreme ends of the plotted line?\n",
      "Generate random text, e.g., using random.choice(\"abcdefg \"),\n",
      "taking care to include the space character.  You will need to\n",
      "import random first.  Use the string\n",
      "concatenation operator to accumulate characters into a (very)\n",
      "long string.  Then tokenize this string, and generate the Zipf\n",
      "plot as before, and compare the two plots.  What do you make of\n",
      "Zipf's Law in the light of this?\n",
      "\n",
      "\n",
      "★ Modify the text generation program in 2.2 further, to\n",
      "do the following tasks:\n",
      "Store the n most likely words in a list words then randomly\n",
      "choose a word from the list using random.choice().  (You will need\n",
      "to import random first.)\n",
      "Select a particular genre, such as a section of the Brown Corpus,\n",
      "or a genesis translation, one of the Gutenberg texts, or one of the Web texts.  Train\n",
      "the model on this corpus and get it to generate random text.  You\n",
      "may have to experiment with different start words. How intelligible\n",
      "is the text?  Discuss the strengths and weaknesses of this method of\n",
      "generating random text.\n",
      "Now train your system using two distinct genres and experiment\n",
      "with generating text in the hybrid genre.  Discuss your observations.\n",
      "\n",
      "\n",
      "★ Define a function find_language() that takes a string\n",
      "as its argument, and returns a list of languages that have that\n",
      "string as a word.  Use the udhr corpus and limit your searches\n",
      "to files in the Latin-1 encoding.\n",
      "★ What is the branching factor of the noun hypernym hierarchy?\n",
      "I.e. for every noun synset that has hyponyms — or children in the\n",
      "hypernym hierarchy — how many do they have on average?\n",
      "You can get all noun synsets using wn.all_synsets('n').\n",
      "★ The polysemy of a word is the number of senses it has.\n",
      "Using WordNet, we can determine that the noun dog has 7 senses\n",
      "with: len(wn.synsets('dog', 'n')).\n",
      "Compute the average polysemy of nouns, verbs, adjectives and\n",
      "adverbs according to WordNet.\n",
      "★ Use one of the predefined similarity measures to score\n",
      "the similarity of each of the following pairs of words.\n",
      "Rank the pairs in order of decreasing similarity.\n",
      "How close is your ranking to the order given here,\n",
      "an order that was established experimentally\n",
      "by (Miller & Charles, 1998):\n",
      "car-automobile, gem-jewel, journey-voyage, boy-lad,\n",
      "coast-shore, asylum-madhouse, magician-wizard, midday-noon,\n",
      "furnace-stove, food-fruit, bird-cock, bird-crane, tool-implement,\n",
      "brother-monk, lad-brother, crane-implement, journey-car,\n",
      "monk-oracle, cemetery-woodland, food-rooster, coast-hill,\n",
      "forest-graveyard, shore-woodland, monk-slave, coast-forest,\n",
      "lad-wizard, chord-smile, glass-magician, rooster-voyage, noon-string.\n",
      "\n",
      "\n",
      "\n",
      "About this document...\n",
      "UPDATED FOR NLTK 3.0.\n",
      "This is a chapter from Natural Language Processing with Python,\n",
      "by Steven Bird, Ewan Klein and Edward Loper,\n",
      "Copyright © 2019 the authors.\n",
      "It is distributed with the Natural Language Toolkit [http://nltk.org/],\n",
      "Version 3.0, under the terms of the\n",
      "Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\n",
      "[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\n",
      "This document was built on\n",
      "Wed  4 Sep 2019 11:40:48 ACST\n",
      "vectors[2] = [0, 10, 4, 50, 13, 43, 4, 19, 0, 0, 4, 33, 21, 28, 2, 2, 1, 0, 2, 0, 0, 0, 1, 2, 2, 1, 0, 1, 8, 17, 3, 3, 1, 10, 1, 10, 0, 0, 40, 1, 9, 7, 4, 269, 49, 3, 12, 4, 1, 2, 1, 2, 0, 1, 2, 1, 11, 9, 0, 1, 3, 0, 0, 0, 39, 5, 0, 3, 0, 135, 0, 0, 6, 2, 1, 19, 12, 0, 0, 3, 3, 1, 15, 0, 0, 0, 5, 5, 1, 0, 1, 2, 0, 6, 1, 203, 0, 1, 2, 19, 3, 1, 0, 1, 1, 2, 5, 0, 1, 0, 5, 4, 5, 0, 1, 0, 2, 0, 0, 0, 188, 0, 1, 6, 2, 0, 0, 0, 1, 27, 2, 3, 7, 1, 16, 1, 273, 104, 38, 9, 0, 0, 19, 2, 3, 3, 21, 21, 16, 7, 39, 1, 2, 0, 1, 0, 0, 0, 0, 9, 2, 9, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 6, 10, 3, 1, 1, 0, 3, 0, 0, 0, 0, 0, 1, 0, 2, 1, 16, 12, 49, 0, 1, 0, 6, 5, 11, 27, 0, 0, 6, 1, 132, 1, 9, 1, 8, 0, 6, 1025, 1, 0, 12, 0, 0, 3, 2, 11, 7, 26, 42, 20, 1, 0, 0, 0, 0, 48, 18, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 14, 1, 4, 1, 0, 0, 0, 1, 2, 4, 91, 1, 1, 0, 0, 0, 0, 1, 0, 9, 25, 3, 0, 0, 0, 2, 1, 0, 2, 2, 20, 0, 0, 0, 0, 1, 1, 7, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 8, 0, 4, 0, 14, 0, 19, 0, 0, 1, 0, 2, 2, 2, 1, 0, 9, 83, 0, 13, 0, 0, 0, 1, 0, 1, 25, 5, 1, 0, 0, 1, 4, 0, 0, 11, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 21, 0, 1, 1, 0, 0, 7, 1, 1, 1, 0, 0, 0, 0, 0, 0, 3, 1, 7, 0, 10, 0, 4, 1, 3, 9, 0, 0, 0, 2, 5, 5, 0, 37, 8, 2, 47, 97, 0, 10, 0, 3, 0, 1, 3, 1, 1, 0, 0, 0, 2, 0, 22, 9, 4, 9, 16, 0, 19, 2, 3, 0, 9, 11, 8, 1, 0, 0, 1, 8, 0, 0, 0, 0, 1, 1, 0, 0, 0, 31, 9, 1, 1, 3, 1, 0, 2, 0, 0, 3, 0, 0, 0, 0, 4, 19, 0, 3, 0, 0, 11, 0, 4, 16, 0, 7, 3, 0, 0, 0, 2, 22, 0, 15, 0, 1, 0, 0, 6, 0, 0, 0, 0, 2, 0, 0, 10, 2, 2, 0, 9, 1, 1, 0, 2, 0, 5, 0, 2, 12, 7, 44, 69, 16, 5, 0, 226, 5, 2, 91, 2, 3, 3, 173, 3, 0, 1, 0, 0, 5, 0, 7, 0, 0, 0, 2, 11, 2, 14, 14, 1, 11, 1, 2, 9, 8, 2, 93, 1, 0, 2, 26, 12, 3, 3, 0, 1, 1, 52, 4, 0, 1, 2, 0, 0, 4, 0, 1, 9, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 5, 0, 3, 0, 1, 0, 0, 0, 1, 0, 0, 5, 2, 3, 1, 0, 0, 4, 1, 1, 3, 0, 0, 0, 0, 22, 0, 3, 1, 0, 0, 2, 1, 1, 1, 0, 15, 13, 0, 1, 0, 4, 2, 1, 1, 1, 7, 9, 59, 1, 9, 1, 6, 1, 2, 1, 9, 18, 0, 4, 4, 0, 2, 2, 2, 0, 0, 11, 0, 2, 1, 0, 0, 30, 4, 0, 0, 9, 3, 54, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 1, 0, 0, 0, 1, 0, 0, 7, 0, 0, 0, 2, 2, 0, 0, 0, 10, 2, 1, 1, 4, 6, 7, 0, 1, 0, 0, 16, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 4, 3, 0, 0, 0, 4, 1, 2, 0, 0, 1, 0, 4, 4, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 11, 2, 7, 0, 0, 1, 0, 8, 2, 0, 3, 0, 0, 0, 0, 3, 0, 4, 0, 12, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 112, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 1, 5, 1, 0, 1, 5, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 6, 0, 30, 1, 0, 0, 8, 1, 2, 4, 1, 0, 0, 0, 0, 3, 2, 1, 0, 0, 0, 0, 5, 3, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 10, 0, 3, 0, 6, 7, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 1, 2, 0, 0, 0, 19, 0, 0, 0, 0, 0, 1, 4, 2, 0, 12, 2, 0, 1, 0, 2, 0, 0, 3, 0, 0, 1, 3, 0, 3, 0, 2, 1, 4, 0, 0, 0, 1, 0, 0, 0, 0, 3, 2, 8, 1, 0, 0, 0, 0, 1, 0, 43, 0, 0, 2, 2, 1, 0, 5, 2, 0, 0, 0, 1, 0, 1, 1, 0, 0, 2, 2, 4, 4, 0, 0, 2, 5, 0, 2, 4, 2, 7, 2, 0, 0, 0, 0, 28, 1, 2, 0, 0, 1, 0, 0, 5, 0, 25, 16, 0, 0, 1, 0, 1, 0, 3, 2, 0, 0, 0, 4, 1, 0, 0, 0, 0, 6, 0, 0, 1, 0, 6, 2, 1, 0, 0, 4, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 10, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 0, 0, 0, 1, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 13, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 18, 9, 8, 1, 3, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 2, 1, 3, 1, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 11, 0, 0, 1, 14, 1, 1, 2, 2, 65, 1, 1, 8, 1, 1, 2, 5, 3, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 0, 2, 0, 0, 1, 1, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 5, 1, 1, 8, 0, 5, 1, 0, 0, 2, 0, 18, 0, 0, 5, 0, 2, 1, 0, 6, 16, 1, 1, 6, 0, 2, 79, 0, 0, 0, 0, 0, 0, 2, 0, 12, 0, 0, 2, 0, 0, 1, 2, 5, 0, 4, 0, 2, 0, 1, 32, 5, 0, 1, 0, 13, 3, 0, 11, 1, 0, 0, 0, 0, 7, 5, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 4, 0, 0, 1, 1, 1, 0, 0, 0, 2, 1, 2, 0, 0, 5, 0, 0, 0, 3, 1, 0, 5, 2, 0, 0, 0, 2, 3, 0, 1, 4, 2, 0, 0, 1, 4, 14, 0, 0, 0, 1, 1, 3, 1, 2, 2, 5, 2, 0, 0, 0, 9, 1, 2, 0, 13, 5, 0, 0, 1, 2, 0, 0, 0, 0, 0, 15, 1, 1, 0, 2, 0, 0, 1, 1, 15, 0, 0, 6, 0, 17, 0, 0, 4, 0, 1, 0, 1, 1, 1, 0, 1, 1, 2, 16, 6, 0, 5, 1, 4, 2, 1, 0, 4, 3, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 2, 0, 3, 0, 0, 0, 2, 0, 0, 0, 7, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 12, 0, 1, 0, 0, 17, 3, 1, 3, 0, 0, 0, 3, 1, 0, 3, 0, 1, 0, 1, 0, 0, 0, 0, 2, 1, 1, 4, 0, 6, 0, 3, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 24, 0, 6, 7, 1, 4, 3, 0, 0, 0, 20, 0, 1, 2, 1, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 8, 3, 2, 0, 0, 0, 0, 4, 10, 2, 6, 3, 4, 2, 0, 4, 2, 0, 0, 3, 0, 0, 4, 0, 0, 0, 3, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 3, 0, 0, 14, 0, 2, 2, 0, 0, 1, 11, 0, 1, 25, 0, 0, 0, 6, 0, 0, 4, 7, 0, 1, 0, 0, 0, 7, 16, 0, 961, 0, 0, 0, 10, 0, 6, 1, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 0, 0, 0, 0, 0, 2, 1, 0, 0, 4, 0, 11, 6, 0, 0, 3, 0, 4, 3, 0, 4, 8, 1, 3, 1, 0, 0, 0, 0, 0, 1, 0, 4, 11, 3, 9, 0, 1, 0, 1, 0, 0, 0, 10, 4, 0, 1, 0, 3, 1, 0, 0, 2, 50, 0, 1, 0, 11, 4, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 49, 4, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 1, 0, 19, 1, 1, 0, 4, 0, 0, 0, 38, 1, 2, 4, 1, 96, 97, 0, 3, 1, 0, 0, 0, 0, 1, 2, 3, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 24, 0, 0, 1, 0, 1, 48, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 34, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 3, 1, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 1, 1, 0, 2, 0, 0, 1, 0, 2, 7, 0, 4, 1, 0, 0, 0, 0, 1, 0, 0, 5, 1, 0, 0, 2, 0, 0, 1, 2, 1, 2, 0, 2, 0, 1, 3, 0, 4, 2, 3, 0, 0, 0, 0, 6, 19, 1, 0, 0, 0, 0, 0, 6, 1, 0, 0, 0, 1, 5, 2, 0, 0, 0, 0, 2, 0, 1, 1, 2, 2, 0, 4, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 6, 19, 4, 0, 5, 0, 0, 1, 1, 0, 1, 0, 0, 0, 3, 8, 1, 0, 0, 3, 5, 0, 1, 0, 1, 1, 6, 11, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 3, 1, 0, 0, 0, 0, 0, 4, 4, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 1, 1, 4, 46, 0, 2, 2, 18, 2, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 38, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 3, 0, 0, 6, 0, 4, 0, 0, 0, 0, 7, 23, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 5, 0, 0, 0, 0, 2, 0, 0, 0, 4, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 11, 0, 0, 0, 3, 2, 0, 9, 0, 3, 0, 0, 1, 1, 3, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 10, 0, 0, 3, 1, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 12, 0, 1, 1, 1, 29, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 3, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 6, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 14, 1, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 26, 2, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 0, 1, 3, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 1, 0, 9, 0, 0, 2, 0, 4, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 5, 0, 0, 1, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 8, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 3, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 9, 9, 0, 0, 4, 1, 0, 6, 1, 3, 0, 0, 0, 6, 0, 0, 0, 0, 9, 10, 5, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 6, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 2, 3, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 1, 1, 1, 3, 1, 3, 5, 1, 27, 3, 5, 6, 1, 33, 14, 4, 2, 6, 3, 3, 2, 6, 5, 2, 2, 2, 2, 2, 2, 9, 2, 2, 4, 2, 1, 2, 1, 2, 1, 1, 1, 1, 27, 2, 1, 3, 4, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 6, 1, 1, 1, 2, 6, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, 3, 2, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 5, 2, 2, 2, 4, 1, 1, 1, 4, 3, 1, 2, 1, 1, 1, 1, 7, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 56, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 41, 5, 5, 3, 3, 5, 2, 4, 1, 1, 1, 9, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 1, 29, 16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 18, 1, 1, 3, 1, 8, 1, 2, 1, 1, 1, 2, 1, 1, 7, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 5, 2, 1, 1, 1, 1, 1, 4, 4, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 4, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 14, 1, 4, 1, 1, 1, 1, 6, 1, 1, 1, 1, 14, 1, 2, 1, 1, 2, 2, 1, 1, 1, 6, 7, 1, 35, 6, 1, 1, 1, 1, 1, 1, 1, 5, 1, 4, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 8, 1, 2, 1, 13, 1, 1, 9, 1, 4, 8, 1, 1, 4, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 8, 1, 1, 1, 1, 2, 2, 6, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 11, 2, 1, 1, 4, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 4, 2, 2, 2, 3, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 3, 6, 3, 3, 3, 1, 1, 10, 1, 3, 1, 5, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 4, 2, 3, 9, 3, 6, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 3, 3, 3, 1, 1, 7, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 6, 1, 5, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 3, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 4, 1, 6, 1, 1, 1, 1, 3, 10, 2, 1, 4, 6, 3, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 15, 4, 1, 1, 1, 1, 1, 4, 1, 2, 2, 2, 1, 3, 9, 2, 3, 1, 1, 1, 1, 2, 2, 1, 2, 3, 1, 1, 1, 2, 1, 1, 1, 2, 1, 4, 3, 1, 2, 1, 38, 2, 2, 1, 1, 2, 3, 3, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 4, 2, 1, 2, 1, 4, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 4, 11, 7, 4, 4, 1, 1, 1, 1, 2, 8, 6, 2, 1, 1, 23, 3, 3, 3, 2, 2, 2, 4, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 5, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 4, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 9, 3, 5, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 4, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 2, 1, 1, 1, 1, 2, 2, 3, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 3, 1, 1, 1, 1, 5, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 5, 2, 4, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 16, 13, 1, 1, 2, 2, 2, 1, 1, 41, 101, 1, 2, 3, 1, 3, 1, 2, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 16, 7, 7, 3, 1, 1, 1, 1, 1, 11, 1, 1, 1, 2, 16, 8, 9, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 3, 1, 2, 2, 2, 2, 2, 1, 1, 1, 3, 3, 2, 2, 3, 2, 2, 3, 4, 3, 1, 2, 1, 1, 1, 17, 3, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 8, 1, 1, 2, 1, 1, 5, 4, 1, 5, 4, 2, 3, 1, 1, 1, 4, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 5, 4, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 3, 3, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 3, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "docs[3] = ch03.rst2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3   Processing Raw Text\n",
      "The most important source of texts is undoubtedly the Web.  It's convenient\n",
      "to have existing text collections to explore, such as the corpora we saw\n",
      "in the previous chapters.  However, you probably have your own text sources\n",
      "in mind, and need to learn how to access them.\n",
      "The goal of this chapter is to answer the following questions:\n",
      "\n",
      "How can we write programs to access text from local files and\n",
      "from the web, in order to get hold of an unlimited range of\n",
      "language material?\n",
      "How can we split documents up into individual words and\n",
      "punctuation symbols, so we can carry out the same kinds of\n",
      "analysis we did with text corpora in earlier chapters?\n",
      "How can we write programs to produce formatted output\n",
      "and save it in a file?\n",
      "\n",
      "In order to address these questions, we will be covering\n",
      "key concepts in NLP, including tokenization and stemming.\n",
      "Along the way you will consolidate your Python knowledge and\n",
      "learn about strings, files, and regular expressions.  Since\n",
      "so much text on the web is in HTML format, we will also\n",
      "see how to dispense with markup.\n",
      "\n",
      "Note\n",
      "Important:\n",
      "From this chapter onwards, our program samples will assume you\n",
      "begin your interactive session or your program with the following import\n",
      "statements:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from __future__ import division  # Python 2 users only\n",
      ">>> import nltk, re, pprint\n",
      ">>> from nltk import word_tokenize\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.1   Accessing Text from the Web and from Disk\n",
      "\n",
      "Electronic Books\n",
      "A small sample of texts from Project Gutenberg appears in the NLTK corpus collection.\n",
      "However, you may be interested in analyzing other texts from Project Gutenberg.\n",
      "You can browse the catalog of 25,000 free online books at\n",
      "http://www.gutenberg.org/catalog/, and obtain a URL to an ASCII text file.\n",
      "Although 90% of the texts in Project Gutenberg are in English, it\n",
      "includes material in over 50 other languages, including Catalan, Chinese, Dutch,\n",
      "Finnish, French, German, Italian, Portuguese and Spanish (with more than\n",
      "100 texts each).\n",
      "Text number 2554 is an English translation of Crime and Punishment,\n",
      "and we can access it as follows.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from urllib import request\n",
      ">>> url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
      ">>> response = request.urlopen(url)\n",
      ">>> raw = response.read().decode('utf8')\n",
      ">>> type(raw)\n",
      "<class 'str'>\n",
      ">>> len(raw)\n",
      "1176893\n",
      ">>> raw[:75]\n",
      "'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "The read() process will take a few seconds as it downloads this large book.\n",
      "If you're using an internet proxy which is not correctly detected by Python,\n",
      "you may need to specify the proxy manually, before using urlopen, as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> proxies = {'http': 'http://www.someproxy.com:3128'}\n",
      ">>> request.ProxyHandler(proxies)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The variable raw contains a string with 1,176,893 characters.\n",
      "(We can see that it is a string, using type(raw).)\n",
      "This is the raw content of the book,\n",
      "including many details we are not interested in such as\n",
      "whitespace, line breaks and blank lines.  Notice the \\r and \\n\n",
      "in the opening line of the file, which is how Python displays the\n",
      "special carriage return and line feed characters (the file must\n",
      "have been created on a Windows machine).  For our language\n",
      "processing, we want to break up the string into\n",
      "words and punctuation, as we saw in 1..  This step is\n",
      "called tokenization, and it produces our familiar structure, a list of words\n",
      "and punctuation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tokens = word_tokenize(raw)\n",
      ">>> type(tokens)\n",
      "<class 'list'>\n",
      ">>> len(tokens)\n",
      "254354\n",
      ">>> tokens[:10]\n",
      "['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']\n",
      "\n",
      "\n",
      "\n",
      "Notice that NLTK was needed for tokenization, but not for any of the\n",
      "earlier tasks of opening a URL and reading it into a string.\n",
      "If we now take the further step of creating an NLTK text from this\n",
      "list, we can carry out all of the other linguistic processing we saw\n",
      "in 1., along with the regular list operations\n",
      "like slicing:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = nltk.Text(tokens)\n",
      ">>> type(text)\n",
      "<class 'nltk.text.Text'>\n",
      ">>> text[1024:1062]\n",
      "['CHAPTER', 'I', 'On', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in',\n",
      " 'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in',\n",
      " 'which', 'he', 'lodged', 'in', 'S.', 'Place', 'and', 'walked', 'slowly',\n",
      " ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K.', 'bridge', '.']\n",
      ">>> text.collocations()\n",
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
      "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
      "great deal; Nikodim Fomitch; young man; Ilya Petrovitch; n't know;\n",
      "Project Gutenberg; Dmitri Prokofitch; Andrey Semyonovitch; Hay Market\n",
      "\n",
      "\n",
      "\n",
      "Notice that Project Gutenberg appears as a collocation.\n",
      "This is because each text downloaded from Project Gutenberg contains a header with the\n",
      "name of the text, the author, the names of people who scanned and\n",
      "corrected the text, a license, and so on.  Sometimes this information\n",
      "appears in a footer at the end of the file.  We cannot reliably detect\n",
      "where the content begins and ends, and so have to resort to manual\n",
      "inspection of the file, to discover unique strings that mark the beginning\n",
      "and the end, before trimming raw to be just the content and nothing else:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> raw.find(\"PART I\")\n",
      "5338\n",
      ">>> raw.rfind(\"End of Project Gutenberg's Crime\")\n",
      "1157743\n",
      ">>> raw = raw[5338:1157743] \n",
      ">>> raw.find(\"PART I\")\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "The find() and rfind() (\"reverse find\") methods help us get\n",
      "the right index values to use for slicing the string .\n",
      "We overwrite raw with this slice, so now it begins\n",
      "with \"PART I\" and goes up to (but not including)\n",
      "the phrase that marks the end of the content.\n",
      "This was our first brush with the reality of the web:\n",
      "texts found on the web may contain unwanted material,\n",
      "and there may not be an automatic way to remove it.\n",
      "But with a small amount of extra work we can extract the material we need.\n",
      "\n",
      "\n",
      "Dealing with HTML\n",
      "Much of the text on the web is in the form of HTML documents.\n",
      "You can use a web browser to save a page as text to a local\n",
      "file, then access this as described in the section on files below.\n",
      "However, if you're going to do this often, it's easiest to get Python\n",
      "to do the work directly.  The first step is the same as before,\n",
      "using urlopen.  For fun we'll pick a BBC News story\n",
      "called Blondes to die out in 200 years, an urban legend\n",
      "passed along by the BBC as established scientific fact:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
      ">>> html = request.urlopen(url).read().decode('utf8')\n",
      ">>> html[:60]\n",
      "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN'\n",
      "\n",
      "\n",
      "\n",
      "You can type print(html) to see the HTML content in all its glory,\n",
      "including meta tags, an image map, JavaScript, forms, and tables.\n",
      "To get text out of HTML we will use a Python library called BeautifulSoup,\n",
      "available from http://www.crummy.com/software/BeautifulSoup/:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from bs4 import BeautifulSoup\n",
      ">>> raw = BeautifulSoup(html, 'html.parser').get_text()\n",
      ">>> tokens = word_tokenize(raw)\n",
      ">>> tokens\n",
      "['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', \"'to\", 'die', 'out', ...]\n",
      "\n",
      "\n",
      "\n",
      "This still contains unwanted material concerning site navigation and related\n",
      "stories.  With some trial and error you can find the start and end indexes of the\n",
      "content and select the tokens of interest, and initialize a text as before.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tokens = tokens[110:390]\n",
      ">>> text = nltk.Text(tokens)\n",
      ">>> text.concordance('gene')\n",
      "Displaying 5 of 5 matches:\n",
      "hey say too few people now carry the gene for blondes to last beyond the next\n",
      "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
      "have blonde hair , it must have the gene on both sides of the family in the g\n",
      "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
      "des would disappear is if having the gene was a disadvantage and I do not thin\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Processing Search Engine Results\n",
      "The web can be thought of as a huge corpus of unannotated text.  Web\n",
      "search engines provide an efficient means of searching this large\n",
      "quantity of text for relevant linguistic examples.  The main advantage\n",
      "of search engines is size: since you are searching such a large set of\n",
      "documents, you are more likely to find any linguistic pattern you\n",
      "are interested in.  Furthermore, you can make use of very specific\n",
      "patterns, which would only match one or two examples on a smaller\n",
      "example, but which might match tens of thousands of examples when run\n",
      "on the web.  A second advantage of web search engines is that they are\n",
      "very easy to use.  Thus, they provide a very convenient tool for\n",
      "quickly checking a theory, to see if it is reasonable.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Google hits\n",
      "adore\n",
      "love\n",
      "like\n",
      "prefer\n",
      "\n",
      "\n",
      "\n",
      "absolutely\n",
      "289,000\n",
      "905,000\n",
      "16,200\n",
      "644\n",
      "\n",
      "definitely\n",
      "1,460\n",
      "51,000\n",
      "158,000\n",
      "62,600\n",
      "\n",
      "ratio\n",
      "198:1\n",
      "18:1\n",
      "1:10\n",
      "1:97\n",
      "\n",
      "\n",
      "Table 3.1: Google Hits for Collocations: The number of hits for collocations\n",
      "involving the words absolutely or definitely, followed\n",
      "by one of adore, love, like, or prefer.\n",
      "(Liberman, in LanguageLog, 2005).\n",
      "\n",
      "\n",
      "Unfortunately, search engines have some significant shortcomings.\n",
      "First, the allowable range of search patterns is severely restricted.\n",
      "Unlike local corpora, where you write programs to search for\n",
      "arbitrarily complex patterns, search engines generally\n",
      "only allow you to search for individual words or strings of\n",
      "words, sometimes with wildcards.  Second, search engines give\n",
      "inconsistent results, and can give widely different figures when used\n",
      "at different times or in different geographical regions.  When content has been\n",
      "duplicated across multiple sites, search results may be boosted.\n",
      "Finally, the markup in the result returned by a search engine may change unpredictably,\n",
      "breaking any pattern-based method of locating particular content (a problem\n",
      "which is ameliorated by the use of search engine APIs).\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Search the web for \"the of\" (inside quotes).  Based on the large\n",
      "count, can we conclude that the of is a frequent collocation\n",
      "in English?\n",
      "\n",
      "\n",
      "\n",
      "Processing RSS Feeds\n",
      "\n",
      "\n",
      "The blogosphere is an important source of text, in both formal and informal registers.\n",
      "With the help of a Python library called the Universal Feed Parser,\n",
      "available from https://pypi.python.org/pypi/feedparser, we can access the content\n",
      "of a blog, as shown below:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> import feedparser\n",
      ">>> llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
      ">>> llog['feed']['title']\n",
      "'Language Log'\n",
      ">>> len(llog.entries)\n",
      "15\n",
      ">>> post = llog.entries[2]\n",
      ">>> post.title\n",
      "\"He's My BF\"\n",
      ">>> content = post.content[0].value\n",
      ">>> content[:70]\n",
      "'<p>Today I was chatting with three of our visiting graduate students f'\n",
      ">>> raw = BeautifulSoup(content, 'html.parser').get_text()\n",
      ">>> word_tokenize(raw)\n",
      "['Today', 'I', 'was', 'chatting', 'with', 'three', 'of', 'our', 'visiting',\n",
      "'graduate', 'students', 'from', 'the', 'PRC', '.', 'Thinking', 'that', 'I',\n",
      "'was', 'being', 'au', 'courant', ',', 'I', 'mentioned', 'the', 'expression',\n",
      "'DUI4XIANG4', '\\u5c0d\\u8c61', '(\"', 'boy', '/', 'girl', 'friend', '\"', ...]\n",
      "\n",
      "\n",
      "\n",
      "With some further work, we can write programs to create a small corpus of blog posts,\n",
      "and use this as the basis for our NLP work.\n",
      "\n",
      "\n",
      "\n",
      "Reading Local Files\n",
      "\n",
      "In order to read a local file, we need to use Python's built-in open() function,\n",
      "followed by the read() method.  Suppose you have a file document.txt, you\n",
      "can load its contents like this:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> f = open('document.txt')\n",
      ">>> raw = f.read()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Create a file called document.txt using a text editor, and type in a few lines of text,\n",
      "and save it as plain text.\n",
      "If you are using IDLE, select the New Window command in the File menu, typing\n",
      "the required text into this window, and then saving the file as\n",
      "document.txt inside the directory that IDLE offers in the pop-up dialogue box.\n",
      "Next, in the Python interpreter, open the file using f = open('document.txt'), then\n",
      "inspect its contents using print(f.read()).\n",
      "\n",
      "Various things might have gone wrong when you tried this.\n",
      "If the interpreter couldn't find your file, you would have seen an\n",
      "error like this:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> f = open('document.txt')\n",
      "Traceback (most recent call last):\n",
      "File \"<pyshell#7>\", line 1, in -toplevel-\n",
      "f = open('document.txt')\n",
      "IOError: [Errno 2] No such file or directory: 'document.txt'\n",
      "\n",
      "\n",
      "\n",
      "To check that the file that you are trying to open is really in the\n",
      "right directory, use IDLE's Open command in the File menu;\n",
      "this will display a list of all the files in the directory where\n",
      "IDLE is running. An alternative is to examine the current\n",
      "directory from within Python:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> import os\n",
      ">>> os.listdir('.')\n",
      "\n",
      "\n",
      "\n",
      "Another possible problem you might have encountered when accessing a text file\n",
      "is the newline conventions, which are different for different operating systems.\n",
      "The built-in open() function has a second parameter for controlling how\n",
      "the file is opened: open('document.txt', 'rU') —\n",
      "'r' means to open the file for reading (the default), and\n",
      "'U' stands for \"Universal\", which lets us ignore the different\n",
      "conventions used for marking newlines.\n",
      "Assuming that you can open the file, there are several methods for reading it.\n",
      "The read() method creates a string with the contents of the entire file:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> f.read()\n",
      "'Time flies like an arrow.\\nFruit flies like a banana.\\n'\n",
      "\n",
      "\n",
      "\n",
      "Recall that the '\\n' characters are newlines; this\n",
      "is equivalent to pressing Enter on a keyboard and starting a new line.\n",
      "\n",
      "We can also read a file one line at a time using a for loop:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> f = open('document.txt', 'rU')\n",
      ">>> for line in f:\n",
      "...     print(line.strip())\n",
      "Time flies like an arrow.\n",
      "Fruit flies like a banana.\n",
      "\n",
      "\n",
      "\n",
      "Here we use the strip() method to remove the newline character at the end of\n",
      "the input line.\n",
      "NLTK's corpus files can also be accessed using these methods.  We simply\n",
      "have to use nltk.data.find() to get the filename for any corpus item.\n",
      "Then we can open and read it in the way we just demonstrated above:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')\n",
      ">>> raw = open(path, 'rU').read()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Extracting Text from PDF, MSWord and other Binary Formats\n",
      "ASCII text and HTML text are human readable formats.  Text often comes in binary\n",
      "formats — like PDF and MSWord — that can only be opened using specialized\n",
      "software.  Third-party libraries such as pypdf and pywin32\n",
      "provide access to\n",
      "these formats.  Extracting text from multi-column documents is particularly\n",
      "challenging.  For once-off conversion of a few documents,\n",
      "it is simpler to open the document with a suitable application, then save it as text\n",
      "to your local drive, and access it as described below.\n",
      "If the document is already on the web, you can enter its URL in Google's search box.\n",
      "The search result often includes a link to an HTML version of the document,\n",
      "which you can save as text.\n",
      "\n",
      "\n",
      "Capturing User Input\n",
      "Sometimes we want to capture the text that a user inputs when she is\n",
      "interacting with our program. To prompt the user\n",
      "to type a line of input, call the Python function input().\n",
      "After saving the input to a variable, we can\n",
      "manipulate it just as we have done for other strings.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> s = input(\"Enter some text: \")\n",
      "Enter some text: On an exceptionally hot evening early in July\n",
      ">>> print(\"You typed\", len(word_tokenize(s)), \"words.\")\n",
      "You typed 8 words.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The NLP Pipeline\n",
      "3.1 summarizes what we have covered in this section, including the process\n",
      "of building a vocabulary that we saw in 1..  (One step, normalization,\n",
      "will be discussed in 3.6.)\n",
      "\n",
      "\n",
      "Figure 3.1: The Processing Pipeline: We open a URL and read its HTML content,\n",
      "remove the markup and select a slice of characters;\n",
      "this is then tokenized and optionally converted into an nltk.Text object;\n",
      "we can also lowercase all the words and extract the vocabulary.\n",
      "\n",
      "There's a lot going on in this pipeline.  To understand it properly, it helps to be\n",
      "clear about the type of each variable that it mentions.  We find out the type\n",
      "of any Python object x using type(x), e.g. type(1) is <int>\n",
      "since 1 is an integer.\n",
      "When we load the contents of a URL or file, and when we strip out HTML markup,\n",
      "we are dealing with strings, Python's <str> data type.\n",
      "(We will learn more about strings in 3.2):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> raw = open('document.txt').read()\n",
      ">>> type(raw)\n",
      "<class 'str'>\n",
      "\n",
      "\n",
      "\n",
      "When we tokenize a string we produce a list (of words), and this is Python's <list>\n",
      "type.  Normalizing and sorting lists produces other lists:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tokens = word_tokenize(raw)\n",
      ">>> type(tokens)\n",
      "<class 'list'>\n",
      ">>> words = [w.lower() for w in tokens]\n",
      ">>> type(words)\n",
      "<class 'list'>\n",
      ">>> vocab = sorted(set(words))\n",
      ">>> type(vocab)\n",
      "<class 'list'>\n",
      "\n",
      "\n",
      "\n",
      "The type of an object determines what operations you can perform on it.\n",
      "So, for example, we can append to a list but not to a string:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> vocab.append('blog')\n",
      ">>> raw.append('blog')\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "AttributeError: 'str' object has no attribute 'append'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Similarly, we can concatenate strings with strings, and lists with\n",
      "lists, but we cannot concatenate strings with lists:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> query = 'Who knows?'\n",
      ">>> beatles = ['john', 'paul', 'george', 'ringo']\n",
      ">>> query + beatles\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "TypeError: cannot concatenate 'str' and 'list' objects\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.2   Strings: Text Processing at the Lowest Level\n",
      "It's time to examine a fundamental data type that we've been studiously avoiding\n",
      "so far.  In earlier chapters we focused on a text as a list of words.  We didn't\n",
      "look too closely at words and how they are handled in the programming\n",
      "language.  By using NLTK's corpus interface we were able to ignore\n",
      "the files that these texts had come from.  The contents of a word, and\n",
      "of a file, are represented by programming languages as a fundamental\n",
      "data type known as a string.  In this section we explore strings\n",
      "in detail, and show the connection between strings, words, texts and files.\n",
      "\n",
      "Basic Operations with Strings\n",
      "Strings are specified using single quotes \n",
      "or double quotes , as shown below.\n",
      "If a string contains a single quote, we must backslash-escape\n",
      "the quote  so Python knows a literal quote character is intended,\n",
      "or else put the string in double quotes .\n",
      "Otherwise, the quote inside the string \n",
      "will be interpreted as a close quote, and the Python interpreter\n",
      "will report a syntax error:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> monty = 'Monty Python' \n",
      ">>> monty\n",
      "'Monty Python'\n",
      ">>> circus = \"Monty Python's Flying Circus\" \n",
      ">>> circus\n",
      "\"Monty Python's Flying Circus\"\n",
      ">>> circus = 'Monty Python\\'s Flying Circus' \n",
      ">>> circus\n",
      "\"Monty Python's Flying Circus\"\n",
      ">>> circus = 'Monty Python's Flying Circus' \n",
      "  File \"<stdin>\", line 1\n",
      "    circus = 'Monty Python's Flying Circus'\n",
      "                           ^\n",
      "SyntaxError: invalid syntax\n",
      "\n",
      "\n",
      "\n",
      "Sometimes strings go over several lines.  Python provides us with various\n",
      "ways of entering them.  In the next example, a sequence of two strings is\n",
      "joined into a single string.\n",
      "We need to use backslash  or parentheses  so that\n",
      "the interpreter knows that the statement is not complete after the first line.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> couplet = \"Shall I compare thee to a Summer's day?\"\\\n",
      "...           \"Thou are more lovely and more temperate:\" \n",
      ">>> print(couplet)\n",
      "Shall I compare thee to a Summer's day?Thou are more lovely and more temperate:\n",
      ">>> couplet = (\"Rough winds do shake the darling buds of May,\"\n",
      "...           \"And Summer's lease hath all too short a date:\") \n",
      ">>> print(couplet)\n",
      "Rough winds do shake the darling buds of May,And Summer's lease hath all too short a date:\n",
      "\n",
      "\n",
      "\n",
      "Unfortunately the above methods do not give us a newline between\n",
      "the two lines of the sonnet.  Instead, we can use a triple-quoted\n",
      "string as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> couplet = \"\"\"Shall I compare thee to a Summer's day?\n",
      "... Thou are more lovely and more temperate:\"\"\"\n",
      ">>> print(couplet)\n",
      "Shall I compare thee to a Summer's day?\n",
      "Thou are more lovely and more temperate:\n",
      ">>> couplet = '''Rough winds do shake the darling buds of May,\n",
      "... And Summer's lease hath all too short a date:'''\n",
      ">>> print(couplet)\n",
      "Rough winds do shake the darling buds of May,\n",
      "And Summer's lease hath all too short a date:\n",
      "\n",
      "\n",
      "\n",
      "Now that we can define strings, we can try some simple operations on them.\n",
      "First let's look at the + operation, known as concatenation .\n",
      "It produces a new string that is a copy of the\n",
      "two original strings pasted together end-to-end.  Notice that\n",
      "concatenation doesn't do anything clever like insert a space between\n",
      "the words.  We can even multiply strings :\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> 'very' + 'very' + 'very' \n",
      "'veryveryvery'\n",
      ">>> 'very' * 3 \n",
      "'veryveryvery'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Try running the following code, then try to use your understanding\n",
      "of the string + and * operations to figure out how it works.\n",
      "Be careful to distinguish between the string ' ', which\n",
      "is a single whitespace character, and '', which is the empty string.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> a = [1, 2, 3, 4, 5, 6, 7, 6, 5, 4, 3, 2, 1]\n",
      ">>> b = [' ' * 2 * (7 - i) + 'very' * i for i in a]\n",
      ">>> for line in b:\n",
      "...     print(line)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We've seen that the addition and multiplication operations apply to\n",
      "strings, not just numbers.  However, note that we cannot use\n",
      "subtraction or division with strings:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> 'very' - 'y'\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "TypeError: unsupported operand type(s) for -: 'str' and 'str'\n",
      ">>> 'very' / 2\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "TypeError: unsupported operand type(s) for /: 'str' and 'int'\n",
      "\n",
      "\n",
      "\n",
      "These error messages are another example of Python telling us that we\n",
      "have got our data types in a muddle. In the first case, we are told\n",
      "that the operation of subtraction (i.e., -) cannot apply to\n",
      "objects of type str (strings), while in the second, we are told that\n",
      "division cannot take str and int as its two operands.\n",
      "\n",
      "\n",
      "Printing Strings\n",
      "So far, when we have wanted to look at the contents of a variable or\n",
      "see the result of a calculation, we have just typed the variable name\n",
      "into the interpreter.  We can also see the contents of a variable\n",
      "using the print statement:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(monty)\n",
      "Monty Python\n",
      "\n",
      "\n",
      "\n",
      "Notice that there are no quotation marks this time.  When we inspect\n",
      "a variable by typing its name in the interpreter, the interpreter prints\n",
      "the Python representation of its value.  Since it's a string,\n",
      "the result is quoted.  However, when we tell the\n",
      "interpreter to print the contents of the variable, we don't see\n",
      "quotation characters since there are none inside the string.\n",
      "The print statement allows us to display more than one item on a line\n",
      "in various ways, as shown below:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> grail = 'Holy Grail'\n",
      ">>> print(monty + grail)\n",
      "Monty PythonHoly Grail\n",
      ">>> print(monty, grail)\n",
      "Monty Python Holy Grail\n",
      ">>> print(monty, \"and the\", grail)\n",
      "Monty Python and the Holy Grail\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accessing Individual Characters\n",
      "As we saw in 2 for lists, strings are indexed, starting from zero.\n",
      "When we index a string, we get one of its characters (or letters).  A single character is nothing special — it's just\n",
      "a string of length 1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> monty[0]\n",
      "'M'\n",
      ">>> monty[3]\n",
      "'t'\n",
      ">>> monty[5]\n",
      "' '\n",
      "\n",
      "\n",
      "\n",
      "As with lists, if we try to access an index that is outside of the string we get an error:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> monty[20]\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in ?\n",
      "IndexError: string index out of range\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Again as with lists, we can use negative indexes for strings,\n",
      "where -1 is the index of the last character .\n",
      "Positive and negative indexes give us two ways to refer to\n",
      "any position in a string.  In this case, when the string had a length of 12,\n",
      "indexes 5 and -7 both refer to the same character (a space).\n",
      "(Notice that 5 = len(monty) - 7.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> monty[-1] \n",
      "'n'\n",
      ">>> monty[5]\n",
      "' '\n",
      ">>> monty[-7]\n",
      "' '\n",
      "\n",
      "\n",
      "\n",
      "We can write for loops to iterate over the characters\n",
      "in strings.  This print function includes the optional end=' '\n",
      "parameter, which is how we tell Python to print a space instead of a newline at the end.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent = 'colorless green ideas sleep furiously'\n",
      ">>> for char in sent:\n",
      "...     print(char, end=' ')\n",
      "...\n",
      "c o l o r l e s s   g r e e n   i d e a s   s l e e p   f u r i o u s l y\n",
      "\n",
      "\n",
      "\n",
      "We can count individual characters as well.  We should ignore the case\n",
      "distinction by normalizing everything to lowercase, and filter out non-alphabetic characters:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import gutenberg\n",
      ">>> raw = gutenberg.raw('melville-moby_dick.txt')\n",
      ">>> fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())\n",
      ">>> fdist.most_common(5)\n",
      "[('e', 117092), ('t', 87996), ('a', 77916), ('o', 69326), ('n', 65617)]\n",
      ">>> [char for (char, count) in fdist.most_common()]\n",
      "['e', 't', 'a', 'o', 'n', 'i', 's', 'h', 'r', 'l', 'd', 'u', 'm', 'c', 'w',\n",
      "'f', 'g', 'p', 'b', 'y', 'v', 'k', 'q', 'j', 'x', 'z']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[sb]explain this tuple unpacking somewhere?\n",
      "\n",
      "\n",
      "This gives us the letters of the alphabet, with the most frequently occurring letters\n",
      "listed first (this is quite complicated and we'll explain it more carefully below).\n",
      "You might like to visualize the distribution using fdist.plot().\n",
      "The relative character frequencies of a text can be used in automatically identifying\n",
      "the language of the text.\n",
      "\n",
      "\n",
      "Accessing Substrings\n",
      "\n",
      "\n",
      "Figure 3.2: String Slicing: The string \"Monty Python\" is shown along with its positive and\n",
      "negative indexes; two substrings are selected using \"slice\" notation.\n",
      "The slice [m,n] contains the characters from position m through n-1.\n",
      "\n",
      "A substring is any continuous section of a string that we want to pull out for\n",
      "further processing.  We can easily access substrings using the same slice notation\n",
      "we used for lists (see 3.2).\n",
      "For example, the following code accesses the substring starting at index 6,\n",
      "up to (but not including) index 10:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> monty[6:10]\n",
      "'Pyth'\n",
      "\n",
      "\n",
      "\n",
      "Here we see the characters are 'P', 'y', 't', and 'h' which correspond\n",
      "to monty[6] ... monty[9] but not monty[10]. This is because\n",
      "a slice starts at the first index but finishes one before the end index.\n",
      "We can also slice with negative indexes — the same basic rule of starting\n",
      "from the start index and stopping one before the end index applies;\n",
      "here we stop before the space character.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> monty[-12:-7]\n",
      "'Monty'\n",
      "\n",
      "\n",
      "\n",
      "As with list slices, if we omit the first value, the substring begins at the start\n",
      "of the string.  If we omit the second value, the substring continues to the end\n",
      "of the string:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> monty[:5]\n",
      "'Monty'\n",
      ">>> monty[6:]\n",
      "'Python'\n",
      "\n",
      "\n",
      "\n",
      "We test if a string contains a particular substring using the in operator, as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> phrase = 'And now for something completely different'\n",
      ">>> if 'thing' in phrase:\n",
      "...     print('found \"thing\"')\n",
      "found \"thing\"\n",
      "\n",
      "\n",
      "\n",
      "We can also find the position of a substring within a string, using find():\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> monty.find('Python')\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Make up a sentence and assign it to a variable, e.g. sent = 'my sentence...'.\n",
      "Now write slice expressions to pull out individual words.  (This is obviously\n",
      "not a convenient way to process the words of a text!)\n",
      "\n",
      "\n",
      "\n",
      "More operations on strings\n",
      "Python has comprehensive support for processing strings.  A summary, including some operations\n",
      "we haven't seen yet, is shown in 3.2.  For more information on strings, type\n",
      "help(str) at the Python prompt.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Method\n",
      "Functionality\n",
      "\n",
      "\n",
      "\n",
      "s.find(t)\n",
      "index of first instance of string t inside s (-1 if not found)\n",
      "\n",
      "s.rfind(t)\n",
      "index of last instance of string t inside s (-1 if not found)\n",
      "\n",
      "s.index(t)\n",
      "like s.find(t) except it raises ValueError if not found\n",
      "\n",
      "s.rindex(t)\n",
      "like s.rfind(t) except it raises ValueError if not found\n",
      "\n",
      "s.join(text)\n",
      "combine the words of the text into a string using s as the glue\n",
      "\n",
      "s.split(t)\n",
      "split s into a list wherever a t is found (whitespace by default)\n",
      "\n",
      "s.splitlines()\n",
      "split s into a list of strings, one per line\n",
      "\n",
      "s.lower()\n",
      "a lowercased version of the string s\n",
      "\n",
      "s.upper()\n",
      "an uppercased version of the string s\n",
      "\n",
      "s.title()\n",
      "a titlecased version of the string s\n",
      "\n",
      "s.strip()\n",
      "a copy of s without leading or trailing whitespace\n",
      "\n",
      "s.replace(t, u)\n",
      "replace instances of t with u inside s\n",
      "\n",
      "\n",
      "Table 3.2: Useful String Methods: operations on strings in addition to the string tests\n",
      "shown in 4.2; all methods produce a new string or list\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Difference between Lists and Strings\n",
      "Strings and lists are both kinds of sequence.  We can pull them\n",
      "apart by indexing and slicing them, and we can join them together\n",
      "by concatenating them.  However, we cannot join strings and lists:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> query = 'Who knows?'\n",
      ">>> beatles = ['John', 'Paul', 'George', 'Ringo']\n",
      ">>> query[2]\n",
      "'o'\n",
      ">>> beatles[2]\n",
      "'George'\n",
      ">>> query[:2]\n",
      "'Wh'\n",
      ">>> beatles[:2]\n",
      "['John', 'Paul']\n",
      ">>> query + \" I don't\"\n",
      "\"Who knows? I don't\"\n",
      ">>> beatles + 'Brian'\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "TypeError: can only concatenate list (not \"str\") to list\n",
      ">>> beatles + ['Brian']\n",
      "['John', 'Paul', 'George', 'Ringo', 'Brian']\n",
      "\n",
      "\n",
      "\n",
      "When we open a file\n",
      "for reading into a Python program, we get a string\n",
      "corresponding to the contents of the whole file. If we use a for loop to\n",
      "process the elements of this string, all we can pick out are the\n",
      "individual characters — we don't get to choose the\n",
      "granularity. By contrast, the elements of a list can be as big or\n",
      "small as we like: for example, they could be paragraphs, sentences,\n",
      "phrases, words, characters. So lists have the advantage that we\n",
      "can be flexible about the elements they contain, and\n",
      "correspondingly flexible about any downstream processing.\n",
      "Consequently, one of the first things we are likely to do in a piece of NLP\n",
      "code is tokenize a string into a list of strings (3.7).\n",
      "Conversely, when we want to write our results to a file, or to a terminal,\n",
      "we will usually format them as a string (3.9).\n",
      "Lists and strings do not have exactly the same functionality.\n",
      "Lists have the added power that you can change their elements:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> beatles[0] = \"John Lennon\"\n",
      ">>> del beatles[-1]\n",
      ">>> beatles\n",
      "['John Lennon', 'Paul', 'George']\n",
      "\n",
      "\n",
      "\n",
      "On the other hand if we try to do that with a string\n",
      "— changing the 0th character in query to 'F' — we get:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> query[0] = 'F'\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in ?\n",
      "TypeError: object does not support item assignment\n",
      "\n",
      "\n",
      "\n",
      " This is because strings are immutable — you can't change a\n",
      "string once you have created it.  However, lists are mutable,\n",
      "and their contents can be modified at any time.  As a result, lists\n",
      "support operations that modify the original value rather than producing a new value.\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Consolidate your knowledge of strings by trying some of the exercises on\n",
      "strings at the end of this chapter.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.3   Text Processing with Unicode\n",
      "Our programs will often need to deal with different languages, and\n",
      "different character sets.  The concept of \"plain text\" is a fiction.\n",
      "If you live in the English-speaking world you probably use ASCII,\n",
      "possibly without realizing it.  If you live in Europe you might use\n",
      "one of the extended Latin character sets, containing such characters\n",
      "as \"ø\" for Danish and Norwegian, \"ő\" for Hungarian,\n",
      "\"ñ\" for Spanish and Breton, and \"ň\" for Czech and\n",
      "Slovak. In this section, we will give an overview of how to use\n",
      "Unicode for processing texts that use non-ASCII character sets.\n",
      "\n",
      "What is Unicode?\n",
      "Unicode supports over a million characters.  Each\n",
      "character is assigned a number, called a code point.  In Python, code\n",
      "points are written in the form \\uXXXX, where XXXX is the number\n",
      "in 4-digit hexadecimal form.\n",
      "Within a program, we can manipulate Unicode strings just like normal strings.\n",
      "However, when Unicode characters are stored in files or displayed on a terminal,\n",
      "they must be encoded as a stream of bytes.  Some encodings (such\n",
      "as ASCII and Latin-2) use a single byte per code point, so they can only support a\n",
      "small subset of Unicode, enough for a single language.  Other encodings\n",
      "(such as UTF-8) use multiple bytes and can represent the full range of\n",
      "Unicode characters.\n",
      "Text in files will be in a particular encoding, so we need some\n",
      "mechanism for translating it into Unicode — translation into\n",
      "Unicode is called decoding. Conversely, to write out Unicode to a\n",
      "file or a terminal, we first need to translate it into a suitable\n",
      "encoding — this translation out of Unicode is called encoding,\n",
      "and is illustrated in 3.3.\n",
      "\n",
      "\n",
      "Figure 3.3: Unicode Decoding and Encoding\n",
      "\n",
      "From a Unicode perspective, characters are abstract entities which can\n",
      "be realized as one or more glyphs. Only glyphs can appear on a\n",
      "screen or be printed on paper. A font is a mapping from characters to glyphs.\n",
      "\n",
      "\n",
      "Extracting encoded text from files\n",
      "Let's assume that we have a small text file, and that we know how it\n",
      "is encoded. For example, polish-lat2.txt, as the name suggests, is\n",
      "a snippet of Polish text (from the Polish Wikipedia; see\n",
      "http://pl.wikipedia.org/wiki/Biblioteka_Pruska).  This file is encoded as Latin-2,\n",
      "also known as ISO-8859-2. The function nltk.data.find() locates the\n",
      "file for us.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')\n",
      "\n",
      "\n",
      "\n",
      "The Python open() function can read encoded data\n",
      "into Unicode strings, and write out Unicode strings in encoded\n",
      "form.  It takes a parameter to\n",
      "specify the encoding of the file being read or written. So let's open\n",
      "our Polish file\n",
      "with the encoding 'latin2' and  inspect the contents of the file:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> f = open(path, encoding='latin2')\n",
      ">>> for line in f:\n",
      "...    line = line.strip()\n",
      "...    print(line)\n",
      "Pruska Biblioteka Państwowa. Jej dawne zbiory znane pod nazwą\n",
      "\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\n",
      "Niemców pod koniec II wojny światowej na Dolny Śląsk, zostały\n",
      "odnalezione po 1945 r. na terytorium Polski. Trafiły do Biblioteki\n",
      "Jagiellońskiej w Krakowie, obejmują ponad 500 tys. zabytkowych\n",
      "archiwaliów, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\n",
      "\n",
      "\n",
      "\n",
      "If this does not display correctly on your terminal, or if we want\n",
      "to see the underlying numerical values (or \"codepoints\") of the characters,\n",
      "then we can convert all non-ASCII characters into their two-digit \\xXX\n",
      "and four-digit \\uXXXX representations:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> f = open(path, encoding='latin2')\n",
      ">>> for line in f:\n",
      "...     line = line.strip()\n",
      "...     print(line.encode('unicode_escape'))\n",
      "b'Pruska Biblioteka Pa\\\\u0144stwowa. Jej dawne zbiory znane pod nazw\\\\u0105'\n",
      "b'\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez'\n",
      "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y'\n",
      "b'odnalezione po 1945 r. na terytorium Polski. Trafi\\\\u0142y do Biblioteki'\n",
      "b'Jagiello\\\\u0144skiej w Krakowie, obejmuj\\\\u0105 ponad 500 tys. zabytkowych'\n",
      "b'archiwali\\\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.'\n",
      "\n",
      "\n",
      "\n",
      "The first line above illustrates a Unicode escape string\n",
      "preceded by the \\u escape string, namely \\u0144 . The relevant\n",
      "Unicode character will be dislayed on the screen as the glyph\n",
      "ń.  In the third line of the preceding example, we see\n",
      "\\xf3, which corresponds to the glyph ó, and is within the\n",
      "128-255 range.\n",
      "In Python 3, source code is encoded using UTF-8 by default, and you can\n",
      "include Unicode characters in strings if you are using IDLE or another program editor\n",
      "that supports Unicode.\n",
      "Arbitrary Unicode characters can be included using the\n",
      "\\uXXXX escape sequence.\n",
      "We find the integer ordinal of a character using ord(). For example:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> ord('ń')\n",
      "324\n",
      "\n",
      "\n",
      "\n",
      "The hexadecimal 4 digit notation for 324 is 0144 (type hex(324) to discover this),\n",
      "and we can define a string with the appropriate escape sequence.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nacute = '\\u0144'\n",
      ">>> nacute\n",
      "'ń'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "There are many factors determining what glyphs are rendered\n",
      "on your screen. If you are sure that you have the correct encoding,\n",
      "but your Python code is still failing to produce the glyphs you\n",
      "expected, you should also check that you have the necessary fonts\n",
      "installed on your system. It may be necessary to configure your locale\n",
      "to render UTF-8 encoded characters, then use print(nacute.encode('utf8'))\n",
      "in order to see the ń displayed in your terminal.\n",
      "\n",
      "We can also see how this character is represented as a sequence of bytes inside\n",
      "a text file:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nacute.encode('utf8')\n",
      "b'\\xc5\\x84'\n",
      "\n",
      "\n",
      "\n",
      "The module unicodedata lets us inspect the properties of Unicode\n",
      "characters. In the following example, we select all characters in the\n",
      "third line of our Polish text outside the ASCII range and print their\n",
      "UTF-8 byte sequence, followed by their code point integer using the\n",
      "standard Unicode convention (i.e., prefixing the hex digits with\n",
      "U+), followed by their Unicode name.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> import unicodedata\n",
      ">>> lines = open(path, encoding='latin2').readlines()\n",
      ">>> line = lines[2]\n",
      ">>> print(line.encode('unicode_escape'))\n",
      "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y\\\\n'\n",
      ">>> for c in line: \n",
      "...     if ord(c) > 127:\n",
      "...         print('{} U+{:04x} {}'.format(c.encode('utf8'), ord(c), unicodedata.name(c)))\n",
      "b'\\xc3\\xb3' U+00f3 LATIN SMALL LETTER O WITH ACUTE\n",
      "b'\\xc5\\x9b' U+015b LATIN SMALL LETTER S WITH ACUTE\n",
      "b'\\xc5\\x9a' U+015a LATIN CAPITAL LETTER S WITH ACUTE\n",
      "b'\\xc4\\x85' U+0105 LATIN SMALL LETTER A WITH OGONEK\n",
      "b'\\xc5\\x82' U+0142 LATIN SMALL LETTER L WITH STROKE\n",
      "\n",
      "\n",
      "\n",
      "If you replace\n",
      "c.encode('utf8') in  with c, and if your system supports UTF-8,\n",
      "you should see an output like the following:\n",
      "\n",
      "ó U+00f3 LATIN SMALL LETTER O WITH ACUTE\n",
      "ś U+015b LATIN SMALL LETTER S WITH ACUTE\n",
      "Ś U+015a LATIN CAPITAL LETTER S WITH ACUTE\n",
      "ą U+0105 LATIN SMALL LETTER A WITH OGONEK\n",
      "ł U+0142 LATIN SMALL LETTER L WITH STROKE\n",
      "\n",
      "Alternatively, you may need to replace the encoding 'utf8' in the\n",
      "example by 'latin2', again depending on the details of your system.\n",
      "The next examples illustrate how Python string methods and the re\n",
      "module can work with Unicode characters. (We will take a close look at\n",
      "the re module in the following section. The \\w matches a \"word\n",
      "character\", cf 3.4).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> line.find('zosta\\u0142y')\n",
      "54\n",
      ">>> line = line.lower()\n",
      ">>> line\n",
      "'niemców pod koniec ii wojny światowej na dolny śląsk, zostały\\n'\n",
      ">>> line.encode('unicode_escape')\n",
      "b'niemc\\\\xf3w pod koniec ii wojny \\\\u015bwiatowej na dolny \\\\u015bl\\\\u0105sk, zosta\\\\u0142y\\\\n'\n",
      ">>> import re\n",
      ">>> m = re.search('\\u015b\\w*', line)\n",
      ">>> m.group()\n",
      "'\\u015bwiatowej'\n",
      "\n",
      "\n",
      "\n",
      "NLTK tokenizers allow Unicode strings as input, and\n",
      "correspondingly yield Unicode strings as output.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> word_tokenize(line)\n",
      "['niemców', 'pod', 'koniec', 'ii', 'wojny', 'światowej', 'na', 'dolny', 'śląsk', ',', 'zostały']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Using your local encoding in Python\n",
      "If you are used to working with characters in a particular local\n",
      "encoding, you probably want to be able to use your standard methods\n",
      "for inputting and editing strings in a Python file. In order to do this,\n",
      "you need to include the string '# -*- coding: <coding> -*-' as the\n",
      "first or second line of your file. Note that <coding> has to be a\n",
      "string like 'latin-1', 'big5' or 'utf-8' (see 3.4).\n",
      "\n",
      "\n",
      "Figure 3.4: Unicode and IDLE: UTF-8 encoded string literals in the IDLE editor;\n",
      "this requires that an appropriate font is set in IDLE's preferences;\n",
      "here we have chosen Courier CE.\n",
      "\n",
      "\n",
      "The above example also illustrates how regular expressions can use\n",
      "encoded strings.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.4   Regular Expressions for Detecting Word Patterns\n",
      "Many linguistic processing tasks involve pattern matching.\n",
      "For example, we can find words ending with ed using\n",
      "endswith('ed').  We saw a variety of such \"word tests\"\n",
      "in 4.2.\n",
      "Regular expressions give us a more powerful and flexible\n",
      "method for describing the character patterns we are interested in.\n",
      "\n",
      "Note\n",
      "There are many other published introductions to regular expressions,\n",
      "organized around the syntax of regular expressions and applied to searching\n",
      "text files.  Instead of doing this again, we focus on the use of regular expressions\n",
      "at different stages of linguistic processing.  As usual, we'll adopt\n",
      "a problem-based approach and present new features only as they are\n",
      "needed to solve practical problems.  In our discussion we will mark\n",
      "regular expressions using chevrons like this: «patt».\n",
      "\n",
      "To use regular expressions in Python we need to import the re\n",
      "library using: import re.  We also need a list of words to search;\n",
      "we'll use the Words Corpus again (4).  We\n",
      "will preprocess it to remove any proper names.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> import re\n",
      ">>> wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Using Basic Meta-Characters\n",
      "Let's find words ending with ed using the regular expression «ed$».\n",
      "We will use the re.search(p, s) function to check whether the pattern p can be found\n",
      "somewhere inside the string s.\n",
      "We need to specify the characters of interest, and use the dollar sign which has a\n",
      "special behavior in the context of regular expressions in that it matches\n",
      "the end of the word:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> [w for w in wordlist if re.search('ed$', w)]\n",
      "['abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', ...]\n",
      "\n",
      "\n",
      "\n",
      "The . wildcard symbol matches any single character.\n",
      "Suppose we have room in a crossword puzzle for an 8-letter word\n",
      "with j as its third letter and t as its sixth letter.\n",
      "In place of each blank cell we use a period:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> [w for w in wordlist if re.search('^..j..t..$', w)]\n",
      "['abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', ...]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "The caret symbol ^ matches the start of a string, just like the $ matches\n",
      "the end.  What results do we get with the above example if we leave out\n",
      "both of these, and search for «..j..t..»?\n",
      "\n",
      "Finally, the ? symbol specifies that the previous character is optional.\n",
      "Thus «^e-?mail$» will match both email and e-mail.\n",
      "We could count the total number of occurrences of this word (in either spelling)\n",
      "in a text using sum(1 for w in text if re.search('^e-?mail$', w)).\n",
      "\n",
      "\n",
      "Ranges and Closures\n",
      "\n",
      "\n",
      "Figure 3.5: T9: Text on 9 Keys\n",
      "\n",
      "The T9 system is used for entering text on mobile phones (see 3.5).  Two or more words that\n",
      "are entered with the same sequence of keystrokes are known as textonyms.\n",
      "For example, both hole and golf are entered by pressing\n",
      "the sequence 4653.  What other words\n",
      "could be produced with the same sequence?  Here we use the regular expression\n",
      "«^[ghi][mno][jlk][def]$»:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> [w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]\n",
      "['gold', 'golf', 'hold', 'hole']\n",
      "\n",
      "\n",
      "\n",
      "The first part of the expression, «^[ghi]», matches the start of\n",
      "a word followed by g, h, or i.  The next part of the expression,\n",
      "«[mno]», constrains the second character to be\n",
      "m, n, or o.  The third and fourth characters are also constrained.\n",
      "Only four words satisfy all these constraints.\n",
      "Note that the order of characters inside the square brackets is not significant, so we\n",
      "could have written «^[hig][nom][ljk][fed]$» and matched the same\n",
      "words.\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Look for some \"finger-twisters\", by searching for words that only use part\n",
      "of the number-pad.  For example «^[ghijklmno]+$», or\n",
      "more concisely, «^[g-o]+$», will match words\n",
      "that only use keys 4, 5, 6 in the center row, and «^[a-fj-o]+$»\n",
      "will match words that use keys 2, 3, 5, 6 in the top-right corner.\n",
      "What do - and + mean?\n",
      "\n",
      "Let's explore the + symbol a bit further.  Notice that it can be applied to\n",
      "individual letters, or to bracketed sets of letters:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
      ">>> [w for w in chat_words if re.search('^m+i+n+e+$', w)]\n",
      "['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee', 'miiiiiinnnnnnnnnneeeeeeee', 'mine',\n",
      "'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']\n",
      ">>> [w for w in chat_words if re.search('^[ha]+$', w)]\n",
      "['a', 'aaaaaaaaaaaaaaaaa', 'aaahhhh', 'ah', 'ahah', 'ahahah', 'ahh',\n",
      "'ahhahahaha', 'ahhh', 'ahhhh', 'ahhhhhh', 'ahhhhhhhhhhhhhh', 'h', 'ha', 'haaa',\n",
      "'hah', 'haha', 'hahaaa', 'hahah', 'hahaha', 'hahahaa', 'hahahah', 'hahahaha', ...]\n",
      "\n",
      "\n",
      "\n",
      "It should be clear that + simply means \"one or more instances of the preceding item\",\n",
      "which could be an individual character like m, a set like [fed] or a range like [d-f].\n",
      "Now let's replace + with *, which means \"zero or more instances of the preceding item\".\n",
      "The regular expression «^m*i*n*e*$» will match everything that we found using\n",
      "«^m+i+n+e+$», but also words where some of the letters don't appear at all,\n",
      "e.g. me, min, and mmmmm.\n",
      "Note that the + and * symbols are sometimes referred to as Kleene closures,\n",
      "or simply closures.\n",
      "The ^ operator has another function when it appears as the first\n",
      "character inside square brackets.  For\n",
      "example «[^aeiouAEIOU]» matches any character other than a vowel.\n",
      "We can search the NPS Chat Corpus for words that are made up entirely of non-vowel\n",
      "characters using «^[^aeiouAEIOU]+$» to find items like these:\n",
      ":):):), grrr, cyb3r and zzzzzzzz.  Notice this includes\n",
      "non-alphabetic characters.\n",
      "Here are some more examples of regular expressions being used to find tokens\n",
      "that match a particular pattern, illustrating the use of some new symbols:\n",
      "\\, {}, (), and |:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> wsj = sorted(set(nltk.corpus.treebank.words()))\n",
      ">>> [w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)]\n",
      "['0.0085', '0.05', '0.1', '0.16', '0.2', '0.25', '0.28', '0.3', '0.4', '0.5',\n",
      "'0.50', '0.54', '0.56', '0.60', '0.7', '0.82', '0.84', '0.9', '0.95', '0.99',\n",
      "'1.01', '1.1', '1.125', '1.14', '1.1650', '1.17', '1.18', '1.19', '1.2', ...]\n",
      ">>> [w for w in wsj if re.search('^[A-Z]+\\$$', w)]\n",
      "['C$', 'US$']\n",
      ">>> [w for w in wsj if re.search('^[0-9]{4}$', w)]\n",
      "['1614', '1637', '1787', '1901', '1903', '1917', '1925', '1929', '1933', ...]\n",
      ">>> [w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]\n",
      "['10-day', '10-lap', '10-year', '100-share', '12-point', '12-year', ...]\n",
      ">>> [w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]\n",
      "['black-and-white', 'bread-and-butter', 'father-in-law', 'machine-gun-toting',\n",
      "'savings-and-loan']\n",
      ">>> [w for w in wsj if re.search('(ed|ing)$', w)]\n",
      "['62%-owned', 'Absorbed', 'According', 'Adopting', 'Advanced', 'Advancing', ...]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Study the above examples and try to work out what the\n",
      "\\, {}, (), and | notations mean before\n",
      "you read on.\n",
      "\n",
      "You probably worked out that a backslash means that the following character is\n",
      "deprived of its special powers and must literally match a specific character in the\n",
      "word.  Thus, while . is special, \\. only matches a period.\n",
      "The braced expressions, like {3,5}, specify the number of repeats of the previous item.\n",
      "The pipe character indicates a choice between the material on its left or its right.\n",
      "Parentheses indicate the scope of an operator: they can be used together with\n",
      "the pipe (or disjunction) symbol like this: «w(i|e|ai|oo)t», matching wit,\n",
      "wet, wait, and woot.  It is instructive to see what happens when\n",
      "you omit the parentheses from the last expression above, and search for\n",
      "«ed|ing$».\n",
      "The meta-characters we have seen are summarized in 3.3.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Operator\n",
      "Behavior\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "Wildcard, matches any character\n",
      "\n",
      "^abc\n",
      "Matches some pattern abc at the start of a string\n",
      "\n",
      "abc$\n",
      "Matches some pattern abc at the end of a string\n",
      "\n",
      "[abc]\n",
      "Matches one of a set of characters\n",
      "\n",
      "[A-Z0-9]\n",
      "Matches one of a range of characters\n",
      "\n",
      "ed|ing|s\n",
      "Matches one of the specified strings (disjunction)\n",
      "\n",
      "*\n",
      "Zero or more of previous item, e.g. a*, [a-z]* (also known as Kleene Closure)\n",
      "\n",
      "+\n",
      "One or more of previous item, e.g. a+, [a-z]+\n",
      "\n",
      "?\n",
      "Zero or one of the previous item (i.e. optional), e.g. a?, [a-z]?\n",
      "\n",
      "{n}\n",
      "Exactly n repeats where n is a non-negative integer\n",
      "\n",
      "{n,}\n",
      "At least n repeats\n",
      "\n",
      "{,n}\n",
      "No more than n repeats\n",
      "\n",
      "{m,n}\n",
      "At least m and no more than n repeats\n",
      "\n",
      "a(b|c)+\n",
      "Parentheses that indicate the scope of the operators\n",
      "\n",
      "\n",
      "Table 3.3: Basic Regular Expression Meta-Characters, Including Wildcards, Ranges and Closures\n",
      "\n",
      "\n",
      "To the Python interpreter, a regular expression is just like any other string.\n",
      "If the string contains a backslash followed by particular characters, it will\n",
      "interpret these specially.  For example \\b would be interpreted as the\n",
      "backspace character.  In general, when using regular expressions containing\n",
      "backslash, we should instruct the interpreter not to look inside the string\n",
      "at all, but simply to pass it directly to the re library for processing.\n",
      "We do this by prefixing the string with the letter r, to indicate that\n",
      "it is a raw string.  For example, the raw string r'\\band\\b'\n",
      "contains two \\b symbols that are interpreted by the re library\n",
      "as matching word boundaries instead of backspace characters.\n",
      "If you get into the habit of using r'...' for regular expressions\n",
      "— as we will do from now on — you will avoid having to think about\n",
      "these complications.\n",
      "\n",
      "\n",
      "\n",
      "3.5   Useful Applications of Regular Expressions\n",
      "The above examples all involved searching for words w\n",
      "that match some regular expression regexp using re.search(regexp, w).\n",
      "Apart from checking if a regular expression matches a word, we can use\n",
      "regular expressions to extract material from words, or to modify words\n",
      "in specific ways.\n",
      "\n",
      "Extracting Word Pieces\n",
      "The re.findall() (\"find all\") method finds all (non-overlapping)\n",
      "matches of the given regular expression.  Let's find all the vowels in\n",
      "a word, then count them:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> word = 'supercalifragilisticexpialidocious'\n",
      ">>> re.findall(r'[aeiou]', word)\n",
      "['u', 'e', 'a', 'i', 'a', 'i', 'i', 'i', 'e', 'i', 'a', 'i', 'o', 'i', 'o', 'u']\n",
      ">>> len(re.findall(r'[aeiou]', word))\n",
      "16\n",
      "\n",
      "\n",
      "\n",
      "Let's look for all sequences of two or more vowels in some text,\n",
      "and determine their relative frequency:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> wsj = sorted(set(nltk.corpus.treebank.words()))\n",
      ">>> fd = nltk.FreqDist(vs for word in wsj\n",
      "...                       for vs in re.findall(r'[aeiou]{2,}', word))\n",
      ">>> fd.most_common(12)\n",
      "[('io', 549), ('ea', 476), ('ie', 331), ('ou', 329), ('ai', 261), ('ia', 253),\n",
      "('ee', 217), ('oo', 174), ('ua', 109), ('au', 106), ('ue', 105), ('ui', 95)]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "In the W3C Date Time Format, dates are represented like this: 2009-12-31.\n",
      "Replace the ? in the following Python code with a regular expression,\n",
      "in order to convert the string '2009-12-31' to a list of integers\n",
      "[2009, 12, 31]:\n",
      "[int(n) for n in re.findall(?, '2009-12-31')]\n",
      "\n",
      "\n",
      "\n",
      "Doing More with Word Pieces\n",
      "Once we can use re.findall() to extract material from words, there's\n",
      "interesting things to do with the pieces, like glue them back together or\n",
      "plot them.\n",
      "It is sometimes noted that English text is highly redundant, and it is still\n",
      "easy to read when word-internal vowels are left out.  For example,\n",
      "declaration becomes dclrtn, and inalienable becomes inlnble,\n",
      "retaining any initial or final vowel sequences.   The regular expression\n",
      "in our next example matches initial vowel sequences, final vowel sequences, and all consonants;\n",
      "everything else is ignored.  This three-way disjunction is processed left-to-right,\n",
      "if one of the three parts matches the word, any later parts of the regular\n",
      "expression are ignored.\n",
      "We use re.findall() to extract all the matching\n",
      "pieces, and ''.join() to join them together (see 3.9 for\n",
      "more about the join operation).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> regexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'\n",
      ">>> def compress(word):\n",
      "...     pieces = re.findall(regexp, word)\n",
      "...     return ''.join(pieces)\n",
      "...\n",
      ">>> english_udhr = nltk.corpus.udhr.words('English-Latin1')\n",
      ">>> print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))\n",
      "Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\n",
      "of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\n",
      "of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\n",
      "rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\n",
      "and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\n",
      "\n",
      "\n",
      "\n",
      "Next, let's combine regular expressions with conditional frequency\n",
      "distributions.  Here we will extract all consonant-vowel sequences\n",
      "from the words of Rotokas, such as ka and si.  Since each of\n",
      "these is a pair, it can be used to initialize a conditional frequency\n",
      "distribution.  We then tabulate the frequency of each pair:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> rotokas_words = nltk.corpus.toolbox.words('rotokas.dic')\n",
      ">>> cvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
      ">>> cfd = nltk.ConditionalFreqDist(cvs)\n",
      ">>> cfd.tabulate()\n",
      "    a    e    i    o    u\n",
      "k  418  148   94  420  173\n",
      "p   83   31  105   34   51\n",
      "r  187   63   84   89   79\n",
      "s    0    0  100    2    1\n",
      "t   47    8    0  148   37\n",
      "v   93   27  105   48   49\n",
      "\n",
      "\n",
      "\n",
      "Examining the rows for s and t, we see they are in partial\n",
      "\"complementary distribution\", which is evidence that they are not\n",
      "distinct phonemes in the language.  Thus, we could conceivably drop\n",
      "s from the Rotokas alphabet and simply have a pronunciation rule\n",
      "that the letter t is pronounced s when followed by\n",
      "i.  (Note that the single entry having su, namely kasuari,\n",
      "'cassowary' is borrowed from English.)\n",
      "If we want to be able to inspect the words behind the numbers in the above table,\n",
      "it would be helpful to have an index, allowing us to quickly find the list of words\n",
      "that contains a given consonant-vowel pair, e.g. cv_index['su'] should give us\n",
      "all words containing su.  Here's how we can do this:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> cv_word_pairs = [(cv, w) for w in rotokas_words\n",
      "...                          for cv in re.findall(r'[ptksvr][aeiou]', w)]\n",
      ">>> cv_index = nltk.Index(cv_word_pairs)\n",
      ">>> cv_index['su']\n",
      "['kasuari']\n",
      ">>> cv_index['po']\n",
      "['kaapo', 'kaapopato', 'kaipori', 'kaiporipie', 'kaiporivira', 'kapo', 'kapoa',\n",
      "'kapokao', 'kapokapo', 'kapokapo', 'kapokapoa', 'kapokapoa', 'kapokapora', ...]\n",
      "\n",
      "\n",
      "\n",
      "This program processes each word w in turn, and for each one, finds every\n",
      "substring that matches the regular expression «[ptksvr][aeiou]».\n",
      "In the case of the word kasuari, it finds ka, su and ri.\n",
      "Therefore, the cv_word_pairs list will contain ('ka', 'kasuari'),\n",
      "('su', 'kasuari') and ('ri', 'kasuari').  One further step, using\n",
      "nltk.Index(), converts this into a useful index.\n",
      "\n",
      "\n",
      "Finding Word Stems\n",
      "When we use a web search engine, we usually don't mind (or even notice)\n",
      "if the words in the document differ from our search terms in having\n",
      "different endings.  A query for laptops finds documents\n",
      "containing laptop and vice versa.\n",
      "Indeed, laptop and laptops are just two forms of the\n",
      "same dictionary word (or lemma).\n",
      "For some language processing tasks we want to ignore word endings, and just\n",
      "deal with word stems.\n",
      "There are various ways we can pull out the stem of a word.  Here's a simple-minded\n",
      "approach which just strips off anything that looks like a suffix:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def stem(word):\n",
      "...     for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
      "...         if word.endswith(suffix):\n",
      "...             return word[:-len(suffix)]\n",
      "...     return word\n",
      "\n",
      "\n",
      "\n",
      "Although we will ultimately use NLTK's built-in stemmers, it's interesting\n",
      "to see how we can use regular expressions for this task.  Our first step is\n",
      "to build up a disjunction of all the suffixes.  We need to enclose it in parentheses\n",
      "in order to limit the scope of the disjunction.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')\n",
      "['ing']\n",
      "\n",
      "\n",
      "\n",
      "Here, re.findall() just gave us the suffix even though the regular expression\n",
      "matched the entire word.  This is because the parentheses have a second function,\n",
      "to select substrings to be extracted.  If we want to use the parentheses to\n",
      "specify the scope of the disjunction, but not to select the material to be output,\n",
      "we have to add ?:,\n",
      "which is just one of many arcane subtleties of regular expressions.\n",
      "Here's the revised version.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')\n",
      "['processing']\n",
      "\n",
      "\n",
      "\n",
      "However, we'd actually like to split the word into stem and suffix.\n",
      "So we should just parenthesize both parts of the regular expression:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')\n",
      "[('process', 'ing')]\n",
      "\n",
      "\n",
      "\n",
      "This looks promising, but still has a problem.  Let's look at a different\n",
      "word, processes:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')\n",
      "[('processe', 's')]\n",
      "\n",
      "\n",
      "\n",
      "The regular expression incorrectly found an -s suffix instead of\n",
      "an -es suffix.  This demonstrates another subtlety: the star operator\n",
      "is \"greedy\" and the .* part of the expression tries to consume as much of the input\n",
      "as possible.  If we use the \"non-greedy\" version of the star operator, written *?,\n",
      "we get what we want:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')\n",
      "[('process', 'es')]\n",
      "\n",
      "\n",
      "\n",
      "This works even when we allow an empty suffix, by making the content of the\n",
      "second parentheses optional:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')\n",
      "[('language', '')]\n",
      "\n",
      "\n",
      "\n",
      "This approach still has many problems (can you spot them?) but we will move\n",
      "on to define a function to perform stemming, and apply it to a whole text:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def stem(word):\n",
      "...     regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
      "...     stem, suffix = re.findall(regexp, word)[0]\n",
      "...     return stem\n",
      "...\n",
      ">>> raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
      "... is no basis for a system of government.  Supreme executive power derives from\n",
      "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
      ">>> tokens = word_tokenize(raw)\n",
      ">>> [stem(t) for t in tokens]\n",
      "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'ly', 'in', 'pond', 'distribut',\n",
      "'sword', 'i', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'Supreme',\n",
      "'execut', 'power', 'deriv', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',',\n",
      "'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n",
      "\n",
      "\n",
      "\n",
      "Notice that our regular expression removed the s from ponds but also from is\n",
      "and basis.  It produced some non-words like distribut and deriv, but these\n",
      "are acceptable stems in some applications.\n",
      "\n",
      "\n",
      "Searching Tokenized Text\n",
      "You can use a special kind of regular expression for searching across multiple words\n",
      "in a text (where a text is a list of tokens).  For example, \"<a> <man>\" finds all\n",
      "instances of a man in the text.  The angle brackets are used to mark token boundaries,\n",
      "and any whitespace between the angle brackets is ignored (behaviors that are unique\n",
      "to NLTK's findall() method for texts).  In the following example, we include\n",
      "<.*>  which will match any single token, and enclose it in parentheses so only the\n",
      "matched word (e.g. monied) and not the matched phrase (e.g. a monied man)\n",
      "is produced.  The second example finds three-word phrases ending with the word bro\n",
      ".  The last example finds sequences of three or more words starting with\n",
      "the letter l .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import gutenberg, nps_chat\n",
      ">>> moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\n",
      ">>> moby.findall(r\"<a> (<.*>) <man>\") \n",
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n",
      ">>> chat = nltk.Text(nps_chat.words())\n",
      ">>> chat.findall(r\"<.*> <.*> <bro>\") \n",
      "you rule bro; telling you bro; u twizted bro\n",
      ">>> chat.findall(r\"<l.*>{3,}\") \n",
      "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
      "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Consolidate your understanding of regular expression patterns and substitutions using\n",
      "nltk.re_show(p, s) which annotates the string s to show every place where\n",
      "pattern p was matched, and nltk.app.nemo() which provides a graphical\n",
      "interface for exploring regular expressions.  For more practice, try some\n",
      "of the exercises on regular expressions at the end of this chapter.\n",
      "\n",
      "\n",
      "It is easy to build search patterns when the linguistic phenomenon we're\n",
      "studying is tied to particular words.  In some cases, a little creativity\n",
      "will go a long way.  For instance, searching a large text corpus for\n",
      "expressions of the form x and other ys allows us to discover\n",
      "hypernyms (cf 5):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import brown\n",
      ">>> hobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\n",
      ">>> hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")\n",
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n",
      "\n",
      "\n",
      "\n",
      "With enough text, this approach would give us a useful store\n",
      "of information about the taxonomy of objects, without the need for\n",
      "any manual labor.  However, our search results will usually\n",
      "contain false positives, i.e. cases that we would want to exclude.\n",
      "For example, the result: demands and other factors suggests\n",
      "that demand is an instance of the type factor, but this\n",
      "sentence is actually about wage demands.  Nevertheless, we could\n",
      "construct our own ontology of English concepts by manually correcting\n",
      "the output of such searches.\n",
      "\n",
      "Note\n",
      "This combination of automatic and manual processing is the most common\n",
      "way for new corpora to be constructed.  We will return to this in\n",
      "11..\n",
      "\n",
      "Searching corpora also suffers from the problem of false negatives,\n",
      "i.e. omitting cases that we would want to include.  It is risky to\n",
      "conclude that some linguistic phenomenon doesn't exist in a corpus\n",
      "just because we couldn't find any instances of a search pattern.\n",
      "Perhaps we just didn't think carefully enough about suitable patterns.\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Look for instances of the pattern as x as y to discover\n",
      "information about entities and their properties.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.6   Normalizing Text\n",
      "In earlier program examples we have often converted text to lowercase before\n",
      "doing anything with its words, e.g. set(w.lower() for w in text).\n",
      "By using lower(), we have normalized the text to lowercase so that\n",
      "the distinction between The and the is ignored.  Often we want\n",
      "to go further than this, and strip off any affixes, a task known as stemming.\n",
      "A further step is to make sure that the resulting form is a known word in a dictionary,\n",
      "a task known as lemmatization.  We discuss each of these in turn.  First, we need\n",
      "to define the data we will use in this section:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
      "... is no basis for a system of government.  Supreme executive power derives from\n",
      "... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
      ">>> tokens = word_tokenize(raw)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Stemmers\n",
      "NLTK includes several off-the-shelf stemmers, and if you ever need\n",
      "a stemmer you should use one of these in preference to crafting your own\n",
      "using regular expressions, since these handle a wide range of irregular cases.\n",
      "The Porter and Lancaster stemmers follow their own rules for stripping affixes.\n",
      "Observe that the Porter stemmer correctly handles the word lying\n",
      "(mapping it to lie), while the Lancaster stemmer does not.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> porter = nltk.PorterStemmer()\n",
      ">>> lancaster = nltk.LancasterStemmer()\n",
      ">>> [porter.stem(t) for t in tokens]\n",
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond',\n",
      "'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern',\n",
      "'.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from',\n",
      "'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n",
      ">>> [lancaster.stem(t) for t in tokens]\n",
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut',\n",
      "'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem',\n",
      "'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not',\n",
      "'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n",
      "\n",
      "\n",
      "\n",
      "Stemming is not a well-defined process, and we typically pick the stemmer that best\n",
      "suits the application we have in mind.  The Porter Stemmer is a good choice if you\n",
      "are indexing some texts and want to support search using alternative forms of\n",
      "words (illustrated in 3.6, which uses object oriented\n",
      "programming techniques that are outside the scope of this book, string formatting\n",
      "techniques to be covered in 3.9, and the enumerate() function\n",
      "to be explained in 4.2).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "class IndexedText(object):\n",
      "\n",
      "    def __init__(self, stemmer, text):\n",
      "        self._text = text\n",
      "        self._stemmer = stemmer\n",
      "        self._index = nltk.Index((self._stem(word), i)\n",
      "                                 for (i, word) in enumerate(text))\n",
      "\n",
      "    def concordance(self, word, width=40):\n",
      "        key = self._stem(word)\n",
      "        wc = int(width/4)                # words of context\n",
      "        for i in self._index[key]:\n",
      "            lcontext = ' '.join(self._text[i-wc:i])\n",
      "            rcontext = ' '.join(self._text[i:i+wc])\n",
      "            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n",
      "            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n",
      "            print(ldisplay, rdisplay)\n",
      "\n",
      "    def _stem(self, word):\n",
      "        return self._stemmer.stem(word).lower()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> porter = nltk.PorterStemmer()\n",
      ">>> grail = nltk.corpus.webtext.words('grail.txt')\n",
      ">>> text = IndexedText(porter, grail)\n",
      ">>> text.concordance('lie')\n",
      "r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no\n",
      " beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\n",
      "       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !\n",
      "doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well\n",
      "ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which\n",
      "   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\n",
      "h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\n",
      "not stop our fight ' til each one of you lies dead , and the Holy Grail returns t\n",
      "\n",
      "\n",
      "Example 3.6 (code_stemmer_indexing.py): Figure 3.6: Indexing a Text Using a Stemmer\n",
      "\n",
      "\n",
      "\n",
      "Lemmatization\n",
      "The WordNet lemmatizer only removes affixes if the resulting word is in its dictionary.\n",
      "This additional checking process makes the lemmatizer slower than the above stemmers.\n",
      "Notice that it doesn't handle lying, but it converts women to woman.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> wnl = nltk.WordNetLemmatizer()\n",
      ">>> [wnl.lemmatize(t) for t in tokens]\n",
      "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond',\n",
      "'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of',\n",
      "'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a',\n",
      "'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical',\n",
      "'aquatic', 'ceremony', '.']\n",
      "\n",
      "\n",
      "\n",
      "The WordNet lemmatizer is a good choice if you want to compile the vocabulary\n",
      "of some texts and want a list of valid lemmas (or lexicon headwords).\n",
      "\n",
      "Note\n",
      "Another normalization task involves identifying non-standard words\n",
      "including numbers, abbreviations, and dates, and mapping any such tokens\n",
      "to a special vocabulary.  For example, every decimal number could be\n",
      "mapped to a single token 0.0, and every acronym could be mapped to AAA.\n",
      "This keeps the vocabulary small and improves the accuracy of many\n",
      "language modeling tasks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.7   Regular Expressions for Tokenizing Text\n",
      "Tokenization is the task of cutting a string into\n",
      "identifiable linguistic units that constitute a piece of language data.\n",
      "Although it is a fundamental task, we have been able to\n",
      "delay it until now because many corpora are already tokenized,\n",
      "and because NLTK includes some tokenizers.\n",
      "Now that you are familiar with regular expressions,\n",
      "you can learn how to use them to tokenize text, and to\n",
      "have much more control over the process.\n",
      "\n",
      "Simple Approaches to Tokenization\n",
      "The very simplest method for tokenizing text is to split on whitespace.\n",
      "Consider the following text from Alice's Adventures in Wonderland:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\n",
      "... though), 'I won't have any pepper in my kitchen AT ALL. Soup does very\n",
      "... well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\"\n",
      "\n",
      "\n",
      "\n",
      "We could split this raw text on whitespace using raw.split().\n",
      "To do the same using a regular expression, it is not enough to match\n",
      "any space characters in the string  since this results\n",
      "in tokens that contain a \\n newline character; instead we need\n",
      "to match any number of spaces, tabs, or newlines :\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> re.split(r' ', raw) \n",
      "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in',\n",
      "'a', 'very', 'hopeful', 'tone\\nthough),', \"'I\", \"won't\", 'have', 'any', 'pepper',\n",
      "'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very\\nwell', 'without--Maybe',\n",
      "\"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n",
      ">>> re.split(r'[ \\t\\n]+', raw) \n",
      "[\"'When\", \"I'M\", 'a', \"Duchess,'\", 'she', 'said', 'to', 'herself,', '(not', 'in',\n",
      "'a', 'very', 'hopeful', 'tone', 'though),', \"'I\", \"won't\", 'have', 'any', 'pepper',\n",
      "'in', 'my', 'kitchen', 'AT', 'ALL.', 'Soup', 'does', 'very', 'well', 'without--Maybe',\n",
      "\"it's\", 'always', 'pepper', 'that', 'makes', 'people', \"hot-tempered,'...\"]\n",
      "\n",
      "\n",
      "\n",
      "The regular expression «[ \\t\\n]+» matches one or more space, tab (\\t)\n",
      "or newline (\\n).  Other whitespace characters, such as carriage-return and\n",
      "form-feed should really be included too.  Instead, we will use a built-in\n",
      "re abbreviation, \\s, which means any whitespace character.  The above\n",
      "statement can be rewritten as re.split(r'\\s+', raw).\n",
      "\n",
      "Note\n",
      "Important:\n",
      "Remember to prefix regular expressions with the letter r\n",
      "(meaning \"raw\"), which instructs the Python\n",
      "interpreter to treat the string literally, rather than\n",
      "processing any backslashed characters it contains.\n",
      "\n",
      "Splitting on whitespace gives us tokens like '(not' and 'herself,'.\n",
      "An alternative is to use the fact that Python provides us with a\n",
      "character class \\w for word characters, equivalent to [a-zA-Z0-9_].\n",
      "It also defines the complement of this class \\W, i.e. all characters\n",
      "other than letters, digits or underscore.  We can use \\W in a simple\n",
      "regular expression to split the input on anything other than a word character:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> re.split(r'\\W+', raw)\n",
      "['', 'When', 'I', 'M', 'a', 'Duchess', 'she', 'said', 'to', 'herself', 'not', 'in',\n",
      "'a', 'very', 'hopeful', 'tone', 'though', 'I', 'won', 't', 'have', 'any', 'pepper',\n",
      "'in', 'my', 'kitchen', 'AT', 'ALL', 'Soup', 'does', 'very', 'well', 'without',\n",
      "'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered',\n",
      "'']\n",
      "\n",
      "\n",
      "\n",
      "Observe that this gives us empty strings at the start and the end (to understand\n",
      "why, try doing 'xx'.split('x')).  We get the same tokens, but without the empty strings,\n",
      "with re.findall(r'\\w+', raw), using a pattern that matches the words instead of the spaces.\n",
      "Now that we're matching the words, we're in a position to extend the regular expression\n",
      "to cover a wider range of cases.\n",
      "The regular expression «\\w+|\\S\\w*» will first try to match any sequence\n",
      "of word characters.  If no match is found, it will try to match any\n",
      "non-whitespace character (\\S is the complement of \\s) followed by\n",
      "further word characters.  This means that punctuation is grouped with any following\n",
      "letters (e.g. 's) but that sequences of two or more punctuation\n",
      "characters are separated.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> re.findall(r'\\w+|\\S\\w*', raw)\n",
      "[\"'When\", 'I', \"'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',',\n",
      "'(not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'I\", 'won', \"'t\",\n",
      "'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does',\n",
      "'very', 'well', 'without', '-', '-Maybe', 'it', \"'s\", 'always', 'pepper', 'that',\n",
      "'makes', 'people', 'hot', '-tempered', ',', \"'\", '.', '.', '.']\n",
      "\n",
      "\n",
      "\n",
      "Let's generalize the \\w+ in the above expression\n",
      "to permit word-internal hyphens and apostrophes: «\\w+([-']\\w+)*».\n",
      "This expression means \\w+ followed by zero or more instances of [-']\\w+;\n",
      "it would match hot-tempered and it's.\n",
      "(We need to include ?: in this expression for reasons discussed earlier.)\n",
      "We'll also add a pattern to match quote characters so these are kept separate\n",
      "from the text they enclose.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw))\n",
      "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',',\n",
      "'(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I',\n",
      "\"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup',\n",
      "'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper',\n",
      "'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\n",
      "\n",
      "\n",
      "\n",
      "The above expression also included «[-.(]+» which causes the double hyphen,\n",
      "ellipsis, and open parenthesis to be tokenized separately.\n",
      "3.4 lists the regular expression character class symbols we have\n",
      "seen in this section, in addition to some other useful symbols.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Symbol\n",
      "Function\n",
      "\n",
      "\n",
      "\n",
      "\\b\n",
      "Word boundary (zero width)\n",
      "\n",
      "\\d\n",
      "Any decimal digit (equivalent to [0-9])\n",
      "\n",
      "\\D\n",
      "Any non-digit character (equivalent to [^0-9])\n",
      "\n",
      "\\s\n",
      "Any whitespace character (equivalent to [ \\t\\n\\r\\f\\v])\n",
      "\n",
      "\\S\n",
      "Any non-whitespace character (equivalent to [^ \\t\\n\\r\\f\\v])\n",
      "\n",
      "\\w\n",
      "Any alphanumeric character (equivalent to [a-zA-Z0-9_])\n",
      "\n",
      "\\W\n",
      "Any non-alphanumeric character (equivalent to [^a-zA-Z0-9_])\n",
      "\n",
      "\\t\n",
      "The tab character\n",
      "\n",
      "\\n\n",
      "The newline character\n",
      "\n",
      "\n",
      "Table 3.4: Regular Expression Symbols\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NLTK's Regular Expression Tokenizer\n",
      "The function nltk.regexp_tokenize() is similar to re.findall() (as\n",
      "we've been using it for tokenization).  However, nltk.regexp_tokenize()\n",
      "is more efficient for this task, and avoids the need for special treatment of parentheses.\n",
      "For readability we break up the regular expression over several lines\n",
      "and add a comment about each line.  The special (?x) \"verbose flag\"\n",
      "tells Python to strip out the embedded whitespace and comments.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = 'That U.S.A. poster-print costs $12.40...'\n",
      ">>> pattern = r'''(?x)     # set flag to allow verbose regexps\n",
      "...     (?:[A-Z]\\.)+       # abbreviations, e.g. U.S.A.\n",
      "...   | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
      "...   | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
      "...   | \\.\\.\\.             # ellipsis\n",
      "...   | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
      "... '''\n",
      ">>> nltk.regexp_tokenize(text, pattern)\n",
      "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']\n",
      "\n",
      "\n",
      "\n",
      "When using the verbose flag, you can no longer use ' ' to match\n",
      "a space character; use \\s instead.\n",
      "The regexp_tokenize() function has an optional gaps parameter.\n",
      "When set to True, the regular expression specifies the gaps\n",
      "between tokens, as with re.split().\n",
      "\n",
      "Note\n",
      "We can evaluate a tokenizer by comparing the resulting tokens with a\n",
      "wordlist, and reporting any tokens that don't appear in the wordlist,\n",
      "using set(tokens).difference(wordlist).  You'll probably want to\n",
      "lowercase all the tokens first.\n",
      "\n",
      "\n",
      "\n",
      "Further Issues with Tokenization\n",
      "Tokenization turns out to be a far more difficult task than you might have expected.\n",
      "No single solution works well across-the-board, and we\n",
      "must decide what counts as a token depending on the application domain.\n",
      "When developing a tokenizer it helps to have access to raw text which\n",
      "has been manually tokenized, in order to compare the output of your tokenizer\n",
      "with high-quality (or \"gold-standard\") tokens.  The NLTK corpus\n",
      "collection includes a sample of Penn Treebank data, including the raw\n",
      "Wall Street Journal text (nltk.corpus.treebank_raw.raw()) and\n",
      "the tokenized version (nltk.corpus.treebank.words()).\n",
      "A final issue for tokenization is the presence of contractions, such\n",
      "as didn't.  If we are analyzing the meaning\n",
      "of a sentence, it would probably be more useful to normalize this\n",
      "form to two separate forms: did and n't (or not).\n",
      "We can do this work with the help of a lookup table.\n",
      "\n",
      "\n",
      "\n",
      "3.8   Segmentation\n",
      "This section discusses more advanced concepts, which you may prefer to skip on the\n",
      "first time through this chapter.\n",
      "Tokenization is an instance of a more general problem of segmentation.\n",
      "In this section we will look at two other instances of this problem,\n",
      "which use radically different techniques to the ones we have seen so far\n",
      "in this chapter.\n",
      "\n",
      "Sentence Segmentation\n",
      "Manipulating texts at the level of individual words often presupposes\n",
      "the ability to divide a text into individual sentences.  As we have\n",
      "seen, some corpora already provide access at the sentence level.  In\n",
      "the following example, we compute the average number of words per\n",
      "sentence in the Brown Corpus:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())\n",
      "20.250994070456922\n",
      "\n",
      "\n",
      "\n",
      "In other cases, the text is only available as a stream of characters.  Before\n",
      "tokenizing the text into words, we need to segment it into sentences.  NLTK facilitates\n",
      "this by including the Punkt sentence segmenter (Kiss & Strunk, 2006).\n",
      "Here is an example of its use in segmenting the text of a novel.\n",
      "(Note that if the segmenter's internal data has been updated by the time you read this,\n",
      "you will see different output):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
      ">>> sents = nltk.sent_tokenize(text)\n",
      ">>> pprint.pprint(sents[79:89])\n",
      "['\"Nonsense!\"',\n",
      " 'said Gregory, who was very rational when anyone else\\nattempted paradox.',\n",
      " '\"Why do all the clerks and navvies in the\\n'\n",
      " 'railway trains look so sad and tired, so very sad and tired?',\n",
      " 'I will\\ntell you.',\n",
      " 'It is because they know that the train is going right.',\n",
      " 'It\\n'\n",
      " 'is because they know that whatever place they have taken a ticket\\n'\n",
      " 'for that place they will reach.',\n",
      " 'It is because after they have\\n'\n",
      " 'passed Sloane Square they know that the next station must be\\n'\n",
      " 'Victoria, and nothing but Victoria.',\n",
      " 'Oh, their wild rapture!',\n",
      " 'oh,\\n'\n",
      " 'their eyes like stars and their souls again in Eden, if the next\\n'\n",
      " 'station were unaccountably Baker Street!\"',\n",
      " '\"It is you who are unpoetical,\" replied the poet Syme.']\n",
      "\n",
      "\n",
      "\n",
      "Notice that this example is really a single sentence, reporting the speech of Mr Lucian Gregory.\n",
      "However, the quoted speech contains several sentences, and these have been split into individual\n",
      "strings.  This is reasonable behavior for most applications.\n",
      "Sentence segmentation is difficult because period is used to mark abbreviations,\n",
      "and some periods simultaneously mark an abbreviation and terminate a sentence,\n",
      "as often happens with acronyms like U.S.A.\n",
      "For another approach to sentence segmentation, see 2.\n",
      "\n",
      "\n",
      "Word Segmentation\n",
      "For some writing systems, tokenizing text is made more difficult by the fact that there\n",
      "is no visual representation of word boundaries.\n",
      "For example, in Chinese, the three-character string: 爱国人\n",
      "(ai4 \"love\" (verb), guo2 \"country\", ren2 \"person\") could\n",
      "be tokenized as 爱国 / 人, \"country-loving person\"\n",
      "or as 爱 / 国人, \"love country-person.\"\n",
      "A similar problem arises in the processing of spoken language, where the\n",
      "hearer must segment a continuous speech stream into individual words.\n",
      "A particularly challenging version of this problem arises when we don't\n",
      "know the words in advance.  This is the problem faced by a language learner,\n",
      "such as a child hearing utterances from a parent.  Consider the\n",
      "following artificial example, where word boundaries have been removed:\n",
      "\n",
      "  (1)\n",
      "  a.doyouseethekitty\n",
      "\n",
      "  b.seethedoggy\n",
      "\n",
      "  c.doyoulikethekitty\n",
      "\n",
      "  d.likethedoggy\n",
      "\n",
      "Our first challenge is simply to represent the problem: we need to find\n",
      "a way to separate text content from the segmentation.  We can do this by\n",
      "annotating each character with a boolean value to indicate whether or\n",
      "not a word-break appears after the character (an idea that will be used\n",
      "heavily for \"chunking\" in 7.).\n",
      "Let's assume that the learner is given the utterance breaks,\n",
      "since these often correspond to extended pauses.  Here is a possible representation,\n",
      "including the initial and target segmentations:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
      ">>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
      ">>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
      "\n",
      "\n",
      "\n",
      "Observe that the segmentation strings consist of zeros and ones.  They\n",
      "are one character shorter than the source text, since a text of length\n",
      "n can only be broken up in n-1 places.\n",
      "The segment() function in 3.7 demonstrates that we can\n",
      "get back to the original segmented text from the above representation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def segment(text, segs):\n",
      "    words = []\n",
      "    last = 0\n",
      "    for i in range(len(segs)):\n",
      "        if segs[i] == '1':\n",
      "            words.append(text[last:i+1])\n",
      "            last = i+1\n",
      "    words.append(text[last:])\n",
      "    return words\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
      ">>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
      ">>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
      ">>> segment(text, seg1)\n",
      "['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      ">>> segment(text, seg2)\n",
      "['do', 'you', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'do', 'you',\n",
      "'like', 'the', 'kitty', 'like', 'the', 'doggy']\n",
      "\n",
      "\n",
      "Example 3.7 (code_segment.py): Figure 3.7: Reconstruct Segmented Text from String Representation:\n",
      "seg1 and seg2 represent the initial and final\n",
      "segmentations of some hypothetical child-directed speech;\n",
      "the segment() function can use them to reproduce the\n",
      "segmented text.\n",
      "\n",
      "Now the segmentation task becomes a search problem: find the bit string that causes\n",
      "the text string to be correctly segmented into words.\n",
      "We assume the learner is acquiring words and storing them in an internal lexicon.\n",
      "Given a suitable lexicon, it is possible to reconstruct the source text as a sequence of\n",
      "lexical items.  Following (Brent, 1995), we can define an objective function,\n",
      "a scoring function whose value we will try to optimize, based on\n",
      "the size of the lexicon (number of characters in the words plus an extra\n",
      "delimiter character to mark the end of each word) and the amount of information needed to\n",
      "reconstruct the source text from the lexicon.  We illustrate this in\n",
      "3.8.\n",
      "\n",
      "\n",
      "Figure 3.8: Calculation of Objective Function: Given a hypothetical segmentation\n",
      "of the source text (on the left), derive a lexicon and a derivation\n",
      "table that permit the source text to be reconstructed, then total\n",
      "up the number of characters used by each lexical item (including a boundary\n",
      "marker) and the number of lexical items used by each derivation, to serve as a score of the quality of\n",
      "the segmentation; smaller values of the score indicate a better segmentation.\n",
      "\n",
      "It is a simple matter to implement this objective function, as shown in\n",
      "3.9.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def evaluate(text, segs):\n",
      "    words = segment(text, segs)\n",
      "    text_size = len(words)\n",
      "    lexicon_size = sum(len(word) + 1 for word in set(words))\n",
      "    return text_size + lexicon_size\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
      ">>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
      ">>> seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
      ">>> seg3 = \"0000100100000011001000000110000100010000001100010000001\"\n",
      ">>> segment(text, seg3)\n",
      "['doyou', 'see', 'thekitt', 'y', 'see', 'thedogg', 'y', 'doyou', 'like',\n",
      " 'thekitt', 'y', 'like', 'thedogg', 'y']\n",
      ">>> evaluate(text, seg3)\n",
      "47\n",
      ">>> evaluate(text, seg2)\n",
      "48\n",
      ">>> evaluate(text, seg1)\n",
      "64\n",
      "\n",
      "\n",
      "Example 3.9 (code_evaluate.py): Figure 3.9: Computing the Cost of Storing the Lexicon and Reconstructing the Source Text\n",
      "\n",
      "The final step is to search for the pattern of zeros and ones that minimizes this objective\n",
      "function, shown in 3.10.  Notice that the best segmentation includes \"words\" like\n",
      "thekitty, since there's not enough evidence in the data to split this any further.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "from random import randint\n",
      "\n",
      "def flip(segs, pos):\n",
      "    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:]\n",
      "\n",
      "def flip_n(segs, n):\n",
      "    for i in range(n):\n",
      "        segs = flip(segs, randint(0, len(segs)-1))\n",
      "    return segs\n",
      "\n",
      "def anneal(text, segs, iterations, cooling_rate):\n",
      "    temperature = float(len(segs))\n",
      "    while temperature > 0.5:\n",
      "        best_segs, best = segs, evaluate(text, segs)\n",
      "        for i in range(iterations):\n",
      "            guess = flip_n(segs, round(temperature))\n",
      "            score = evaluate(text, guess)\n",
      "            if score < best:\n",
      "                best, best_segs = score, guess\n",
      "        score, segs = best, best_segs\n",
      "        temperature = temperature / cooling_rate\n",
      "        print(evaluate(text, segs), segment(text, segs))\n",
      "    print()\n",
      "    return segs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
      ">>> seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
      ">>> anneal(text, seg1, 5000, 1.2)\n",
      "61 ['doyouseetheki', 'tty', 'see', 'thedoggy', 'doyouliketh', 'ekittylike', 'thedoggy']\n",
      "59 ['doy', 'ouseetheki', 'ttysee', 'thedoggy', 'doy', 'o', 'ulikethekittylike', 'thedoggy']\n",
      "57 ['doyou', 'seetheki', 'ttysee', 'thedoggy', 'doyou', 'liketh', 'ekittylike', 'thedoggy']\n",
      "55 ['doyou', 'seethekit', 'tysee', 'thedoggy', 'doyou', 'likethekittylike', 'thedoggy']\n",
      "54 ['doyou', 'seethekit', 'tysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "52 ['doyou', 'seethekittysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "'0000100100000001001000000010000100010000000100010000000'\n",
      "\n",
      "\n",
      "Example 3.10 (code_anneal.py): Figure 3.10: Non-Deterministic Search Using Simulated Annealing: begin searching\n",
      "with phrase segmentations only; randomly perturb the zeros and ones\n",
      "proportional to the \"temperature\"; with each iteration the temperature\n",
      "is lowered and the perturbation of boundaries is reduced. As this\n",
      "search algorithm is non-deterministic, you may see a slightly different\n",
      "result.\n",
      "\n",
      "With enough data, it is possible to automatically segment text into words with a reasonable\n",
      "degree of accuracy.  Such methods can be applied to tokenization for writing systems that\n",
      "don't have any visual representation of word boundaries.\n",
      "\n",
      "\n",
      "\n",
      "3.9   Formatting: From Lists to Strings\n",
      "Often we write a program to report a single data item, such as a particular element\n",
      "in a corpus that meets some complicated criterion, or a single summary statistic\n",
      "such as a word-count or the performance of a tagger.  More often, we write a program\n",
      "to produce a structured result; for example, a tabulation of numbers or linguistic forms,\n",
      "or a reformatting of the original data.  When the results to be presented are linguistic,\n",
      "textual output is usually the most natural choice.  However, when the results are numerical,\n",
      "it may be preferable to produce graphical output.  In this section you will learn about\n",
      "a variety of ways to present program output.\n",
      "\n",
      "From Lists to Strings\n",
      "The simplest kind of structured object we use for text processing is lists of words.\n",
      "When we want to output these to a display or a file, we must convert\n",
      "these lists into strings.  To do this in Python we use the join() method, and specify\n",
      "the string to be used as the \"glue\".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']\n",
      ">>> ' '.join(silly)\n",
      "'We called him Tortoise because he taught us .'\n",
      ">>> ';'.join(silly)\n",
      "'We;called;him;Tortoise;because;he;taught;us;.'\n",
      ">>> ''.join(silly)\n",
      "'WecalledhimTortoisebecausehetaughtus.'\n",
      "\n",
      "\n",
      "\n",
      "So ' '.join(silly) means: take all the items in silly and\n",
      "concatenate them as one big string, using ' ' as a spacer between\n",
      "the items.  I.e. join() is a method of the string that you want to\n",
      "use as the glue.  (Many people find this notation for join() counter-intuitive.)\n",
      "The join() method only works on a list of strings — what we have been calling a text\n",
      "— a complex type that enjoys some privileges in Python.\n",
      "\n",
      "\n",
      "Strings and Formats\n",
      "We have seen that there are two ways to display the contents of an object:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> word = 'cat'\n",
      ">>> sentence = \"\"\"hello\n",
      "... world\"\"\"\n",
      ">>> print(word)\n",
      "cat\n",
      ">>> print(sentence)\n",
      "hello\n",
      "world\n",
      ">>> word\n",
      "'cat'\n",
      ">>> sentence\n",
      "'hello\\nworld'\n",
      "\n",
      "\n",
      "\n",
      "The print command yields Python's attempt to produce the most human-readable form of an object.\n",
      "The second method — naming the variable at a prompt — shows us a string\n",
      "that can be used to recreate this object.  It is important to keep in mind that both of\n",
      "these are just strings, displayed for the benefit of you, the user.  They do not give\n",
      "us any clue as to the actual internal representation of the object.\n",
      "There are many other useful ways to display an object as a string of\n",
      "characters.  This may be for the benefit of a human reader, or because\n",
      "we want to export our data to a particular file format for use\n",
      "in an external program.\n",
      "Formatted output typically contains a combination of variables and\n",
      "pre-specified strings, e.g. given a frequency distribution fdist\n",
      "we could do:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fdist = nltk.FreqDist(['dog', 'cat', 'dog', 'cat', 'dog', 'snake', 'dog', 'cat'])\n",
      ">>> for word in sorted(fdist):\n",
      "...     print(word, '->', fdist[word], end='; ')\n",
      "cat -> 3; dog -> 4; snake -> 1;\n",
      "\n",
      "\n",
      "\n",
      "Print statements that contain alternating variables and constants can be difficult to read and\n",
      "maintain.  Another solution is to use string formatting.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> for word in sorted(fdist):\n",
      "...    print('{}->{};'.format(word, fdist[word]), end=' ')\n",
      "cat->3; dog->4; snake->1;\n",
      "\n",
      "\n",
      "\n",
      "To understand what is going on here, let's test out the\n",
      "format string on its own.  (By now this will be\n",
      "your usual method of exploring new syntax.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> '{}->{};'.format ('cat', 3)\n",
      "'cat->3;'\n",
      "\n",
      "\n",
      "\n",
      "The curly brackets '{}'  mark the presence of a replacement\n",
      "field: this acts as a\n",
      "placeholder for the string values of objects that are\n",
      "passed to the str.format() method. We can embed occurrences of '{}'\n",
      "inside a string, then replace them with strings by calling format() with\n",
      "appropriate arguments.  A string containing\n",
      "replacement fields is called a format string.\n",
      "Let's unpack the above code further, in order to see this\n",
      "behavior up close:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> '{}->'.format('cat')\n",
      "'cat->'\n",
      ">>> '{}'.format(3)\n",
      "'3'\n",
      ">>> 'I want a {} right now'.format('coffee')\n",
      "'I want a coffee right now'\n",
      "\n",
      "\n",
      "\n",
      "We can have any number of placeholders, but  the str.format method\n",
      "must be called with exactly the same number of arguments.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> '{} wants a {} {}'.format ('Lee', 'sandwich', 'for lunch')\n",
      "'Lee wants a sandwich for lunch'\n",
      ">>> '{} wants a {} {}'.format ('sandwich', 'for lunch')\n",
      "Traceback (most recent call last):\n",
      "...\n",
      "    '{} wants a {} {}'.format ('sandwich', 'for lunch')\n",
      "IndexError: tuple index out of range\n",
      "\n",
      "\n",
      "\n",
      "Arguments to format() are consumed left to right, and any\n",
      "superfluous arguments are simply ignored.\n",
      "\n",
      "System Message: ERROR/3 (ch03.rst2, line 2262)\n",
      "Unexpected indentation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> '{} wants a {}'.format ('Lee', 'sandwich', 'for lunch')\n",
      "'Lee wants a sandwich'\n",
      "\n",
      "\n",
      "\n",
      "The field name in a format string can start with a number, which\n",
      "refers to a positional argument of format(). Something like\n",
      "'from {} to {}'\n",
      "is equivalent to  'from {0} to {1}', but we can use the numbers to get\n",
      "non-default orders:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> 'from {1} to {0}'.format('A', 'B')\n",
      "'from B to A'\n",
      "\n",
      "\n",
      "\n",
      "We can also provide the values for the placeholders indirectly. Here's\n",
      "an example using a for loop:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> template = 'Lee wants a {} right now'\n",
      ">>> menu = ['sandwich', 'spam fritter', 'pancake']\n",
      ">>> for snack in menu:\n",
      "...     print(template.format(snack))\n",
      "...\n",
      "Lee wants a sandwich right now\n",
      "Lee wants a spam fritter right now\n",
      "Lee wants a pancake right now\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lining Things Up\n",
      "\n",
      "So far our format strings generated output of arbitrary width\n",
      "on the page (or screen).  We can add padding to obtain output of a given\n",
      "width by inserting into the brackets a colon ':' followed by an integer. So {:6}\n",
      "specifies that we want a string that is\n",
      "padded to width 6.  It is right-justified by default for numbers ,\n",
      "but we can precede the width specifier with a '<' alignment option to make numbers left-justified .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> '{:6}'.format(41) \n",
      "'    41'\n",
      ">>> '{:<6}' .format(41) \n",
      "'41    '\n",
      "\n",
      "\n",
      "\n",
      "Strings are left-justified by default, but can be right-justified with\n",
      "the '>' alignment option.\n",
      "\n",
      "System Message: ERROR/3 (ch03.rst2, line 2310)\n",
      "Unexpected indentation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> '{:6}'.format('dog') \n",
      "'dog   '\n",
      ">>> '{:>6}'.format('dog') \n",
      " '   dog'\n",
      "\n",
      "\n",
      "\n",
      "Other control characters can be used to specify the sign and precision\n",
      "of floating point numbers; for example {:.4f} indicates that four\n",
      "digits should be displayed after the decimal point for a floating\n",
      "point number.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> import math\n",
      ">>> '{:.4f}'.format(math.pi)\n",
      "'3.1416'\n",
      "\n",
      "\n",
      "\n",
      "The string formatting is smart enough to know that if you include a\n",
      "'%' in your format specification, then you want to represent the\n",
      "value as a percentage; there's no need to multiply by 100.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> count, total = 3205, 9375\n",
      ">>> \"accuracy for {} words: {:.4%}\".format(total, count / total)\n",
      "'accuracy for 9375 words: 34.1867%'\n",
      "\n",
      "\n",
      "\n",
      "An important use of formatting strings is for tabulating data.\n",
      "Recall that in 1 we saw\n",
      "data being tabulated from a conditional frequency distribution.\n",
      "Let's perform the tabulation ourselves, exercising full control\n",
      "of headings and column widths, as shown in 3.11.\n",
      "Note the clear separation between the language processing work,\n",
      "and the tabulation of results.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def tabulate(cfdist, words, categories):\n",
      "    print('{:16}'.format('Category'), end=' ')                    # column headings\n",
      "    for word in words:\n",
      "        print('{:>6}'.format(word), end=' ')\n",
      "    print()\n",
      "    for category in categories:\n",
      "        print('{:16}'.format(category), end=' ')                  # row heading\n",
      "        for word in words:                                        # for each word\n",
      "            print('{:6}'.format(cfdist[category][word]), end=' ') # print table cell\n",
      "        print()                                                   # end the row\n",
      "\n",
      ">>> from nltk.corpus import brown\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...           (genre, word)\n",
      "...           for genre in brown.categories()\n",
      "...           for word in brown.words(categories=genre))\n",
      ">>> genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
      ">>> modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
      ">>> tabulate(cfd, modals, genres)\n",
      "Category            can  could    may  might   must   will\n",
      "news                 93     86     66     38     50    389\n",
      "religion             82     59     78     12     54     71\n",
      "hobbies             268     58    131     22     83    264\n",
      "science_fiction      16     49      4     12      8     16\n",
      "romance              74    193     11     51     45     43\n",
      "humor                16     30      8      8      9     13\n",
      "\n",
      "\n",
      "Example 3.11 (code_modal_tabulate.py): Figure 3.11: Frequency of Modals in Different Sections of the Brown Corpus\n",
      "\n",
      "Recall from the listing in 3.6 that we used a format string\n",
      "'{:{width}}' and bound a value to the width parameter in\n",
      "format().  This allows us to specify the width of a field using a\n",
      "variable.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> '{:{width}}'.format('Monty Python', width=15)\n",
      "'Monty Python   '\n",
      "\n",
      "\n",
      "\n",
      "We could use this to automatically customize the column to be\n",
      "just wide enough to accommodate all the words, using\n",
      "width = max(len(w) for w in words).\n",
      "\n",
      "\n",
      "Writing Results to a File\n",
      "We have seen how to read text from files (3.1).\n",
      "It is often useful to write output to files as well.  The following\n",
      "code opens a file output.txt for writing, and saves the program\n",
      "output to the file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> output_file = open('output.txt', 'w')\n",
      ">>> words = set(nltk.corpus.genesis.words('english-kjv.txt'))\n",
      ">>> for word in sorted(words):\n",
      "...     print(word, file=output_file)\n",
      "\n",
      "\n",
      "\n",
      "When we write non-text data to a file we must convert it to a string first.\n",
      "We can do this conversion using formatting strings, as we saw above.\n",
      "Let's write the total number of words to our file:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> len(words)\n",
      "2789\n",
      ">>> str(len(words))\n",
      "'2789'\n",
      ">>> print(str(len(words)), file=output_file)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Caution!\n",
      "You should avoid filenames that contain space characters like\n",
      "output file.txt, or that are identical except for case\n",
      "distinctions, e.g. Output.txt and output.TXT.\n",
      "\n",
      "\n",
      "\n",
      "Text Wrapping\n",
      "When the output of our program is text-like, instead of tabular,\n",
      "it will usually be necessary to wrap it so that it can be displayed\n",
      "conveniently.  Consider the following output, which overflows its line,\n",
      "and which uses a complicated print statement:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> saying = ['After', 'all', 'is', 'said', 'and', 'done', ',',\n",
      "...           'more', 'is', 'said', 'than', 'done', '.']\n",
      ">>> for word in saying:\n",
      "...     print(word, '(' + str(len(word)) + '),', end=' ')\n",
      "After (5), all (3), is (2), said (4), and (3), done (4), , (1), more (4), is (2), said (4), than (4), done (4), . (1),\n",
      "\n",
      "\n",
      "\n",
      "We can take care of line wrapping with the help of Python's textwrap module.\n",
      "For maximum clarity we will separate each step onto its own line:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from textwrap import fill\n",
      ">>> pieces = [\"{} {}\".format(word, len(word)) for word in saying]\n",
      ">>> output = ' '.join(pieces)\n",
      ">>> wrapped = fill(output)\n",
      ">>> print(wrapped)\n",
      "After (5), all (3), is (2), said (4), and (3), done (4), , (1), more\n",
      "(4), is (2), said (4), than (4), done (4), . (1),\n",
      "\n",
      "\n",
      "\n",
      "Notice that there is a linebreak between more and its following number.\n",
      "If we wanted to avoid this, we could\n",
      "redefine the formatting string so that it contained no spaces,\n",
      "e.g. '%s_(%d),', then instead of printing the value of wrapped,\n",
      "we could print wrapped.replace('_', ' ').\n",
      "\n",
      "\n",
      "\n",
      "3.10   Summary\n",
      "\n",
      "In this book we view a text as a list of words.  A \"raw text\" is a potentially\n",
      "long string containing words and whitespace formatting, and is how we\n",
      "typically store and visualize a text.\n",
      "A string is specified in Python using single or double quotes: 'Monty Python', \"Monty Python\".\n",
      "The characters of a string are accessed using indexes, counting from zero:\n",
      "'Monty Python'[0] gives the value M.  The length of a string is\n",
      "found using len().\n",
      "Substrings are accessed using slice notation: 'Monty Python'[1:5]\n",
      "gives the value onty.  If the start index is omitted, the\n",
      "substring begins at the start of the string; if the end index is omitted,\n",
      "the slice continues to the end of the string.\n",
      "Strings can be split into lists: 'Monty Python'.split() gives\n",
      "['Monty', 'Python'].  Lists can be joined into strings:\n",
      "'/'.join(['Monty', 'Python']) gives 'Monty/Python'.\n",
      "We can read text from a file input.txt using text = open('input.txt').read().\n",
      "We can read text from url using text = request.urlopen(url).read().decode('utf8').\n",
      "We can iterate over the lines of a text file using for line in open(f).\n",
      "We can write text to a file by opening the file for writing\n",
      "output_file = open('output.txt', 'w'), then adding content to the\n",
      "file print(\"Monty Python\", file=output_file).\n",
      "Texts found on the web may contain unwanted material (such as headers, footers, markup),\n",
      "that need to be removed before we do any linguistic processing.\n",
      "Tokenization is the segmentation of a text into basic units — or tokens —\n",
      "such as words and punctuation.\n",
      "Tokenization based on whitespace is inadequate for many applications because it\n",
      "bundles punctuation together with words.\n",
      "NLTK provides an off-the-shelf tokenizer nltk.word_tokenize().\n",
      "Lemmatization is a process that maps the various forms of a word (such as appeared, appears)\n",
      "to the canonical or citation form of the word, also known as the lexeme or lemma (e.g. appear).\n",
      "Regular expressions are a powerful and flexible method of specifying\n",
      "patterns. Once we have imported the re module, we can use\n",
      "re.findall() to find all substrings in a string that match a pattern.\n",
      "If a regular expression string includes a backslash, you should tell Python not to\n",
      "preprocess the string, by using a raw string with an r prefix: r'regexp'.\n",
      "When backslash is used before certain characters, e.g. \\n, this takes on\n",
      "a special meaning (newline character); however, when backslash is used\n",
      "before regular expression wildcards and operators, e.g. \\., \\|, \\$,\n",
      "these characters lose their special meaning and are matched literally.\n",
      "A string formatting expression template % arg_tuple consists of a\n",
      "format string template that contains conversion specifiers\n",
      "like %-6s and %0.2d.\n",
      "\n",
      "\n",
      "\n",
      "3.11   Further Reading\n",
      "Extra materials for this chapter are posted at http://nltk.org/, including links to freely\n",
      "available resources on the web.  Remember to consult the Python reference materials\n",
      "at http://docs.python.org/.  (For example, this documentation covers \"universal newline support,\"\n",
      "explaining how to work with the different newline conventions used by various\n",
      "operating systems.)\n",
      "For more examples of processing words with NLTK, see the\n",
      "tokenization, stemming and corpus HOWTOs at http://nltk.org/howto.\n",
      "Chapters 2 and 3 of (Jurafsky & Martin, 2008) contain more advanced\n",
      "material on regular expressions and morphology.  For more extensive\n",
      "discussion of text processing with Python see (Mertz, 2003).\n",
      "For information about normalizing non-standard words see (Sproat et al, 2001)\n",
      "There are many references for regular expressions, both practical and\n",
      "theoretical.  For an introductory\n",
      "tutorial to using regular expressions in Python,\n",
      "see Kuchling's Regular Expression HOWTO,\n",
      "http://www.amk.ca/python/howto/regex/.\n",
      "For a comprehensive and detailed manual\n",
      "in using regular expressions, covering their syntax in most major\n",
      "programming languages, including Python, see (Friedl, 2002).\n",
      "Other presentations include Section 2.1 of (Jurafsky & Martin, 2008),\n",
      "and Chapter 3 of (Mertz, 2003).\n",
      "There are many online resources for Unicode.  Useful discussions\n",
      "of Python's facilities for handling Unicode are:\n",
      "\n",
      "Ned Batchelder, Pragmatic Unicode, http://nedbatchelder.com/text/unipain.html\n",
      "Unicode HOWTO, Python Documentation,\n",
      "http://docs.python.org/3/howto/unicode.html\n",
      "David Beazley, Mastering Python 3 I/O,\n",
      "http://pyvideo.org/video/289/pycon-2010--mastering-python-3-i-o\n",
      "Joel Spolsky, The Absolute Minimum Every Software Developer\n",
      "Absolutely, Positively Must Know About Unicode and Character Sets\n",
      "(No Excuses!), http://www.joelonsoftware.com/articles/Unicode.html\n",
      "\n",
      "The problem of tokenizing Chinese text is a major focus of SIGHAN,\n",
      "the ACL Special Interest Group on Chinese Language Processing\n",
      "http://sighan.org/.  Our method for segmenting English text\n",
      "follows (Brent, 1995); this work falls in the area of\n",
      "language acquisition (Niyogi, 2006).\n",
      "Collocations are a special case of multiword expressions.\n",
      "A multiword expression is a small phrase whose meaning\n",
      "and other properties cannot be predicted from its words alone,\n",
      "e.g. part of speech (Baldwin & Kim, 2010).\n",
      "Simulated annealing is a heuristic for finding\n",
      "a good approximation to the optimum value of\n",
      "a function in a large, discrete search space,\n",
      "based on an analogy with annealing in metallurgy.\n",
      "The technique is described in many Artificial Intelligence texts.\n",
      "The approach to discovering hyponyms in text using search\n",
      "patterns like x and other ys is described by (Hearst, 1992).\n",
      "\n",
      "\n",
      "3.12   Exercises\n",
      "\n",
      "☼ Define a string s = 'colorless'.  Write a Python statement\n",
      "that changes this to \"colourless\" using only the slice and\n",
      "concatenation operations.\n",
      "\n",
      "☼ We can use the slice notation to remove morphological endings on\n",
      "words.  For example, 'dogs'[:-1] removes the last character of\n",
      "dogs, leaving dog.  Use slice notation to remove the\n",
      "affixes from these words (we've inserted a hyphen to\n",
      "indicate the affix boundary, but omit this from your strings):\n",
      "dish-es, run-ning, nation-ality, un-do,\n",
      "pre-heat.\n",
      "\n",
      "☼ We saw how we can generate an IndexError by indexing beyond the end\n",
      "of a string.  Is it possible to construct an index that goes too far to\n",
      "the left, before the start of the string?\n",
      "\n",
      "☼ We can specify a \"step\" size for the slice. The following\n",
      "returns every second character within the slice: monty[6:11:2].\n",
      "It also works in the reverse direction: monty[10:5:-2]\n",
      "Try these for yourself, then experiment with different step values.\n",
      "\n",
      "☼ What happens if you ask the interpreter to evaluate monty[::-1]?\n",
      "Explain why this is a reasonable result.\n",
      "\n",
      "☼ Describe the class of strings matched by the following regular\n",
      "expressions.\n",
      "\n",
      "[a-zA-Z]+\n",
      "[A-Z][a-z]*\n",
      "p[aeiou]{,2}t\n",
      "\\d+(\\.\\d+)?\n",
      "([^aeiou][aeiou][^aeiou])*\n",
      "\\w+|[^\\w\\s]+\n",
      "\n",
      "Test your answers using nltk.re_show().\n",
      "\n",
      "☼ Write regular expressions to match the following classes of strings:\n",
      "\n",
      "\n",
      "A single determiner (assume that a, an, and the\n",
      "are the only determiners).\n",
      "An arithmetic expression using integers, addition, and\n",
      "multiplication, such as 2*3+8.\n",
      "\n",
      "\n",
      "\n",
      "☼ Write a utility function that takes a URL as its argument, and returns\n",
      "the contents of the URL, with all HTML markup removed.  Use from urllib import request\n",
      "and then request.urlopen('http://nltk.org/').read().decode('utf8') to access the contents of the URL.\n",
      "\n",
      "☼\n",
      "Save some text into a file corpus.txt.  Define a function load(f)\n",
      "that reads from the file named in its sole argument, and returns a string\n",
      "containing the text of the file.\n",
      "\n",
      "Use nltk.regexp_tokenize() to create a tokenizer that tokenizes\n",
      "the various kinds of punctuation in this text.  Use one multi-line\n",
      "regular expression, with inline comments, using the verbose flag (?x).\n",
      "Use nltk.regexp_tokenize() to create a tokenizer that tokenizes\n",
      "the following kinds of expression: monetary amounts; dates; names\n",
      "of people and organizations.\n",
      "\n",
      "\n",
      "☼ Rewrite the following loop as a list comprehension:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
      ">>> result = []\n",
      ">>> for word in sent:\n",
      "...     word_len = (word, len(word))\n",
      "...     result.append(word_len)\n",
      ">>> result\n",
      "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "☼ Define a string raw containing a sentence of your own choosing.\n",
      "Now, split raw on some character other than space, such as 's'.\n",
      "\n",
      "☼ Write a for loop to print out the characters of a string, one per line.\n",
      "\n",
      "☼ What is the difference between calling split on a string\n",
      "with no argument or with ' ' as the argument,\n",
      "e.g. sent.split() versus sent.split(' ')?  What happens\n",
      "when the string being split contains tab characters, consecutive\n",
      "space characters, or a sequence of tabs and spaces?  (In IDLE you\n",
      "will need to use '\\t' to enter a tab character.)\n",
      "\n",
      "☼ Create a variable words containing a list of words.\n",
      "Experiment with words.sort() and sorted(words).\n",
      "What is the difference?\n",
      "\n",
      "☼ Explore the difference between strings and integers by typing\n",
      "the following at a Python prompt: \"3\" * 7 and 3 * 7.\n",
      "Try converting between strings and integers using\n",
      "int(\"3\") and str(3).\n",
      "\n",
      "☼ Use a text editor to create a file\n",
      "called prog.py containing the single line monty = 'Monty Python'.\n",
      "Next, start up a new session with the\n",
      "Python interpreter, and enter the expression monty at the prompt.\n",
      "You will get an error from the interpreter. Now, try the following\n",
      "(note that you have to leave off the .py part of the filename):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from prog import monty\n",
      ">>> monty\n",
      "\n",
      "\n",
      "\n",
      "This time, Python should return with a value. You can also try\n",
      "import prog, in which case Python should be able to\n",
      "evaluate the expression prog.monty at the prompt.\n",
      "\n",
      "☼ What happens when the formatting strings %6s and %-6s\n",
      "are used to display strings that are longer than six characters?\n",
      "\n",
      "◑ Read in some text from a corpus, tokenize it, and print the list of\n",
      "all wh-word types that occur.  (wh-words in English\n",
      "are used in questions, relative clauses and exclamations:\n",
      "who, which, what, and so on.) Print\n",
      "them in order.  Are any words duplicated in this list, because of\n",
      "the presence of case distinctions or punctuation?\n",
      "\n",
      "◑ Create a file consisting of words and (made up) frequencies, where each\n",
      "line consists of a word, the space character, and a positive integer,\n",
      "e.g. fuzzy 53.  Read the file into a Python list using open(filename).readlines().\n",
      "Next, break each line into its two fields using split(), and\n",
      "convert the number into an integer using int().  The result should\n",
      "be a list of the form: [['fuzzy', 53], ...].\n",
      "\n",
      "◑ Write code to access a favorite webpage and extract some text from it.\n",
      "For example, access a weather site and extract the forecast top\n",
      "temperature for your town or city today.\n",
      "\n",
      "◑ Write a function unknown() that takes a URL as its argument,\n",
      "and returns a list of unknown words that occur on that webpage.\n",
      "In order to do this, extract all substrings consisting of lowercase letters\n",
      "(using re.findall()) and remove any items from this set that occur\n",
      "in the Words Corpus (nltk.corpus.words).  Try to categorize these words\n",
      "manually and discuss your findings.\n",
      "\n",
      "◑ Examine the results of processing the URL\n",
      "http://news.bbc.co.uk/ using the regular expressions suggested\n",
      "above. You will see that there is still a fair amount of\n",
      "non-textual data there, particularly Javascript commands. You may\n",
      "also find that sentence breaks have not been properly\n",
      "preserved. Define further regular expressions that improve the\n",
      "extraction of text from this web page.\n",
      "\n",
      "◑ Are you able to write a regular expression to tokenize text in such\n",
      "a way that the word don't is tokenized into do and n't?\n",
      "Explain why this regular expression won't work: «n't|\\w+».\n",
      "\n",
      "◑ Try to write code to convert text into hAck3r, using regular expressions\n",
      "and substitution, where\n",
      "e → 3,\n",
      "i → 1,\n",
      "o → 0,\n",
      "l → |,\n",
      "s → 5,\n",
      ". → 5w33t!,\n",
      "ate → 8.\n",
      "Normalize the text to lowercase before converting it.\n",
      "Add more substitutions of your own.  Now try to map\n",
      "s to two different values: $ for word-initial s,\n",
      "and 5 for word-internal s.\n",
      "\n",
      "◑ Pig Latin is a simple transformation of English text.  Each word of the\n",
      "text is converted as follows: move any consonant (or consonant cluster)\n",
      "that appears at the start of the word to the end,\n",
      "then append ay, e.g. string → ingstray,\n",
      "idle → idleay.  http://en.wikipedia.org/wiki/Pig_Latin\n",
      "\n",
      "Write a function to convert a word to Pig Latin.\n",
      "Write code that converts text, instead of individual words.\n",
      "Extend it further to preserve capitalization, to keep qu together\n",
      "(i.e. so that quiet becomes ietquay), and to detect when y\n",
      "is used as a consonant (e.g. yellow) vs a vowel (e.g. style).\n",
      "\n",
      "\n",
      "◑ Download some text from a language that has vowel harmony (e.g. Hungarian),\n",
      "extract the vowel sequences of words, and create a vowel bigram table.\n",
      "\n",
      "◑ Python's random module includes a function choice() which\n",
      "randomly chooses an item from a sequence, e.g. choice(\"aehh \") will\n",
      "produce one of four possible characters, with the letter h being\n",
      "twice as frequent as the others.  Write a generator expression\n",
      "that produces a sequence of 500 randomly chosen letters drawn from the\n",
      "string \"aehh \", and put this\n",
      "expression inside a call to the ''.join() function, to concatenate\n",
      "them into one long string.  You should get a result that looks like\n",
      "uncontrolled sneezing or maniacal laughter: he  haha ee  heheeh eha.\n",
      "Use split() and join() again to normalize the whitespace in\n",
      "this string.\n",
      "\n",
      "◑ Consider the numeric expressions in the following sentence from\n",
      "the MedLine Corpus: The corresponding free cortisol fractions in these\n",
      "sera were 4.53 +/- 0.15% and 8.16 +/- 0.23%, respectively.\n",
      "Should we say that the numeric expression 4.53 +/- 0.15% is three\n",
      "words?  Or should we say that it's a single compound word?  Or should\n",
      "we say that it is actually nine words, since it's read \"four point\n",
      "five three, plus or minus zero point fifteen percent\"?  Or should we say that\n",
      "it's not a \"real\" word at all, since it wouldn't appear in any dictionary?\n",
      "Discuss these different possibilities.  Can you think of application domains\n",
      "that motivate at least two of these answers?\n",
      "\n",
      "◑ Readability measures are used to score the reading difficulty of a\n",
      "text, for the purposes of selecting texts of appropriate difficulty\n",
      "for language learners.  Let us define\n",
      "μw to be the average number of letters per word, and\n",
      "μs to be the average number of words per sentence, in\n",
      "a given text.  The Automated Readability Index (ARI) of the text\n",
      "is defined to be:\n",
      "4.71 μw + 0.5 μs - 21.43.\n",
      "Compute the ARI score for various sections of the Brown Corpus, including\n",
      "section f (lore) and j (learned).  Make use of the fact that\n",
      "nltk.corpus.brown.words() produces a sequence of words, while\n",
      "nltk.corpus.brown.sents() produces a sequence of sentences.\n",
      "\n",
      "◑ Use the Porter Stemmer to normalize some tokenized text, calling\n",
      "the stemmer on each word.  Do the same thing with the Lancaster Stemmer\n",
      "and see if you observe any differences.\n",
      "\n",
      "◑ Define the variable saying to contain the list\n",
      "['After', 'all', 'is', 'said', 'and', 'done', ',', 'more',\n",
      "'is', 'said', 'than', 'done', '.'].  Process this list using a for loop, and store the\n",
      "length of each word in a new list lengths.  Hint: begin by assigning the\n",
      "empty list to lengths, using lengths = []. Then each time\n",
      "through the loop, use append() to add another length value to\n",
      "the list. Now do the same thing using a list comprehension.\n",
      "\n",
      "◑ Define a variable silly to contain the string:\n",
      "'newly formed bland ideas are inexpressible in an infuriating\n",
      "way'.  (This happens to be the legitimate interpretation that\n",
      "bilingual English-Spanish speakers can assign to Chomsky's\n",
      "famous nonsense phrase, colorless green ideas sleep furiously\n",
      "according to Wikipedia).  Now write code to perform the following tasks:\n",
      "\n",
      "Split silly into a list of strings, one per\n",
      "word, using Python's split() operation, and save\n",
      "this to a variable called bland.\n",
      "Extract the second letter of each word in silly and join\n",
      "them into a string, to get 'eoldrnnnna'.\n",
      "Combine the words in bland back into a single string, using join().\n",
      "Make sure the words in the resulting string are separated with\n",
      "whitespace.\n",
      "Print the words of silly in alphabetical order, one per line.\n",
      "\n",
      "\n",
      "◑ The index() function can be used to look up items in sequences.\n",
      "For example, 'inexpressible'.index('e') tells us the index of the\n",
      "first position of the letter e.\n",
      "\n",
      "What happens when you look up a substring, e.g. 'inexpressible'.index('re')?\n",
      "Define a variable words containing a list of words.  Now use words.index()\n",
      "to look up the position of an individual word.\n",
      "Define a variable silly as in the exercise above.\n",
      "Use the index() function in combination with list slicing to\n",
      "build a list phrase consisting of all the words up to (but not\n",
      "including) in in silly.\n",
      "\n",
      "\n",
      "◑ Write code to convert nationality adjectives like Canadian and\n",
      "Australian to their corresponding nouns Canada and Australia\n",
      "(see http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names).\n",
      "\n",
      "◑ Read the LanguageLog post on phrases of the form as best as p can\n",
      "and as best p can, where p is a pronoun.   Investigate this\n",
      "phenomenon with the help of a corpus and the findall() method\n",
      "for searching tokenized text described in\n",
      "3.5.\n",
      "http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html\n",
      "\n",
      "◑ Study the lolcat version of the book of Genesis,\n",
      "accessible as nltk.corpus.genesis.words('lolcat.txt'), and\n",
      "the rules for converting text into lolspeak at\n",
      "http://www.lolcatbible.com/index.php?title=How_to_speak_lolcat.\n",
      "Define regular expressions to convert English words into\n",
      "corresponding lolspeak words.\n",
      "\n",
      "◑ Read about the re.sub() function for string substitution\n",
      "using regular expressions, using help(re.sub) and by consulting\n",
      "the further readings for this chapter.  Use re.sub in writing code\n",
      "to remove HTML tags from an HTML file, and to normalize whitespace.\n",
      "\n",
      "★ An interesting challenge for tokenization is words that have been\n",
      "split across a line-break.  E.g. if long-term is split, then we\n",
      "have the string long-\\nterm.\n",
      "\n",
      "Write a regular expression that identifies words that are\n",
      "hyphenated at a line-break.  The expression will need to include the\n",
      "\\n character.\n",
      "Use re.sub() to remove the \\n character from these\n",
      "words.\n",
      "How might you identify words that should not remain hyphenated\n",
      "once the newline is removed, e.g. 'encyclo-\\npedia'?x\n",
      "\n",
      "\n",
      "★ Read the Wikipedia entry on Soundex.  Implement this\n",
      "algorithm in Python.\n",
      "\n",
      "★ Obtain raw texts from two or more genres and compute their respective\n",
      "reading difficulty scores as in the earlier exercise on reading difficulty.\n",
      "E.g. compare ABC Rural News and ABC Science News (nltk.corpus.abc).\n",
      "Use Punkt to perform sentence segmentation.\n",
      "\n",
      "★ Rewrite the following nested loop as a nested list comprehension:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> words = ['attribution', 'confabulation', 'elocution',\n",
      "...          'sequoia', 'tenacious', 'unidirectional']\n",
      ">>> vsequences = set()\n",
      ">>> for word in words:\n",
      "...     vowels = []\n",
      "...     for char in word:\n",
      "...         if char in 'aeiou':\n",
      "...             vowels.append(char)\n",
      "...     vsequences.add(''.join(vowels))\n",
      ">>> sorted(vsequences)\n",
      "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "★ Use WordNet to create a semantic index for a text collection.\n",
      "Extend the concordance search program in 3.6,\n",
      "indexing each word using the offset of its first synset,\n",
      "e.g. wn.synsets('dog')[0].offset (and optionally the\n",
      "offset of some of its ancestors in the hypernym hierarchy).\n",
      "\n",
      "★ With the help of a multilingual corpus such as the\n",
      "Universal Declaration of Human Rights Corpus (nltk.corpus.udhr),\n",
      "and NLTK's frequency distribution and rank correlation functionality\n",
      "(nltk.FreqDist, nltk.spearman_correlation),\n",
      "develop a system that guesses the language of a previously unseen text.\n",
      "For simplicity, work with a single character encoding and just a few\n",
      "languages.\n",
      "\n",
      "★ Write a program that processes a text and discovers\n",
      "cases where a word has been used with a novel sense.\n",
      "For each word, compute the WordNet similarity\n",
      "between all synsets of the word and all synsets of the\n",
      "words in its context.  (Note that this is a crude\n",
      "approach; doing it well is a difficult, open research problem.)\n",
      "\n",
      "★ Read the article on normalization of non-standard words\n",
      "(Sproat et al, 2001), and implement a similar system for text normalization.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "About this document...\n",
      "UPDATED FOR NLTK 3.0.\n",
      "This is a chapter from Natural Language Processing with Python,\n",
      "by Steven Bird, Ewan Klein and Edward Loper,\n",
      "Copyright © 2019 the authors.\n",
      "It is distributed with the Natural Language Toolkit [http://nltk.org/],\n",
      "Version 3.0, under the terms of the\n",
      "Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\n",
      "[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\n",
      "This document was built on\n",
      "Wed  4 Sep 2019 11:40:48 ACST\n",
      "vectors[3] = [0, 7, 3, 27, 31, 130, 11, 27, 0, 0, 4, 107, 53, 14, 0, 1, 1, 2, 4, 0, 8, 0, 1, 0, 0, 0, 0, 5, 10, 19, 4, 4, 2, 1, 2, 6, 0, 0, 37, 0, 17, 6, 1, 262, 10, 6, 20, 6, 1, 1, 2, 2, 2, 3, 0, 6, 16, 9, 2, 0, 6, 0, 0, 0, 53, 1, 0, 9, 0, 201, 0, 0, 18, 49, 10, 15, 6, 0, 0, 2, 3, 3, 56, 0, 4, 4, 34, 3, 1, 0, 2, 0, 0, 4, 0, 205, 2, 0, 1, 16, 4, 0, 0, 0, 1, 0, 4, 1, 1, 1, 5, 14, 2, 0, 0, 0, 0, 0, 1, 0, 55, 0, 2, 11, 0, 0, 1, 0, 5, 90, 7, 32, 10, 6, 15, 1, 419, 82, 58, 11, 1, 3, 22, 2, 1, 0, 26, 25, 15, 3, 7, 0, 3, 0, 0, 0, 0, 0, 1, 19, 4, 18, 0, 6, 1, 0, 0, 0, 1, 0, 0, 0, 1, 28, 0, 2, 11, 16, 10, 1, 2, 5, 0, 0, 0, 0, 0, 2, 0, 1, 1, 14, 9, 10, 1, 1, 0, 34, 5, 30, 21, 0, 0, 6, 0, 216, 2, 15, 4, 1, 0, 5, 1108, 0, 0, 4, 1, 0, 3, 2, 26, 2, 40, 72, 2, 2, 0, 0, 2, 8, 86, 19, 0, 2, 2, 2, 1, 0, 0, 0, 8, 39, 10, 0, 3, 3, 0, 0, 0, 3, 13, 14, 72, 5, 0, 0, 0, 2, 0, 1, 0, 23, 5, 4, 0, 0, 0, 1, 5, 0, 8, 1, 25, 0, 1, 0, 0, 0, 1, 11, 1, 1, 0, 0, 2, 1, 0, 2, 0, 0, 0, 8, 0, 6, 0, 28, 2, 18, 0, 0, 1, 1, 1, 1, 0, 1, 0, 4, 57, 3, 31, 1, 0, 1, 2, 0, 3, 18, 0, 2, 0, 0, 2, 6, 1, 0, 1, 6, 48, 10, 1, 4, 2, 0, 0, 4, 0, 21, 0, 1, 0, 0, 1, 6, 2, 0, 0, 0, 0, 1, 0, 1, 0, 18, 5, 14, 0, 10, 0, 10, 1, 1, 15, 0, 4, 5, 3, 4, 0, 0, 105, 10, 2, 41, 156, 0, 0, 0, 4, 3, 0, 3, 2, 0, 2, 3, 0, 0, 1, 29, 16, 11, 9, 17, 1, 25, 0, 2, 8, 5, 19, 13, 5, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 1, 0, 84, 12, 0, 0, 0, 1, 0, 3, 0, 0, 95, 0, 0, 0, 0, 10, 25, 0, 3, 0, 0, 34, 0, 2, 28, 0, 17, 3, 1, 0, 0, 0, 3, 1, 20, 0, 0, 0, 0, 8, 5, 0, 2, 0, 3, 0, 1, 15, 2, 3, 0, 10, 1, 2, 0, 3, 1, 2, 0, 1, 68, 16, 85, 30, 59, 6, 13, 310, 12, 0, 63, 30, 1, 2, 84, 0, 2, 1, 19, 0, 4, 1, 14, 5, 0, 0, 14, 18, 1, 19, 2, 2, 2, 1, 25, 122, 197, 15, 84, 9, 3, 4, 72, 18, 5, 10, 0, 11, 13, 54, 6, 2, 2, 4, 0, 2, 5, 0, 4, 9, 4, 0, 0, 0, 0, 5, 0, 0, 4, 1, 3, 1, 1, 0, 2, 0, 1, 0, 1, 4, 3, 0, 0, 18, 0, 2, 2, 0, 1, 1, 0, 1, 3, 0, 0, 2, 0, 4, 0, 2, 1, 1, 0, 3, 5, 13, 0, 0, 12, 5, 0, 0, 1, 2, 16, 4, 0, 0, 6, 11, 31, 1, 5, 0, 0, 2, 2, 4, 8, 19, 0, 12, 8, 0, 2, 11, 3, 2, 10, 30, 35, 2, 1, 0, 0, 56, 24, 1, 0, 7, 12, 44, 0, 29, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 1, 3, 0, 5, 0, 1, 0, 0, 0, 0, 5, 3, 0, 0, 0, 5, 1, 0, 0, 1, 29, 10, 0, 0, 0, 0, 14, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 14, 0, 0, 1, 0, 1, 0, 2, 0, 0, 6, 2, 0, 3, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 9, 0, 2, 2, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 5, 10, 0, 4, 0, 7, 5, 13, 2, 16, 6, 0, 0, 2, 0, 2, 54, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 4, 1, 0, 72, 1, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 1, 3, 0, 0, 0, 0, 1, 4, 0, 4, 1, 0, 0, 1, 0, 0, 5, 0, 4, 1, 1, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 21, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 7, 3, 12, 0, 10, 13, 0, 3, 1, 0, 0, 3, 1, 0, 0, 0, 3, 0, 21, 0, 0, 0, 14, 1, 0, 0, 12, 2, 0, 0, 2, 0, 3, 7, 0, 0, 1, 1, 0, 4, 1, 1, 0, 3, 2, 0, 0, 0, 7, 1, 11, 0, 0, 13, 12, 0, 0, 2, 2, 0, 0, 2, 1, 2, 0, 4, 3, 0, 0, 4, 2, 1, 0, 71, 0, 0, 1, 0, 2, 2, 3, 2, 0, 0, 0, 0, 1, 2, 2, 0, 0, 2, 2, 0, 1, 0, 0, 3, 3, 0, 0, 6, 0, 9, 0, 0, 0, 0, 0, 20, 0, 2, 0, 0, 0, 0, 0, 19, 0, 3, 55, 0, 4, 2, 0, 0, 6, 2, 0, 1, 0, 0, 8, 4, 0, 0, 6, 2, 3, 0, 1, 2, 16, 4, 0, 1, 22, 1, 11, 5, 0, 4, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 9, 21, 0, 0, 0, 0, 0, 2, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 2, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0, 3, 1, 8, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 8, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 5, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 4, 9, 0, 1, 2, 0, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 6, 1, 0, 0, 1, 1, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 14, 0, 2, 0, 23, 1, 1, 2, 1, 79, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 5, 1, 1, 5, 3, 1, 1, 1, 1, 0, 0, 11, 11, 2, 0, 1, 0, 0, 0, 1, 0, 4, 0, 0, 0, 0, 0, 0, 5, 0, 4, 1, 2, 0, 2, 5, 1, 0, 14, 0, 9, 2, 2, 0, 10, 0, 39, 9, 0, 2, 0, 3, 1, 0, 3, 38, 6, 0, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 17, 0, 29, 0, 0, 6, 4, 0, 0, 8, 1, 0, 26, 0, 2, 5, 1, 23, 6, 0, 0, 0, 19, 7, 0, 46, 15, 0, 0, 1, 0, 9, 14, 6, 0, 0, 2, 0, 0, 0, 0, 0, 0, 8, 0, 3, 12, 8, 9, 4, 1, 1, 3, 8, 2, 2, 0, 23, 0, 1, 0, 4, 1, 1, 9, 0, 1, 6, 0, 1, 6, 0, 3, 2, 1, 0, 0, 1, 16, 3, 0, 1, 3, 2, 0, 3, 5, 0, 5, 20, 1, 2, 0, 3, 26, 1, 1, 9, 16, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 57, 4, 12, 0, 1, 2, 1, 0, 0, 6, 1, 37, 1, 0, 17, 11, 4, 2, 0, 0, 13, 42, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 6, 0, 1, 0, 0, 0, 1, 0, 4, 2, 10, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 9, 0, 2, 0, 0, 13, 2, 0, 3, 0, 1, 3, 0, 1, 0, 0, 0, 1, 0, 0, 0, 13, 0, 1, 1, 1, 0, 0, 0, 9, 0, 5, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 4, 0, 0, 0, 0, 3, 3, 6, 5, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 8, 14, 1, 0, 7, 1, 5, 0, 2, 0, 0, 4, 2, 26, 19, 0, 1, 0, 0, 0, 0, 11, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 1, 0, 2, 9, 1, 3, 5, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 4, 0, 1, 9, 0, 2, 7, 0, 2, 3, 9, 1, 13, 22, 0, 0, 0, 38, 0, 0, 1, 11, 0, 1, 0, 0, 4, 9, 17, 0, 780, 5, 1, 2, 23, 0, 26, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 2, 2, 0, 0, 1, 7, 4, 1, 0, 3, 0, 18, 3, 0, 1, 11, 0, 1, 7, 0, 9, 3, 0, 4, 2, 0, 4, 0, 2, 3, 0, 0, 0, 15, 0, 14, 0, 1, 0, 0, 2, 0, 2, 0, 9, 3, 1, 0, 1, 13, 0, 0, 0, 6, 0, 3, 1, 5, 6, 7, 5, 0, 1, 0, 0, 7, 1, 0, 2, 0, 1, 0, 6, 3, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 12, 0, 4, 15, 3, 172, 59, 0, 8, 0, 3, 2, 0, 1, 3, 5, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 10, 0, 0, 1, 1, 3, 36, 3, 3, 6, 1, 2, 0, 0, 1, 0, 0, 10, 5, 3, 3, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 51, 0, 0, 0, 0, 10, 5, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 11, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 9, 9, 3, 0, 1, 0, 0, 0, 16, 4, 10, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 8, 8, 0, 0, 0, 0, 1, 0, 5, 0, 6, 0, 0, 0, 0, 14, 31, 1, 0, 0, 0, 0, 0, 3, 7, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 8, 0, 2, 0, 3, 0, 0, 6, 0, 2, 9, 0, 0, 0, 2, 0, 0, 0, 1, 4, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 24, 2, 2, 1, 0, 0, 0, 1, 2, 0, 0, 2, 1, 3, 4, 15, 5, 0, 0, 0, 3, 2, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 90, 0, 3, 0, 7, 3, 8, 69, 0, 0, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 4, 1, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 1, 7, 4, 2, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 1, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 15, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 4, 0, 0, 0, 0, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 2, 2, 1, 0, 16, 1, 0, 0, 1, 2, 0, 2, 0, 0, 1, 5, 2, 0, 0, 1, 5, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 4, 0, 0, 0, 0, 2, 1, 0, 0, 0, 24, 0, 0, 5, 0, 0, 0, 2, 0, 0, 4, 1, 0, 0, 0, 3, 0, 3, 0, 1, 0, 0, 2, 0, 3, 6, 12, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 11, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 4, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 4, 2, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 7, 1, 0, 0, 3, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 1, 3, 1, 0, 4, 0, 0, 0, 2, 0, 0, 0, 6, 2, 0, 0, 1, 0, 0, 1, 0, 0, 3, 0, 0, 0, 3, 0, 1, 0, 0, 0, 3, 0, 1, 0, 11, 1, 0, 4, 0, 2, 0, 1, 0, 0, 0, 0, 0, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 16, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 2, 2, 0, 0, 4, 0, 0, 5, 2, 0, 0, 0, 3, 9, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 1, 0, 0, 2, 0, 3, 0, 0, 15, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 13, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 18, 1, 1, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 1, 1, 1, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 6, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 11, 8, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 0, 9, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 6, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 1, 0, 3, 1, 0, 0, 3, 0, 1, 0, 2, 1, 4, 1, 3, 0, 4, 0, 4, 2, 1, 1, 3, 2, 1, 4, 2, 1, 1, 1, 2, 1, 2, 1, 1, 3, 1, 3, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 10, 1, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 4, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 7, 0, 0, 0, 5, 0, 0, 16, 0, 0, 0, 0, 0, 78, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 9, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 4, 2, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 6, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 1, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 14, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 0, 0, 0, 0, 0, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 3, 2, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 3, 0, 9, 0, 2, 0, 0, 1, 0, 3, 4, 0, 9, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 1, 0, 1, 0, 17, 0, 0, 1, 0, 0, 0, 0, 0, 2, 4, 2, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 8, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 0, 5, 0, 0, 1, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 15, 5, 3, 1, 1, 5, 3, 10, 2, 7, 1, 1, 3, 4, 3, 2, 6, 4, 19, 7, 1, 2, 2, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 4, 1, 1, 2, 4, 2, 1, 1, 1, 3, 2, 1, 5, 2, 1, 1, 3, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 4, 2, 5, 1, 2, 1, 2, 2, 1, 1, 1, 1, 6, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 3, 1, 2, 4, 3, 2, 3, 3, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 4, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 16, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 1, 2, 1, 9, 1, 2, 2, 2, 1, 1, 1, 3, 2, 3, 1, 1, 1, 4, 2, 10, 3, 2, 8, 4, 1, 9, 2, 10, 8, 5, 3, 5, 1, 1, 1, 8, 5, 2, 1, 2, 12, 6, 1, 8, 4, 2, 1, 4, 4, 4, 4, 4, 4, 1, 1, 1, 5, 1, 2, 1, 1, 2, 2, 1, 2, 2, 3, 1, 1, 2, 1, 2, 1, 6, 3, 2, 2, 1, 1, 1, 1, 1, 1, 5, 1, 2, 1, 2, 2, 3, 2, 4, 1, 1, 1, 2, 2, 1, 1, 2, 1, 4, 1, 1, 1, 1, 1, 3, 1, 2, 1, 2, 5, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 10, 5, 1, 7, 1, 2, 1, 7, 1, 3, 6, 2, 2, 1, 1, 1, 1, 1, 5, 2, 2, 1, 2, 2, 2, 2, 8, 1, 2, 2, 2, 2, 2, 2, 2, 3, 6, 6, 3, 6, 3, 3, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 7, 3, 23, 1, 1, 2, 3, 4, 4, 2, 3, 4, 5, 1, 1, 1, 1, 1, 3, 1, 2, 1, 4, 1, 1, 1, 1, 1, 4, 3, 1, 2, 4, 4, 1, 1, 1, 1, 1, 1, 4, 1, 3, 2, 1, 2, 1, 1, 1, 1, 9, 5, 1, 1, 2, 6, 1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 2, 1, 1, 5, 1, 1, 1, 5, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 3, 3, 2, 3, 1, 1, 2, 5, 2, 1, 1, 1, 2, 2, 1, 6, 3, 3, 2, 1, 7, 1, 2, 2, 3, 1, 1, 1, 1, 1, 1, 2, 1, 7, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 5, 1, 17, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 5, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 4, 2, 1, 3, 2, 1, 1, 2, 1, 2, 1, 9, 1, 1, 1, 7, 30, 1, 1, 11, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 5, 1, 1, 1, 4, 2, 1, 2, 1, 6, 4, 1, 1, 1, 1, 2, 2, 1, 2, 1, 4, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 6, 3, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 6, 6, 1, 1, 1, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 4, 1, 1, 11, 9, 8, 8, 8, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 3, 2, 1, 1, 3, 3, 1, 1, 1, 1, 1, 5, 6, 7, 8, 7, 4, 3, 4, 6, 4, 4, 5, 2, 4, 2, 3, 3, 1, 1, 2, 2, 4, 3, 5, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 3, 2, 1, 1, 5, 1, 10, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 2, 2, 1, 3, 2, 2, 1, 3, 3, 2, 2, 2, 1, 3, 1, 2, 3, 1, 1, 1, 1, 1, 2, 4, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 3, 2, 2, 1, 1, 4, 5, 1, 1, 2, 1, 1, 1, 1, 6, 1, 6, 6, 12, 6, 6, 6, 7, 1, 3, 2, 2, 4, 1, 3, 1, 2, 1, 1, 3, 1, 3, 2, 1, 1, 1, 1, 4, 3, 2, 1, 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 3, 7, 6, 1, 1, 3, 2, 4, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 5, 1, 18, 1, 1, 1, 11, 2, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 4, 1, 2, 2, 2, 2, 1, 1, 1, 4, 8, 4, 6, 3, 1, 1, 4, 23, 3, 1, 1, 2, 2, 1, 3, 2, 1, 1, 2, 2, 4, 1, 1, 1, 2, 1, 1, 2, 2, 3, 1, 12, 2, 2, 1, 1, 1, 1, 5, 2, 2, 2, 1, 2, 3, 2, 8, 3, 1, 1, 1, 14, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 1, 1, 12, 1, 1, 1, 1, 4, 1, 1, 3, 1, 1, 1, 2, 1, 3, 1, 1, 7, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 7, 4, 8, 5, 1, 1, 1, 2, 1, 1, 9, 2, 2, 2, 2, 1, 1, 1, 1, 1, 4, 2, 1, 2, 2, 4, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 4, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 4, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "docs[4] = ch04.rst2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4   Writing Structured Programs\n",
      "By now you will have a sense of the capabilities of the Python programming language\n",
      "for processing natural language.  However, if you're new to Python or to programming, you may\n",
      "still be wrestling with Python and not feel like you are in full control yet.  In this chapter we'll\n",
      "address the following questions:\n",
      "\n",
      "How can you write well-structured, readable programs that you and others will be able to re-use easily?\n",
      "How do the fundamental building blocks work, such as loops, functions and assignment?\n",
      "What are some of the pitfalls with Python programming and how can you avoid them?\n",
      "\n",
      "Along the way, you will consolidate your knowledge of fundamental programming\n",
      "constructs, learn more about using features of the Python language in a natural\n",
      "and concise way, and learn some useful techniques in visualizing natural language data.\n",
      "As before, this chapter contains many examples and\n",
      "exercises (and as before, some exercises introduce new material).\n",
      "Readers new to programming should work through them carefully\n",
      "and consult other introductions to programming if necessary;\n",
      "experienced programmers can quickly skim this chapter.\n",
      "In the other chapters of this book, we have organized the programming\n",
      "concepts as dictated by the needs of NLP.  Here we revert to a more\n",
      "conventional approach where the material is more closely tied to\n",
      "the structure of the programming language.  There's not room for\n",
      "a complete presentation of the language, so we'll just focus on\n",
      "the language constructs and idioms that are most important for NLP.\n",
      "\n",
      "4.1   Back to the Basics\n",
      "\n",
      "Assignment\n",
      "Assignment would seem to be the most elementary programming concept, not\n",
      "deserving a separate discussion.  However, there are some surprising subtleties\n",
      "here.  Consider the following code fragment:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> foo = 'Monty'\n",
      ">>> bar = foo \n",
      ">>> foo = 'Python' \n",
      ">>> bar\n",
      "'Monty'\n",
      "\n",
      "\n",
      "\n",
      "This behaves exactly as expected.  When we write bar = foo in the above\n",
      "code ,\n",
      "the value of foo (the string 'Monty') is assigned to bar.\n",
      "That is, bar is a copy of foo, so when we overwrite\n",
      "foo with a new string 'Python' on line , the value\n",
      "of bar is not affected.\n",
      "However, assignment statements do not always involve making copies in this way.\n",
      "Assignment always copies the value of an expression, but a value is not\n",
      "always what you might expect it to be.  In particular,\n",
      "the \"value\" of a structured object such as a list is actually just a\n",
      "reference to the object.  In the following example,\n",
      " assigns the reference of foo to the new variable bar.\n",
      "Now when we modify something inside foo on line , we can see\n",
      "that the contents of bar have also been changed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> foo = ['Monty', 'Python']\n",
      ">>> bar = foo \n",
      ">>> foo[1] = 'Bodkin' \n",
      ">>> bar\n",
      "['Monty', 'Bodkin']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figure 4.1: List Assignment and Computer Memory: Two list objects foo and bar reference\n",
      "the same location in the computer's memory; updating foo will also modify bar,\n",
      "and vice versa.\n",
      "\n",
      "The line bar = foo  does not copy the contents of the\n",
      "variable, only its \"object reference\".\n",
      "To understand what is going on here, we need to\n",
      "know how lists are stored in the computer's memory.\n",
      "In 4.1, we see that a list foo is\n",
      "a reference to an object stored at location 3133 (which is\n",
      "itself a series of pointers to other locations holding strings).\n",
      "When we assign bar = foo, it is just the object reference\n",
      "3133 that gets copied.\n",
      "This behavior extends to other aspects of the language, such as\n",
      "parameter passing (4.4).\n",
      "\n",
      "Let's experiment some more, by creating a variable empty holding the\n",
      "empty list, then using it three times on the next line.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> empty = []\n",
      ">>> nested = [empty, empty, empty]\n",
      ">>> nested\n",
      "[[], [], []]\n",
      ">>> nested[1].append('Python')\n",
      ">>> nested\n",
      "[['Python'], ['Python'], ['Python']]\n",
      "\n",
      "\n",
      "\n",
      "Observe that changing one of the items inside our nested list of lists changed them all.\n",
      "This is because each of the three elements is actually just a reference to one and the\n",
      "same list in memory.\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Use multiplication to create a list of lists: nested = [[]] * 3.\n",
      "Now modify one of the elements of the list, and observe that all the\n",
      "elements are changed.  Use Python's id() function to find out\n",
      "the numerical identifier for any object, and verify that\n",
      "id(nested[0]), id(nested[1]), and id(nested[2]) are\n",
      "all the same.\n",
      "\n",
      "Now, notice that when we assign a new value to one of the elements of the list,\n",
      "it does not propagate to the others:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nested = [[]] * 3\n",
      ">>> nested[1].append('Python')\n",
      ">>> nested[1] = ['Monty']\n",
      ">>> nested\n",
      "[['Python'], ['Monty'], ['Python']]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We began with a list containing three references to a single empty list object.  Then we\n",
      "modified that object by appending 'Python' to it, resulting in a list containing\n",
      "three references to a single list object ['Python'].\n",
      "Next, we overwrote one of those references with a reference to a new object ['Monty'].\n",
      "This last step modified one of the three object references inside the nested list.\n",
      "However, the ['Python'] object wasn't changed, and is still referenced from two places in\n",
      "our nested list of lists.  It is crucial to appreciate this difference between\n",
      "modifying an object via an object reference, and overwriting an object reference.\n",
      "\n",
      "Note\n",
      "Important:\n",
      "To copy the items from a list foo to a new list bar, you can write\n",
      "bar = foo[:].  This copies the object references inside the list.\n",
      "To copy a structure without copying any object references, use copy.deepcopy().\n",
      "\n",
      "\n",
      "\n",
      "Equality\n",
      "Python provides two ways to check that a pair of items are the same.\n",
      "The is operator tests for object identity.  We can use it to\n",
      "verify our earlier observations about objects.  First we create\n",
      "a list containing several copies of the same object, and demonstrate\n",
      "that they are not only identical according to ==, but also\n",
      "that they are one and the same object:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> size = 5\n",
      ">>> python = ['Python']\n",
      ">>> snake_nest = [python] * size\n",
      ">>> snake_nest[0] == snake_nest[1] == snake_nest[2] == snake_nest[3] == snake_nest[4]\n",
      "True\n",
      ">>> snake_nest[0] is snake_nest[1] is snake_nest[2] is snake_nest[3] is snake_nest[4]\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "Now let's put a new python in this nest.  We can easily show that the objects are not\n",
      "all identical:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> import random\n",
      ">>> position = random.choice(range(size))\n",
      ">>> snake_nest[position] = ['Python']\n",
      ">>> snake_nest\n",
      "[['Python'], ['Python'], ['Python'], ['Python'], ['Python']]\n",
      ">>> snake_nest[0] == snake_nest[1] == snake_nest[2] == snake_nest[3] == snake_nest[4]\n",
      "True\n",
      ">>> snake_nest[0] is snake_nest[1] is snake_nest[2] is snake_nest[3] is snake_nest[4]\n",
      "False\n",
      "\n",
      "\n",
      "\n",
      "You can do several pairwise tests to discover which position contains the interloper,\n",
      "but the id() function makes detection easier:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> [id(snake) for snake in snake_nest]\n",
      "[4557855488, 4557854763, 4557855488, 4557855488, 4557855488]\n",
      "\n",
      "\n",
      "\n",
      "This reveals that the second item of the list has a distinct identifier.  If you try\n",
      "running this code snippet yourself, expect to see different numbers in the resulting list,\n",
      "and also the interloper may be in a different position.\n",
      "\n",
      "\n",
      "Having two kinds of equality might seem strange.  However, it's really just the\n",
      "type-token distinction, familiar from natural language, here showing up in\n",
      "a programming language.\n",
      "\n",
      "\n",
      "Conditionals\n",
      "In the condition part of an if statement, a\n",
      "nonempty string or list is evaluated as true, while an empty string or\n",
      "list evaluates as false.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> mixed = ['cat', '', ['dog'], []]\n",
      ">>> for element in mixed:\n",
      "...     if element:\n",
      "...         print(element)\n",
      "...\n",
      "cat\n",
      "['dog']\n",
      "\n",
      "\n",
      "\n",
      "That is, we don't need to say if len(element) > 0: in the\n",
      "condition.\n",
      "What's the difference between using if...elif as opposed to using\n",
      "a couple of if statements in a row? Well, consider the following\n",
      "situation:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> animals = ['cat', 'dog']\n",
      ">>> if 'cat' in animals:\n",
      "...     print(1)\n",
      "... elif 'dog' in animals:\n",
      "...     print(2)\n",
      "...\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "Since the if clause of the statement is satisfied, Python never\n",
      "tries to evaluate the elif clause, so we never get to print out\n",
      "2. By contrast, if we replaced the elif by an if, then we\n",
      "would print out both 1 and 2. So an elif clause\n",
      "potentially gives us more information than a bare if clause; when\n",
      "it evaluates to true, it tells us not only that the condition is\n",
      "satisfied, but also that the condition of the main if clause was\n",
      "not satisfied.\n",
      "\n",
      "\n",
      "The functions all() and any() can be applied to a list (or other sequence) to\n",
      "check whether all or any items meet some condition:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent = ['No', 'good', 'fish', 'goes', 'anywhere', 'without', 'a', 'porpoise', '.']\n",
      ">>> all(len(w) > 4 for w in sent)\n",
      "False\n",
      ">>> any(len(w) > 4 for w in sent)\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4.2   Sequences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "So far, we have seen two kinds of sequence object: strings and lists.  Another\n",
      "kind of sequence is called a tuple.\n",
      "Tuples are formed with the comma operator , and typically enclosed\n",
      "using parentheses.  We've actually seen them in the\n",
      "previous chapters, and sometimes referred to them as \"pairs\", since\n",
      "there were always two members.  However, tuples can have any number\n",
      "of members.  Like lists and strings, tuples can be indexed \n",
      "and sliced , and have a length .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> t = 'walk', 'fem', 3 \n",
      ">>> t\n",
      "('walk', 'fem', 3)\n",
      ">>> t[0] \n",
      "'walk'\n",
      ">>> t[1:] \n",
      "('fem', 3)\n",
      ">>> len(t) \n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Caution!\n",
      "Tuples are constructed using the comma operator.  Parentheses are a more\n",
      "general feature of Python syntax, designed for grouping.\n",
      "A tuple containing the single element 'snark' is defined by adding a\n",
      "trailing comma, like this: \"'snark',\".  The empty tuple is a special\n",
      "case, and is defined using empty parentheses ().\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Let's compare strings, lists and tuples directly, and do the indexing, slice, and length\n",
      "operation on each type:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> raw = 'I turned off the spectroroute'\n",
      ">>> text = ['I', 'turned', 'off', 'the', 'spectroroute']\n",
      ">>> pair = (6, 'turned')\n",
      ">>> raw[2], text[3], pair[1]\n",
      "('t', 'the', 'turned')\n",
      ">>> raw[-3:], text[-3:], pair[-3:]\n",
      "('ute', ['off', 'the', 'spectroroute'], (6, 'turned'))\n",
      ">>> len(raw), len(text), len(pair)\n",
      "(29, 5, 2)\n",
      "\n",
      "\n",
      "\n",
      "Notice in this code sample that we computed multiple values on a\n",
      "single line, separated by commas.  These comma-separated expressions\n",
      "are actually just tuples — Python allows us to omit the\n",
      "parentheses around tuples if there is no ambiguity. When we print a\n",
      "tuple, the parentheses are always displayed. By using tuples in this\n",
      "way, we are implicitly aggregating items together.\n",
      "\n",
      "Operating on Sequence Types\n",
      "We can iterate over the items in a sequence s in a variety of useful ways,\n",
      "as shown in 4.1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Python Expression\n",
      "Comment\n",
      "\n",
      "\n",
      "\n",
      "for item in s\n",
      "iterate over the items of s\n",
      "\n",
      "for item in sorted(s)\n",
      "iterate over the items of s in order\n",
      "\n",
      "for item in set(s)\n",
      "iterate over unique elements of s\n",
      "\n",
      "for item in reversed(s)\n",
      "iterate over elements of s in reverse\n",
      "\n",
      "for item in set(s).difference(t)\n",
      "iterate over elements of s not in t\n",
      "\n",
      "\n",
      "Table 4.1: Various ways to iterate over sequences\n",
      "\n",
      "\n",
      "The sequence functions illustrated in 4.1 can be combined\n",
      "in various ways; for example, to get unique elements of s sorted\n",
      "in reverse, use reversed(sorted(set(s))).\n",
      "We can randomize the contents of a list s before iterating over\n",
      "them, using random.shuffle(s).\n",
      "We can convert between these sequence types.  For example,\n",
      "tuple(s) converts any kind of sequence into a tuple, and\n",
      "list(s) converts any kind of sequence into a list.\n",
      "We can convert a list of strings to a single string using the\n",
      "join() function, e.g. ':'.join(words).\n",
      "Some other objects, such as a FreqDist, can be converted into a\n",
      "sequence (using list() or sorted()) and support iteration, e.g.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> raw = 'Red lorry, yellow lorry, red lorry, yellow lorry.'\n",
      ">>> text = word_tokenize(raw)\n",
      ">>> fdist = nltk.FreqDist(text)\n",
      ">>> sorted(fdist)\n",
      "[',', '.', 'Red', 'lorry', 'red', 'yellow']\n",
      ">>> for key in fdist:\n",
      "...     print(key + ':', fdist[key], end='; ')\n",
      "...\n",
      "lorry: 4; red: 1; .: 1; ,: 3; Red: 1; yellow: 2\n",
      "\n",
      "\n",
      "\n",
      "In the next example, we use tuples to re-arrange the\n",
      "contents of our list.  (We can omit the parentheses\n",
      "because the comma has higher precedence than assignment.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> words = ['I', 'turned', 'off', 'the', 'spectroroute']\n",
      ">>> words[2], words[3], words[4] = words[3], words[4], words[2]\n",
      ">>> words\n",
      "['I', 'turned', 'the', 'spectroroute', 'off']\n",
      "\n",
      "\n",
      "\n",
      "This is an idiomatic and readable way to move items inside a list.\n",
      "It is equivalent to the following traditional way of doing such\n",
      "tasks that does not use tuples (notice that this method needs a\n",
      "temporary variable tmp).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tmp = words[2]\n",
      ">>> words[2] = words[3]\n",
      ">>> words[3] = words[4]\n",
      ">>> words[4] = tmp\n",
      "\n",
      "\n",
      "\n",
      "As we have seen, Python has sequence functions such as sorted() and reversed()\n",
      "that rearrange the items of a sequence.  There are also functions that\n",
      "modify the structure of a sequence and which can be handy for\n",
      "language processing.  Thus, zip() takes\n",
      "the items of two or more sequences and \"zips\" them together into a single list of tuples.\n",
      "Given a sequence s, enumerate(s) returns pairs consisting of\n",
      "an index and the item at that index.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> words = ['I', 'turned', 'off', 'the', 'spectroroute']\n",
      ">>> tags = ['noun', 'verb', 'prep', 'det', 'noun']\n",
      ">>> zip(words, tags)\n",
      "<zip object at ...>\n",
      ">>> list(zip(words, tags))\n",
      "[('I', 'noun'), ('turned', 'verb'), ('off', 'prep'),\n",
      "('the', 'det'), ('spectroroute', 'noun')]\n",
      ">>> list(enumerate(words))\n",
      "[(0, 'I'), (1, 'turned'), (2, 'off'), (3, 'the'), (4, 'spectroroute')]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "It is a widespread feature of Python 3 and NLTK 3 to only perform\n",
      "computation when required (a feature known as \"lazy evaluation\").\n",
      "If you ever see a result like <zip object at 0x10d005448> when\n",
      "you expect to see a sequence, you can force the object to be\n",
      "evaluated just by putting it in a context that expects a sequence,\n",
      "like list(x), or for item in x.\n",
      "\n",
      "For some NLP tasks it is necessary to cut up a sequence into two or more parts.\n",
      "For instance, we might want to \"train\" a system on 90% of the data and test it\n",
      "on the remaining 10%.  To do this we decide the location where we want to\n",
      "cut the data , then cut the sequence at that location .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = nltk.corpus.nps_chat.words()\n",
      ">>> cut = int(0.9 * len(text)) \n",
      ">>> training_data, test_data = text[:cut], text[cut:] \n",
      ">>> text == training_data + test_data \n",
      "True\n",
      ">>> len(training_data) / len(test_data) \n",
      "9.0\n",
      "\n",
      "\n",
      "\n",
      "We can verify that none of the original data is lost during this process, nor is it duplicated\n",
      ".  We can also verify that the ratio of the sizes of the two pieces is what\n",
      "we intended .\n",
      "\n",
      "\n",
      "Combining Different Sequence Types\n",
      "Let's combine our knowledge of these three sequence types, together with list\n",
      "comprehensions, to perform the task of sorting the words in a string by\n",
      "their length.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> words = 'I turned off the spectroroute'.split() \n",
      ">>> wordlens = [(len(word), word) for word in words] \n",
      ">>> wordlens.sort() \n",
      ">>> ' '.join(w for (_, w) in wordlens) \n",
      "'I off the turned spectroroute'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Each of the above lines of code contains a significant feature.\n",
      "A simple string is actually an object with methods defined on it such as split() .\n",
      "We use a list comprehension to build a list of tuples ,\n",
      "where each tuple consists of a number (the word length) and the\n",
      "word, e.g. (3, 'the').  We use the sort() method \n",
      "to sort the list in-place.  Finally, we discard the length\n",
      "information and join the words back into a single string .\n",
      "(The underscore  is just a regular Python variable,\n",
      "but we can use underscore by convention to indicate that we will\n",
      "not use its value.)\n",
      "We began by talking about the commonalities in these sequence types,\n",
      "but the above code illustrates important differences in their\n",
      "roles.  First, strings appear at the beginning and the end: this is\n",
      "typical in the context where our program is reading in some text and\n",
      "producing output for us to read.  Lists and tuples are used in the\n",
      "middle, but for different purposes.  A list is typically a sequence of\n",
      "objects all having the same type, of arbitrary length.  We often\n",
      "use lists to hold sequences of words.  In contrast,\n",
      "a tuple is typically a collection of objects of different types, of\n",
      "fixed length.  We often use a tuple to hold a record,\n",
      "a collection of different fields relating to some entity.\n",
      "This distinction between the use of lists and tuples takes some\n",
      "getting used to,\n",
      "so here is another example:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> lexicon = [\n",
      "...     ('the', 'det', ['Di:', 'D@']),\n",
      "...     ('off', 'prep', ['Qf', 'O:f'])\n",
      "... ]\n",
      "\n",
      "\n",
      "\n",
      "Here, a lexicon is represented as a list because it is a\n",
      "collection of objects of a single type — lexical entries —\n",
      "of no predetermined length.  An individual entry is represented as a\n",
      "tuple because it is a collection of objects with different\n",
      "interpretations, such as the orthographic form, the part of speech,\n",
      "and the pronunciations (represented in the SAMPA computer-readable\n",
      "phonetic alphabet http://www.phon.ucl.ac.uk/home/sampa/).\n",
      "Note that these pronunciations are stored using a list. (Why?)\n",
      "\n",
      "Note\n",
      "A good way to decide when to use tuples vs lists is to ask whether\n",
      "the interpretation of an item depends on its position.  For example,\n",
      "a tagged token combines two strings having different interpretation,\n",
      "and we choose to interpret the first item as the token and the\n",
      "second item as the tag.  Thus we use tuples like this: ('grail', 'noun');\n",
      "a tuple of the form ('noun', 'grail') would be nonsensical since\n",
      "it would be a word noun tagged grail.\n",
      "In contrast, the elements of a text are all tokens, and position is\n",
      "not significant.  Thus we use lists like this: ['venetian', 'blind'];\n",
      "a list of the form ['blind', 'venetian'] would be equally valid.\n",
      "The linguistic meaning of the words might be different, but the\n",
      "interpretation of list items as tokens is unchanged.\n",
      "\n",
      "The distinction between lists and tuples has been described in terms of\n",
      "usage.  However, there is a more fundamental difference: in Python,\n",
      "lists are mutable, while tuples are immutable.  In other\n",
      "words, lists can be modified, while tuples cannot.  Here are some of\n",
      "the operations on lists that do in-place modification of the list.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> lexicon.sort()\n",
      ">>> lexicon[1] = ('turned', 'VBD', ['t3:nd', 't3`nd'])\n",
      ">>> del lexicon[0]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Convert lexicon to a tuple, using lexicon = tuple(lexicon),\n",
      "then try each of the above operations, to confirm that none of\n",
      "them is permitted on tuples.\n",
      "\n",
      "\n",
      "\n",
      "Generator Expressions\n",
      "We've been making heavy use of list comprehensions, for compact and readable\n",
      "processing of texts.  Here's an example where we tokenize and normalize a text:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = '''\"When I use a word,\" Humpty Dumpty said in rather a scornful tone,\n",
      "... \"it means just what I choose it to mean - neither more nor less.\"'''\n",
      ">>> [w.lower() for w in word_tokenize(text)]\n",
      "['``', 'when', 'i', 'use', 'a', 'word', ',', \"''\", 'humpty', 'dumpty', 'said', ...]\n",
      "\n",
      "\n",
      "\n",
      "Suppose we now want to process these words further.  We can do this by inserting the above\n",
      "expression inside a call to some other function ,\n",
      "but Python allows us to omit the brackets .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> max([w.lower() for w in word_tokenize(text)]) \n",
      "'word'\n",
      ">>> max(w.lower() for w in word_tokenize(text)) \n",
      "'word'\n",
      "\n",
      "\n",
      "\n",
      "The second line uses a generator expression.  This is more than a notational convenience:\n",
      "in many language processing situations, generator expressions will be more efficient.\n",
      "In , storage for the list object must be allocated\n",
      "before the value of max() is computed.  If the text is\n",
      "very large, this could be slow.  In , the data is streamed to the calling\n",
      "function.  Since the calling function simply has to find the maximum value — the\n",
      "word which comes latest in lexicographic sort order — it can process the stream\n",
      "of data without having to store anything more than the maximum value seen so far.\n",
      "\n",
      "\n",
      "\n",
      "4.3   Questions of Style\n",
      "Programming is as much an art as a science.  The undisputed \"bible\" of programming,\n",
      "a 2,500 page multi-volume work by Donald Knuth, is called\n",
      "The Art of Computer Programming.  Many books have been written on\n",
      "Literate Programming, recognizing that humans, not just computers,\n",
      "must read and understand programs.  Here we pick up on some issues of\n",
      "programming style that have important ramifications for the readability\n",
      "of your code, including code layout, procedural vs declarative style,\n",
      "and the use of loop variables.\n",
      "\n",
      "Python Coding Style\n",
      "When writing programs you make many subtle choices about names,\n",
      "spacing, comments, and so on.  When you look at code written by\n",
      "other people, needless differences in style make it harder\n",
      "to interpret the code.  Therefore, the designers of the Python\n",
      "language have published a style guide for Python code, available\n",
      "at http://www.python.org/dev/peps/pep-0008/.\n",
      "The underlying value presented in the style guide is consistency,\n",
      "for the purpose of maximizing the readability of code.\n",
      "We briefly review some of its key recommendations here, and refer\n",
      "readers to the full guide for detailed discussion with examples.\n",
      "\n",
      "Code layout should use four spaces per indentation level.  You should make sure that\n",
      "when you write Python code in a file, you\n",
      "avoid tabs for indentation, since these can be misinterpreted by\n",
      "different text editors and the indentation can be messed up.\n",
      "Lines should be less than 80 characters long; if necessary you can\n",
      "break a line inside parentheses, brackets, or braces, because\n",
      "Python is able to detect that the line continues over to the next line.\n",
      "If you need to break a line outside parentheses, brackets, or braces,\n",
      "you can often add extra parentheses, and you can always add a backslash at\n",
      "the end of the line that is broken:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> if (len(syllables) > 4 and len(syllables[2]) == 3 and\n",
      "...    syllables[2][2] in [aeiou] and syllables[2][3] == syllables[1][3]):\n",
      "...     process(syllables)\n",
      ">>> if len(syllables) > 4 and len(syllables[2]) == 3 and \\\n",
      "...    syllables[2][2] in [aeiou] and syllables[2][3] == syllables[1][3]:\n",
      "...     process(syllables)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Typing spaces instead of tabs soon becomes a chore.  Many programming\n",
      "editors have built-in support for Python, and can automatically indent\n",
      "code and highlight any syntax errors (including indentation errors).\n",
      "For a list of Python-aware editors, please see\n",
      "http://wiki.python.org/moin/PythonEditors.\n",
      "\n",
      "\n",
      "\n",
      "Procedural vs Declarative Style\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We have just seen how the same task can be performed in different\n",
      "ways, with implications for efficiency.  Another factor influencing\n",
      "program development is programming style.  Consider the following\n",
      "program to compute the average length of words in the Brown Corpus:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tokens = nltk.corpus.brown.words(categories='news')\n",
      ">>> count = 0\n",
      ">>> total = 0\n",
      ">>> for token in tokens:\n",
      "...     count += 1\n",
      "...     total += len(token)\n",
      ">>> total / count\n",
      "4.401545438271973\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this program we use the variable count to keep track of the\n",
      "number of tokens seen, and total to store the combined length of\n",
      "all words.  This is a low-level style, not far removed from machine\n",
      "code, the primitive operations performed by the computer's CPU.\n",
      "The two variables are just like a CPU's registers, accumulating values\n",
      "at many intermediate stages, values that are meaningless until the end.\n",
      "We say that this program is written in a procedural style, dictating\n",
      "the machine operations step by step.  Now consider the following\n",
      "program that computes the same thing:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> total = sum(len(t) for t in tokens)\n",
      ">>> print(total / len(tokens))\n",
      "4.401...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The first line uses a generator expression to sum the token lengths,\n",
      "while the second line computes the average as before.\n",
      "Each line of code performs a complete, meaningful task, which\n",
      "can be understood in terms of high-level properties like:\n",
      "\"total is the sum of the lengths of the tokens\".\n",
      "Implementation details are left to the Python interpreter.\n",
      "The second program uses a built-in function, and constitutes\n",
      "programming at a more abstract level; the resulting code is\n",
      "more declarative.  Let's look at an extreme example:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> word_list = []\n",
      ">>> i = 0\n",
      ">>> while i < len(tokens):\n",
      "...     j = 0\n",
      "...     while j < len(word_list) and word_list[j] <= tokens[i]:\n",
      "...         j += 1\n",
      "...     if j == 0 or tokens[i] != word_list[j-1]:\n",
      "...         word_list.insert(j, tokens[i])\n",
      "...     i += 1\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "The equivalent declarative version uses familiar built-in functions,\n",
      "and its purpose is instantly recognizable:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> word_list = sorted(set(tokens))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Another case where a loop variable seems to be necessary is for printing\n",
      "a counter with each line of output.  Instead, we can use enumerate(), which\n",
      "processes a sequence s and produces a tuple of the form (i, s[i]) for each\n",
      "item in s, starting with (0, s[0]).  Here we enumerate the key-value pairs of the\n",
      "frequency distribution, resulting in nested tuples (rank, (word, count)).\n",
      "We print rank+1 so that the counting appears to start from 1,\n",
      "as required when producing a list of ranked items.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fd = nltk.FreqDist(nltk.corpus.brown.words())\n",
      ">>> cumulative = 0.0\n",
      ">>> most_common_words = [word for (word, count) in fd.most_common()]\n",
      ">>> for rank, word in enumerate(most_common_words):\n",
      "...     cumulative += fd.freq(word)\n",
      "...     print(\"%3d %6.2f%% %s\" % (rank + 1, cumulative * 100, word))\n",
      "...     if cumulative > 0.25:\n",
      "...         break\n",
      "...\n",
      "  1   5.40% the\n",
      "  2  10.42% ,\n",
      "  3  14.67% .\n",
      "  4  17.78% of\n",
      "  5  20.19% and\n",
      "  6  22.40% to\n",
      "  7  24.29% a\n",
      "  8  25.97% in\n",
      "\n",
      "\n",
      "\n",
      "It's sometimes tempting to use loop variables to store a maximum or minimum value\n",
      "seen so far.  Let's use this method to find the longest word in a text.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = nltk.corpus.gutenberg.words('milton-paradise.txt')\n",
      ">>> longest = ''\n",
      ">>> for word in text:\n",
      "...     if len(word) > len(longest):\n",
      "...         longest = word\n",
      ">>> longest\n",
      "'unextinguishable'\n",
      "\n",
      "\n",
      "\n",
      "However, a more transparent solution uses two list comprehensions,\n",
      "both having forms that should be familiar by now:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> maxlen = max(len(word) for word in text)\n",
      ">>> [word for word in text if len(word) == maxlen]\n",
      "['unextinguishable', 'transubstantiate', 'inextinguishable', 'incomprehensible']\n",
      "\n",
      "\n",
      "\n",
      "Note that our first solution found the first word having the longest length, while the\n",
      "second solution found all of the longest words (which is usually what we would want).\n",
      "Although there's a theoretical efficiency difference between the two solutions,\n",
      "the main overhead is reading the data into main memory; once it's there, a second pass\n",
      "through the data is effectively instantaneous.  We also need to balance our concerns about\n",
      "program efficiency with programmer efficiency.  A fast but cryptic solution\n",
      "will be harder to understand and maintain.\n",
      "\n",
      "\n",
      "Some Legitimate Uses for Counters\n",
      "\n",
      "\n",
      "There are cases where we still want to use loop variables in a list comprehension.\n",
      "For example, we need to use a loop variable to extract successive overlapping n-grams\n",
      "from a list:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
      ">>> n = 3\n",
      ">>> [sent[i:i+n] for i in range(len(sent)-n+1)]\n",
      "[['The', 'dog', 'gave'],\n",
      " ['dog', 'gave', 'John'],\n",
      " ['gave', 'John', 'the'],\n",
      " ['John', 'the', 'newspaper']]\n",
      "\n",
      "\n",
      "\n",
      "It is quite tricky to get the range of the loop variable right.\n",
      "Since this is a common operation in NLP, NLTK\n",
      "supports it with functions bigrams(text) and trigrams(text), and\n",
      "a general purpose ngrams(text, n).\n",
      "Here's an example of how we can use loop variables in\n",
      "building multidimensional structures.\n",
      "For example, to build an array with m rows and n columns,\n",
      "where each cell is a set, we could use a nested list comprehension:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> m, n = 3, 7\n",
      ">>> array = [[set() for i in range(n)] for j in range(m)]\n",
      ">>> array[2][5].add('Alice')\n",
      ">>> pprint.pprint(array)\n",
      "[[set(), set(), set(), set(), set(), set(), set()],\n",
      " [set(), set(), set(), set(), set(), set(), set()],\n",
      " [set(), set(), set(), set(), set(), {'Alice'}, set()]]\n",
      "\n",
      "\n",
      "\n",
      "Observe that the loop variables i and j are not used\n",
      "anywhere in the resulting object, they are just needed for a syntactically\n",
      "correct for statement.  As another example of this usage, observe\n",
      "that the expression ['very' for i in range(3)] produces a list\n",
      "containing three instances of 'very', with no integers in sight.\n",
      "Note that it would be incorrect to do this work using multiplication,\n",
      "for reasons concerning object copying that were discussed earlier in this section.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> array = [[set()] * n] * m\n",
      ">>> array[2][5].add(7)\n",
      ">>> pprint.pprint(array)\n",
      "[[{7}, {7}, {7}, {7}, {7}, {7}, {7}],\n",
      " [{7}, {7}, {7}, {7}, {7}, {7}, {7}],\n",
      " [{7}, {7}, {7}, {7}, {7}, {7}, {7}]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration is an important programming device.\n",
      "It is tempting to adopt idioms from other languages.\n",
      "However, Python offers some elegant and highly readable alternatives,\n",
      "as we have seen.\n",
      "\n",
      "\n",
      "\n",
      "4.4   Functions: The Foundation of Structured Programming\n",
      "\n",
      "\n",
      "\n",
      "Functions provide an effective way to package and re-use program code,\n",
      "as already explained in 3.\n",
      "For example, suppose we find that we often want to read text from an HTML file.\n",
      "This involves several steps: opening the file, reading it in, normalizing\n",
      "whitespace, and stripping HTML markup.  We can collect these steps into a\n",
      "function, and give it a name such as get_text(), as shown in 4.2.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "import re\n",
      "def get_text(file):\n",
      "    \"\"\"Read text from a file, normalizing whitespace and stripping HTML markup.\"\"\"\n",
      "    text = open(file).read()\n",
      "    text = re.sub(r'<.*?>', ' ', text)\n",
      "    text = re.sub('\\s+', ' ', text)\n",
      "    return text\n",
      "\n",
      "\n",
      "Example 4.2 (code_get_text.py): Figure 4.2: Read text from a file\n",
      "\n",
      "Now, any time we want to get cleaned-up text from an HTML file, we can just call\n",
      "get_text() with the name of the file as its only argument.  It will return\n",
      "a string, and we can assign this to a variable, e.g.:\n",
      "contents = get_text(\"test.html\").  Each time we want to use this series of\n",
      "steps we only have to call the function.\n",
      "Using functions has the benefit of saving space in our program.  More\n",
      "importantly, our choice of name for the function helps make the program readable.\n",
      "In the case of the above example, whenever our program needs to read cleaned-up\n",
      "text from a file we don't have to clutter the program with four lines of code, we\n",
      "simply need to call get_text().  This naming helps to provide some \"semantic\n",
      "interpretation\" — it helps a reader of our program to see what the program \"means\".\n",
      "Notice that the above function definition contains a string.  The first string inside\n",
      "a function definition is called a docstring.  Not only does it document the\n",
      "purpose of the function to someone reading the code, it is accessible to a programmer\n",
      "who has loaded the code from a file:\n",
      "\n",
      "|   >>> help(get_text)\n",
      "|   Help on function get_text in module __main__:\n",
      "|\n",
      "|   get_text(file)\n",
      "|       Read text from a file, normalizing whitespace and stripping HTML markup.\n",
      "\n",
      "We have seen that functions help to make our work reusable and readable.  They\n",
      "also help make it reliable.  When we re-use code that has already been developed\n",
      "and tested, we can be more confident that it handles a variety of cases correctly.\n",
      "We also remove the risk that we forget some important step, or introduce a bug.\n",
      "The program that calls our function also has increased reliability.  The author\n",
      "of that program is dealing with a shorter program, and its components behave\n",
      "transparently.\n",
      "To summarize, as its name suggests, a function captures functionality.\n",
      "It is a segment of code that can be given a meaningful name and which performs\n",
      "a well-defined task.  Functions allow us to abstract away from the details,\n",
      "to see a bigger picture, and to program more effectively.\n",
      "The rest of this section takes a closer look at functions, exploring the\n",
      "mechanics and discussing ways to make your programs easier to read.\n",
      "\n",
      "Function Inputs and Outputs\n",
      "We pass information to functions using a function's parameters,\n",
      "the parenthesized list of variables and constants following\n",
      "the function's name in the function definition.  Here's a complete example:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def repeat(msg, num):  \n",
      "...     return ' '.join([msg] * num)\n",
      ">>> monty = 'Monty Python'\n",
      ">>> repeat(monty, 3) \n",
      "'Monty Python Monty Python Monty Python'\n",
      "\n",
      "\n",
      "\n",
      "We first define the function to take two parameters, msg and num\n",
      ". Then we call the function and pass it two arguments, monty and 3\n",
      "; these arguments fill the \"placeholders\" provided by the parameters and\n",
      "provide values for the occurrences of msg and num in the function body.\n",
      "It is not necessary to have any parameters, as we see in the following example:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def monty():\n",
      "...     return \"Monty Python\"\n",
      ">>> monty()\n",
      "'Monty Python'\n",
      "\n",
      "\n",
      "\n",
      "A function usually communicates its results back to the calling program via the return statement,\n",
      "as we have just seen.  To the calling program, it looks as if the function call had been replaced\n",
      "with the function's result, e.g.:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> repeat(monty(), 3)\n",
      "'Monty Python Monty Python Monty Python'\n",
      ">>> repeat('Monty Python', 3)\n",
      "'Monty Python Monty Python Monty Python'\n",
      "\n",
      "\n",
      "\n",
      "A Python function is not required to have a return statement.\n",
      "Some functions do their work as a side effect, printing a result,\n",
      "modifying a file, or updating the contents of a parameter to the function\n",
      "(such functions are called \"procedures\" in some other programming languages).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Consider the following three sort functions.\n",
      "The third one is dangerous because a programmer could\n",
      "use it without realizing that it had modified its input.\n",
      "In general, functions should modify the contents of a parameter\n",
      "(my_sort1()), or return a value (my_sort2()),\n",
      "not both (my_sort3()).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def my_sort1(mylist):      # good: modifies its argument, no return value\n",
      "...     mylist.sort()\n",
      ">>> def my_sort2(mylist):      # good: doesn't touch its argument, returns value\n",
      "...     return sorted(mylist)\n",
      ">>> def my_sort3(mylist):      # bad: modifies its argument and also returns it\n",
      "...     mylist.sort()\n",
      "...     return mylist\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Parameter Passing\n",
      "Back in 4.1 you saw that assignment works on values,\n",
      "but that the value of a structured object is a reference to that object.  The same\n",
      "is true for functions.  Python interprets function parameters as values (this is\n",
      "known as call-by-value).  In the following code, set_up() has two parameters,\n",
      "both of which are modified inside the function.  We begin by assigning an empty string\n",
      "to w and an empty list to p.  After calling the function, w is unchanged,\n",
      "while p is changed:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def set_up(word, properties):\n",
      "...     word = 'lolcat'\n",
      "...     properties.append('noun')\n",
      "...     properties = 5\n",
      "...\n",
      ">>> w = ''\n",
      ">>> p = []\n",
      ">>> set_up(w, p)\n",
      ">>> w\n",
      "''\n",
      ">>> p\n",
      "['noun']\n",
      "\n",
      "\n",
      "\n",
      "Notice that w was not changed by the function.\n",
      "When we called set_up(w, p), the value of w (an empty string) was assigned to\n",
      "a new variable word.  Inside the function, the value of word was modified.\n",
      "However, that change did not propagate to w.  This parameter passing is\n",
      "identical to the following sequence of assignments:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> w = ''\n",
      ">>> word = w\n",
      ">>> word = 'lolcat'\n",
      ">>> w\n",
      "''\n",
      "\n",
      "\n",
      "\n",
      "Let's look at what happened with the list p.\n",
      "When we called set_up(w, p), the value of p (a reference to an empty\n",
      "list) was assigned to a new local variable properties,\n",
      "so both variables now reference the same memory location.\n",
      "The function modifies properties, and this change is also\n",
      "reflected in the value of p as we saw.  The function also\n",
      "assigned a new value to properties (the number 5); this\n",
      "did not modify the contents at that memory location, but\n",
      "created a new local variable.\n",
      "This behavior is just as if we had done the following sequence of assignments:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> p = []\n",
      ">>> properties = p\n",
      ">>> properties.append('noun')\n",
      ">>> properties = 5\n",
      ">>> p\n",
      "['noun']\n",
      "\n",
      "\n",
      "\n",
      "Thus, to understand Python's call-by-value parameter passing,\n",
      "it is enough to understand how assignment works.  Remember that you\n",
      "can use the id() function and is operator to check your\n",
      "understanding of object identity after each statement.\n",
      "\n",
      "\n",
      "Variable Scope\n",
      "Function definitions create a new, local scope for variables.\n",
      "When you assign to a new variable inside the body of a function,\n",
      "the name is only defined within that function.  The name is not\n",
      "visible outside the function, or in other functions.  This behavior\n",
      "means you can choose variable names without being concerned about\n",
      "collisions with names used in your other function definitions.\n",
      "When you refer to an existing name from within the body\n",
      "of a function, the Python interpreter first tries to resolve\n",
      "the name with respect to the names that are local to the function.\n",
      "If nothing is found, the interpreter checks if it is a global\n",
      "name within the module.  Finally, if that does not succeed, the\n",
      "interpreter checks if the name is a Python built-in.  This is\n",
      "the so-called LGB rule of name resolution: local,\n",
      "then global, then built-in.\n",
      "\n",
      "Caution!\n",
      "A function can enable access to a global variable using the\n",
      "global declaration.  However, this practice should be\n",
      "avoided as much as possible.  Defining global variables\n",
      "inside a function introduces dependencies on context\n",
      "and limits the portability (or reusability) of the function.\n",
      "In general you should use parameters for function inputs\n",
      "and return values for function outputs.\n",
      "\n",
      "\n",
      "\n",
      "Checking Parameter Types\n",
      "Python does not allow us to declare the type of a variable when we write a program,\n",
      "and this permits us to define functions that are flexible\n",
      "about the type of their arguments.  For example, a tagger might expect\n",
      "a sequence of words, but it wouldn't care whether this sequence is expressed\n",
      "as a list or a tuple (or an iterator, another sequence type that is\n",
      "outside the scope of the current discussion).\n",
      "However, often we want to write programs for later use by others, and want\n",
      "to program in a defensive style, providing useful warnings when functions\n",
      "have not been invoked correctly.  The author of the following tag()\n",
      "function assumed that its argument would always be a string.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def tag(word):\n",
      "...     if word in ['a', 'the', 'all']:\n",
      "...         return 'det'\n",
      "...     else:\n",
      "...         return 'noun'\n",
      "...\n",
      ">>> tag('the')\n",
      "'det'\n",
      ">>> tag('knight')\n",
      "'noun'\n",
      ">>> tag([\"'Tis\", 'but', 'a', 'scratch']) \n",
      "'noun'\n",
      "\n",
      "\n",
      "\n",
      "The function returns sensible values for the arguments 'the' and 'knight',\n",
      "but look what happens when it is passed a list  — it fails to\n",
      "complain, even though the result which it returns is clearly incorrect.\n",
      "The author of this function could take some extra steps to\n",
      "ensure that the word parameter of the tag() function is a string.\n",
      "A naive approach would be to check the type of the argument using\n",
      "if not type(word) is str, and if word is not a string, to simply\n",
      "return Python's special empty value, None. This is a slight improvement, because\n",
      "the function is checking the type of the argument, and trying to return a \"special\", diagnostic\n",
      "value for the wrong input.\n",
      "However, it is also dangerous because the calling program\n",
      "may not detect that None is intended as a \"special\" value, and this diagnostic\n",
      "return value may then be\n",
      "propagated to other parts of the program with unpredictable consequences.\n",
      "This approach also fails if the word is a Unicode string, which has\n",
      "type unicode, not str.\n",
      "Here's a better solution, using an assert statement together with Python's basestring\n",
      "type that generalizes over both unicode and str.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def tag(word):\n",
      "...     assert isinstance(word, basestring), \"argument to tag() must be a string\"\n",
      "...     if word in ['a', 'the', 'all']:\n",
      "...         return 'det'\n",
      "...     else:\n",
      "...         return 'noun'\n",
      "\n",
      "\n",
      "\n",
      "If the assert statement fails, it will produce an error that cannot be ignored,\n",
      "since it halts program execution.\n",
      "Additionally, the error message is easy to interpret.  Adding assertions to\n",
      "a program helps you find logical errors, and is a kind of defensive programming.\n",
      "A more fundamental approach is to document the parameters to each function\n",
      "using docstrings as described later in this section.\n",
      "\n",
      "\n",
      "\n",
      "Functional Decomposition\n",
      "Well-structured programs usually make extensive use of functions.\n",
      "When a block of program code grows longer than 10-20 lines, it is a\n",
      "great help to readability if the code is broken up into one or more\n",
      "functions, each one having a clear purpose.  This is analogous to\n",
      "the way a good essay is divided into paragraphs, each expressing one main idea.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Functions provide an important kind of abstraction.\n",
      "They allow us to group multiple actions into a single, complex action,\n",
      "and associate a name with it.\n",
      "(Compare this with the way we combine the actions of\n",
      "go and bring back into a single more complex action fetch.)\n",
      "When we use functions, the main program can be written at a higher level\n",
      "of abstraction, making its structure transparent, e.g.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> data = load_corpus()\n",
      ">>> results = analyze(data)\n",
      ">>> present(results)\n",
      "\n",
      "\n",
      "\n",
      "Appropriate use of functions makes programs more readable and maintainable.\n",
      "Additionally, it becomes possible to reimplement a function\n",
      "— replacing the function's body with more efficient code —\n",
      "without having to be concerned with the rest of the program.\n",
      "Consider the freq_words function in 4.3.\n",
      "It updates the contents of a frequency distribution that is\n",
      "passed in as a parameter, and it also prints a list of the\n",
      "n most frequent words.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "from urllib import request\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "def freq_words(url, freqdist, n):\n",
      "    html = request.urlopen(url).read().decode('utf8')\n",
      "    raw = BeautifulSoup(html, 'html.parser').get_text()\n",
      "    for word in word_tokenize(raw):\n",
      "        freqdist[word.lower()] += 1\n",
      "    result = []\n",
      "    for word, count in freqdist.most_common(n):\n",
      "        result = result + [word]\n",
      "    print(result)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> constitution = \"http://www.archives.gov/exhibits/charters/constitution_transcript.html\"\n",
      ">>> fd = nltk.FreqDist()\n",
      ">>> freq_words(constitution, fd, 30)\n",
      "['the', ',', 'of', 'and', 'shall', '.', 'be', 'to', ';', 'in', 'states',\n",
      "'or', 'united', 'a', 'state', 'by', 'for', 'any', '=', 'which', 'president',\n",
      "'all', 'on', 'may', 'such', 'as', 'have', ')', '(', 'congress']\n",
      "\n",
      "\n",
      "Example 4.3 (code_freq_words1.py): Figure 4.3: Poorly Designed Function to Compute Frequent Words\n",
      "\n",
      "This function has a number of problems.\n",
      "The function has two side-effects: it modifies the contents of its second\n",
      "parameter, and it prints a selection of the results it has computed.\n",
      "The function would be easier to understand and to reuse elsewhere if we\n",
      "initialize the FreqDist() object inside the function (in the same place\n",
      "it is populated), and if we moved the selection and display of results to the\n",
      "calling program. Given that its task is to identify frequent words, it\n",
      "should probably just return a list, not the whole frequency distribution.\n",
      "In 4.4 we refactor this function,\n",
      "and simplify its interface by dropping the freqdist parameter.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "from urllib import request\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "def freq_words(url, n):\n",
      "    html = request.urlopen(url).read().decode('utf8')\n",
      "    text = BeautifulSoup(html, 'html.parser').get_text()\n",
      "    freqdist = nltk.FreqDist(word.lower() for word in word_tokenize(text))\n",
      "    return [word for (word, _) in fd.most_common(n)]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> freq_words(constitution, 30)\n",
      "['the', ',', 'of', 'and', 'shall', '.', 'be', 'to', ';', 'in', 'states',\n",
      "'or', 'united', 'a', 'state', 'by', 'for', 'any', '=', 'which', 'president',\n",
      "'all', 'on', 'may', 'such', 'as', 'have', ')', '(', 'congress']\n",
      "\n",
      "\n",
      "Example 4.4 (code_freq_words2.py): Figure 4.4: Well-Designed Function to Compute Frequent Words\n",
      "\n",
      "The readability and usability of the freq_words function is improved.\n",
      "\n",
      "Note\n",
      "We have used _ as a variable name. This is no different to any other\n",
      "variable except it signals to the reader that we don't have a use\n",
      "for the information it holds.\n",
      "\n",
      "\n",
      "\n",
      "Documenting Functions\n",
      "If we have done a good job at decomposing our program into functions, then it should\n",
      "be easy to describe the purpose of each function in plain language, and provide\n",
      "this in the docstring at the top of the function definition.  This statement\n",
      "should not explain how the functionality is implemented; in fact it should be possible\n",
      "to re-implement the function using a different method without changing this\n",
      "statement.\n",
      "For the simplest functions, a one-line docstring is usually adequate (see 4.2).\n",
      "You should provide a triple-quoted string containing a complete sentence on a single line.\n",
      "For non-trivial functions, you should still provide a one sentence summary on the first line,\n",
      "since many docstring processing tools index this string.  This should be followed by\n",
      "a blank line, then a more detailed description of the functionality\n",
      "(see http://www.python.org/dev/peps/pep-0257/ for more information in docstring\n",
      "conventions).\n",
      "\n",
      "Docstrings can include a doctest block, illustrating the use of\n",
      "the function and the expected output.  These can be tested automatically\n",
      "using Python's docutils module.\n",
      "Docstrings should document the type of each parameter to the function, and the return\n",
      "type.  At a minimum, that can be done in plain text.  However, note that NLTK uses\n",
      "the Sphinx markup language to document parameters.  This format\n",
      "can be automatically converted into richly structured\n",
      "API documentation (see http://nltk.org/), and includes special handling of certain\n",
      "\"fields\" such as param which allow the inputs and outputs of functions to be\n",
      "clearly documented.  4.5 illustrates\n",
      "a complete docstring.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def accuracy(reference, test):\n",
      "    \"\"\"\n",
      "    Calculate the fraction of test items that equal the corresponding reference items.\n",
      "\n",
      "    Given a list of reference values and a corresponding list of test values,\n",
      "    return the fraction of corresponding values that are equal.\n",
      "    In particular, return the fraction of indexes\n",
      "    {0<i<=len(test)} such that C{test[i] == reference[i]}.\n",
      "\n",
      "        >>> accuracy(['ADJ', 'N', 'V', 'N'], ['N', 'N', 'V', 'ADJ'])\n",
      "        0.5\n",
      "\n",
      "    :param reference: An ordered list of reference values\n",
      "    :type reference: list\n",
      "    :param test: A list of values to compare against the corresponding\n",
      "        reference values\n",
      "    :type test: list\n",
      "    :return: the accuracy score\n",
      "    :rtype: float\n",
      "    :raises ValueError: If reference and length do not have the same length\n",
      "    \"\"\"\n",
      "\n",
      "    if len(reference) != len(test):\n",
      "        raise ValueError(\"Lists must have the same length.\")\n",
      "    num_correct = 0\n",
      "    for x, y in zip(reference, test):\n",
      "        if x == y:\n",
      "            num_correct += 1\n",
      "    return float(num_correct) / len(reference)\n",
      "\n",
      "\n",
      "Example 4.5 (code_sphinx.py): Figure 4.5: Illustration of a complete docstring, consisting of a one-line summary,\n",
      "a more detailed explanation, a doctest example, and Sphinx markup\n",
      "specifying the parameters, types, return type, and exceptions.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4.5   Doing More with Functions\n",
      "This section discusses more advanced features, which you may prefer to skip on the\n",
      "first time through this chapter.\n",
      "\n",
      "Functions as Arguments\n",
      "So far the arguments we have passed into functions have been simple objects like\n",
      "strings, or structured objects like lists.  Python also lets us pass a function as\n",
      "an argument to another function.  Now we can abstract out the operation, and apply\n",
      "a different operation on the same data.  As the following examples show,\n",
      "we can pass the built-in function len() or a user-defined function last_letter()\n",
      "as arguments to another function:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent = ['Take', 'care', 'of', 'the', 'sense', ',', 'and', 'the',\n",
      "...         'sounds', 'will', 'take', 'care', 'of', 'themselves', '.']\n",
      ">>> def extract_property(prop):\n",
      "...     return [prop(word) for word in sent]\n",
      "...\n",
      ">>> extract_property(len)\n",
      "[4, 4, 2, 3, 5, 1, 3, 3, 6, 4, 4, 4, 2, 10, 1]\n",
      ">>> def last_letter(word):\n",
      "...     return word[-1]\n",
      ">>> extract_property(last_letter)\n",
      "['e', 'e', 'f', 'e', 'e', ',', 'd', 'e', 's', 'l', 'e', 'e', 'f', 's', '.']\n",
      "\n",
      "\n",
      "\n",
      "The objects len and last_letter can be\n",
      "passed around like lists and dictionaries.  Notice that parentheses\n",
      "are only used after a function name if we are invoking the function;\n",
      "when we are simply treating the function as an object these are omitted.\n",
      "Python provides us with one more way to define functions as arguments\n",
      "to other functions, so-called lambda expressions.  Supposing there\n",
      "was no need to use the above last_letter() function in multiple places,\n",
      "and thus no need to give it a name.  We can equivalently write the following:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> extract_property(lambda w: w[-1])\n",
      "['e', 'e', 'f', 'e', 'e', ',', 'd', 'e', 's', 'l', 'e', 'e', 'f', 's', '.']\n",
      "\n",
      "\n",
      "\n",
      "Our next example illustrates passing a function to the sorted() function.\n",
      "When we call the latter with a single argument (the list to be sorted),\n",
      "it uses the built-in comparison function cmp().\n",
      "However, we can supply our own sort function, e.g. to sort by decreasing\n",
      "length.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sorted(sent)\n",
      "[',', '.', 'Take', 'and', 'care', 'care', 'of', 'of', 'sense', 'sounds',\n",
      "'take', 'the', 'the', 'themselves', 'will']\n",
      ">>> sorted(sent, cmp)\n",
      "[',', '.', 'Take', 'and', 'care', 'care', 'of', 'of', 'sense', 'sounds',\n",
      "'take', 'the', 'the', 'themselves', 'will']\n",
      ">>> sorted(sent, lambda x, y: cmp(len(y), len(x)))\n",
      "['themselves', 'sounds', 'sense', 'Take', 'care', 'will', 'take', 'care',\n",
      "'the', 'and', 'the', 'of', 'of', ',', '.']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accumulative Functions\n",
      "These functions start by initializing some storage, and iterate over\n",
      "input to build it up, before returning some final object (a large structure\n",
      "or aggregated result).  A standard way to do this is to initialize an\n",
      "empty list, accumulate the material, then return the list, as shown\n",
      "in function search1() in 4.6.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def search1(substring, words):\n",
      "    result = []\n",
      "    for word in words:\n",
      "        if substring in word:\n",
      "            result.append(word)\n",
      "    return result\n",
      "\n",
      "def search2(substring, words):\n",
      "    for word in words:\n",
      "        if substring in word:\n",
      "            yield word\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> for item in search1('zz', nltk.corpus.brown.words()):\n",
      "...     print(item, end=\" \")\n",
      "Grizzlies' fizzled Rizzuto huzzahs dazzler jazz Pezza ...\n",
      ">>> for item in search2('zz', nltk.corpus.brown.words()):\n",
      "...     print(item, end=\" \")\n",
      "Grizzlies' fizzled Rizzuto huzzahs dazzler jazz Pezza ...\n",
      "\n",
      "\n",
      "Example 4.6 (code_search_examples.py): Figure 4.6: Accumulating Output into a List\n",
      "\n",
      "The function search2() is a generator.\n",
      "The first time this function is called, it gets as far as the yield\n",
      "statement and pauses.  The calling program gets the first word and does\n",
      "any necessary processing.  Once the calling program is ready for another\n",
      "word, execution of the function is continued from where it stopped, until\n",
      "the next time it encounters a yield statement.  This approach is\n",
      "typically more efficient, as the function only generates the data as it is\n",
      "required by the calling program, and does not need to allocate additional\n",
      "memory to store the output (cf. our discussion of generator expressions above).\n",
      "Here's a more sophisticated example of a generator which produces\n",
      "all permutations of a list of words.  In order to force the permutations()\n",
      "function to generate all its output, we wrap it with a call to list() .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def permutations(seq):\n",
      "...     if len(seq) <= 1:\n",
      "...         yield seq\n",
      "...     else:\n",
      "...         for perm in permutations(seq[1:]):\n",
      "...             for i in range(len(perm)+1):\n",
      "...                 yield perm[:i] + seq[0:1] + perm[i:]\n",
      "...\n",
      ">>> list(permutations(['police', 'fish', 'buffalo'])) \n",
      "[['police', 'fish', 'buffalo'], ['fish', 'police', 'buffalo'],\n",
      " ['fish', 'buffalo', 'police'], ['police', 'buffalo', 'fish'],\n",
      " ['buffalo', 'police', 'fish'], ['buffalo', 'fish', 'police']]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "The permutations function uses a technique called recursion,\n",
      "discussed below in 4.7.\n",
      "The ability to generate permutations of a set of words is\n",
      "useful for creating data to test a grammar (8.).\n",
      "\n",
      "\n",
      "\n",
      "Higher-Order Functions\n",
      "Python provides some higher-order functions that are standard\n",
      "features of functional programming languages such as Haskell.\n",
      "We illustrate them here, alongside the equivalent expression\n",
      "using list comprehensions.\n",
      "Let's start by defining a function is_content_word()\n",
      "which checks whether a word is from the open class of content words.\n",
      "We use this function as the first parameter of filter(),\n",
      "which applies the function to each item in the sequence contained\n",
      "in its second parameter, and only retains the items for which\n",
      "the function returns True.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def is_content_word(word):\n",
      "...     return word.lower() not in ['a', 'of', 'the', 'and', 'will', ',', '.']\n",
      ">>> sent = ['Take', 'care', 'of', 'the', 'sense', ',', 'and', 'the',\n",
      "...         'sounds', 'will', 'take', 'care', 'of', 'themselves', '.']\n",
      ">>> list(filter(is_content_word, sent))\n",
      "['Take', 'care', 'sense', 'sounds', 'take', 'care', 'themselves']\n",
      ">>> [w for w in sent if is_content_word(w)]\n",
      "['Take', 'care', 'sense', 'sounds', 'take', 'care', 'themselves']\n",
      "\n",
      "\n",
      "\n",
      "Another higher-order function is map(), which applies a function\n",
      "to every item in a sequence.  It is a general version of the\n",
      "extract_property() function we saw in 4.5.\n",
      "Here is a simple way to find the average length of a sentence in the news\n",
      "section of the Brown Corpus, followed by an equivalent version with list comprehension\n",
      "calculation:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> lengths = list(map(len, nltk.corpus.brown.sents(categories='news')))\n",
      ">>> sum(lengths) / len(lengths)\n",
      "21.75081116158339\n",
      ">>> lengths = [len(sent) for sent in nltk.corpus.brown.sents(categories='news')]\n",
      ">>> sum(lengths) / len(lengths)\n",
      "21.75081116158339\n",
      "\n",
      "\n",
      "\n",
      "In the above examples we specified a user-defined function is_content_word()\n",
      "and a built-in function len().  We can also provide a lambda expression.\n",
      "Here's a pair of equivalent examples which count the number of vowels in each word.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> list(map(lambda w: len(filter(lambda c: c.lower() in \"aeiou\", w)), sent))\n",
      "[2, 2, 1, 1, 2, 0, 1, 1, 2, 1, 2, 2, 1, 3, 0]\n",
      ">>> [len(c for c in w if c.lower() in \"aeiou\") for w in sent]\n",
      "[2, 2, 1, 1, 2, 0, 1, 1, 2, 1, 2, 2, 1, 3, 0]\n",
      "\n",
      "\n",
      "\n",
      "The solutions based on list comprehensions are usually more readable than the\n",
      "solutions based on higher-order functions, and we have favored the former\n",
      "approach throughout this book.\n",
      "\n",
      "\n",
      "Named Arguments\n",
      "When there are a lot of parameters it is easy to get confused about the\n",
      "correct order.  Instead we can refer to parameters by name, and even assign\n",
      "them a default value just in case one was not provided by the calling\n",
      "program.  Now the parameters can be specified in any order, and can be omitted.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def repeat(msg='<empty>', num=1):\n",
      "...     return msg * num\n",
      ">>> repeat(num=3)\n",
      "'<empty><empty><empty>'\n",
      ">>> repeat(msg='Alice')\n",
      "'Alice'\n",
      ">>> repeat(num=5, msg='Alice')\n",
      "'AliceAliceAliceAliceAlice'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "These are called keyword arguments.\n",
      "If we mix these two kinds of parameters, then we must ensure that the unnamed parameters precede the named ones.\n",
      "It has to be this way, since unnamed parameters are defined by position.  We can define a function that takes\n",
      "an arbitrary number of unnamed and named parameters, and access them via an in-place list of arguments *args and\n",
      "an \"in-place dictionary\" of keyword arguments **kwargs.\n",
      "(Dictionaries will be presented in 3.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def generic(*args, **kwargs):\n",
      "...     print(args)\n",
      "...     print(kwargs)\n",
      "...\n",
      ">>> generic(1, \"African swallow\", monty=\"python\")\n",
      "(1, 'African swallow')\n",
      "{'monty': 'python'}\n",
      "\n",
      "\n",
      "\n",
      "When *args appears as a function parameter, it actually corresponds to all the unnamed parameters of\n",
      "the function.  Here's another illustration of this aspect of Python syntax, for the zip() function which\n",
      "operates on a variable number of arguments.  We'll use the variable name *song to demonstrate that\n",
      "there's nothing special about the name *args.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> song = [['four', 'calling', 'birds'],\n",
      "...         ['three', 'French', 'hens'],\n",
      "...         ['two', 'turtle', 'doves']]\n",
      ">>> list(zip(song[0], song[1], song[2]))\n",
      "[('four', 'three', 'two'), ('calling', 'French', 'turtle'), ('birds', 'hens', 'doves')]\n",
      ">>> list(zip(*song))\n",
      "[('four', 'three', 'two'), ('calling', 'French', 'turtle'), ('birds', 'hens', 'doves')]\n",
      "\n",
      "\n",
      "\n",
      "It should be clear from the above example that typing *song is just a convenient\n",
      "shorthand, and equivalent to typing out song[0], song[1], song[2].\n",
      "Here's another example of the use of keyword arguments in a function\n",
      "definition, along with three equivalent ways to call the function:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def freq_words(file, min=1, num=10):\n",
      "...     text = open(file).read()\n",
      "...     tokens = word_tokenize(text)\n",
      "...     freqdist = nltk.FreqDist(t for t in tokens if len(t) >= min)\n",
      "...     return freqdist.most_common(num)\n",
      ">>> fw = freq_words('ch01.rst', 4, 10)\n",
      ">>> fw = freq_words('ch01.rst', min=4, num=10)\n",
      ">>> fw = freq_words('ch01.rst', num=10, min=4)\n",
      "\n",
      "\n",
      "\n",
      "A side-effect of having named arguments is that they permit optionality.  Thus we\n",
      "can leave out any arguments where we are happy with the default value:\n",
      "freq_words('ch01.rst', min=4), freq_words('ch01.rst', 4).\n",
      "Another common use of optional arguments is to permit a flag.\n",
      "Here's a revised version of the same function that reports its\n",
      "progress if a verbose flag is set:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def freq_words(file, min=1, num=10, verbose=False):\n",
      "...     freqdist = FreqDist()\n",
      "...     if verbose: print(\"Opening\", file)\n",
      "...     text = open(file).read()\n",
      "...     if verbose: print(\"Read in %d characters\" % len(file))\n",
      "...     for word in word_tokenize(text):\n",
      "...         if len(word) >= min:\n",
      "...             freqdist[word] += 1\n",
      "...             if verbose and freqdist.N() % 100 == 0: print(\".\", sep=\"\")\n",
      "...     if verbose: print\n",
      "...     return freqdist.most_common(num)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Caution!\n",
      "Take care not to use a mutable object as the default value of\n",
      "a parameter.  A series of calls to the function will use the\n",
      "same object, sometimes with bizarre results as we will see in\n",
      "the discussion of debugging below.\n",
      "\n",
      "\n",
      "Caution!\n",
      "If your program will work with a lot of files, it is a good idea to\n",
      "close any open files once they are no longer required. Python will\n",
      "close open files automatically if you use the with statement:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> with open(\"lexicon.txt\") as f:\n",
      "...     data = f.read()\n",
      "...     # process the data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4.6   Program Development\n",
      "Programming is a skill that is acquired over several years of\n",
      "experience with a variety of programming languages and tasks.  Key\n",
      "high-level abilities are algorithm design and its manifestation in\n",
      "structured programming.  Key low-level abilities include familiarity\n",
      "with the syntactic constructs of the language, and knowledge of a\n",
      "variety of diagnostic methods for trouble-shooting a program which\n",
      "does not exhibit the expected behavior.\n",
      "This section describes the internal structure of a program module and\n",
      "how to organize a multi-module program.  Then it describes various\n",
      "kinds of error that arise during program development, what you can\n",
      "do to fix them and, better still, to avoid them in the first place.\n",
      "\n",
      "Structure of a Python Module\n",
      "The purpose of a program module is to bring logically-related definitions and functions\n",
      "together in order to facilitate re-use and abstraction.  Python modules are nothing\n",
      "more than individual .py files.  For example, if you were working\n",
      "with a particular corpus format, the functions to read and write the format could be\n",
      "kept together.  Constants used by both formats, such as field separators,\n",
      "or a EXTN = \".inf\" filename extension, could be shared.  If the format was updated,\n",
      "you would know that only one file needed to be changed.  Similarly, a module could\n",
      "contain code for creating and manipulating a particular data structure such as\n",
      "syntax trees, or code for performing a particular processing task such as\n",
      "plotting corpus statistics.\n",
      "When you start writing Python modules, it helps to have some\n",
      "examples to emulate.  You can locate the code for any NLTK module on your\n",
      "system using the __file__ variable, e.g.:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nltk.metrics.distance.__file__\n",
      "'/usr/lib/python2.5/site-packages/nltk/metrics/distance.pyc'\n",
      "\n",
      "\n",
      "\n",
      "This returns the location of the compiled .pyc file for the module, and\n",
      "you'll probably see a different location on your machine. The file that you will need\n",
      "to open is the corresponding .py source file, and this will be in the same\n",
      "directory as the .pyc file.\n",
      "Alternatively, you can view the latest version of this module on the web\n",
      "at http://code.google.com/p/nltk/source/browse/trunk/nltk/nltk/metrics/distance.py.\n",
      "Like every other NLTK module, distance.py begins with a group of comment\n",
      "lines giving a one-line title of the module and identifying the authors.\n",
      "(Since the code is distributed, it also includes the URL where the\n",
      "code is available, a copyright statement, and license information.)\n",
      "Next is the module-level docstring, a triple-quoted multiline string\n",
      "containing information about the module that will be printed when\n",
      "someone types help(nltk.metrics.distance).\n",
      "\n",
      "\n",
      "# Natural Language Toolkit: Distance Metrics\n",
      "#\n",
      "# Copyright (C) 2001-2019 NLTK Project\n",
      "# Author: Edward Loper <edloper@gmail.com>\n",
      "#         Steven Bird <stevenbird1@gmail.com>\n",
      "#         Tom Lippincott <tom@cs.columbia.edu>\n",
      "# URL: <http://nltk.org/>\n",
      "# For license information, see LICENSE.TXT\n",
      "#\n",
      "\n",
      "\"\"\"\n",
      "Distance Metrics.\n",
      "\n",
      "Compute the distance between two items (usually strings).\n",
      "As metrics, they must satisfy the following three requirements:\n",
      "\n",
      "1. d(a, a) = 0\n",
      "2. d(a, b) >= 0\n",
      "3. d(a, c) <= d(a, b) + d(b, c)\n",
      "\"\"\"\n",
      "\n",
      "After this comes all the import statements required for the module,\n",
      "then any global variables,\n",
      "followed by a series of function definitions that make up most\n",
      "of the module.  Other modules define \"classes,\" the main building block\n",
      "of object-oriented programming, which falls outside the scope of this book.\n",
      "(Most NLTK modules also include a demo() function which can be used\n",
      "to see examples of the module in use.)\n",
      "\n",
      "Note\n",
      "Some module variables and functions are only used within the module.\n",
      "These should have names beginning with an underscore, e.g. _helper(),\n",
      "since this will hide the name.  If another module imports this one,\n",
      "using the idiom: from module import *, these names will not be imported.\n",
      "You can optionally list the externally accessible names of a module using\n",
      "a special built-in variable like this: __all__ = ['edit_distance', 'jaccard_distance'].\n",
      "\n",
      "\n",
      "\n",
      "Multi-Module Programs\n",
      "Some programs bring together a diverse range of tasks, such as loading data from\n",
      "a corpus, performing some analysis tasks on the data, then visualizing it.\n",
      "We may already have stable modules that take care of loading data and producing visualizations.\n",
      "Our work might involve coding up the analysis task, and just invoking functions\n",
      "from the existing modules.  This scenario is depicted in 4.7.\n",
      "\n",
      "\n",
      "Figure 4.7: Structure of a Multi-Module Program: The main program my_program.py imports functions\n",
      "from two other modules; unique analysis tasks are localized to the main program, while\n",
      "common loading and visualization tasks are kept apart to facilitate re-use and abstraction.\n",
      "\n",
      "By dividing our work into several modules and using import statements to\n",
      "access functions defined elsewhere, we can keep the individual modules simple\n",
      "and easy to maintain.  This approach will also result in a growing collection\n",
      "of modules, and make it possible for us to build sophisticated systems involving\n",
      "a hierarchy of modules.  Designing such systems well is a\n",
      "complex software engineering task, and beyond the scope of this book.\n",
      "\n",
      "\n",
      "Sources of Error\n",
      "Mastery of programming depends on having a variety of problem-solving skills to\n",
      "draw upon when the program doesn't work as expected.  Something as trivial as\n",
      "a mis-placed symbol might cause the program to behave very differently.\n",
      "We call these \"bugs\" because they are tiny in comparison to the damage\n",
      "they can cause.  They creep into our code unnoticed, and it's only much later\n",
      "when we're running the program on some new data that their presence is detected.\n",
      "Sometimes, fixing one bug only reveals another, and we get the distinct impression\n",
      "that the bug is on the move.  The only reassurance we have is that bugs are\n",
      "spontaneous and not the fault of the programmer.\n",
      "Flippancy aside, debugging code is hard because there are so many ways for\n",
      "it to be faulty.  Our understanding of the input data, the algorithm, or\n",
      "even the programming language, may be at fault.  Let's look at examples\n",
      "of each of these.\n",
      "First, the input data may contain some unexpected characters.\n",
      "For example, WordNet synset names have the form tree.n.01, with three\n",
      "components separated using periods.  The NLTK WordNet module initially\n",
      "decomposed these names using split('.').  However, this method broke when\n",
      "someone tried to look up the word PhD, which has the synset\n",
      "name ph.d..n.01, containing four periods instead of the expected two.\n",
      "The solution was to use rsplit('.', 2) to do at most two splits, using\n",
      "the rightmost instances of the period, and leaving the ph.d. string intact.\n",
      "Although several people had tested\n",
      "the module before it was released, it was some weeks before someone detected\n",
      "the problem (see http://code.google.com/p/nltk/issues/detail?id=297).\n",
      "Second, a supplied function might not behave as expected.\n",
      "For example, while testing NLTK's interface to WordNet, one of the\n",
      "authors noticed that no synsets had any antonyms defined, even though\n",
      "the underlying database provided a large quantity of antonym information.\n",
      "What looked like a bug in the WordNet interface turned out to\n",
      "be a misunderstanding about WordNet itself: antonyms are defined for\n",
      "lemmas, not for synsets.  The only \"bug\" was a misunderstanding\n",
      "of the interface (see http://code.google.com/p/nltk/issues/detail?id=98).\n",
      "\n",
      "\n",
      "Third, our understanding of Python's semantics may be at fault.\n",
      "It is easy to make the wrong assumption about the relative\n",
      "scope of two operators.\n",
      "For example, \"%s.%s.%02d\" % \"ph.d.\", \"n\", 1 produces a run-time\n",
      "error TypeError: not enough arguments for format string.\n",
      "This is because the percent operator has higher precedence than\n",
      "the comma operator.  The fix is to add parentheses in order to\n",
      "force the required scope.  As another example, suppose we are\n",
      "defining a function to collect all tokens of a text having a\n",
      "given length.  The function has parameters for the text and\n",
      "the word length, and an extra parameter that allows the initial\n",
      "value of the result to be given as a parameter:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def find_words(text, wordlength, result=[]):\n",
      "...     for word in text:\n",
      "...         if len(word) == wordlength:\n",
      "...             result.append(word)\n",
      "...     return result\n",
      ">>> find_words(['omg', 'teh', 'lolcat', 'sitted', 'on', 'teh', 'mat'], 3) \n",
      "['omg', 'teh', 'teh', 'mat']\n",
      ">>> find_words(['omg', 'teh', 'lolcat', 'sitted', 'on', 'teh', 'mat'], 2, ['ur']) \n",
      "['ur', 'on']\n",
      ">>> find_words(['omg', 'teh', 'lolcat', 'sitted', 'on', 'teh', 'mat'], 3) \n",
      "['omg', 'teh', 'teh', 'mat', 'omg', 'teh', 'teh', 'mat']\n",
      "\n",
      "\n",
      "\n",
      "The first time we call find_words() , we get all three-letter\n",
      "words as expected.  The second time we specify an initial value for the result,\n",
      "a one-element list ['ur'], and as expected, the result has this word along with the\n",
      "other two-letter word in our text.  Now, the next time we call find_words() \n",
      "we use the same parameters as in , but we get a different result!\n",
      "Each time we call find_words() with no third parameter, the result will\n",
      "simply extend the result of the previous call, rather than start with the\n",
      "empty result list as specified in the function definition.  The program's\n",
      "behavior is not as expected because we incorrectly assumed that the default\n",
      "value was created at the time the function was invoked.  However, it is\n",
      "created just once, at the time the Python interpreter loads the function.\n",
      "This one list object is used whenever no explicit value is provided to the function.\n",
      "\n",
      "\n",
      "Debugging Techniques\n",
      "Since most code errors result from the programmer making incorrect assumptions,\n",
      "the first thing to do when you detect a bug is to check your assumptions.\n",
      "Localize the problem by adding print statements to the program, showing the\n",
      "value of important variables, and showing how far the program has progressed.\n",
      "If the program produced an \"exception\" — a run-time error —\n",
      "the interpreter will print a stack trace,\n",
      "pinpointing the location of program execution at the time of the error.\n",
      "If the program depends on input data, try to reduce this to the smallest\n",
      "size while still producing the error.\n",
      "Once you have localized the problem to a particular function, or to a line\n",
      "of code, you need to work out what is going wrong.  It is often helpful to\n",
      "recreate the situation using the interactive command line.  Define some\n",
      "variables then copy-paste the offending line of code into the session\n",
      "and see what happens.  Check your understanding of the code by reading\n",
      "some documentation, and examining other code samples that purport to do\n",
      "the same thing that you are trying to do.  Try explaining your code to\n",
      "someone else, in case they can see where things are going wrong.\n",
      "Python provides a debugger which allows you to monitor the execution\n",
      "of your program, specify line numbers where execution will stop (i.e. breakpoints),\n",
      "and step through sections of code and inspect the value of variables.\n",
      "You can invoke the debugger on your code as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> import pdb\n",
      ">>> import mymodule\n",
      ">>> pdb.run('mymodule.myfunction()')\n",
      "\n",
      "\n",
      "\n",
      "It will present you with a prompt (Pdb) where you can type instructions\n",
      "to the debugger.  Type help to see the full list of commands.\n",
      "Typing step (or just s) will execute the current line and\n",
      "stop.  If the current line calls a function, it will enter the function\n",
      "and stop at the first line.  Typing next (or just n) is similar,\n",
      "but it stops execution at the next line in the current function.  The\n",
      "break (or b) command can be used to create or list breakpoints.  Type\n",
      "continue (or c) to continue execution as far as the next breakpoint.\n",
      "Type the name of any variable to inspect its value.\n",
      "We can use the Python debugger to locate the problem in our find_words()\n",
      "function.  Remember that the problem arose the second time the function was\n",
      "called.  We'll start by calling the function without using the debugger ,\n",
      "using the smallest possible input.  The second time, we'll call it with the\n",
      "debugger .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> import pdb\n",
      ">>> find_words(['cat'], 3) \n",
      "['cat']\n",
      ">>> pdb.run(\"find_words(['dog'], 3)\") \n",
      "> <string>(1)<module>()\n",
      "(Pdb) step\n",
      "--Call--\n",
      "> <stdin>(1)find_words()\n",
      "(Pdb) args\n",
      "text = ['dog']\n",
      "wordlength = 3\n",
      "result = ['cat']\n",
      "\n",
      "\n",
      "\n",
      "Here we typed just two commands into the debugger: step took us inside\n",
      "the function, and args showed the values of its arguments (or parameters).\n",
      "We see immediately that result has an initial value of ['cat'], and not\n",
      "the empty list as expected.  The debugger has helped us to localize the problem,\n",
      "prompting us to check our understanding of Python functions.\n",
      "\n",
      "\n",
      "Defensive Programming\n",
      "In order to avoid some of the pain of debugging, it helps to adopt\n",
      "some defensive programming habits.  Instead of writing a 20-line\n",
      "program then testing it, build the program bottom-up out of\n",
      "small pieces that are known to work.  Each time you combine these\n",
      "pieces to make a larger unit, test it carefully to see that it works\n",
      "as expected.  Consider adding assert statements to your code,\n",
      "specifying properties of a variable, e.g. assert(isinstance(text, list)).\n",
      "If the value of the text variable later becomes a string when your\n",
      "code is used in some larger context, this will raise an AssertionError\n",
      "and you will get immediate notification of the problem.\n",
      "Once you think you've found the bug, view your solution as a hypothesis.\n",
      "Try to predict the effect of your bugfix before re-running the program.\n",
      "If the bug isn't fixed, don't fall into the trap of blindly changing\n",
      "the code in the hope that it will magically start working again.\n",
      "Instead, for each change, try to articulate a hypothesis about what\n",
      "is wrong and why the change will fix the problem.  Then undo the change\n",
      "if the problem was not resolved.\n",
      "As you develop your program, extend its functionality, and fix any bugs,\n",
      "it helps to maintain a suite of test cases.\n",
      "This is called regression testing, since it is meant to detect\n",
      "situations where the code \"regresses\" — where a change to the\n",
      "code has an unintended side-effect of breaking something that\n",
      "used to work.  Python provides a simple regression testing framework\n",
      "in the form of the doctest module.  This module searches a file\n",
      "of code or documentation for blocks of text that look like\n",
      "an interactive Python session, of the form you have already seen\n",
      "many times in this book.  It executes the Python commands it finds,\n",
      "and tests that their output matches the output supplied in the original\n",
      "file.  Whenever there is a mismatch, it reports the expected and actual\n",
      "values.  For details please consult the doctest documentation at\n",
      "http://docs.python.org/library/doctest.html.  Apart from its\n",
      "value for regression testing, the doctest module is useful for\n",
      "ensuring that your software documentation stays in sync with your\n",
      "code.\n",
      "Perhaps the most important defensive programming strategy is to\n",
      "set out your code clearly, choose meaningful variable and function\n",
      "names, and simplify the code wherever possible by decomposing it into\n",
      "functions and modules with well-documented interfaces.\n",
      "\n",
      "\n",
      "\n",
      "4.7   Algorithm Design\n",
      "This section discusses more advanced concepts, which you may prefer to skip on the\n",
      "first time through this chapter.\n",
      "A major part of algorithmic problem solving is selecting or adapting\n",
      "an appropriate algorithm for the problem at hand.  Sometimes there are\n",
      "several alternatives, and choosing the best one depends on knowledge\n",
      "about how each alternative performs as the size of the data grows.\n",
      "Whole books are written on this topic, and we only have space to introduce\n",
      "some key concepts and elaborate on the approaches that are most prevalent\n",
      "in natural language processing.\n",
      "The best known strategy is known as divide-and-conquer.\n",
      "We attack a problem of size n by dividing it into two problems of size n/2,\n",
      "solve these problems, and combine their results into a solution of the original problem.\n",
      "For example, suppose that we had a pile of cards with a single word written on each card.\n",
      "We could sort this pile by splitting it in half and giving it to two other people\n",
      "to sort (they could do the same in turn).  Then, when two sorted piles come back, it\n",
      "is an easy task to merge them into a single sorted pile.\n",
      "See 4.8 for an illustration of this process.\n",
      "\n",
      "\n",
      "Figure 4.8: Sorting by Divide-and-Conquer: to sort an array, we split it in half and\n",
      "sort each half (recursively); we merge each sorted half back into a whole\n",
      "list (again recursively); this algorithm is known as \"Merge Sort\".\n",
      "\n",
      "Another example is the process of looking up a word in a dictionary.  We open\n",
      "the book somewhere around the middle and compare our word with the current\n",
      "page.  If it's earlier in the dictionary we repeat the process on the first\n",
      "half; if its later we use the second half.  This search method is called\n",
      "binary search since it splits the problem in half at every step.\n",
      "In another approach to algorithm design, we attack a problem\n",
      "by transforming it into an instance of a problem we already know how to solve.\n",
      "For example, in order to detect duplicate entries in a list, we can pre-sort\n",
      "the list, then scan through it once to check if any adjacent pairs of elements\n",
      "are identical.\n",
      "\n",
      "Recursion\n",
      "The above examples of sorting and searching have a striking property:\n",
      "to solve a problem of size n, we have to break it in half and\n",
      "then work on one or more problems of size n/2.\n",
      "A common way to implement such methods uses recursion.\n",
      "We define a function f which simplifies the problem,\n",
      "and calls itself to solve one or more easier instances\n",
      "of the same problem.  It then combines the results into a solution\n",
      "for the original problem.\n",
      "For example, suppose we have a set of n words, and want to\n",
      "calculate how many different ways they can be combined to make a\n",
      "sequence of words.  If we have only one word (n=1), there is\n",
      "just one way to make it into a sequence.  If we have a set of two\n",
      "words, there are two ways to put them into a sequence.  For three\n",
      "words there are six possibilities.  In general, for n words,\n",
      "there are n × n-1 × … × 2 × 1\n",
      "ways (i.e. the factorial of n).  We can code this up as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def factorial1(n):\n",
      "...     result = 1\n",
      "...     for i in range(n):\n",
      "...         result *= (i+1)\n",
      "...     return result\n",
      "\n",
      "\n",
      "\n",
      "However, there is also a recursive algorithm for solving this problem,\n",
      "based on the following observation.  Suppose we have a way to\n",
      "construct all orderings for n-1 distinct words.  Then\n",
      "for each such ordering, there are n places where we can\n",
      "insert a new word: at the start, the end, or any of the n-2\n",
      "boundaries between the words.  Thus we simply multiply the number\n",
      "of solutions found for n-1 by the value of n.\n",
      "We also need the base case, to say that if we have a single\n",
      "word, there's just one ordering.  We can code this up as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def factorial2(n):\n",
      "...     if n == 1:\n",
      "...         return 1\n",
      "...     else:\n",
      "...         return n * factorial2(n-1)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "These two algorithms solve the same problem.  One uses iteration\n",
      "while the other uses recursion.\n",
      "We can use recursion to navigate a deeply-nested object, such as the\n",
      "WordNet hypernym hierarchy.  Let's count the size of the hypernym\n",
      "hierarchy rooted at a given synset s.  We'll do this by finding the\n",
      "size of each hyponym of s, then adding these together\n",
      "(we will also add 1 for the synset itself).  The following\n",
      "function size1() does this work; notice that the body of\n",
      "the function includes a recursive call to size1():\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def size1(s):\n",
      "...     return 1 + sum(size1(child) for child in s.hyponyms())\n",
      "\n",
      "\n",
      "\n",
      "We can also design an iterative solution to this problem which processes\n",
      "the hierarchy in layers.  The first layer is the synset itself ,\n",
      "then all the hyponyms of the synset, then all the hyponyms of the\n",
      "hyponyms.  Each time through the loop it computes the next layer\n",
      "by finding the hyponyms of everything in the last layer .\n",
      "It also maintains a total of the number of synsets encountered so far .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def size2(s):\n",
      "...     layer = [s] \n",
      "...     total = 0\n",
      "...     while layer:\n",
      "...         total += len(layer) \n",
      "...         layer = [h for c in layer for h in c.hyponyms()] \n",
      "...     return total\n",
      "\n",
      "\n",
      "\n",
      "Not only is the iterative solution much longer, it is harder to interpret.\n",
      "It forces us to think procedurally, and keep track of what is happening with\n",
      "the layer and total variables through time.  Let's satisfy ourselves\n",
      "that both solutions give the same result.  We'll use another form of the import\n",
      "statement, allowing us to abbreviate the name wordnet to wn:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import wordnet as wn\n",
      ">>> dog = wn.synset('dog.n.01')\n",
      ">>> size1(dog)\n",
      "190\n",
      ">>> size2(dog)\n",
      "190\n",
      "\n",
      "\n",
      "\n",
      "As a final example of recursion, let's use it to construct\n",
      "a deeply-nested object.\n",
      "A letter trie is a data structure that can be used\n",
      "for indexing a lexicon, one letter at a time.  (The name\n",
      "is based on the word retrieval).\n",
      "For example, if trie\n",
      "contained a letter trie, then trie['c'] would be a smaller\n",
      "trie which held all words starting with c.\n",
      "4.9 demonstrates the recursive process of building a trie,\n",
      "using Python dictionaries (3).\n",
      "To insert the word chien (French for dog),\n",
      "we split off the c and recursively insert hien\n",
      "into the sub-trie trie['c'].  The recursion continues\n",
      "until there are no letters remaining in the word, when we\n",
      "store the intended value (in this case, the word dog).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def insert(trie, key, value):\n",
      "    if key:\n",
      "        first, rest = key[0], key[1:]\n",
      "        if first not in trie:\n",
      "            trie[first] = {}\n",
      "        insert(trie[first], rest, value)\n",
      "    else:\n",
      "        trie['value'] = value\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> trie = {}\n",
      ">>> insert(trie, 'chat', 'cat')\n",
      ">>> insert(trie, 'chien', 'dog')\n",
      ">>> insert(trie, 'chair', 'flesh')\n",
      ">>> insert(trie, 'chic', 'stylish')\n",
      ">>> trie = dict(trie)               # for nicer printing\n",
      ">>> trie['c']['h']['a']['t']['value']\n",
      "'cat'\n",
      ">>> pprint.pprint(trie, width=40)\n",
      "{'c': {'h': {'a': {'t': {'value': 'cat'}},\n",
      "                  {'i': {'r': {'value': 'flesh'}}},\n",
      "             'i': {'e': {'n': {'value': 'dog'}}}\n",
      "                  {'c': {'value': 'stylish'}}}}}\n",
      "\n",
      "\n",
      "Example 4.9 (code_trie.py): Figure 4.9: Building a Letter Trie: A recursive function that builds a nested dictionary\n",
      "structure; each level of nesting contains all words with a given prefix,\n",
      "and a sub-trie containing all possible continuations.\n",
      "\n",
      "\n",
      "Caution!\n",
      "Despite the simplicity of recursive programming, it comes with a cost.\n",
      "Each time a function is called, some state information needs to be\n",
      "pushed on a stack, so that once the function has completed, execution\n",
      "can continue from where it left off.  For this reason, iterative\n",
      "solutions are often more efficient than recursive solutions.\n",
      "\n",
      "\n",
      "\n",
      "Space-Time Tradeoffs\n",
      "We can sometimes significantly speed up the execution of a program by building an auxiliary\n",
      "data structure, such as an index.  The listing in 4.10 implements a simple\n",
      "text retrieval system for the Movie Reviews Corpus.  By indexing the document collection it\n",
      "provides much faster lookup.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def raw(file):\n",
      "    contents = open(file).read()\n",
      "    contents = re.sub(r'<.*?>', ' ', contents)\n",
      "    contents = re.sub('\\s+', ' ', contents)\n",
      "    return contents\n",
      "\n",
      "def snippet(doc, term):\n",
      "    text = ' '*30 + raw(doc) + ' '*30\n",
      "    pos = text.index(term)\n",
      "    return text[pos-30:pos+30]\n",
      "\n",
      "print(\"Building Index...\")\n",
      "files = nltk.corpus.movie_reviews.abspaths()\n",
      "idx = nltk.Index((w, f) for f in files for w in raw(f).split())\n",
      "\n",
      "query = ''\n",
      "while query != \"quit\":\n",
      "    query = input(\"query> \")     # use raw_input() in Python 2\n",
      "    if query in idx:\n",
      "        for doc in idx[query]:\n",
      "            print(snippet(doc, query))\n",
      "    else:\n",
      "        print(\"Not found\")\n",
      "\n",
      "\n",
      "Example 4.10 (code_search_documents.py): Figure 4.10: A Simple Text Retrieval System\n",
      "\n",
      "A more subtle example of a space-time tradeoff involves replacing the tokens of a corpus\n",
      "with integer identifiers.  We create a vocabulary for the corpus, a list in which each\n",
      "word is stored once, then invert this list so that we can look up any word to find its\n",
      "identifier.  Each document is preprocessed, so that a list of words becomes a list of integers.\n",
      "Any language models can now work with integers.  See the listing in 4.11\n",
      "for an example of how to do this for a tagged corpus.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def preprocess(tagged_corpus):\n",
      "    words = set()\n",
      "    tags = set()\n",
      "    for sent in tagged_corpus:\n",
      "        for word, tag in sent:\n",
      "            words.add(word)\n",
      "            tags.add(tag)\n",
      "    wm = dict((w, i) for (i, w) in enumerate(words))\n",
      "    tm = dict((t, i) for (i, t) in enumerate(tags))\n",
      "    return [[(wm[w], tm[t]) for (w, t) in sent] for sent in tagged_corpus]\n",
      "\n",
      "\n",
      "Example 4.11 (code_strings_to_ints.py): Figure 4.11: Preprocess tagged corpus data, converting all words and tags to integers\n",
      "\n",
      "Another example of a space-time tradeoff is maintaining a vocabulary list.\n",
      "If you need to process an input text to check that all words are in an\n",
      "existing vocabulary, the vocabulary should be stored as a set, not a list.\n",
      "The elements of a set are automatically indexed, so testing membership\n",
      "of a large set will be much faster than testing membership of the\n",
      "corresponding list.\n",
      "We can test this claim using the timeit module.\n",
      "The Timer class has two parameters, a statement which\n",
      "is executed multiple times, and setup code that is executed\n",
      "once at the beginning.  We will simulate a vocabulary of\n",
      "100,000 items using a list  or set \n",
      "of integers.  The test statement will generate a random\n",
      "item which has a 50% chance of being in the vocabulary .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from timeit import Timer\n",
      ">>> vocab_size = 100000\n",
      ">>> setup_list = \"import random; vocab = range(%d)\" % vocab_size \n",
      ">>> setup_set = \"import random; vocab = set(range(%d))\" % vocab_size \n",
      ">>> statement = \"random.randint(0, %d) in vocab\" % (vocab_size * 2) \n",
      ">>> print(Timer(statement, setup_list).timeit(1000))\n",
      "2.78092288971\n",
      ">>> print(Timer(statement, setup_set).timeit(1000))\n",
      "0.0037260055542\n",
      "\n",
      "\n",
      "\n",
      "Performing 1000 list membership tests takes a total of 2.8 seconds,\n",
      "while the equivalent tests on a set take a mere 0.0037 seconds,\n",
      "or three orders of magnitude faster!\n",
      "\n",
      "\n",
      "Dynamic Programming\n",
      "Dynamic programming is a general technique for designing algorithms\n",
      "which is widely used in natural language processing.  The term\n",
      "'programming' is used in a different sense to what you might expect,\n",
      "to mean planning or scheduling.  Dynamic programming is used when a\n",
      "problem contains overlapping sub-problems.  Instead of computing\n",
      "solutions to these sub-problems repeatedly, we simply store them in a\n",
      "lookup table.\n",
      "In the remainder of this section we will introduce dynamic programming,\n",
      "but in a rather different context to syntactic parsing.\n",
      "Pingala was an Indian author who lived around the 5th century B.C.,\n",
      "and wrote a treatise on Sanskrit prosody called the Chandas Shastra.\n",
      "Virahanka extended this work around the 6th century A.D., studying the\n",
      "number of ways of combining short and long syllables to create a meter\n",
      "of length n.  Short syllables, marked S, take up one unit of length, while\n",
      "long syllables, marked L, take two.\n",
      "Pingala found, for example, that there are five ways to\n",
      "construct a meter of length 4: V4 = {LL, SSL, SLS,\n",
      "LSS, SSSS}.  Observe that we can split V4 into two\n",
      "subsets, those starting with L and those starting with\n",
      "S, as shown in (1).\n",
      "\n",
      "  (1)\n",
      "V4 =\n",
      "  LL, LSS\n",
      "    i.e. L prefixed to each item of V2 = {L, SS}\n",
      "  SSL, SLS, SSSS\n",
      "    i.e. S prefixed to each item of V3 = {SL, LS, SSS}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def virahanka1(n):\n",
      "    if n == 0:\n",
      "        return [\"\"]\n",
      "    elif n == 1:\n",
      "        return [\"S\"]\n",
      "    else:\n",
      "        s = [\"S\" + prosody for prosody in virahanka1(n-1)]\n",
      "        l = [\"L\" + prosody for prosody in virahanka1(n-2)]\n",
      "        return s + l\n",
      "\n",
      "def virahanka2(n):\n",
      "    lookup = [[\"\"], [\"S\"]]\n",
      "    for i in range(n-1):\n",
      "        s = [\"S\" + prosody for prosody in lookup[i+1]]\n",
      "        l = [\"L\" + prosody for prosody in lookup[i]]\n",
      "        lookup.append(s + l)\n",
      "    return lookup[n]\n",
      "\n",
      "def virahanka3(n, lookup={0:[\"\"], 1:[\"S\"]}):\n",
      "    if n not in lookup:\n",
      "        s = [\"S\" + prosody for prosody in virahanka3(n-1)]\n",
      "        l = [\"L\" + prosody for prosody in virahanka3(n-2)]\n",
      "        lookup[n] = s + l\n",
      "    return lookup[n]\n",
      "\n",
      "from nltk import memoize\n",
      "@memoize\n",
      "def virahanka4(n):\n",
      "    if n == 0:\n",
      "        return [\"\"]\n",
      "    elif n == 1:\n",
      "        return [\"S\"]\n",
      "    else:\n",
      "        s = [\"S\" + prosody for prosody in virahanka4(n-1)]\n",
      "        l = [\"L\" + prosody for prosody in virahanka4(n-2)]\n",
      "        return s + l\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> virahanka1(4)\n",
      "['SSSS', 'SSL', 'SLS', 'LSS', 'LL']\n",
      ">>> virahanka2(4)\n",
      "['SSSS', 'SSL', 'SLS', 'LSS', 'LL']\n",
      ">>> virahanka3(4)\n",
      "['SSSS', 'SSL', 'SLS', 'LSS', 'LL']\n",
      ">>> virahanka4(4)\n",
      "['SSSS', 'SSL', 'SLS', 'LSS', 'LL']\n",
      "\n",
      "\n",
      "Example 4.12 (code_virahanka.py): Figure 4.12: Four Ways to Compute Sanskrit Meter: (i) recursive; (ii) bottom-up dynamic programming;\n",
      "(iii) top-down dynamic programming; and (iv) built-in memoization.\n",
      "\n",
      "With this observation, we can write a little recursive function called\n",
      "virahanka1() to compute these meters, shown in 4.12.\n",
      "Notice that, in order to compute V4 we first compute\n",
      "V3 and V2.  But to compute V3,\n",
      "we need to first compute V2 and V1.  This call\n",
      "structure is depicted in (2).\n",
      "\n",
      "  (2)\n",
      "As you can see, V2 is computed twice.\n",
      "This might not seem like a significant problem, but\n",
      "it turns out to be rather wasteful as n gets large:\n",
      "to compute V20 using this recursive technique, we\n",
      "would compute V2 4,181 times;\n",
      "and for V40 we would compute V2 63,245,986 times!\n",
      "A much better alternative is to store the value of V2 in a table\n",
      "and look it up whenever we need it.  The same goes for other values, such\n",
      "as V3 and so on.  Function virahanka2() implements a\n",
      "dynamic programming approach to the problem.  It works by filling up a\n",
      "table (called lookup) with solutions to all smaller instances of the\n",
      "problem, stopping as soon as we reach the value we're interested in.\n",
      "At this point we read off the value and return it.  Crucially, each\n",
      "sub-problem is only ever solved once.\n",
      "Notice that the approach taken in virahanka2() is to solve smaller\n",
      "problems on the way to solving larger problems.  Accordingly, this is known as the\n",
      "bottom-up approach to dynamic programming.  Unfortunately it turns out\n",
      "to be quite wasteful for some applications, since it\n",
      "may compute solutions to sub-problems that are never required for\n",
      "solving the main problem.  This wasted computation can be avoided\n",
      "using the top-down approach to dynamic programming, which is\n",
      "illustrated in the function virahanka3() in 4.12.\n",
      "Unlike the bottom-up approach, this approach is recursive.  It avoids\n",
      "the huge wastage of virahanka1() by checking whether it has\n",
      "previously stored the result.  If not, it computes the result\n",
      "recursively and stores it in the table.  The last step is to return\n",
      "the stored result.  The final method, in virahanka4(),\n",
      "is to use a Python \"decorator\" called memoize,\n",
      "which takes care of the housekeeping work done\n",
      "by virahanka3() without cluttering up the program.\n",
      "This \"memoization\" process stores the result of each previous\n",
      "call to the function along with the parameters that were used.\n",
      "If the function is subsequently called with the same parameters,\n",
      "it returns the stored result instead of recalculating it.\n",
      "(This aspect of Python syntax is beyond the scope of this book.)\n",
      "This concludes our brief introduction to dynamic programming.\n",
      "We will encounter it again in 4.\n",
      "\n",
      "\n",
      "\n",
      "4.8   A Sample of Python Libraries\n",
      "Python has hundreds of third-party libraries, specialized software packages that extend\n",
      "the functionality of Python.  NLTK is one such library.  To realize the full power\n",
      "of Python programming, you should become familiar with several other libraries.\n",
      "Most of these will need to be manually installed on your computer.\n",
      "\n",
      "Matplotlib\n",
      "Python has some libraries that are useful for visualizing language data.\n",
      "The Matplotlib package supports sophisticated\n",
      "plotting functions with a MATLAB-style interface, and is available from\n",
      "http://matplotlib.sourceforge.net/.\n",
      "So far we have focused on textual presentation and the use of formatted print\n",
      "statements to get output lined up in columns.  It is often very useful to display\n",
      "numerical data in graphical form, since this often makes it easier to detect\n",
      "patterns.  For example, in 3.7 we saw a table of numbers\n",
      "showing the frequency of particular modal verbs in the Brown Corpus, classified\n",
      "by genre.  The program in 4.13 presents the same information in graphical\n",
      "format.  The output is shown in 4.14 (a color figure in the graphical display).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "from numpy import arange\n",
      "from matplotlib import pyplot\n",
      "\n",
      "colors = 'rgbcmyk' # red, green, blue, cyan, magenta, yellow, black\n",
      "\n",
      "def bar_chart(categories, words, counts):\n",
      "    \"Plot a bar chart showing counts for each word by category\"\n",
      "    ind = arange(len(words))\n",
      "    width = 1 / (len(categories) + 1)\n",
      "    bar_groups = []\n",
      "    for c in range(len(categories)):\n",
      "        bars = pyplot.bar(ind+c*width, counts[categories[c]], width,\n",
      "                         color=colors[c % len(colors)])\n",
      "        bar_groups.append(bars)\n",
      "    pyplot.xticks(ind+width, words)\n",
      "    pyplot.legend([b[0] for b in bar_groups], categories, loc='upper left')\n",
      "    pyplot.ylabel('Frequency')\n",
      "    pyplot.title('Frequency of Six Modal Verbs by Genre')\n",
      "    pyplot.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> genres = ['news', 'religion', 'hobbies', 'government', 'adventure']\n",
      ">>> modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
      ">>> cfdist = nltk.ConditionalFreqDist(\n",
      "...              (genre, word)\n",
      "...              for genre in genres\n",
      "...              for word in nltk.corpus.brown.words(categories=genre)\n",
      "...              if word in modals)\n",
      "...\n",
      ">>> counts = {}\n",
      ">>> for genre in genres:\n",
      "...     counts[genre] = [cfdist[genre][word] for word in modals]\n",
      ">>> bar_chart(genres, modals, counts)\n",
      "\n",
      "\n",
      "Example 4.13 (code_modal_plot.py): Figure 4.13: Frequency of Modals in Different Sections of the Brown Corpus\n",
      "\n",
      "\n",
      "\n",
      "Figure 4.14: Bar Chart Showing Frequency of Modals in Different Sections of Brown Corpus: this\n",
      "visualization was produced by the program in 4.13.\n",
      "\n",
      "\n",
      "From the bar chart it is immediately obvious that may and must have\n",
      "almost identical relative frequencies.  The same goes for could and might.\n",
      "It is also possible to generate such data visualizations on the fly.\n",
      "For example, a web page with form input could permit visitors to specify\n",
      "search parameters, submit the form, and see a dynamically generated\n",
      "visualization.\n",
      "To do this we have to specify the Agg backend for matplotlib,\n",
      "which is a library for producing raster (pixel) images .\n",
      "Next, we use all the same Matplotlib methods as before, but instead of displaying the\n",
      "result on a graphical terminal using pyplot.show(), we save it to a file\n",
      "using pyplot.savefig() .  We specify the filename\n",
      "then print HTML markup that directs the web browser to load the file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from matplotlib import use, pyplot\n",
      ">>> use('Agg') \n",
      ">>> pyplot.savefig('modals.png') \n",
      ">>> print('Content-Type: text/html')\n",
      ">>> print()\n",
      ">>> print('<html><body>')\n",
      ">>> print('<img src=\"modals.png\"/>')\n",
      ">>> print('</body></html>')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NetworkX\n",
      "The NetworkX package is for defining and manipulating structures consisting of\n",
      "nodes and edges, known as graphs.  It is\n",
      "available from https://networkx.lanl.gov/.\n",
      "NetworkX can be used in conjunction with Matplotlib to\n",
      "visualize networks, such as WordNet (the semantic network we\n",
      "introduced in 5).  The program in 4.15\n",
      "initializes an empty graph  then traverses\n",
      "the WordNet hypernym hierarchy adding edges to\n",
      "the graph .\n",
      "Notice that the traversal is recursive ,\n",
      "applying the programming technique discussed in\n",
      "4.7.  The resulting display is shown in 4.16.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "import networkx as nx\n",
      "import matplotlib\n",
      "from nltk.corpus import wordnet as wn\n",
      "\n",
      "def traverse(graph, start, node):\n",
      "    graph.depth[node.name] = node.shortest_path_distance(start)\n",
      "    for child in node.hyponyms():\n",
      "        graph.add_edge(node.name, child.name) \n",
      "        traverse(graph, start, child) \n",
      "\n",
      "def hyponym_graph(start):\n",
      "    G = nx.Graph() \n",
      "    G.depth = {}\n",
      "    traverse(G, start, start)\n",
      "    return G\n",
      "\n",
      "def graph_draw(graph):\n",
      "    nx.draw_graphviz(graph,\n",
      "         node_size = [16 * graph.degree(n) for n in graph],\n",
      "         node_color = [graph.depth[n] for n in graph],\n",
      "         with_labels = False)\n",
      "    matplotlib.pyplot.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> dog = wn.synset('dog.n.01')\n",
      ">>> graph = hyponym_graph(dog)\n",
      ">>> graph_draw(graph)\n",
      "\n",
      "\n",
      "Example 4.15 (code_networkx.py): Figure 4.15: Using the NetworkX and Matplotlib Libraries\n",
      "\n",
      "\n",
      "\n",
      "Figure 4.16: Visualization with NetworkX and Matplotlib: Part of the WordNet hypernym\n",
      "hierarchy is displayed, starting with dog.n.01 (the darkest node in the middle);\n",
      "node size is based on the number of children of the node, and color is based on\n",
      "the distance of the node from dog.n.01; this visualization was produced\n",
      "by the program in 4.15.\n",
      "\n",
      "\n",
      "\n",
      "csv\n",
      "Language analysis work often involves data tabulations, containing information\n",
      "about lexical items, or the participants in an empirical study, or the linguistic\n",
      "features extracted from a corpus.  Here's a fragment of a simple lexicon, in CSV format:\n",
      "\n",
      "sleep, sli:p, v.i, a condition of body and mind ...\n",
      "walk, wo:k, v.intr, progress by lifting and setting down each foot ...\n",
      "wake, weik, intrans, cease to sleep\n",
      "\n",
      "We can use Python's CSV library to read and write files stored in this format.\n",
      "For example, we can open a CSV file called lexicon.csv \n",
      "and iterate over its rows :\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> import csv\n",
      ">>> input_file = open(\"lexicon.csv\", \"rb\") \n",
      ">>> for row in csv.reader(input_file): \n",
      "...     print(row)\n",
      "['sleep', 'sli:p', 'v.i', 'a condition of body and mind ...']\n",
      "['walk', 'wo:k', 'v.intr', 'progress by lifting and setting down each foot ...']\n",
      "['wake', 'weik', 'intrans', 'cease to sleep']\n",
      "\n",
      "\n",
      "\n",
      "Each row is just a list of strings.  If any fields contain numerical\n",
      "data, they will appear as strings, and will have to be converted using\n",
      "int() or float().\n",
      "\n",
      "\n",
      "NumPy\n",
      "The NumPy package provides substantial support for numerical processing in Python.\n",
      "NumPy has a multi-dimensional array object, which is easy to initialize and access:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from numpy import array\n",
      ">>> cube = array([ [[0,0,0], [1,1,1], [2,2,2]],\n",
      "...                [[3,3,3], [4,4,4], [5,5,5]],\n",
      "...                [[6,6,6], [7,7,7], [8,8,8]] ])\n",
      ">>> cube[1,1,1]\n",
      "4\n",
      ">>> cube[2].transpose()\n",
      "array([[6, 7, 8],\n",
      "       [6, 7, 8],\n",
      "       [6, 7, 8]])\n",
      ">>> cube[2,1:]\n",
      "array([[7, 7, 7],\n",
      "       [8, 8, 8]])\n",
      "\n",
      "\n",
      "\n",
      "NumPy includes linear algebra functions.  Here we perform\n",
      "singular value decomposition on a matrix, an operation used\n",
      "in latent semantic analysis to help identify implicit\n",
      "concepts in a document collection.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from numpy import linalg\n",
      ">>> a=array([[4,0], [3,-5]])\n",
      ">>> u,s,vt = linalg.svd(a)\n",
      ">>> u\n",
      "array([[-0.4472136 , -0.89442719],\n",
      "       [-0.89442719,  0.4472136 ]])\n",
      ">>> s\n",
      "array([ 6.32455532,  3.16227766])\n",
      ">>> vt\n",
      "array([[-0.70710678,  0.70710678],\n",
      "       [-0.70710678, -0.70710678]])\n",
      "\n",
      "\n",
      "\n",
      "NLTK's clustering package nltk.cluster makes extensive use of NumPy arrays,\n",
      "and includes support for k-means clustering, Gaussian EM clustering,\n",
      "group average agglomerative clustering, and dendrogram plots.\n",
      "For details, type help(nltk.cluster).\n",
      "\n",
      "\n",
      "Other Python Libraries\n",
      "There are many other Python libraries, and you can search for them with the\n",
      "help of the Python Package Index http://pypi.python.org/.\n",
      "Many libraries provide an interface to external software, such\n",
      "as relational databases (e.g. mysql-python)\n",
      "and large document collections (e.g. PyLucene).\n",
      "Many other libraries give access to file formats\n",
      "such as PDF, MSWord, and XML (pypdf, pywin32, xml.etree),\n",
      "RSS feeds (e.g. feedparser),\n",
      "and electronic mail (e.g. imaplib, email).\n",
      "\n",
      "\n",
      "\n",
      "4.9   Summary\n",
      "\n",
      "Python's assignment and parameter passing use object references;\n",
      "e.g. if a is a list and we assign b = a, then any operation\n",
      "on a will modify b, and vice versa.\n",
      "The is operation tests if two objects are identical internal objects,\n",
      "while == tests if two objects are equivalent.  This distinction\n",
      "parallels the type-token distinction.\n",
      "Strings, lists and tuples are different kinds of sequence object, supporting\n",
      "common operations such as indexing, slicing, len(), sorted(), and\n",
      "membership testing using in.\n",
      "A declarative programming style usually produces more compact,\n",
      "readable code; manually-incremented loop variables are usually\n",
      "unnecessary; when a sequence must be enumerated, use enumerate().\n",
      "Functions are an essential programming abstraction: key concepts\n",
      "to understand are parameter passing, variable scope, and docstrings.\n",
      "A function serves as a namespace: names defined inside a function are not visible\n",
      "outside that function, unless those names are declared to be global.\n",
      "Modules permit logically-related material to be localized in a file.\n",
      "A module serves as a namespace: names defined in a module — such as variables\n",
      "and functions — are not visible to other modules, unless those names are imported.\n",
      "Dynamic programming is an algorithm design technique used widely in NLP\n",
      "that stores the results of previous computations in order to avoid\n",
      "unnecessary recomputation.\n",
      "\n",
      "\n",
      "\n",
      "4.10   Further Reading\n",
      "This chapter has touched on many topics in programming, some specific to Python,\n",
      "and some quite general.  We've just scratched the surface, and you may want\n",
      "to read more about these topics, starting with the further materials for\n",
      "this chapter available at http://nltk.org/.\n",
      "The Python website provides extensive documentation.  It is important to\n",
      "understand the built-in functions and standard types, described at\n",
      "http://docs.python.org/library/functions.html and\n",
      "http://docs.python.org/library/stdtypes.html.\n",
      "We have learnt about generators and their importance for efficiency;\n",
      "for information about iterators, a closely related topic,\n",
      "see http://docs.python.org/library/itertools.html.\n",
      "Consult your favorite Python book for more information on such topics.\n",
      "An excellent resource for using Python for multimedia processing,\n",
      "including working with sound files, is (Guzdial, 2005).\n",
      "When using the online Python documentation, be aware that\n",
      "your installed version might be different from the version\n",
      "of the documentation you are reading.  You can easily\n",
      "check what version you have, with import sys; sys.version.\n",
      "Version-specific documentation is available at\n",
      "http://www.python.org/doc/versions/.\n",
      "Algorithm design is a rich field within computer science.  Some\n",
      "good starting points are (Harel, 2004), (Levitin, 2004), (Knuth, 2006).\n",
      "Useful guidance on the practice of software development is provided\n",
      "in (Hunt & Thomas, 2000) and (McConnell, 2004).\n",
      "\n",
      "\n",
      "4.11   Exercises\n",
      "\n",
      "☼ Find out more about sequence objects using Python's help facility.\n",
      "In the interpreter, type help(str), help(list), and help(tuple).\n",
      "This will give you a full list of the functions supported by each type.\n",
      "Some functions have special names flanked with underscore; as the\n",
      "help documentation shows, each such function corresponds to something\n",
      "more familiar.  For example x.__getitem__(y) is just a long-winded\n",
      "way of saying x[y].\n",
      "\n",
      "☼ Identify three operations that can be performed on both tuples\n",
      "and lists.  Identify three list operations that cannot be performed on\n",
      "tuples.  Name a context where using a list instead of a tuple generates\n",
      "a Python error.\n",
      "\n",
      "☼ Find out how to create a tuple consisting of a single item.\n",
      "There are at least two ways to do this.\n",
      "\n",
      "☼ Create a list words = ['is', 'NLP', 'fun', '?'].  Use\n",
      "a series of assignment statements (e.g. words[1] = words[2])\n",
      "and a temporary variable tmp to transform this list into the\n",
      "list ['NLP', 'is', 'fun', '!'].  Now do the same transformation\n",
      "using tuple assignment.\n",
      "\n",
      "☼ Read about the built-in comparison function cmp, by\n",
      "typing help(cmp).  How does it differ in behavior from\n",
      "the comparison operators?\n",
      "\n",
      "☼ Does the method for creating a sliding window of n-grams\n",
      "behave correctly for the two limiting cases: n = 1, and n = len(sent)?\n",
      "\n",
      "☼ We pointed out that when empty strings and empty lists occur\n",
      "in the condition part of an if clause, they evaluate to\n",
      "False. In this case, they are said to be occurring in a\n",
      "Boolean context.\n",
      "Experiment with different kind of non-Boolean expressions in Boolean\n",
      "contexts, and see whether they evaluate as True or False.\n",
      "\n",
      "☼ Use the inequality operators to compare strings, e.g.\n",
      "'Monty' < 'Python'.  What happens when you do 'Z' < 'a'?\n",
      "Try pairs of strings which have a common prefix, e.g. 'Monty' < 'Montague'.\n",
      "Read up on \"lexicographical sort\" in order to understand what is\n",
      "going on here.  Try comparing structured objects, e.g.\n",
      "('Monty', 1) < ('Monty', 2).  Does this behave as expected?\n",
      "\n",
      "☼ Write code that removes whitespace at the beginning and end of a\n",
      "string, and normalizes whitespace between words to be a single\n",
      "space character.\n",
      "\n",
      "do this task using split() and join()\n",
      "do this task using regular expression substitutions\n",
      "\n",
      "\n",
      "☼ Write a program to sort words by length.  Define a helper function\n",
      "cmp_len which uses the cmp comparison function on word lengths.\n",
      "\n",
      "◑ Create a list of words and store it in a variable sent1.\n",
      "Now assign sent2 = sent1.  Modify one of the items in sent1\n",
      "and verify that sent2 has changed.\n",
      "\n",
      "Now try the same exercise but instead assign sent2 = sent1[:].\n",
      "Modify sent1 again and see what happens to sent2.  Explain.\n",
      "Now define text1 to be a list of lists of strings (e.g. to\n",
      "represent a text consisting of multiple sentences.  Now assign\n",
      "text2 = text1[:], assign a new value to one of the words,\n",
      "e.g. text1[1][1] = 'Monty'.  Check what this did to text2.\n",
      "Explain.\n",
      "Load Python's deepcopy() function (i.e. from copy import deepcopy),\n",
      "consult its documentation, and test that it makes a fresh copy of any\n",
      "object.\n",
      "\n",
      "\n",
      "◑ Initialize an n-by-m list of lists of empty strings using list\n",
      "multiplication, e.g. word_table = [[''] * n] * m.  What happens\n",
      "when you set one of its values, e.g. word_table[1][2] = \"hello\"?\n",
      "Explain why this happens.  Now write an expression using range()\n",
      "to construct a list of lists, and show that it does not have this problem.\n",
      "\n",
      "◑ Write code to initialize a two-dimensional array of sets called\n",
      "word_vowels and process a list of words, adding each\n",
      "word to word_vowels[l][v] where l is the length of the word and v is\n",
      "the number of vowels it contains.\n",
      "\n",
      "◑ Write a function novel10(text) that prints any word that\n",
      "appeared in the last 10% of a text that had not been encountered earlier.\n",
      "\n",
      "◑ Write a program that takes a sentence expressed as a single string,\n",
      "splits it and counts up the words.  Get it to print out each word and the\n",
      "word's frequency, one per line, in alphabetical order.\n",
      "\n",
      "◑ Read up on Gematria, a method for assigning numbers to words, and for\n",
      "mapping between words having the same number to discover the hidden meaning of\n",
      "texts (http://en.wikipedia.org/wiki/Gematria, http://essenes.net/gemcal.htm).\n",
      "\n",
      "Write a function gematria() that sums the numerical values of\n",
      "the letters of a word, according to the letter values in letter_vals:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> letter_vals = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':80, 'g':3, 'h':8,\n",
      "... 'i':10, 'j':10, 'k':20, 'l':30, 'm':40, 'n':50, 'o':70, 'p':80, 'q':100,\n",
      "... 'r':200, 's':300, 't':400, 'u':6, 'v':6, 'w':800, 'x':60, 'y':10, 'z':7}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Process a corpus (e.g. nltk.corpus.state_union) and for each document, count how\n",
      "many of its words have the number 666.\n",
      "\n",
      "Write a function decode() to process a text, randomly replacing words with\n",
      "their Gematria equivalents, in order to discover the \"hidden meaning\" of the text.\n",
      "\n",
      "\n",
      "\n",
      "◑ Write a function shorten(text, n) to process a text, omitting the n\n",
      "most frequently occurring words of the text.  How readable is it?\n",
      "\n",
      "◑ Write code to print out an index for a lexicon, allowing someone\n",
      "to look up words according to their meanings (or pronunciations; whatever\n",
      "properties are contained in lexical entries).\n",
      "\n",
      "◑ Write a list comprehension that sorts a list of WordNet synsets for\n",
      "proximity to a given synset.  For example, given the synsets\n",
      "minke_whale.n.01, orca.n.01, novel.n.01, and tortoise.n.01,\n",
      "sort them according to their shortest_path_distance() from right_whale.n.01.\n",
      "\n",
      "◑ Write a function that takes a list of words (containing duplicates) and\n",
      "returns a list of words (with no duplicates) sorted by decreasing frequency.\n",
      "E.g. if the input list contained 10 instances of the word table and 9 instances\n",
      "of the word chair, then table would appear before chair in the output\n",
      "list.\n",
      "\n",
      "◑ Write a function that takes a text and a vocabulary as its arguments\n",
      "and returns the set of words that appear in the text but not in the\n",
      "vocabulary.  Both arguments can be represented as lists of strings.\n",
      "Can you do this in a single line, using set.difference()?\n",
      "\n",
      "◑ Import the itemgetter() function from the operator module in Python's\n",
      "standard library (i.e. from operator import itemgetter).  Create a list\n",
      "words containing several words.  Now try calling:\n",
      "sorted(words, key=itemgetter(1)), and sorted(words, key=itemgetter(-1)).\n",
      "Explain what itemgetter() is doing.\n",
      "\n",
      "◑ Write a recursive function lookup(trie, key) that looks up a key in a trie,\n",
      "and returns the value it finds.  Extend the function to return a word when it is uniquely\n",
      "determined by its prefix (e.g. vanguard is the only word that starts with vang-,\n",
      "so lookup(trie, 'vang') should return the same thing as lookup(trie, 'vanguard')).\n",
      "\n",
      "◑ Read up on \"keyword linkage\" (chapter 5 of (Scott & Tribble, 2006)).  Extract keywords from\n",
      "NLTK's Shakespeare Corpus and using the NetworkX package, plot keyword linkage networks.\n",
      "\n",
      "◑ Read about string edit distance and the Levenshtein Algorithm.\n",
      "Try the implementation provided in nltk.edit_distance().\n",
      "In what way is this using dynamic programming?  Does it use the bottom-up or\n",
      "top-down approach?\n",
      "[See also http://norvig.com/spell-correct.html]\n",
      "\n",
      "◑ The Catalan numbers arise in many applications of combinatorial mathematics,\n",
      "including the counting of parse trees (6).  The series\n",
      "can be defined as follows: C0 = 1, and\n",
      "Cn+1 = Σ0..n (CiCn-i).\n",
      "\n",
      "Write a recursive function to compute nth Catalan number Cn.\n",
      "Now write another function that does this computation using dynamic programming.\n",
      "Use the timeit module to compare the performance of these functions as n\n",
      "increases.\n",
      "\n",
      "\n",
      "★\n",
      "Reproduce some of the results of (Zhao & Zobel, 2007) concerning authorship identification.\n",
      "\n",
      "★ Study gender-specific lexical choice, and see if you can\n",
      "reproduce some of the results of http://www.clintoneast.com/articles/words.php\n",
      "\n",
      "★ Write a recursive function that pretty prints a trie in alphabetically\n",
      "sorted order, e.g.:\n",
      "\n",
      "chair: 'flesh'\n",
      "---t: 'cat'\n",
      "--ic: 'stylish'\n",
      "---en: 'dog'\n",
      "\n",
      "\n",
      "★ With the help of the trie data structure, write a recursive\n",
      "function that processes text, locating the uniqueness point in\n",
      "each word, and discarding the remainder of each word.  How much compression does this\n",
      "give?  How readable is the resulting text?\n",
      "\n",
      "★ Obtain some raw text, in the form of a single, long string.\n",
      "Use Python's textwrap module to break it up into multiple lines.\n",
      "Now write code to add extra spaces between words, in order to justify\n",
      "the output.  Each line must have the same width, and spaces must be\n",
      "approximately evenly distributed across each line.  No line can\n",
      "begin or end with a space.\n",
      "\n",
      "★ Develop a simple extractive summarization tool, that prints the\n",
      "sentences of a document which contain the highest total word\n",
      "frequency.  Use FreqDist() to count word frequencies, and use\n",
      "sum to sum the frequencies of the words in each sentence.\n",
      "Rank the sentences according to their score.  Finally, print the n\n",
      "highest-scoring sentences in document order.  Carefully review the\n",
      "design of your program, especially your approach to this double\n",
      "sorting.  Make sure the program is written as clearly as possible.\n",
      "\n",
      "★\n",
      "Read the following article on semantic orientation of adjectives.\n",
      "Use the NetworkX package to visualize\n",
      "a network of adjectives with edges to indicate same vs different\n",
      "semantic orientation.  http://www.aclweb.org/anthology/P97-1023\n",
      "\n",
      "★\n",
      "Design an algorithm to find the \"statistically improbable\n",
      "phrases\" of a document collection.\n",
      "http://www.amazon.com/gp/search-inside/sipshelp.html\n",
      "\n",
      "★ Write a program to implement a brute-force algorithm for\n",
      "discovering word squares, a kind of n × n crossword\n",
      "in which the entry in the nth row is the same as the entry\n",
      "in the nth column.  For discussion, see\n",
      "http://itre.cis.upenn.edu/~myl/languagelog/archives/002679.html\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "About this document...\n",
      "UPDATED FOR NLTK 3.0.\n",
      "This is a chapter from Natural Language Processing with Python,\n",
      "by Steven Bird, Ewan Klein and Edward Loper,\n",
      "Copyright © 2019 the authors.\n",
      "It is distributed with the Natural Language Toolkit [http://nltk.org/],\n",
      "Version 3.0, under the terms of the\n",
      "Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\n",
      "[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\n",
      "This document was built on\n",
      "Wed  4 Sep 2019 11:40:48 ACST\n",
      "vectors[4] = [0, 10, 9, 29, 12, 81, 6, 21, 0, 0, 1, 33, 17, 0, 0, 0, 3, 0, 52, 0, 0, 0, 5, 0, 1, 0, 1, 1, 30, 14, 7, 2, 0, 9, 0, 12, 9, 0, 38, 1, 12, 9, 2, 205, 14, 6, 25, 4, 14, 3, 5, 6, 0, 1, 0, 2, 7, 8, 0, 0, 6, 0, 0, 1, 62, 0, 0, 6, 0, 78, 0, 0, 3, 6, 0, 5, 14, 0, 0, 3, 0, 4, 21, 0, 7, 0, 26, 0, 5, 0, 6, 0, 0, 0, 1, 153, 7, 3, 9, 32, 4, 0, 0, 1, 0, 0, 8, 1, 2, 2, 5, 3, 2, 0, 0, 0, 0, 0, 0, 0, 29, 0, 0, 12, 1, 0, 0, 0, 4, 125, 8, 12, 3, 17, 22, 2, 440, 49, 56, 5, 3, 5, 33, 11, 0, 0, 28, 28, 16, 6, 3, 6, 1, 0, 0, 0, 0, 0, 0, 13, 4, 35, 0, 10, 0, 0, 0, 0, 2, 0, 0, 0, 0, 13, 0, 5, 18, 14, 3, 3, 4, 5, 0, 0, 0, 4, 0, 0, 0, 1, 2, 16, 8, 44, 1, 3, 1, 12, 2, 30, 81, 1, 0, 4, 1, 191, 0, 11, 0, 5, 0, 5, 579, 0, 0, 6, 2, 2, 5, 12, 20, 8, 33, 153, 1, 9, 1, 6, 1, 2, 56, 15, 0, 0, 8, 3, 1, 0, 1, 0, 2, 10, 24, 2, 6, 5, 0, 1, 0, 0, 5, 15, 114, 2, 2, 1, 0, 0, 0, 0, 0, 78, 24, 2, 2, 1, 0, 1, 9, 3, 17, 1, 29, 0, 1, 0, 0, 1, 0, 2, 1, 0, 1, 1, 1, 0, 0, 3, 0, 2, 0, 3, 0, 6, 0, 13, 32, 24, 0, 0, 2, 0, 2, 4, 1, 1, 0, 6, 75, 2, 18, 0, 1, 0, 14, 1, 0, 63, 0, 0, 0, 0, 0, 17, 1, 0, 16, 4, 10, 3, 1, 7, 0, 0, 0, 1, 0, 44, 1, 0, 2, 1, 2, 6, 1, 1, 8, 1, 1, 0, 0, 7, 0, 6, 3, 18, 0, 8, 0, 7, 0, 1, 22, 0, 0, 7, 7, 0, 9, 1, 68, 3, 10, 113, 183, 0, 5, 0, 1, 0, 0, 1, 2, 6, 0, 4, 0, 0, 0, 25, 40, 13, 19, 13, 1, 9, 2, 7, 0, 4, 16, 18, 3, 0, 0, 2, 5, 1, 0, 1, 1, 0, 0, 0, 2, 0, 79, 10, 0, 1, 0, 8, 0, 6, 0, 1, 16, 0, 0, 0, 0, 9, 28, 0, 2, 0, 1, 9, 0, 5, 20, 0, 10, 7, 2, 0, 1, 2, 3, 0, 16, 3, 3, 0, 8, 9, 0, 0, 0, 5, 2, 0, 2, 11, 1, 4, 1, 12, 1, 0, 1, 5, 0, 0, 0, 1, 41, 19, 42, 3, 42, 0, 0, 261, 10, 0, 127, 11, 3, 0, 45, 0, 3, 9, 5, 1, 7, 2, 16, 9, 0, 4, 16, 60, 1, 49, 1, 1, 14, 0, 40, 4, 47, 14, 91, 6, 1, 3, 49, 10, 4, 31, 2, 11, 8, 32, 2, 0, 4, 9, 1, 12, 8, 0, 0, 17, 4, 3, 0, 0, 0, 5, 1, 2, 8, 1, 0, 0, 2, 0, 5, 0, 3, 13, 0, 1, 1, 1, 0, 4, 2, 4, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 6, 1, 3, 0, 3, 7, 1, 1, 3, 6, 3, 40, 0, 1, 1, 3, 51, 216, 2, 46, 2, 2, 2, 3, 6, 28, 67, 1, 11, 1, 8, 4, 6, 0, 0, 3, 21, 3, 1, 3, 2, 0, 59, 24, 0, 0, 16, 5, 74, 0, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 1, 2, 0, 1, 0, 1, 10, 0, 0, 0, 8, 2, 0, 1, 0, 9, 5, 1, 0, 5, 19, 7, 0, 0, 0, 0, 7, 0, 0, 8, 0, 0, 1, 19, 1, 1, 8, 0, 4, 12, 0, 2, 7, 6, 16, 22, 3, 0, 2, 0, 2, 1, 9, 1, 0, 4, 5, 10, 3, 3, 5, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 3, 1, 0, 0, 0, 16, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 14, 1, 0, 0, 0, 0, 0, 0, 0, 0, 18, 92, 2, 0, 0, 0, 0, 0, 2, 0, 0, 3, 3, 0, 1, 0, 1, 4, 0, 0, 2, 4, 0, 0, 0, 1, 5, 0, 6, 0, 3, 1, 7, 0, 0, 0, 0, 0, 0, 0, 1, 0, 13, 1, 0, 0, 0, 0, 3, 7, 0, 2, 1, 0, 1, 5, 2, 1, 0, 0, 0, 0, 8, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 9, 4, 0, 2, 1, 0, 2, 0, 3, 0, 4, 3, 0, 0, 0, 8, 9, 26, 0, 15, 20, 2, 5, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 7, 4, 0, 0, 30, 1, 0, 0, 0, 0, 5, 7, 0, 0, 1, 4, 0, 0, 2, 3, 0, 0, 4, 0, 0, 0, 9, 2, 14, 0, 3, 8, 14, 1, 0, 0, 2, 0, 2, 3, 14, 1, 2, 3, 0, 0, 0, 1, 0, 2, 0, 23, 0, 0, 1, 0, 3, 0, 8, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 4, 2, 0, 1, 10, 1, 3, 9, 0, 11, 0, 0, 1, 0, 0, 42, 0, 2, 0, 0, 0, 0, 2, 7, 0, 4, 12, 0, 0, 0, 1, 0, 14, 0, 0, 0, 0, 0, 2, 2, 0, 0, 3, 0, 6, 0, 1, 3, 6, 2, 1, 2, 6, 2, 7, 17, 1, 5, 0, 2, 1, 0, 0, 0, 0, 0, 5, 1, 1, 4, 16, 0, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 8, 0, 0, 0, 2, 5, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 1, 5, 0, 10, 4, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 6, 0, 0, 0, 0, 0, 0, 0, 4, 3, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 3, 10, 2, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 7, 0, 0, 0, 2, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 6, 0, 0, 17, 0, 0, 1, 14, 3, 1, 3, 3, 150, 1, 1, 7, 1, 1, 3, 6, 5, 1, 1, 3, 1, 13, 1, 2, 5, 1, 1, 1, 0, 0, 0, 2, 2, 1, 9, 1, 3, 0, 0, 0, 1, 1, 2, 1, 0, 0, 0, 1, 1, 9, 0, 1, 0, 3, 0, 5, 0, 0, 0, 7, 0, 5, 1, 3, 0, 0, 0, 12, 0, 0, 4, 0, 6, 2, 0, 14, 59, 4, 0, 4, 0, 1, 11, 0, 0, 0, 0, 0, 0, 13, 0, 9, 0, 0, 2, 3, 0, 0, 11, 0, 0, 47, 0, 13, 6, 0, 33, 1, 0, 2, 0, 13, 3, 0, 32, 20, 0, 0, 1, 0, 21, 5, 1, 0, 0, 3, 0, 0, 0, 2, 0, 0, 3, 1, 1, 0, 1, 0, 0, 0, 0, 3, 13, 0, 0, 0, 15, 0, 0, 2, 4, 0, 1, 7, 0, 1, 2, 0, 0, 0, 0, 1, 1, 7, 0, 0, 0, 11, 2, 0, 0, 13, 2, 0, 3, 1, 0, 3, 42, 0, 3, 0, 1, 15, 18, 0, 0, 1, 2, 0, 0, 3, 3, 0, 0, 5, 0, 0, 25, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 34, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 14, 1, 0, 1, 0, 0, 8, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 0, 0, 6, 0, 0, 0, 0, 0, 0, 1, 2, 0, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 1, 0, 0, 0, 19, 9, 0, 0, 0, 1, 7, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 1, 1, 0, 0, 0, 4, 0, 4, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 10, 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 0, 0, 1, 0, 1, 3, 1, 7, 0, 0, 0, 0, 0, 0, 0, 11, 0, 4, 18, 0, 10, 2, 0, 2, 0, 3, 0, 0, 7, 0, 1, 4, 0, 1, 0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 2, 0, 7, 11, 4, 3, 4, 0, 0, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 1, 0, 1, 18, 0, 2, 2, 0, 2, 2, 32, 0, 1, 54, 0, 0, 0, 24, 0, 0, 3, 4, 0, 0, 3, 0, 0, 21, 19, 0, 378, 0, 2, 2, 9, 0, 11, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 3, 0, 0, 2, 0, 13, 3, 0, 0, 25, 2, 0, 23, 1, 3, 4, 0, 15, 0, 0, 4, 0, 1, 3, 0, 0, 0, 9, 5, 39, 0, 0, 1, 0, 4, 0, 1, 10, 5, 1, 1, 0, 1, 3, 0, 0, 3, 12, 0, 2, 2, 27, 7, 9, 26, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 12, 0, 1, 12, 2, 215, 85, 0, 7, 2, 2, 3, 0, 0, 11, 2, 1, 4, 1, 0, 0, 0, 0, 0, 0, 4, 2, 3, 0, 0, 7, 0, 1, 41, 0, 6, 0, 0, 1, 0, 0, 0, 0, 0, 8, 4, 4, 0, 0, 0, 0, 3, 0, 1, 0, 4, 0, 0, 85, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 30, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 5, 1, 3, 2, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 5, 0, 0, 2, 2, 3, 0, 1, 2, 0, 0, 2, 0, 1, 4, 4, 0, 0, 0, 0, 11, 9, 4, 0, 0, 0, 0, 0, 6, 6, 0, 0, 1, 5, 4, 0, 0, 1, 1, 0, 3, 0, 3, 1, 3, 4, 0, 4, 4, 1, 0, 0, 0, 0, 2, 0, 0, 0, 10, 0, 0, 5, 1, 0, 0, 10, 0, 0, 0, 0, 6, 7, 5, 1, 3, 0, 2, 0, 0, 0, 0, 6, 0, 2, 1, 3, 7, 5, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 4, 0, 8, 1, 0, 0, 0, 0, 0, 3, 1, 0, 0, 1, 0, 0, 2, 1, 0, 13, 41, 0, 0, 0, 13, 5, 23, 5, 0, 1, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 7, 0, 0, 1, 0, 0, 8, 0, 0, 0, 1, 3, 5, 2, 0, 8, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 2, 0, 4, 0, 4, 0, 0, 1, 1, 4, 1, 0, 0, 0, 0, 0, 1, 9, 0, 9, 0, 0, 0, 0, 1, 0, 10, 3, 0, 0, 0, 0, 0, 1, 0, 3, 0, 1, 4, 25, 0, 3, 3, 5, 5, 8, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 5, 7, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 1, 1, 0, 0, 0, 12, 0, 1, 3, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 20, 1, 0, 7, 12, 2, 2, 1, 0, 0, 2, 1, 0, 0, 0, 4, 7, 0, 1, 1, 0, 0, 6, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 1, 0, 0, 0, 0, 1, 0, 4, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 15, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 4, 0, 4, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 3, 3, 3, 2, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 6, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 1, 6, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 6, 0, 1, 3, 2, 0, 0, 0, 0, 0, 3, 0, 4, 0, 9, 0, 0, 5, 0, 1, 0, 4, 0, 0, 2, 0, 1, 0, 1, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 3, 0, 0, 0, 3, 0, 1, 0, 0, 0, 1, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 4, 11, 2, 1, 0, 0, 3, 1, 4, 0, 0, 12, 1, 0, 0, 0, 1, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 5, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 2, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 28, 0, 0, 1, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 10, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 3, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 5, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 14, 0, 0, 41, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 5, 10, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 1, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 3, 1, 3, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 21, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 3, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 5, 14, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 3, 0, 0, 0, 4, 8, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 2, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 1, 2, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 4, 1, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 6, 8, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 16, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 2, 10, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 3, 0, 3, 0, 2, 3, 0, 7, 0, 0, 4, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 5, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 10, 0, 19, 0, 1, 2, 2, 0, 0, 0, 1, 2, 4, 0, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 2, 6, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 7, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 4, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 6, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 15, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 7, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 1, 0, 1, 1, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 4, 1, 1, 3, 1, 1, 19, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 2, 3, 2, 2, 1, 24, 1, 2, 4, 1, 2, 1, 2, 2, 1, 4, 3, 1, 8, 2, 1, 24, 1, 1, 3, 1, 2, 1, 14, 10, 1, 4, 1, 1, 3, 1, 1, 1, 1, 6, 1, 1, 2, 2, 4, 1, 1, 10, 3, 6, 1, 1, 1, 1, 6, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 3, 5, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 5, 1, 1, 2, 1, 1, 1, 1, 1, 6, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 7, 6, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 6, 2, 1, 1, 3, 1, 2, 2, 1, 1, 2, 2, 1, 2, 4, 1, 8, 1, 1, 1, 3, 1, 1, 11, 1, 1, 1, 1, 1, 8, 1, 1, 2, 2, 2, 2, 1, 1, 5, 1, 3, 2, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 1, 5, 2, 1, 1, 1, 1, 1, 4, 1, 1, 3, 1, 1, 5, 2, 1, 2, 1, 9, 2, 1, 1, 4, 2, 2, 2, 1, 1, 1, 5, 1, 1, 1, 1, 13, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 3, 1, 2, 1, 1, 3, 1, 2, 5, 5, 2, 2, 1, 1, 1, 6, 1, 1, 1, 3, 3, 2, 4, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 7, 5, 4, 1, 7, 7, 7, 1, 1, 5, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 4, 7, 3, 2, 2, 10, 3, 3, 3, 1, 3, 5, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 10, 1, 1, 3, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 3, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 11, 3, 1, 6, 12, 3, 6, 1, 2, 1, 1, 1, 2, 1, 1, 8, 2, 7, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 4, 3, 4, 1, 1, 1, 1, 1, 15, 3, 1, 2, 2, 1, 5, 3, 10, 1, 2, 1, 1, 1, 2, 30, 1, 1, 4, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 4, 5, 4, 1, 1, 1, 2, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 17, 1, 1, 1, 1, 4, 2, 4, 6, 6, 1, 2, 7, 4, 1, 6, 2, 4, 1, 1, 6, 1, 1, 1, 1, 3, 5, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 12, 1, 1, 1, 1, 2, 3, 3, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 4, 1, 3, 5, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 4, 2, 1, 1, 1, 2, 1, 1, 1, 1, 5, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "docs[5] = 5. Categorizing and Tagging Words\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5. Categorizing and Tagging Words\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Back in elementary school you learnt the difference between nouns, verbs,\n",
      "adjectives, and adverbs.  These \"word classes\" are not just\n",
      "the idle invention of grammarians, but are useful categories for many\n",
      "language processing tasks.  As we will see, they arise from simple analysis\n",
      "of the distribution of words in text.  The goal of this chapter is to\n",
      "answer the following questions:\n",
      "\n",
      "What are lexical categories and how are they used in natural language processing?\n",
      "What is a good Python data structure for storing words and their categories?\n",
      "How can we automatically tag each word of a text with its word class?\n",
      "\n",
      "Along the way, we'll cover some fundamental techniques in NLP, including\n",
      "sequence labeling, n-gram models, backoff, and evaluation.  These techniques\n",
      "are useful in many areas, and tagging gives us a simple context in which\n",
      "to present them.  We will also see how tagging is the second step in the typical\n",
      "NLP pipeline, following tokenization.\n",
      "The process of classifying words into their parts of speech and\n",
      "labeling them accordingly is known as part-of-speech tagging,\n",
      "POS-tagging, or simply tagging.  Parts of speech\n",
      "are also known as word classes or lexical categories.\n",
      "The collection of tags\n",
      "used for a particular task is known as a tagset.  Our emphasis\n",
      "in this chapter is on exploiting tags, and tagging text automatically.\n",
      "\n",
      "1   Using a Tagger\n",
      "A part-of-speech tagger, or POS-tagger, processes a sequence of words, and attaches a\n",
      "part of speech tag to each word (don't forget to import nltk):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = word_tokenize(\"And now for something completely different\")\n",
      ">>> nltk.pos_tag(text)\n",
      "[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'),\n",
      "('completely', 'RB'), ('different', 'JJ')]\n",
      "\n",
      "\n",
      "\n",
      "Here we see that and is CC, a coordinating conjunction;\n",
      "now and completely are RB, or adverbs;\n",
      "for is IN, a preposition;\n",
      "something is NN, a noun; and\n",
      "different is JJ, an adjective.\n",
      "\n",
      "Note\n",
      "NLTK provides documentation for each tag, which can be queried using\n",
      "the tag, e.g. nltk.help.upenn_tagset('RB'), or a regular\n",
      "expression, e.g. nltk.help.upenn_tagset('NN.*').\n",
      "Some corpora have README files with tagset documentation,\n",
      "see nltk.corpus.???.readme(), substituting in the name\n",
      "of the corpus.\n",
      "\n",
      "Let's look at another example, this time including some homonyms:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
      ">>> nltk.pos_tag(text)\n",
      "[('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'),\n",
      "('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')]\n",
      "\n",
      "\n",
      "\n",
      "Notice that refuse and permit both appear as a\n",
      "present tense verb (VBP) and a noun (NN).\n",
      "E.g. refUSE is a verb meaning \"deny,\" while REFuse is\n",
      "a noun meaning \"trash\" (i.e. they are not homophones).\n",
      "Thus, we need to know which word is being used in order to pronounce\n",
      "the text correctly.  (For this reason,\n",
      "text-to-speech systems usually perform POS-tagging.)\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Many words, like ski and race, can be used as nouns\n",
      "or verbs with no difference in pronunciation.  Can you think of\n",
      "others?  Hint: think of a commonplace object and try to put\n",
      "the word to before it to see if it can also be a verb, or\n",
      "think of an action and try to put the before it to see if\n",
      "it can also be a noun.  Now make up a sentence with both uses\n",
      "of this word, and run the POS-tagger on this sentence.\n",
      "\n",
      "Lexical categories like \"noun\" and part-of-speech tags like NN seem to have\n",
      "their uses, but the details will be obscure to many readers.  You might wonder what\n",
      "justification there is for introducing this extra level of information.\n",
      "Many of these categories arise from superficial analysis the distribution\n",
      "of words in text.  Consider the following analysis involving\n",
      "woman (a noun), bought (a verb),\n",
      "over (a preposition), and the (a determiner).\n",
      "The text.similar() method takes a word w, finds all contexts\n",
      "w1w w2,\n",
      "then finds all words w' that appear in the same context,\n",
      "i.e. w1w'w2.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
      ">>> text.similar('woman')\n",
      "Building word-context index...\n",
      "man day time year car moment world family house boy child country job\n",
      "state girl place war way case question\n",
      ">>> text.similar('bought')\n",
      "made done put said found had seen given left heard been brought got\n",
      "set was called felt in that told\n",
      ">>> text.similar('over')\n",
      "in on to of and for with from at by that into as up out down through\n",
      "about all is\n",
      ">>> text.similar('the')\n",
      "a his this their its her an that our any all one these my in your no\n",
      "some other and\n",
      "\n",
      "\n",
      "\n",
      "Observe that searching for woman finds nouns;\n",
      "searching for bought mostly finds verbs;\n",
      "searching for over generally finds prepositions;\n",
      "searching for the finds several determiners.\n",
      "A tagger can correctly identify the tags on these words\n",
      "in the context of a sentence, e.g. The woman bought over $150,000\n",
      "worth of clothes.\n",
      "A tagger can also model our knowledge of unknown words,\n",
      "e.g. we can guess that scrobbling is probably a verb,\n",
      "with the root scrobble,\n",
      "and likely to occur in contexts like he was scrobbling.\n",
      "\n",
      "\n",
      "2   Tagged Corpora\n",
      "\n",
      "2.1   Representing Tagged Tokens\n",
      "By convention in NLTK, a tagged token is represented using a\n",
      "tuple consisting of the token and the tag.\n",
      "We can create one of these special tuples from the standard string\n",
      "representation of a tagged token, using the function str2tuple():\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tagged_token = nltk.tag.str2tuple('fly/NN')\n",
      ">>> tagged_token\n",
      "('fly', 'NN')\n",
      ">>> tagged_token[0]\n",
      "'fly'\n",
      ">>> tagged_token[1]\n",
      "'NN'\n",
      "\n",
      "\n",
      "\n",
      "We can construct a list of tagged tokens directly from a string.  The first\n",
      "step is to tokenize the string\n",
      "to access the individual word/tag strings, and then to convert\n",
      "each of these into a tuple (using str2tuple()).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent = '''\n",
      "... The/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN\n",
      "... other/AP topics/NNS ,/, AMONG/IN them/PPO the/AT Atlanta/NP and/CC\n",
      "... Fulton/NP-tl County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS\n",
      "... said/VBD ``/`` ARE/BER well/QL operated/VBN and/CC follow/VB generally/RB\n",
      "... accepted/VBN practices/NNS which/WDT inure/VB to/IN the/AT best/JJT\n",
      "... interest/NN of/IN both/ABX governments/NNS ''/'' ./.\n",
      "... '''\n",
      ">>> [nltk.tag.str2tuple(t) for t in sent.split()]\n",
      "[('The', 'AT'), ('grand', 'JJ'), ('jury', 'NN'), ('commented', 'VBD'),\n",
      "('on', 'IN'), ('a', 'AT'), ('number', 'NN'), ... ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.2   Reading Tagged Corpora\n",
      "Several of the corpora included with NLTK have been tagged for\n",
      "their part-of-speech. Here's an example of what you might see if you\n",
      "opened a file from the Brown Corpus with a text editor:\n",
      "\n",
      "The/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl\n",
      "said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$\n",
      "recent/jj primary/nn election/nn produced/vbd / no/at\n",
      "evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd\n",
      "place/nn ./.\n",
      "Other corpora use a variety of formats for storing part-of-speech tags.\n",
      "NLTK's corpus readers provide a uniform interface so that you\n",
      "don't have to be concerned with the different file formats.\n",
      "In contrast with the file fragment shown above,\n",
      "the corpus reader for the Brown Corpus represents the data as shown below.\n",
      "Note that part-of-speech tags have been converted to uppercase, since this has\n",
      "become standard practice since the Brown Corpus was published.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nltk.corpus.brown.tagged_words()\n",
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n",
      ">>> nltk.corpus.brown.tagged_words(tagset='universal')\n",
      "[('The', 'DET'), ('Fulton', 'NOUN'), ...]\n",
      "\n",
      "\n",
      "\n",
      "Whenever a corpus contains tagged text, the NLTK corpus interface\n",
      "will have a tagged_words() method.\n",
      "Here are some more examples, again using the output format\n",
      "illustrated for the Brown Corpus:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(nltk.corpus.nps_chat.tagged_words())\n",
      "[('now', 'RB'), ('im', 'PRP'), ('left', 'VBD'), ...]\n",
      ">>> nltk.corpus.conll2000.tagged_words()\n",
      "[('Confidence', 'NN'), ('in', 'IN'), ('the', 'DT'), ...]\n",
      ">>> nltk.corpus.treebank.tagged_words()\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]\n",
      "\n",
      "\n",
      "\n",
      "Not all corpora employ the same set of tags; see the\n",
      "tagset help functionality and the readme() methods\n",
      "mentioned above for documentation.\n",
      "Initially we want to avoid the complications of these tagsets,\n",
      "so we use a built-in mapping to the \"Universal Tagset\":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nltk.corpus.brown.tagged_words(tagset='universal')\n",
      "[('The', 'DET'), ('Fulton', 'NOUN'), ...]\n",
      ">>> nltk.corpus.treebank.tagged_words(tagset='universal')\n",
      "[('Pierre', 'NOUN'), ('Vinken', 'NOUN'), (',', '.'), ...]\n",
      "\n",
      "\n",
      "\n",
      "Tagged corpora for several other languages are distributed with NLTK,\n",
      "including Chinese, Hindi, Portuguese, Spanish, Dutch and Catalan.\n",
      "These usually contain non-ASCII text,\n",
      "and Python always displays this in hexadecimal when printing a larger structure\n",
      "such as a list.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nltk.corpus.sinica_treebank.tagged_words()\n",
      "[('ä', 'Neu'), ('åæ', 'Nad'), ('åç', 'Nba'), ...]\n",
      ">>> nltk.corpus.indian.tagged_words()\n",
      "[('মহিষের', 'NN'), ('সন্তান', 'NN'), (':', 'SYM'), ...]\n",
      ">>> nltk.corpus.mac_morpho.tagged_words()\n",
      "[('Jersei', 'N'), ('atinge', 'V'), ('m\\xe9dia', 'N'), ...]\n",
      ">>> nltk.corpus.conll2002.tagged_words()\n",
      "[('Sao', 'NC'), ('Paulo', 'VMI'), ('(', 'Fpa'), ...]\n",
      ">>> nltk.corpus.cess_cat.tagged_words()\n",
      "[('El', 'da0ms0'), ('Tribunal_Suprem', 'np0000o'), ...]\n",
      "\n",
      "\n",
      "\n",
      "If your environment is set up correctly, with appropriate editors and fonts,\n",
      "you should be able to display individual strings in a human-readable way.\n",
      "For example, 2.1 shows data accessed using\n",
      "nltk.corpus.indian.\n",
      "\n",
      "\n",
      "Figure 2.1: POS-Tagged Data from Four Indian Languages: Bangla, Hindi, Marathi, and Telugu\n",
      "\n",
      "\n",
      "If the corpus is also segmented into sentences, it will have\n",
      "a tagged_sents() method that divides up the tagged words into\n",
      "sentences rather than presenting them as one big list.\n",
      "This will be useful when we come to developing automatic taggers,\n",
      "as they are trained and tested on lists of sentences, not words.\n",
      "\n",
      "\n",
      "2.3   A Universal Part-of-Speech Tagset\n",
      "Tagged corpora use many different conventions for tagging words.\n",
      "To help us get started, we will be looking at a simplified tagset\n",
      "(shown in 2.1).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tag\n",
      "Meaning\n",
      "English Examples\n",
      "\n",
      "\n",
      "\n",
      "ADJ\n",
      "adjective\n",
      "new, good, high, special, big, local\n",
      "\n",
      "ADP\n",
      "adposition\n",
      "on, of, at, with, by, into, under\n",
      "\n",
      "ADV\n",
      "adverb\n",
      "really, already, still, early, now\n",
      "\n",
      "CONJ\n",
      "conjunction\n",
      "and, or, but, if, while, although\n",
      "\n",
      "DET\n",
      "determiner, article\n",
      "the, a, some, most, every, no, which\n",
      "\n",
      "NOUN\n",
      "noun\n",
      "year, home, costs, time, Africa\n",
      "\n",
      "NUM\n",
      "numeral\n",
      "twenty-four, fourth, 1991, 14:24\n",
      "\n",
      "PRT\n",
      "particle\n",
      "at, on, out, over per, that, up, with\n",
      "\n",
      "PRON\n",
      "pronoun\n",
      "he, their, her, its, my, I, us\n",
      "\n",
      "VERB\n",
      "verb\n",
      "is, say, told, given, playing, would\n",
      "\n",
      ".\n",
      "punctuation marks\n",
      ". , ; !\n",
      "\n",
      "X\n",
      "other\n",
      "ersatz, esprit, dunno, gr8, univeristy\n",
      "\n",
      "\n",
      "Table 2.1: Universal Part-of-Speech Tagset\n",
      "\n",
      "\n",
      "Let's see which of these tags are the most common in the news\n",
      "category of the Brown corpus:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import brown\n",
      ">>> brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')\n",
      ">>> tag_fd = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)\n",
      ">>> tag_fd.most_common()\n",
      "[('NOUN', 30640), ('VERB', 14399), ('ADP', 12355), ('.', 11928), ('DET', 11389),\n",
      " ('ADJ', 6706), ('ADV', 3349), ('CONJ', 2717), ('PRON', 2535), ('PRT', 2264),\n",
      " ('NUM', 2166), ('X', 106)]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Plot the above frequency distribution using tag_fd.plot(cumulative=True).\n",
      "What percentage of words are tagged using the first five tags of the above list?\n",
      "\n",
      "We can use these tags to do powerful searches using a graphical\n",
      "POS-concordance tool nltk.app.concordance().  Use it\n",
      "to search for any combination of words and POS tags, e.g.\n",
      "N N N N, hit/VD, hit/VN, or the ADJ man.\n",
      "\n",
      "\n",
      "\n",
      "2.4   Nouns\n",
      "Nouns generally refer to people, places, things, or concepts, e.g.:\n",
      "woman, Scotland, book, intelligence.  Nouns can appear after\n",
      "determiners and adjectives, and can be the subject or object of the\n",
      "verb, as shown in 2.2.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Word\n",
      "After a determiner\n",
      "Subject of the verb\n",
      "\n",
      "\n",
      "\n",
      "woman\n",
      "the woman who I saw yesterday ...\n",
      "the woman sat down\n",
      "\n",
      "Scotland\n",
      "the Scotland I remember as a child ...\n",
      "Scotland has five million people\n",
      "\n",
      "book\n",
      "the book I bought yesterday ...\n",
      "this book recounts the colonization of Australia\n",
      "\n",
      "intelligence\n",
      "the intelligence displayed by the child ...\n",
      "Mary's intelligence impressed her teachers\n",
      "\n",
      "\n",
      "Table 2.2: Syntactic Patterns involving some Nouns\n",
      "\n",
      "\n",
      "The simplified noun tags are N for common nouns like book,\n",
      "and NP for proper nouns like Scotland.\n",
      "Let's inspect some tagged text to see what parts of speech occur before a noun,\n",
      "with the most frequent ones first. To begin with, we construct a list\n",
      "of bigrams whose members are themselves word-tag pairs such as\n",
      "(('The', 'DET'), ('Fulton', 'NP')) and  (('Fulton', 'NP'), ('County', 'N')).\n",
      "Then we construct a FreqDist from the tag parts of the bigrams.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> word_tag_pairs = nltk.bigrams(brown_news_tagged)\n",
      ">>> noun_preceders = [a[1] for (a, b) in word_tag_pairs if b[1] == 'NOUN']\n",
      ">>> fdist = nltk.FreqDist(noun_preceders)\n",
      ">>> [tag for (tag, _) in fdist.most_common()]\n",
      "['NOUN', 'DET', 'ADJ', 'ADP', '.', 'VERB', 'CONJ', 'NUM', 'ADV', 'PRT', 'PRON', 'X']\n",
      "\n",
      "\n",
      "\n",
      "This confirms our assertion that nouns occur after determiners and\n",
      "adjectives, including numeral adjectives (tagged as NUM).\n",
      "\n",
      "\n",
      "\n",
      "2.5   Verbs\n",
      "Verbs are words that describe events and actions, e.g. fall,\n",
      "eat in 2.3.\n",
      "In the context of a sentence, verbs typically express a relation\n",
      "involving the referents of one or more noun phrases.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Word\n",
      "Simple\n",
      "With modifiers and adjuncts (italicized)\n",
      "\n",
      "\n",
      "\n",
      "fall\n",
      "Rome fell\n",
      "Dot com stocks suddenly fell like a stone\n",
      "\n",
      "eat\n",
      "Mice eat cheese\n",
      "John ate the pizza with gusto\n",
      "\n",
      "\n",
      "Table 2.3: Syntactic Patterns involving some Verbs\n",
      "\n",
      "\n",
      "What are the most common verbs in news text?  Let's sort all the verbs by frequency:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> wsj = nltk.corpus.treebank.tagged_words(tagset='universal')\n",
      ">>> word_tag_fd = nltk.FreqDist(wsj)\n",
      ">>> [wt[0] for (wt, _) in word_tag_fd.most_common() if wt[1] == 'VERB']\n",
      "['is', 'said', 'are', 'was', 'be', 'has', 'have', 'will', 'says', 'would',\n",
      " 'were', 'had', 'been', 'could', \"'s\", 'can', 'do', 'say', 'make', 'may',\n",
      " 'did', 'rose', 'made', 'does', 'expected', 'buy', 'take', 'get', 'might',\n",
      " 'sell', 'added', 'sold', 'help', 'including', 'should', 'reported', ...]\n",
      "\n",
      "\n",
      "\n",
      "Note that the items being counted in the frequency distribution are word-tag pairs.\n",
      "Since words and tags are paired, we can treat the word as a condition and the tag\n",
      "as an event, and initialize a conditional frequency distribution with a list of\n",
      "condition-event pairs.  This lets us see a frequency-ordered list of tags given a word:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> cfd1 = nltk.ConditionalFreqDist(wsj)\n",
      ">>> cfd1['yield'].most_common()\n",
      "[('VERB', 28), ('NOUN', 20)]\n",
      ">>> cfd1['cut'].most_common()\n",
      "[('VERB', 25), ('NOUN', 3)]\n",
      "\n",
      "\n",
      "\n",
      "We can reverse the order of the pairs, so that the tags are the conditions, and the\n",
      "words are the events.  Now we can see likely words for a given tag. We\n",
      "will do this for the WSJ tagset rather than the universal tagset:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> wsj = nltk.corpus.treebank.tagged_words()\n",
      ">>> cfd2 = nltk.ConditionalFreqDist((tag, word) for (word, tag) in wsj)\n",
      ">>> list(cfd2['VBN'])\n",
      "['been', 'expected', 'made', 'compared', 'based', 'priced', 'used', 'sold',\n",
      "'named', 'designed', 'held', 'fined', 'taken', 'paid', 'traded', 'said', ...]\n",
      "\n",
      "\n",
      "\n",
      "To clarify the distinction between VBD (past tense) and VBN\n",
      "(past participle), let's find words which can be both VBD and\n",
      "VBN, and see some surrounding text:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> [w for w in cfd1.conditions() if 'VBD' in cfd1[w] and 'VBN' in cfd1[w]]\n",
      "['Asked', 'accelerated', 'accepted', 'accused', 'acquired', 'added', 'adopted', ...]\n",
      ">>> idx1 = wsj.index(('kicked', 'VBD'))\n",
      ">>> wsj[idx1-4:idx1+1]\n",
      "[('While', 'IN'), ('program', 'NN'), ('trades', 'NNS'), ('swiftly', 'RB'),\n",
      " ('kicked', 'VBD')]\n",
      ">>> idx2 = wsj.index(('kicked', 'VBN'))\n",
      ">>> wsj[idx2-4:idx2+1]\n",
      "[('head', 'NN'), ('of', 'IN'), ('state', 'NN'), ('has', 'VBZ'), ('kicked', 'VBN')]\n",
      "\n",
      "\n",
      "\n",
      "In this case, we see that the past participle of kicked is preceded by a form of\n",
      "the auxiliary verb have. Is this generally true?\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Given the list of past participles produced by\n",
      "list(cfd2['VN']), try to collect a list of all the word-tag\n",
      "pairs that immediately precede items in that list.\n",
      "\n",
      "\n",
      "\n",
      "2.6   Adjectives and Adverbs\n",
      "Two other important word classes are adjectives and adverbs.\n",
      "Adjectives describe nouns, and can be used as modifiers\n",
      "(e.g. large in the large pizza), or in predicates (e.g. the\n",
      "pizza is large).  English adjectives can have internal structure\n",
      "(e.g.  fall+ing in the falling\n",
      "stocks).  Adverbs modify verbs to specify the time, manner, place or\n",
      "direction of the event described by the verb (e.g. quickly in\n",
      "the stocks fell quickly).  Adverbs may also modify adjectives\n",
      "(e.g. really in Mary's teacher was really nice).\n",
      "English has several categories of closed class words in addition to\n",
      "prepositions, such as articles (also often called determiners)\n",
      "(e.g., the, a), modals (e.g., should,\n",
      "may), and personal pronouns (e.g., she, they).\n",
      "Each dictionary and grammar classifies these words differently.\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "If you are uncertain about some of these parts of speech, study them using\n",
      "nltk.app.concordance(), or watch some of the Schoolhouse Rock!\n",
      "grammar videos available at YouTube, or consult the Further Reading\n",
      "section at the end of this chapter.\n",
      "\n",
      "\n",
      "\n",
      "2.7   Unsimplified Tags\n",
      "Let's find the most frequent nouns of each noun part-of-speech type.\n",
      "The program in 2.2 finds all tags starting with NN,\n",
      "and provides a few example words for each one.  You will see that\n",
      "there are many variants of NN; the most important contain $\n",
      "for possessive nouns, S for plural nouns (since plural nouns\n",
      "typically end in s) and P for proper nouns.  In addition,\n",
      "most of the tags have suffix modifiers: -NC for citations, -HL\n",
      "for words in headlines and -TL for titles (a feature of Brown tags).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def findtags(tag_prefix, tagged_text):\n",
      "    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\n",
      "                                  if tag.startswith(tag_prefix))\n",
      "    return dict((tag, cfd[tag].most_common(5)) for tag in cfd.conditions())\n",
      "\n",
      ">>> tagdict = findtags('NN', nltk.corpus.brown.tagged_words(categories='news'))\n",
      ">>> for tag in sorted(tagdict):\n",
      "...     print(tag, tagdict[tag])\n",
      "...\n",
      "NN [('year', 137), ('time', 97), ('state', 88), ('week', 85), ('man', 72)]\n",
      "NN$ [(\"year's\", 13), (\"world's\", 8), (\"state's\", 7), (\"nation's\", 6), (\"company's\", 6)]\n",
      "NN$-HL [(\"Golf's\", 1), (\"Navy's\", 1)]\n",
      "NN$-TL [(\"President's\", 11), (\"Army's\", 3), (\"Gallery's\", 3), (\"University's\", 3), (\"League's\", 3)]\n",
      "NN-HL [('sp.', 2), ('problem', 2), ('Question', 2), ('business', 2), ('Salary', 2)]\n",
      "NN-NC [('eva', 1), ('aya', 1), ('ova', 1)]\n",
      "NN-TL [('President', 88), ('House', 68), ('State', 59), ('University', 42), ('City', 41)]\n",
      "NN-TL-HL [('Fort', 2), ('Dr.', 1), ('Oak', 1), ('Street', 1), ('Basin', 1)]\n",
      "NNS [('years', 101), ('members', 69), ('people', 52), ('sales', 51), ('men', 46)]\n",
      "NNS$ [(\"children's\", 7), (\"women's\", 5), (\"janitors'\", 3), (\"men's\", 3), (\"taxpayers'\", 2)]\n",
      "NNS$-HL [(\"Dealers'\", 1), (\"Idols'\", 1)]\n",
      "NNS$-TL [(\"Women's\", 4), (\"States'\", 3), (\"Giants'\", 2), (\"Bros.'\", 1), (\"Writers'\", 1)]\n",
      "NNS-HL [('comments', 1), ('Offenses', 1), ('Sacrifices', 1), ('funds', 1), ('Results', 1)]\n",
      "NNS-TL [('States', 38), ('Nations', 11), ('Masters', 10), ('Rules', 9), ('Communists', 9)]\n",
      "NNS-TL-HL [('Nations', 1)]\n",
      "\n",
      "\n",
      "Example 2.2 (code_findtags.py): Figure 2.2: Program to Find the Most Frequent Noun Tags\n",
      "\n",
      "When we come to constructing part-of-speech taggers later in this chapter,\n",
      "we will use the unsimplified tags.\n",
      "\n",
      "\n",
      "2.8   Exploring Tagged Corpora\n",
      "Let's briefly return to the kinds of exploration of corpora we saw in previous chapters,\n",
      "this time exploiting POS tags.\n",
      "Suppose we're studying the word often and want to see how it is used\n",
      "in text.  We could ask to see the words that follow often\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> brown_learned_text = brown.words(categories='learned')\n",
      ">>> sorted(set(b for (a, b) in nltk.bigrams(brown_learned_text) if a == 'often'))\n",
      "[',', '.', 'accomplished', 'analytically', 'appear', 'apt', 'associated', 'assuming',\n",
      "'became', 'become', 'been', 'began', 'call', 'called', 'carefully', 'chose', ...]\n",
      "\n",
      "\n",
      "\n",
      "However, it's probably more instructive to use the tagged_words()\n",
      "method to look at the part-of-speech tag of the following words:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> brown_lrnd_tagged = brown.tagged_words(categories='learned', tagset='universal')\n",
      ">>> tags = [b[1] for (a, b) in nltk.bigrams(brown_lrnd_tagged) if a[0] == 'often']\n",
      ">>> fd = nltk.FreqDist(tags)\n",
      ">>> fd.tabulate()\n",
      " PRT  ADV  ADP    . VERB  ADJ\n",
      "   2    8    7    4   37    6\n",
      "\n",
      "\n",
      "\n",
      "Notice that the most high-frequency parts of speech following often are verbs.\n",
      "Nouns never appear in this position (in this particular corpus).\n",
      "Next, let's look at some larger context, and find words involving\n",
      "particular sequences of tags and words (in this case \"<Verb> to <Verb>\").\n",
      "In code-three-word-phrase we consider each three-word window in the sentence ,\n",
      "and check if they meet our criterion .  If the tags\n",
      "match, we print the corresponding words .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "from nltk.corpus import brown\n",
      "def process(sentence):\n",
      "    for (w1,t1), (w2,t2), (w3,t3) in nltk.trigrams(sentence): \n",
      "        if (t1.startswith('V') and t2 == 'TO' and t3.startswith('V')): \n",
      "            print(w1, w2, w3) \n",
      "\n",
      ">>> for tagged_sent in brown.tagged_sents():\n",
      "...     process(tagged_sent)\n",
      "...\n",
      "combined to achieve\n",
      "continue to place\n",
      "serve to protect\n",
      "wanted to wait\n",
      "allowed to place\n",
      "expected to become\n",
      "...\n",
      "\n",
      "\n",
      "Example 2.3 (code_three_word_phrase.py): Figure 2.3: Searching for Three-Word Phrases Using POS Tags\n",
      "\n",
      "Finally, let's look for words that are highly ambiguous as to their part of speech tag.\n",
      "Understanding why such words are tagged as they are in each context can help us clarify\n",
      "the distinctions between the tags.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')\n",
      ">>> data = nltk.ConditionalFreqDist((word.lower(), tag)\n",
      "...                                 for (word, tag) in brown_news_tagged)\n",
      ">>> for word in sorted(data.conditions()):\n",
      "...     if len(data[word]) > 3:\n",
      "...         tags = [tag for (tag, _) in data[word].most_common()]\n",
      "...         print(word, ' '.join(tags))\n",
      "...\n",
      "best ADJ ADV NP V\n",
      "better ADJ ADV V DET\n",
      "close ADV ADJ V N\n",
      "cut V N VN VD\n",
      "even ADV DET ADJ V\n",
      "grant NP N V -\n",
      "hit V VD VN N\n",
      "lay ADJ V NP VD\n",
      "left VD ADJ N VN\n",
      "like CNJ V ADJ P -\n",
      "near P ADV ADJ DET\n",
      "open ADJ V N ADV\n",
      "past N ADJ DET P\n",
      "present ADJ ADV V N\n",
      "read V VN VD NP\n",
      "right ADJ N DET ADV\n",
      "second NUM ADV DET N\n",
      "set VN V VD N -\n",
      "that CNJ V WH DET\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Open the POS concordance tool nltk.app.concordance() and load the complete\n",
      "Brown Corpus (simplified tagset).  Now pick some of the above words and see how the tag\n",
      "of the word correlates with the context of the word.\n",
      "E.g. search for near to see all forms mixed together, near/ADJ to see it used\n",
      "as an adjective, near N to see just those cases where a noun follows, and so forth.\n",
      "For a larger set of examples, modify the supplied code so that it lists words having\n",
      "three distinct tags.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3   Mapping Words to Properties Using Python Dictionaries\n",
      "As we have seen, a tagged word of the form (word, tag) is\n",
      "an association between a word and a part-of-speech tag.\n",
      "Once we start doing part-of-speech tagging, we will be creating\n",
      "programs that assign a tag to a word, the tag which is most\n",
      "likely in a given context.  We can think of this process as\n",
      "mapping from words to tags.  The most natural way to\n",
      "store mappings in Python uses the so-called dictionary data type\n",
      "(also known as an associative array or hash array\n",
      "in other programming languages).\n",
      "In this section we look at dictionaries and see how they can\n",
      "represent a variety of language information, including\n",
      "parts of speech.\n",
      "\n",
      "3.1   Indexing Lists vs Dictionaries\n",
      "A text, as we have seen, is treated in Python as a list of words.\n",
      "An important property of lists is that we can \"look up\" a particular\n",
      "item by giving its index, e.g. text1[100].  Notice how we specify\n",
      "a number, and get back a word.  We can think of a list as a simple\n",
      "kind of table, as shown in 3.1.\n",
      "\n",
      "\n",
      "Figure 3.1: List Look-up: we access the contents of a Python list with the help of an integer index.\n",
      "\n",
      "Contrast this situation with frequency distributions (3),\n",
      "where we specify a word, and get back a number, e.g. fdist['monstrous'], which\n",
      "tells us the number of times a given word has occurred in a text.  Look-up using words is\n",
      "familiar to anyone who has used a dictionary.  Some more examples are shown in\n",
      "3.2.\n",
      "\n",
      "\n",
      "Figure 3.2: Dictionary Look-up: we access the entry of a dictionary using a key\n",
      "such as someone's name, a web domain, or an English word;\n",
      "other names for dictionary are map, hashmap, hash, and associative array.\n",
      "\n",
      "In the case of a phonebook, we look up an entry using a name,\n",
      "and get back a number.  When we type a domain name in a web browser,\n",
      "the computer looks this up to get back an IP address.  A word\n",
      "frequency table allows us to look up a word and find its frequency in\n",
      "a text collection.  In all these cases, we are mapping from names to\n",
      "numbers, rather than the other way around as with a list.\n",
      "In general, we would like to be able to map between\n",
      "arbitrary types of information.  3.1 lists a variety\n",
      "of linguistic objects, along with what they map.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Linguistic Object\n",
      "Maps From\n",
      "Maps To\n",
      "\n",
      "\n",
      "\n",
      "Document Index\n",
      "Word\n",
      "List of pages (where word is found)\n",
      "\n",
      "Thesaurus\n",
      "Word sense\n",
      "List of synonyms\n",
      "\n",
      "Dictionary\n",
      "Headword\n",
      "Entry (part-of-speech, sense definitions, etymology)\n",
      "\n",
      "Comparative Wordlist\n",
      "Gloss term\n",
      "Cognates (list of words, one per language)\n",
      "\n",
      "Morph Analyzer\n",
      "Surface form\n",
      "Morphological analysis (list of component morphemes)\n",
      "\n",
      "\n",
      "Table 3.1: Linguistic Objects as Mappings from Keys to Values\n",
      "\n",
      "\n",
      "Most often, we are mapping from a \"word\" to some structured object.\n",
      "For example, a document index maps from a word (which we can represent\n",
      "as a string), to a list of pages (represented as a list of integers).\n",
      "In this section, we will see how to represent such mappings in Python.\n",
      "\n",
      "\n",
      "3.2   Dictionaries in Python\n",
      "Python provides a dictionary data type that can be used for\n",
      "mapping between arbitrary types.  It is like a conventional dictionary,\n",
      "in that it gives you an efficient way to look things up.  However,\n",
      "as we see from 3.1, it has a much wider range of uses.\n",
      "To illustrate, we define pos to be an empty dictionary and then add four\n",
      "entries to it, specifying the part-of-speech of some words.  We add\n",
      "entries to a dictionary using the familiar square bracket notation:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> pos = {}\n",
      ">>> pos\n",
      "{}\n",
      ">>> pos['colorless'] = 'ADJ' \n",
      ">>> pos\n",
      "{'colorless': 'ADJ'}\n",
      ">>> pos['ideas'] = 'N'\n",
      ">>> pos['sleep'] = 'V'\n",
      ">>> pos['furiously'] = 'ADV'\n",
      ">>> pos \n",
      "{'furiously': 'ADV', 'ideas': 'N', 'colorless': 'ADJ', 'sleep': 'V'}\n",
      "\n",
      "\n",
      "\n",
      "So, for example,  says that\n",
      "the part-of-speech of colorless is adjective, or more\n",
      "specifically, that the key 'colorless'\n",
      "is assigned the value 'ADJ'  in dictionary pos.\n",
      "When we inspect the value of pos  we see\n",
      "a set of key-value pairs.  Once we have populated the dictionary\n",
      "in this way, we can employ the keys to retrieve values:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> pos['ideas']\n",
      "'N'\n",
      ">>> pos['colorless']\n",
      "'ADJ'\n",
      "\n",
      "\n",
      "\n",
      "Of course, we might accidentally use a key that hasn't been assigned a value.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> pos['green']\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in ?\n",
      "KeyError: 'green'\n",
      "\n",
      "\n",
      "\n",
      "This raises an important question.  Unlike lists and strings, where we\n",
      "can use len() to work out which integers will be legal indexes,\n",
      "how do we work out the legal keys for a dictionary?  If the dictionary\n",
      "is not too big, we can simply inspect its contents by evaluating the\n",
      "variable pos.  As we saw above (line ), this gives\n",
      "us the key-value pairs.  Notice that they are not in the same order they\n",
      "were originally entered; this is because dictionaries are not sequences\n",
      "but mappings (cf. 3.2), and the keys are not inherently\n",
      "ordered.\n",
      "Alternatively, to just find the keys, we can convert the\n",
      "dictionary to a list  — or use\n",
      "the dictionary in a context where a list is expected,\n",
      "as the parameter of sorted() ,\n",
      "or in a for loop .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> list(pos) \n",
      "['ideas', 'furiously', 'colorless', 'sleep']\n",
      ">>> sorted(pos) \n",
      "['colorless', 'furiously', 'ideas', 'sleep']\n",
      ">>> [w for w in pos if w.endswith('s')] \n",
      "['colorless', 'ideas']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "When you type list(pos) you might see a different order\n",
      "to the one shown above.  If you want to see the keys in order, just sort them.\n",
      "\n",
      "As well as iterating over all keys\n",
      "in the dictionary with a for loop, we can use the for loop\n",
      "as we did for printing lists:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> for word in sorted(pos):\n",
      "...     print(word + \":\", pos[word])\n",
      "...\n",
      "colorless: ADJ\n",
      "furiously: ADV\n",
      "sleep: V\n",
      "ideas: N\n",
      "\n",
      "\n",
      "\n",
      "Finally, the dictionary methods keys(), values() and\n",
      "items() allow us to access the keys, values, and key-value pairs as separate lists.\n",
      "We can even sort tuples , which orders them according to their first element\n",
      "(and if the first elements are the same, it uses their second elements).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> list(pos.keys())\n",
      "['colorless', 'furiously', 'sleep', 'ideas']\n",
      ">>> list(pos.values())\n",
      "['ADJ', 'ADV', 'V', 'N']\n",
      ">>> list(pos.items())\n",
      "[('colorless', 'ADJ'), ('furiously', 'ADV'), ('sleep', 'V'), ('ideas', 'N')]\n",
      ">>> for key, val in sorted(pos.items()): \n",
      "...     print(key + \":\", val)\n",
      "...\n",
      "colorless: ADJ\n",
      "furiously: ADV\n",
      "ideas: N\n",
      "sleep: V\n",
      "\n",
      "\n",
      "\n",
      "We want to be sure that when we look something up in a dictionary, we\n",
      "only get one value for each key. Now\n",
      "suppose we try to use a dictionary to store the fact that the\n",
      "word sleep can be used as both a verb and a noun:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> pos['sleep'] = 'V'\n",
      ">>> pos['sleep']\n",
      "'V'\n",
      ">>> pos['sleep'] = 'N'\n",
      ">>> pos['sleep']\n",
      "'N'\n",
      "\n",
      "\n",
      "\n",
      "Initially, pos['sleep'] is given the value 'V'. But this is\n",
      "immediately overwritten with the new value 'N'.\n",
      "In other words, there can only be one entry in the dictionary for 'sleep'.\n",
      "However, there is a way of storing multiple values in\n",
      "that entry: we use a list value,\n",
      "e.g. pos['sleep'] = ['N', 'V'].  In fact, this is what we\n",
      "saw in 4 for the CMU Pronouncing Dictionary,\n",
      "which stores multiple pronunciations for a single word.\n",
      "\n",
      "\n",
      "3.3   Defining Dictionaries\n",
      "We can use the same key-value pair format to create a dictionary.  There's\n",
      "a couple of ways to do this, and we will normally use the first:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> pos = {'colorless': 'ADJ', 'ideas': 'N', 'sleep': 'V', 'furiously': 'ADV'}\n",
      ">>> pos = dict(colorless='ADJ', ideas='N', sleep='V', furiously='ADV')\n",
      "\n",
      "\n",
      "\n",
      "Note that dictionary keys must be immutable types, such as strings and tuples.\n",
      "If we try to define a dictionary using a mutable key, we get a TypeError:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> pos = {['ideas', 'blogs', 'adventures']: 'N'}\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "TypeError: list objects are unhashable\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.4   Default Dictionaries\n",
      "If we try to access a key that is not in a dictionary, we get an error.\n",
      "However, its often useful if a dictionary can automatically create\n",
      "an entry for this new key and give it a default value, such as zero or\n",
      "the empty list.  For this reason, a special kind of dictionary\n",
      "called a defaultdict is available.\n",
      "In order to use it, we have to supply a parameter which can be used to\n",
      "create the default value, e.g. int, float, str, list, dict,\n",
      "tuple.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from collections import defaultdict\n",
      ">>> frequency = defaultdict(int)\n",
      ">>> frequency['colorless'] = 4\n",
      ">>> frequency['ideas']\n",
      "0\n",
      ">>> pos = defaultdict(list)\n",
      ">>> pos['sleep'] = ['NOUN', 'VERB']\n",
      ">>> pos['ideas']\n",
      "[]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "These default values are actually functions that convert other\n",
      "objects to the specified type (e.g. int(\"2\"), list(\"2\")).\n",
      "When they are called with no parameter — int(), list()\n",
      "— they return 0 and [] respectively.\n",
      "\n",
      "The above examples specified the default value of a dictionary entry to\n",
      "be the default value of a particular data type.  However, we can specify\n",
      "any default value we like, simply by providing the name of a function\n",
      "that can be called with no arguments to create the required value.\n",
      "Let's return to our part-of-speech example, and create a dictionary\n",
      "whose default value for any entry is 'N' .\n",
      "When we access a non-existent entry ,\n",
      "it is automatically added to the dictionary .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> pos = defaultdict(lambda: 'NOUN') \n",
      ">>> pos['colorless'] = 'ADJ'\n",
      ">>> pos['blog'] \n",
      "'NOUN'\n",
      ">>> list(pos.items())\n",
      "[('blog', 'NOUN'), ('colorless', 'ADJ')] # [_automatically-added]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "The above example used a lambda expression, introduced in\n",
      "4.4.  This lambda expression specifies no\n",
      "parameters, so we call it using parentheses with no arguments.\n",
      "Thus, the definitions of f and g below are equivalent:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> f = lambda: 'NOUN'\n",
      ">>> f()\n",
      "'NOUN'\n",
      ">>> def g():\n",
      "...     return 'NOUN'\n",
      ">>> g()\n",
      "'NOUN'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Let's see how default dictionaries could be used in a more substantial\n",
      "language processing task.\n",
      "Many language processing tasks — including tagging — struggle to correctly process\n",
      "the hapaxes of a text.  They can perform better with a fixed vocabulary and a\n",
      "guarantee that no new words will appear.  We can preprocess a text to replace\n",
      "low-frequency words with a special \"out of vocabulary\" token UNK, with\n",
      "the help of a default dictionary.  (Can you work out how to do this without\n",
      "reading on?)\n",
      "We need to create a default dictionary that maps each word to its replacement.\n",
      "The most frequent n words will be mapped to themselves.\n",
      "Everything else will be mapped to UNK.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> alice = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
      ">>> vocab = nltk.FreqDist(alice)\n",
      ">>> v1000 = [word for (word, _) in vocab.most_common(1000)]\n",
      ">>> mapping = defaultdict(lambda: 'UNK')\n",
      ">>> for v in v1000:\n",
      "...     mapping[v] = v\n",
      "...\n",
      ">>> alice2 = [mapping[v] for v in alice]\n",
      ">>> alice2[:100]\n",
      "['UNK', 'Alice', \"'\", 's', 'UNK', 'in', 'UNK', 'by', 'UNK', 'UNK', 'UNK',\n",
      "'UNK', 'CHAPTER', 'I', '.', 'UNK', 'the', 'Rabbit', '-', 'UNK', 'Alice',\n",
      "'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by',\n",
      "'her', 'sister', 'on', 'the', 'UNK', ',', 'and', 'of', 'having', 'nothing',\n",
      "'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'UNK', 'into', 'the',\n",
      "'book', 'her', 'sister', 'was', 'UNK', ',', 'but', 'it', 'had', 'no',\n",
      "'pictures', 'or', 'UNK', 'in', 'it', ',', \"'\", 'and', 'what', 'is', 'the',\n",
      "'use', 'of', 'a', 'book', \",'\", 'thought', 'Alice', \"'\", 'without',\n",
      "'pictures', 'or', 'conversation', \"?'\" ...]\n",
      ">>> len(set(alice2))\n",
      "1001\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.5   Incrementally Updating a Dictionary\n",
      "We can employ dictionaries to count occurrences, emulating the method\n",
      "for tallying words shown in fig-tally.\n",
      "We begin by initializing an empty defaultdict, then process each\n",
      "part-of-speech tag in the text.  If the tag hasn't been seen before,\n",
      "it will have a zero count by default.  Each time we encounter a tag,\n",
      "we increment its count using the += operator.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from collections import defaultdict\n",
      ">>> counts = defaultdict(int)\n",
      ">>> from nltk.corpus import brown\n",
      ">>> for (word, tag) in brown.tagged_words(categories='news', tagset='universal'):\n",
      "...     counts[tag] += 1\n",
      "...\n",
      ">>> counts['NOUN']\n",
      "30640\n",
      ">>> sorted(counts)\n",
      "['ADJ', 'PRT', 'ADV', 'X', 'CONJ', 'PRON', 'VERB', '.', 'NUM', 'NOUN', 'ADP', 'DET']\n",
      "\n",
      ">>> from operator import itemgetter\n",
      ">>> sorted(counts.items(), key=itemgetter(1), reverse=True)\n",
      "[('NOUN', 30640), ('VERB', 14399), ('ADP', 12355), ('.', 11928), ...]\n",
      ">>> [t for t, c in sorted(counts.items(), key=itemgetter(1), reverse=True)]\n",
      "['NOUN', 'VERB', 'ADP', '.', 'DET', 'ADJ', 'ADV', 'CONJ', 'PRON', 'PRT', 'NUM', 'X']\n",
      "\n",
      "\n",
      "Example 3.3 (code_dictionary.py): Figure 3.3: Incrementally Updating a Dictionary, and Sorting by Value\n",
      "\n",
      "The listing in 3.3 illustrates an important idiom for\n",
      "sorting a dictionary by its values, to show words in decreasing\n",
      "order of frequency.  The first parameter of sorted() is the items\n",
      "to sort, a list of tuples consisting of a POS tag and a frequency.\n",
      "The second parameter specifies the sort key using a function itemgetter().\n",
      "In general, itemgetter(n) returns a function that can be called on\n",
      "some other sequence object to obtain the nth element, e.g.:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> pair = ('NP', 8336)\n",
      ">>> pair[1]\n",
      "8336\n",
      ">>> itemgetter(1)(pair)\n",
      "8336\n",
      "\n",
      "\n",
      "\n",
      "The last parameter of sorted() specifies that the items should be returned\n",
      "in reverse order, i.e. decreasing values of frequency.\n",
      "There's a second useful programming idiom at the beginning of\n",
      "3.3, where we initialize a defaultdict and then use a\n",
      "for loop to update its values. Here's a schematic version:\n",
      "\n",
      ">>> my_dictionary = defaultdict(function to create default value)\n",
      ">>> for item in sequence:\n",
      "...      my_dictionary[item_key] is updated with information about item\n",
      "\n",
      "Here's another instance of this pattern, where we index words according to their last two letters:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> last_letters = defaultdict(list)\n",
      ">>> words = nltk.corpus.words.words('en')\n",
      ">>> for word in words:\n",
      "...     key = word[-2:]\n",
      "...     last_letters[key].append(word)\n",
      "...\n",
      ">>> last_letters['ly']\n",
      "['abactinally', 'abandonedly', 'abasedly', 'abashedly', 'abashlessly', 'abbreviately',\n",
      "'abdominally', 'abhorrently', 'abidingly', 'abiogenetically', 'abiologically', ...]\n",
      ">>> last_letters['zy']\n",
      "['blazy', 'bleezy', 'blowzy', 'boozy', 'breezy', 'bronzy', 'buzzy', 'Chazy', ...]\n",
      "\n",
      "\n",
      "\n",
      "The following example uses the same pattern to create an anagram dictionary.\n",
      "(You might experiment with the third line to get an idea of why this program works.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> anagrams = defaultdict(list)\n",
      ">>> for word in words:\n",
      "...     key = ''.join(sorted(word))\n",
      "...     anagrams[key].append(word)\n",
      "...\n",
      ">>> anagrams['aeilnrt']\n",
      "['entrail', 'latrine', 'ratline', 'reliant', 'retinal', 'trenail']\n",
      "\n",
      "\n",
      "\n",
      "Since accumulating words like this is such a common task,\n",
      "NLTK provides a more convenient way of creating a defaultdict(list),\n",
      "in the form of nltk.Index().\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> anagrams = nltk.Index((''.join(sorted(w)), w) for w in words)\n",
      ">>> anagrams['aeilnrt']\n",
      "['entrail', 'latrine', 'ratline', 'reliant', 'retinal', 'trenail']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "nltk.Index is a defaultdict(list) with extra support for\n",
      "initialization.  Similarly,\n",
      "nltk.FreqDist is essentially a defaultdict(int) with extra\n",
      "support for initialization (along with sorting and plotting methods).\n",
      "\n",
      "\n",
      "\n",
      "3.6   Complex Keys and Values\n",
      "We can use default dictionaries with complex keys and values.\n",
      "Let's study the range of possible tags for a word, given the\n",
      "word itself, and the tag of the previous word.  We will see\n",
      "how this information can be used by a POS tagger.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> pos = defaultdict(lambda: defaultdict(int))\n",
      ">>> brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')\n",
      ">>> for ((w1, t1), (w2, t2)) in nltk.bigrams(brown_news_tagged): \n",
      "...     pos[(t1, w2)][t2] += 1 \n",
      "...\n",
      ">>> pos[('DET', 'right')] \n",
      "defaultdict(<class 'int'>, {'ADJ': 11, 'NOUN': 5})\n",
      "\n",
      "\n",
      "\n",
      "This example uses a dictionary whose default value for an entry\n",
      "is a dictionary (whose default value is int(), i.e. zero).\n",
      "Notice how we iterated over the bigrams of the tagged\n",
      "corpus, processing a pair of word-tag pairs for each iteration .\n",
      "Each time through the loop we updated our pos dictionary's\n",
      "entry for (t1, w2), a tag and its following word .\n",
      "When we look up an item in pos we must specify a compound key ,\n",
      "and we get back a dictionary object.\n",
      "A POS tagger could use such information to decide that the\n",
      "word right, when preceded by a determiner, should be tagged as ADJ.\n",
      "\n",
      "\n",
      "3.7   Inverting a Dictionary\n",
      "Dictionaries support efficient lookup, so long as you want to get the value for\n",
      "any key.  If d is a dictionary and k is a key, we type d[k] and\n",
      "immediately obtain the value.  Finding a key given a value is slower and more\n",
      "cumbersome:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> counts = defaultdict(int)\n",
      ">>> for word in nltk.corpus.gutenberg.words('milton-paradise.txt'):\n",
      "...     counts[word] += 1\n",
      "...\n",
      ">>> [key for (key, value) in counts.items() if value == 32]\n",
      "['brought', 'Him', 'virtue', 'Against', 'There', 'thine', 'King', 'mortal',\n",
      "'every', 'been']\n",
      "\n",
      "\n",
      "\n",
      "If we expect to do this kind of \"reverse lookup\" often, it helps to construct\n",
      "a dictionary that maps values to keys.  In the case that no two keys have\n",
      "the same value, this is an easy thing to do.  We just get all the key-value\n",
      "pairs in the dictionary, and create a new dictionary of value-key\n",
      "pairs. The next example also illustrates another way of initializing a\n",
      "dictionary pos with key-value pairs.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> pos = {'colorless': 'ADJ', 'ideas': 'N', 'sleep': 'V', 'furiously': 'ADV'}\n",
      ">>> pos2 = dict((value, key) for (key, value) in pos.items())\n",
      ">>> pos2['N']\n",
      "'ideas'\n",
      "\n",
      "\n",
      "\n",
      "Let's first make our part-of-speech dictionary a bit more realistic\n",
      "and add some more words to pos using the dictionary update() method, to\n",
      "create the situation where multiple keys have the same value. Then the\n",
      "technique just shown for reverse lookup will no longer work (why\n",
      "not?).  Instead, we have to use append() to accumulate the words\n",
      "for each part-of-speech, as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> pos.update({'cats': 'N', 'scratch': 'V', 'peacefully': 'ADV', 'old': 'ADJ'})\n",
      ">>> pos2 = defaultdict(list)\n",
      ">>> for key, value in pos.items():\n",
      "...     pos2[value].append(key)\n",
      "...\n",
      ">>> pos2['ADV']\n",
      "['peacefully', 'furiously']\n",
      "\n",
      "\n",
      "\n",
      "Now we have inverted the pos dictionary, and can look up any part-of-speech and find\n",
      "all words having that part-of-speech.  We can do the same thing even\n",
      "more simply using NLTK's support for indexing as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> pos2 = nltk.Index((value, key) for (key, value) in pos.items())\n",
      ">>> pos2['ADV']\n",
      "['peacefully', 'furiously']\n",
      "\n",
      "\n",
      "\n",
      "A summary of Python's dictionary methods is given in 3.2.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Example\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "d = {}\n",
      "create an empty dictionary and assign it to d\n",
      "\n",
      "d[key] = value\n",
      "assign a value to a given dictionary key\n",
      "\n",
      "d.keys()\n",
      "the list of keys of the dictionary\n",
      "\n",
      "list(d)\n",
      "the list of keys of the dictionary\n",
      "\n",
      "sorted(d)\n",
      "the keys of the dictionary, sorted\n",
      "\n",
      "key in d\n",
      "test whether a particular key is in the dictionary\n",
      "\n",
      "for key in d\n",
      "iterate over the keys of the dictionary\n",
      "\n",
      "d.values()\n",
      "the list of values in the dictionary\n",
      "\n",
      "dict([(k1,v1), (k2,v2), ...])\n",
      "create a dictionary from a list of key-value pairs\n",
      "\n",
      "d1.update(d2)\n",
      "add all items from d2 to d1\n",
      "\n",
      "defaultdict(int)\n",
      "a dictionary whose default value is zero\n",
      "\n",
      "\n",
      "Table 3.2: Python's Dictionary Methods: A summary of commonly-used methods and idioms\n",
      "involving dictionaries.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4   Automatic Tagging\n",
      "In the rest of this chapter we will explore various ways to automatically\n",
      "add part-of-speech tags to text.  We will see that the tag of a word depends\n",
      "on the word and its context within a sentence.  For this reason, we will\n",
      "be working with data at the level of (tagged) sentences rather than words.\n",
      "We'll begin by loading the data we will be using.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import brown\n",
      ">>> brown_tagged_sents = brown.tagged_sents(categories='news')\n",
      ">>> brown_sents = brown.sents(categories='news')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4.1   The Default Tagger\n",
      "The simplest possible tagger assigns the same tag to each token.  This\n",
      "may seem to be a rather banal step, but it establishes an important\n",
      "baseline for tagger performance.  In order to get the best result, we\n",
      "tag each word with the most likely tag.  Let's find out which tag is\n",
      "most likely (now using the unsimplified tagset):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tags = [tag for (word, tag) in brown.tagged_words(categories='news')]\n",
      ">>> nltk.FreqDist(tags).max()\n",
      "'NN'\n",
      "\n",
      "\n",
      "\n",
      "Now we can create a tagger that tags everything as NN.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> raw = 'I do not like green eggs and ham, I do not like them Sam I am!'\n",
      ">>> tokens = nltk.word_tokenize(raw)\n",
      ">>> default_tagger = nltk.DefaultTagger('NN')\n",
      ">>> default_tagger.tag(tokens)\n",
      "[('I', 'NN'), ('do', 'NN'), ('not', 'NN'), ('like', 'NN'), ('green', 'NN'),\n",
      "('eggs', 'NN'), ('and', 'NN'), ('ham', 'NN'), (',', 'NN'), ('I', 'NN'),\n",
      "('do', 'NN'), ('not', 'NN'), ('like', 'NN'), ('them', 'NN'), ('Sam', 'NN'),\n",
      "('I', 'NN'), ('am', 'NN'), ('!', 'NN')]\n",
      "\n",
      "\n",
      "\n",
      "Unsurprisingly, this method performs rather poorly.\n",
      "On a typical corpus, it will tag only about an eighth of the tokens correctly,\n",
      "as we see below:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> default_tagger.evaluate(brown_tagged_sents)\n",
      "0.13089484257215028\n",
      "\n",
      "\n",
      "\n",
      "Default taggers assign their tag to every single word, even words that\n",
      "have never been encountered before.  As it happens, once we have processed\n",
      "several thousand words of English text, most new words will be nouns.\n",
      "As we will see, this means that default taggers can help to improve the\n",
      "robustness of a language processing system.  We will return to them\n",
      "shortly.\n",
      "\n",
      "\n",
      "4.2   The Regular Expression Tagger\n",
      "The regular expression tagger assigns tags to tokens on the basis of\n",
      "matching patterns.  For instance, we might guess that any word ending\n",
      "in ed is the past participle of a verb, and any word ending with\n",
      "'s is a possessive noun.  We can express these as a list of\n",
      "regular expressions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> patterns = [\n",
      "...     (r'.*ing$', 'VBG'),                # gerunds\n",
      "...     (r'.*ed$', 'VBD'),                 # simple past\n",
      "...     (r'.*es$', 'VBZ'),                 # 3rd singular present\n",
      "...     (r'.*ould$', 'MD'),                # modals\n",
      "...     (r'.*\\'s$', 'NN$'),                # possessive nouns\n",
      "...     (r'.*s$', 'NNS'),                  # plural nouns\n",
      "...     (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
      "...     (r'.*', 'NN')                      # nouns (default)\n",
      "... ]\n",
      "\n",
      "\n",
      "\n",
      "Note that these are processed in order, and the first one that matches is applied.\n",
      "Now we can set up a tagger and use it to tag a sentence.  Now its right about a fifth\n",
      "of the time.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> regexp_tagger = nltk.RegexpTagger(patterns)\n",
      ">>> regexp_tagger.tag(brown_sents[3])\n",
      "[('``', 'NN'), ('Only', 'NN'), ('a', 'NN'), ('relative', 'NN'), ('handful', 'NN'),\n",
      "('of', 'NN'), ('such', 'NN'), ('reports', 'NNS'), ('was', 'NNS'), ('received', 'VBD'),\n",
      "(\"''\", 'NN'), (',', 'NN'), ('the', 'NN'), ('jury', 'NN'), ('said', 'NN'), (',', 'NN'),\n",
      "('``', 'NN'), ('considering', 'VBG'), ('the', 'NN'), ('widespread', 'NN'), ...]\n",
      ">>> regexp_tagger.evaluate(brown_tagged_sents)\n",
      "0.20326391789486245\n",
      "\n",
      "\n",
      "\n",
      "The final regular expression «.*» is a catch-all that tags everything as a noun.\n",
      "This is equivalent to the default tagger (only much less efficient).\n",
      "Instead of re-specifying this as part of the regular expression tagger,\n",
      "is there a way to combine this tagger with the default tagger?  We\n",
      "will see how to do this shortly.\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "See if you can come up with patterns to improve the performance of the above\n",
      "regular expression tagger.  (Note that 1\n",
      "describes a way to partially automate such work.)\n",
      "\n",
      "\n",
      "\n",
      "4.3   The Lookup Tagger\n",
      "A lot of high-frequency words do not have the NN tag.\n",
      "Let's find the hundred most frequent words and store their most likely tag.\n",
      "We can then use this information as the model for a \"lookup tagger\"\n",
      "(an NLTK UnigramTagger):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fd = nltk.FreqDist(brown.words(categories='news'))\n",
      ">>> cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
      ">>> most_freq_words = fd.most_common(100)\n",
      ">>> likely_tags = dict((word, cfd[word].max()) for (word, _) in most_freq_words)\n",
      ">>> baseline_tagger = nltk.UnigramTagger(model=likely_tags)\n",
      ">>> baseline_tagger.evaluate(brown_tagged_sents)\n",
      "0.45578495136941344\n",
      "\n",
      "\n",
      "\n",
      "It should come as no surprise by now that simply\n",
      "knowing the tags for the 100 most frequent words enables us to tag a large fraction of\n",
      "tokens correctly (nearly half in fact).\n",
      "Let's see what it does on some untagged input text:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent = brown.sents(categories='news')[3]\n",
      ">>> baseline_tagger.tag(sent)\n",
      "[('``', '``'), ('Only', None), ('a', 'AT'), ('relative', None),\n",
      "('handful', None), ('of', 'IN'), ('such', None), ('reports', None),\n",
      "('was', 'BEDZ'), ('received', None), (\"''\", \"''\"), (',', ','),\n",
      "('the', 'AT'), ('jury', None), ('said', 'VBD'), (',', ','),\n",
      "('``', '``'), ('considering', None), ('the', 'AT'), ('widespread', None),\n",
      "('interest', None), ('in', 'IN'), ('the', 'AT'), ('election', None),\n",
      "(',', ','), ('the', 'AT'), ('number', None), ('of', 'IN'),\n",
      "('voters', None), ('and', 'CC'), ('the', 'AT'), ('size', None),\n",
      "('of', 'IN'), ('this', 'DT'), ('city', None), (\"''\", \"''\"), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "Many words have been assigned a tag of None,\n",
      "because they were not among the 100 most frequent words.\n",
      "In these cases we would like to assign the default tag of NN.\n",
      "In other words, we want to use the lookup table first,\n",
      "and if it is unable to assign a tag, then use the default tagger,\n",
      "a process known as backoff (5).\n",
      "We do this by specifying one tagger as a parameter to the other,\n",
      "as shown below.  Now the lookup tagger will only store word-tag pairs\n",
      "for words other than nouns, and whenever it cannot assign a tag to a\n",
      "word it will invoke the default tagger.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> baseline_tagger = nltk.UnigramTagger(model=likely_tags,\n",
      "...                                      backoff=nltk.DefaultTagger('NN'))\n",
      "\n",
      "\n",
      "\n",
      "Let's put all this together and write a program to create and\n",
      "evaluate lookup taggers having a range of sizes, in 4.1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def performance(cfd, wordlist):\n",
      "    lt = dict((word, cfd[word].max()) for word in wordlist)\n",
      "    baseline_tagger = nltk.UnigramTagger(model=lt, backoff=nltk.DefaultTagger('NN'))\n",
      "    return baseline_tagger.evaluate(brown.tagged_sents(categories='news'))\n",
      "\n",
      "def display():\n",
      "    import pylab\n",
      "    word_freqs = nltk.FreqDist(brown.words(categories='news')).most_common()\n",
      "    words_by_freq = [w for (w, _) in word_freqs]\n",
      "    cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
      "    sizes = 2 ** pylab.arange(15)\n",
      "    perfs = [performance(cfd, words_by_freq[:size]) for size in sizes]\n",
      "    pylab.plot(sizes, perfs, '-bo')\n",
      "    pylab.title('Lookup Tagger Performance with Varying Model Size')\n",
      "    pylab.xlabel('Model Size')\n",
      "    pylab.ylabel('Performance')\n",
      "    pylab.show()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> display()                                  \n",
      "\n",
      "\n",
      "Example 4.1 (code_baseline_tagger.py): Figure 4.1: Lookup Tagger Performance with Varying Model Size\n",
      "\n",
      "\n",
      "\n",
      "Figure 4.2: Lookup Tagger\n",
      "\n",
      "Observe that performance initially increases rapidly as the model size grows, eventually\n",
      "reaching a plateau, when large increases in model size yield little improvement\n",
      "in performance.  (This example used the pylab plotting package, discussed\n",
      "in 4.8.)\n",
      "\n",
      "\n",
      "4.4   Evaluation\n",
      "In the above examples, you will have noticed an emphasis on\n",
      "accuracy scores.  In fact, evaluating the performance of\n",
      "such tools is a central theme in NLP.  Recall the processing\n",
      "pipeline in fig-sds; any errors in the output of one\n",
      "module are greatly multiplied in the downstream modules.\n",
      "We evaluate the performance of a tagger relative to the tags\n",
      "a human expert would assign.  Since we don't usually have access\n",
      "to an expert and impartial human judge, we make do instead with\n",
      "gold standard test data. This is a corpus which has been manually\n",
      "annotated and which is accepted as a standard against which the\n",
      "guesses of an automatic system are assessed. The tagger is regarded as\n",
      "being correct if the tag it guesses for a given word is the same as\n",
      "the gold standard tag.\n",
      "Of course, the humans who designed and carried out the\n",
      "original gold standard annotation were only human. Further\n",
      "analysis might show mistakes in the gold standard, or may\n",
      "eventually lead to a revised tagset and more elaborate guidelines.\n",
      "Nevertheless, the gold standard is by definition \"correct\"\n",
      "as far as the evaluation of an automatic tagger is concerned.\n",
      "\n",
      "Note\n",
      "Developing an annotated corpus is a major undertaking.\n",
      "Apart from the data, it generates sophisticated tools,\n",
      "documentation, and practices for ensuring high quality\n",
      "annotation.  The tagsets and other coding schemes inevitably\n",
      "depend on some theoretical position that is not shared by\n",
      "all, however corpus creators often go to great lengths to\n",
      "make their work as theory-neutral as possible in order to\n",
      "maximize the usefulness of their work.  We will discuss\n",
      "the challenges of creating a corpus in 11..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5   N-Gram Tagging\n",
      "\n",
      "5.1   Unigram Tagging\n",
      "Unigram taggers are based on a simple statistical algorithm:\n",
      "for each token, assign the tag that is most likely for\n",
      "that particular token. For example, it will assign the tag JJ to any\n",
      "occurrence of the word frequent, since frequent is used as an\n",
      "adjective (e.g. a frequent word) more often than it is used as a\n",
      "verb (e.g. I frequent this cafe).\n",
      "A unigram tagger behaves just like a lookup tagger (4),\n",
      "except there is a more convenient technique for setting it up,\n",
      "called training.  In the following code sample,\n",
      "we train a unigram tagger, use it to tag a sentence, then evaluate:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import brown\n",
      ">>> brown_tagged_sents = brown.tagged_sents(categories='news')\n",
      ">>> brown_sents = brown.sents(categories='news')\n",
      ">>> unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
      ">>> unigram_tagger.tag(brown_sents[2007])\n",
      "[('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'),\n",
      "('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'), ('type', 'NN'),\n",
      "(',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'), ('ground', 'NN'),\n",
      "('floor', 'NN'), ('so', 'QL'), ('that', 'CS'), ('entrance', 'NN'), ('is', 'BEZ'),\n",
      "('direct', 'JJ'), ('.', '.')]\n",
      ">>> unigram_tagger.evaluate(brown_tagged_sents)\n",
      "0.9349006503968017\n",
      "\n",
      "\n",
      "\n",
      "We train a UnigramTagger by specifying tagged sentence data as\n",
      "a parameter when we initialize the tagger.  The training process involves\n",
      "inspecting the tag of each word and storing the most likely tag for any word\n",
      "in a dictionary, stored inside the tagger.\n",
      "\n",
      "\n",
      "5.2   Separating the Training and Testing Data\n",
      "Now that we are training a tagger on some data, we must be careful not to test it on the\n",
      "same data, as we did in the above example.  A tagger that simply memorized its training data\n",
      "and made no attempt to construct a general model would get a perfect score, but would also\n",
      "be useless for tagging new text.  Instead, we should split the data, training on 90% and\n",
      "testing on the remaining 10%:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> size = int(len(brown_tagged_sents) * 0.9)\n",
      ">>> size\n",
      "4160\n",
      ">>> train_sents = brown_tagged_sents[:size]\n",
      ">>> test_sents = brown_tagged_sents[size:]\n",
      ">>> unigram_tagger = nltk.UnigramTagger(train_sents)\n",
      ">>> unigram_tagger.evaluate(test_sents)\n",
      "0.811721...\n",
      "\n",
      "\n",
      "\n",
      "Although the score is worse, we now have a better picture of the usefulness of\n",
      "this tagger, i.e. its performance on previously unseen text.\n",
      "\n",
      "\n",
      "5.3   General N-Gram Tagging\n",
      "When we perform a language processing task based on unigrams, we are using\n",
      "one item of context.  In the case of tagging, we only consider the current\n",
      "token, in isolation from any larger context.  Given such a model, the best\n",
      "we can do is tag each word with its a priori most likely tag.\n",
      "This means we would tag a word such as wind with the same tag,\n",
      "regardless of whether it appears in the context the wind or\n",
      "to wind.\n",
      "An n-gram tagger is a generalization of a unigram tagger whose context is\n",
      "the current word together with the part-of-speech tags of the\n",
      "n-1 preceding tokens, as shown in 5.1. The tag to be\n",
      "chosen, tn, is circled, and the context is shaded\n",
      "in grey. In the example of an n-gram tagger shown in 5.1,\n",
      "we have n=3; that is, we consider the tags of the two preceding words in addition\n",
      "to the current word.  An n-gram tagger\n",
      "picks the tag that is most likely in the given context.\n",
      "\n",
      "\n",
      "Figure 5.1: Tagger Context\n",
      "\n",
      "\n",
      "Note\n",
      "A 1-gram tagger is another term for a unigram tagger: i.e.,\n",
      "the context used to tag a token is just the text of the token itself.\n",
      "2-gram taggers are also called bigram taggers, and 3-gram taggers\n",
      "are called trigram taggers.\n",
      "\n",
      "The NgramTagger class uses a tagged training corpus to determine which\n",
      "part-of-speech tag is most likely for each context.  Here we see\n",
      "a special case of an n-gram tagger, namely a bigram tagger.\n",
      "First we train it, then use it to tag untagged sentences:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> bigram_tagger = nltk.BigramTagger(train_sents)\n",
      ">>> bigram_tagger.tag(brown_sents[2007])\n",
      "[('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'),\n",
      "('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'),\n",
      "('type', 'NN'), (',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'),\n",
      "('ground', 'NN'), ('floor', 'NN'), ('so', 'CS'), ('that', 'CS'),\n",
      "('entrance', 'NN'), ('is', 'BEZ'), ('direct', 'JJ'), ('.', '.')]\n",
      ">>> unseen_sent = brown_sents[4203]\n",
      ">>> bigram_tagger.tag(unseen_sent)\n",
      "[('The', 'AT'), ('population', 'NN'), ('of', 'IN'), ('the', 'AT'), ('Congo', 'NP'),\n",
      "('is', 'BEZ'), ('13.5', None), ('million', None), (',', None), ('divided', None),\n",
      "('into', None), ('at', None), ('least', None), ('seven', None), ('major', None),\n",
      "('``', None), ('culture', None), ('clusters', None), (\"''\", None), ('and', None),\n",
      "('innumerable', None), ('tribes', None), ('speaking', None), ('400', None),\n",
      "('separate', None), ('dialects', None), ('.', None)]\n",
      "\n",
      "\n",
      "\n",
      "Notice that the bigram tagger manages to tag every word in a sentence it saw during\n",
      "training, but does badly on an unseen sentence.  As soon as it encounters a new word\n",
      "(i.e., 13.5), it is unable to assign a tag.  It cannot tag the following word\n",
      "(i.e., million) even if it was seen during training, simply because it never\n",
      "saw it during training with a None tag on the previous word.  Consequently, the\n",
      "tagger fails to tag the rest of the sentence.  Its overall accuracy score is very low:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> bigram_tagger.evaluate(test_sents)\n",
      "0.102063...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "As n gets larger, the specificity of the contexts increases,\n",
      "as does the chance that the data we wish to tag contains contexts that\n",
      "were not present in the training data. This is known as the sparse\n",
      "data problem, and is quite pervasive in NLP. As a consequence, there is a\n",
      "trade-off between the accuracy and the coverage of our results (and\n",
      "this is related to the precision/recall trade-off in information\n",
      "retrieval).\n",
      "\n",
      "Caution!\n",
      "n-gram taggers should not consider context that crosses a\n",
      "sentence boundary.  Accordingly, NLTK taggers are designed to work\n",
      "with lists of sentences, where each sentence is a list of words.  At\n",
      "the start of a sentence, tn-1 and preceding\n",
      "tags are set to None.\n",
      "\n",
      "\n",
      "\n",
      "5.4   Combining Taggers\n",
      "One way to address the trade-off between accuracy and coverage is to\n",
      "use the more accurate algorithms when we can, but to fall back on\n",
      "algorithms with wider coverage when necessary. For example, we could\n",
      "combine the results of a bigram tagger, a unigram tagger, and\n",
      "a default tagger, as follows:\n",
      "\n",
      "Try tagging the token with the bigram tagger.\n",
      "If the bigram tagger is unable to find a tag for the token, try\n",
      "the unigram tagger.\n",
      "If the unigram tagger is also unable to find a tag, use a default tagger.\n",
      "\n",
      "Most NLTK taggers permit a backoff-tagger to be specified.\n",
      "The backoff-tagger may itself have a backoff tagger:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> t0 = nltk.DefaultTagger('NN')\n",
      ">>> t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
      ">>> t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
      ">>> t2.evaluate(test_sents)\n",
      "0.844513...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Extend the above example by defining a TrigramTagger called\n",
      "t3, which backs off to t2.\n",
      "\n",
      "Note that we specify the backoff tagger when the tagger is\n",
      "initialized so that training can take advantage of the backoff tagger.\n",
      "Thus, if the bigram tagger would assign the same tag\n",
      "as its unigram backoff tagger in a certain context,\n",
      "the bigram tagger discards the training instance.\n",
      "This keeps the bigram tagger model as small as possible.  We can\n",
      "further specify that a tagger needs to see more than one instance of a\n",
      "context in order to retain it, e.g. nltk.BigramTagger(sents, cutoff=2, backoff=t1)\n",
      "will discard contexts that have only been seen once or twice.\n",
      "\n",
      "\n",
      "5.5   Tagging Unknown Words\n",
      "Our approach to tagging unknown words still uses backoff to a regular-expression tagger\n",
      "or a default tagger.  These are unable to make use of context.  Thus, if our tagger\n",
      "encountered the word blog, not seen during training, it would assign it the same tag,\n",
      "regardless of whether this word appeared in the context the blog or to blog.\n",
      "How can we do better with these unknown words, or out-of-vocabulary items?\n",
      "A useful method to tag unknown words based on context is to limit the vocabulary\n",
      "of a tagger to the most frequent n words, and to replace every other word\n",
      "with a special word UNK using the method shown in 3.\n",
      "During training, a unigram tagger will probably learn that UNK is usually a noun.\n",
      "However, the n-gram taggers will detect contexts in which it has some other tag.\n",
      "For example, if the preceding word is to (tagged TO), then UNK\n",
      "will probably be tagged as a verb.\n",
      "\n",
      "\n",
      "\n",
      "5.6   Storing Taggers\n",
      "Training a tagger on a large corpus may take a significant time.  Instead of training a tagger\n",
      "every time we need one, it is convenient to save a trained tagger in a file for later re-use.\n",
      "Let's save our tagger t2 to a file t2.pkl.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from pickle import dump\n",
      ">>> output = open('t2.pkl', 'wb')\n",
      ">>> dump(t2, output, -1)\n",
      ">>> output.close()\n",
      "\n",
      "\n",
      "\n",
      "Now, in a separate Python process, we can load our saved tagger.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from pickle import load\n",
      ">>> input = open('t2.pkl', 'rb')\n",
      ">>> tagger = load(input)\n",
      ">>> input.close()\n",
      "\n",
      "\n",
      "\n",
      "Now let's check that it can be used for tagging.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = \"\"\"The board's action shows what free enterprise\n",
      "...     is up against in our complex maze of regulatory laws .\"\"\"\n",
      ">>> tokens = text.split()\n",
      ">>> tagger.tag(tokens)\n",
      "[('The', 'AT'), (\"board's\", 'NN$'), ('action', 'NN'), ('shows', 'NNS'),\n",
      "('what', 'WDT'), ('free', 'JJ'), ('enterprise', 'NN'), ('is', 'BEZ'),\n",
      "('up', 'RP'), ('against', 'IN'), ('in', 'IN'), ('our', 'PP$'), ('complex', 'JJ'),\n",
      "('maze', 'NN'), ('of', 'IN'), ('regulatory', 'NN'), ('laws', 'NNS'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5.7   Performance Limitations\n",
      "What is the upper limit to the performance of an n-gram tagger?\n",
      "Consider the case of a trigram tagger.  How many cases of part-of-speech ambiguity does it\n",
      "encounter?  We can determine the answer to this question empirically:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> cfd = nltk.ConditionalFreqDist(\n",
      "...            ((x[1], y[1], z[0]), z[1])\n",
      "...            for sent in brown_tagged_sents\n",
      "...            for x, y, z in nltk.trigrams(sent))\n",
      ">>> ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) > 1]\n",
      ">>> sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()\n",
      "0.049297702068029296\n",
      "\n",
      "\n",
      "\n",
      "Thus, one out of twenty trigrams is ambiguous [EXAMPLES].  Given the\n",
      "current word and the previous two tags, in 5% of cases there is more than one tag\n",
      "that could be legitimately assigned to the current word according to\n",
      "the training data.  Assuming we always pick the most likely tag in\n",
      "such ambiguous contexts, we can derive a lower bound on\n",
      "the performance of a trigram tagger.\n",
      "\n",
      "Another way to investigate the performance of a tagger is to study\n",
      "its mistakes.  Some tags may be harder than others to assign, and\n",
      "it might be possible to treat them specially by pre- or post-processing\n",
      "the data.  A convenient way to look at tagging errors is the\n",
      "confusion matrix.  It charts expected tags (the gold standard)\n",
      "against actual tags generated by a tagger:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> test_tags = [tag for sent in brown.sents(categories='editorial')\n",
      "...                  for (word, tag) in t2.tag(sent)]\n",
      ">>> gold_tags = [tag for (word, tag) in brown.tagged_words(categories='editorial')]\n",
      ">>> print(nltk.ConfusionMatrix(gold_tags, test_tags))           \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Based on such analysis we may decide to modify the tagset.  Perhaps\n",
      "a distinction between tags that is difficult to make can be dropped,\n",
      "since it is not important in the context of some larger processing task.\n",
      "Another way to analyze the performance bound on a tagger comes from\n",
      "the less than 100% agreement between human annotators.  [MORE]\n",
      "In general, observe that the tagging process collapses distinctions:\n",
      "e.g. lexical identity is usually lost when all personal pronouns are\n",
      "tagged PRP.  At the same time, the tagging process introduces\n",
      "new distinctions and removes ambiguities: e.g. deal tagged as VB or NN.\n",
      "This characteristic of collapsing certain distinctions and introducing new\n",
      "distinctions is an important feature of tagging which\n",
      "facilitates classification and prediction.\n",
      "When we introduce finer distinctions in a tagset, an n-gram tagger gets\n",
      "more detailed information about the left-context when it is deciding\n",
      "what tag to assign to a particular word.\n",
      "However, the tagger simultaneously has to do more work to classify the\n",
      "current token, simply because there are more tags to choose from.\n",
      "Conversely, with fewer distinctions (as with the simplified tagset),\n",
      "the tagger has less information about context, and it has a smaller\n",
      "range of choices in classifying the current token.\n",
      "We have seen that ambiguity in the training data leads to an upper limit\n",
      "in tagger performance.  Sometimes more context will resolve the\n",
      "ambiguity.  In other cases however, as noted by (Church, Young, & Bloothooft, 1996), the\n",
      "ambiguity can only be resolved with reference to syntax, or to world\n",
      "knowledge.  Despite these imperfections, part-of-speech tagging has\n",
      "played a central role in the rise of statistical approaches to natural\n",
      "language processing.  In the early 1990s, the surprising accuracy of\n",
      "statistical taggers was a striking demonstration that it was possible\n",
      "to solve one small part of the language understanding problem, namely\n",
      "part-of-speech disambiguation, without reference to deeper sources of\n",
      "linguistic knowledge.  Can this idea be pushed further?  In 7.,\n",
      "we shall see that it can.\n",
      "\n",
      "\n",
      "\n",
      "6   Transformation-Based Tagging\n",
      "A potential issue with n-gram taggers is the size of their n-gram\n",
      "table (or language model).  If tagging is to be employed in a variety\n",
      "of language technologies deployed on mobile computing devices, it is\n",
      "important to strike a balance between model size and tagger\n",
      "performance.  An n-gram tagger with backoff may store trigram and\n",
      "bigram tables, large sparse arrays which may have hundreds of millions\n",
      "of entries.\n",
      "A second issue concerns context.  The only information an n-gram\n",
      "tagger considers from prior context is tags, even though words\n",
      "themselves might be a useful source of information.  It is simply\n",
      "impractical for n-gram models to be conditioned on the identities of\n",
      "words in the context.  In this section we examine Brill tagging,\n",
      "an inductive tagging method which performs very well using models\n",
      "that are only a tiny fraction of the size of n-gram taggers.\n",
      "Brill tagging is a kind of transformation-based learning, named\n",
      "after its inventor.  The\n",
      "general idea is very simple: guess the tag of each word, then go back\n",
      "and fix the mistakes.  In this way, a Brill tagger successively\n",
      "transforms a bad tagging of a text into a better one.  As with n-gram\n",
      "tagging, this is a supervised learning method, since we need\n",
      "annotated training data to figure out whether the tagger's guess is a\n",
      "mistake or not.  However, unlike n-gram tagging, it does\n",
      "not count observations but compiles a list of transformational\n",
      "correction rules.\n",
      "The process of Brill tagging is usually explained by analogy with\n",
      "painting.  Suppose we were painting a tree, with all its details of\n",
      "boughs, branches, twigs and leaves, against a uniform sky-blue\n",
      "background.  Instead of painting the tree first then trying to paint\n",
      "blue in the gaps, it is simpler to paint the whole canvas blue, then\n",
      "\"correct\" the tree section by over-painting the blue background.  In\n",
      "the same fashion we might paint the trunk a uniform brown before going\n",
      "back to over-paint further details with even finer brushes.  Brill\n",
      "tagging uses the same idea: begin with broad brush strokes then fix up\n",
      "the details, with successively finer changes.  Let's look at an\n",
      "example involving the following sentence:\n",
      "\n",
      "  (1)The President said he will ask Congress to increase grants to states\n",
      "for vocational rehabilitation\n",
      "We will examine the operation of two rules:\n",
      "(a) Replace NN with VB when the previous word is TO;\n",
      "(b) Replace TO with IN when the next tag is NNS.\n",
      "6.1\n",
      "illustrates this process, first tagging with the unigram tagger, then\n",
      "applying the rules to fix the errors.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Phrase\n",
      "to\n",
      "increase\n",
      "grants\n",
      "to\n",
      "states\n",
      "for\n",
      "vocational\n",
      "rehabilitation\n",
      "\n",
      "Unigram\n",
      "TO\n",
      "NN\n",
      "NNS\n",
      "TO\n",
      "NNS\n",
      "IN\n",
      "JJ\n",
      "NN\n",
      "\n",
      "Rule 1\n",
      " \n",
      "VB\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "Rule 2\n",
      " \n",
      " \n",
      " \n",
      "IN\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "Output\n",
      "TO\n",
      "VB\n",
      "NNS\n",
      "IN\n",
      "NNS\n",
      "IN\n",
      "JJ\n",
      "NN\n",
      "\n",
      "Gold\n",
      "TO\n",
      "VB\n",
      "NNS\n",
      "IN\n",
      "NNS\n",
      "IN\n",
      "JJ\n",
      "NN\n",
      "\n",
      "\n",
      "Table 6.1: Steps in Brill Tagging\n",
      "\n",
      "\n",
      "In this table we see two rules.  All such rules are generated from a\n",
      "template of the following form: \"replace T1 with\n",
      "T2 in the context C\".  Typical contexts are the\n",
      "identity or the tag of the preceding or following word, or the\n",
      "appearance of a specific tag within 2-3 words of the current word.  During\n",
      "its training phase, the tagger guesses values for T1,\n",
      "T2 and C, to create thousands of candidate rules.\n",
      "Each rule is scored according to its net benefit: the\n",
      "number of incorrect tags that it corrects, less the number of correct\n",
      "tags it incorrectly modifies.\n",
      "\n",
      "Brill taggers have another interesting property: the rules are\n",
      "linguistically interpretable.  Compare this with the n-gram taggers,\n",
      "which employ a potentially massive table of n-grams.  We cannot learn\n",
      "much from direct inspection of such a table, in comparison to the\n",
      "rules learned by the Brill tagger.\n",
      "6.1 demonstrates NLTK's Brill tagger.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.tbl import demo as brill_demo\n",
      ">>> brill_demo.demo()\n",
      "Training Brill tagger on 80 sentences...\n",
      "Finding initial useful rules...\n",
      "    Found 6555 useful rules.\n",
      "\n",
      "           B      |\n",
      "   S   F   r   O  |        Score = Fixed - Broken\n",
      "   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n",
      "   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n",
      "   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n",
      "   e   d   n   r  |  e\n",
      "------------------+-------------------------------------------------------\n",
      "  12  13   1   4  | NN -> VB if the tag of the preceding word is 'TO'\n",
      "   8   9   1  23  | NN -> VBD if the tag of the following word is 'DT'\n",
      "   8   8   0   9  | NN -> VBD if the tag of the preceding word is 'NNS'\n",
      "   6   9   3  16  | NN -> NNP if the tag of words i-2...i-1 is '-NONE-'\n",
      "   5   8   3   6  | NN -> NNP if the tag of the following word is 'NNP'\n",
      "   5   6   1   0  | NN -> NNP if the text of words i-2...i-1 is 'like'\n",
      "   5   5   0   3  | NN -> VBN if the text of the following word is '*-1'\n",
      "   ...\n",
      ">>> print(open(\"errors.out\").read())\n",
      "             left context |    word/test->gold     | right context\n",
      "--------------------------+------------------------+--------------------------\n",
      "                          |      Then/NN->RB       | ,/, in/IN the/DT guests/N\n",
      ", in/IN the/DT guests/NNS |       '/VBD->POS       | honor/NN ,/, the/DT speed\n",
      "'/POS honor/NN ,/, the/DT |    speedway/JJ->NN     | hauled/VBD out/RP four/CD\n",
      "NN ,/, the/DT speedway/NN |     hauled/NN->VBD     | out/RP four/CD drivers/NN\n",
      "DT speedway/NN hauled/VBD |      out/NNP->RP       | four/CD drivers/NNS ,/, c\n",
      "dway/NN hauled/VBD out/RP |      four/NNP->CD      | drivers/NNS ,/, crews/NNS\n",
      "hauled/VBD out/RP four/CD |    drivers/NNP->NNS    | ,/, crews/NNS and/CC even\n",
      "P four/CD drivers/NNS ,/, |     crews/NN->NNS      | and/CC even/RB the/DT off\n",
      "NNS and/CC even/RB the/DT |    official/NNP->JJ    | Indianapolis/NNP 500/CD a\n",
      "                          |     After/VBD->IN      | the/DT race/NN ,/, Fortun\n",
      "ter/IN the/DT race/NN ,/, |    Fortune/IN->NNP     | 500/CD executives/NNS dro\n",
      "s/NNS drooled/VBD like/IN |  schoolboys/NNP->NNS   | over/IN the/DT cars/NNS a\n",
      "olboys/NNS over/IN the/DT |      cars/NN->NNS      | and/CC drivers/NNS ./.\n",
      "\n",
      "\n",
      "Example 6.1 (code_brill_demo.py): Figure 6.1: Brill Tagger Demonstration: the tagger has a collection of\n",
      "templates of the form X -> Y if the preceding word is Z;\n",
      "the variables in these templates are instantiated to particular\n",
      "words and tags to create \"rules\"; the score for a rule is the\n",
      "number of broken examples it corrects minus the number of\n",
      "correct cases it breaks; apart from training a tagger, the\n",
      "demonstration displays residual errors.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7   How to Determine the Category of a Word\n",
      "Now that we have examined word classes in detail, we turn to a more\n",
      "basic question: how do we decide what category a word belongs to in\n",
      "the first place? In general, linguists use morphological, syntactic,\n",
      "and semantic clues to determine the category of a word.\n",
      "\n",
      "7.1   Morphological Clues\n",
      "The internal structure of a word may give useful clues as to the\n",
      "word's category. For example, -ness is a suffix\n",
      "that combines with an adjective to produce a noun, e.g.\n",
      "happy → happiness, ill → illness. So\n",
      "if we encounter a word that ends in -ness, this is very likely\n",
      "to be a noun.  Similarly, -ment is a suffix that combines\n",
      "with some verbs to produce a noun, e.g.\n",
      "govern → government and establish → establishment.\n",
      "English verbs can also be morphologically complex.  For instance, the\n",
      "present participle of a verb ends in -ing, and expresses\n",
      "the idea of ongoing, incomplete action (e.g. falling, eating).\n",
      "The -ing suffix also appears on nouns derived from verbs, e.g. the\n",
      "falling of the leaves (this is known as the gerund).\n",
      "\n",
      "\n",
      "7.2   Syntactic Clues\n",
      "Another source of information is the typical contexts in which a word can\n",
      "occur. For example, assume that we have already determined the\n",
      "category of nouns. Then we might say that a syntactic criterion for an\n",
      "adjective in English is that it can occur immediately before a noun,\n",
      "or immediately following the words be or very. According\n",
      "to these tests, near should be categorized as an adjective:\n",
      "\n",
      "  (2)\n",
      "  a.the near window\n",
      "\n",
      "  b.The end is (very) near.\n",
      "\n",
      "\n",
      "\n",
      "7.3   Semantic Clues\n",
      "Finally, the meaning of a word is a useful clue as to its lexical\n",
      "category.  For example, the best-known definition of a noun is\n",
      "semantic: \"the name of a person, place or thing\". Within modern linguistics,\n",
      "semantic criteria for word classes are treated with suspicion, mainly\n",
      "because they are hard to formalize. Nevertheless, semantic criteria\n",
      "underpin many of our intuitions about word classes, and enable us to\n",
      "make a good guess about the categorization of words in languages that\n",
      "we are unfamiliar with.  For example, if all we know about the Dutch word\n",
      "verjaardag is that it means the same as the English word\n",
      "birthday, then we can guess that verjaardag is a noun in\n",
      "Dutch. However, some care is needed: although we might translate zij\n",
      "is vandaag jarig as it's her birthday today, the word\n",
      "jarig is in fact an adjective in Dutch, and has no exact\n",
      "equivalent in English.\n",
      "\n",
      "\n",
      "7.4   New Words\n",
      "All languages acquire new lexical items. A list of words recently\n",
      "added to the Oxford Dictionary of English includes cyberslacker,\n",
      "fatoush, blamestorm, SARS, cantopop, bupkis, noughties, muggle, and\n",
      "robata. Notice that all these new words are nouns, and this is\n",
      "reflected in calling nouns an open class. By contrast, prepositions\n",
      "are regarded as a closed class. That is, there is a limited set of\n",
      "words belonging to the class (e.g., above, along, at, below, beside,\n",
      "between, during, for, from, in, near, on, outside, over, past,\n",
      "through, towards, under, up, with), and membership of the set only\n",
      "changes very gradually over time.\n",
      "\n",
      "\n",
      "7.5   Morphology in Part of Speech Tagsets\n",
      "\n",
      "Common tagsets often capture some morpho-syntactic information;\n",
      "that is, information about the kind of morphological markings that\n",
      "words receive by virtue of their syntactic role.  Consider, for\n",
      "example, the selection of distinct grammatical forms of the word\n",
      "go illustrated in the following sentences:\n",
      "\n",
      "  (3)\n",
      "  a.Go away!\n",
      "\n",
      "  b.He sometimes goes to the cafe.\n",
      "\n",
      "  c.All the cakes have gone.\n",
      "\n",
      "  d.We went on the excursion.\n",
      "\n",
      "Each of these forms — go, goes, gone, and went —\n",
      "is morphologically distinct from the others. Consider the form,\n",
      "goes. This occurs in a restricted set of grammatical contexts, and\n",
      "requires a third person singular subject. Thus, the\n",
      "following sentences are ungrammatical.\n",
      "\n",
      "  (4)\n",
      "  a.*They sometimes goes to the cafe.\n",
      "\n",
      "  b.*I sometimes goes to the cafe.\n",
      "\n",
      "By contrast, gone is the past participle form; it is required\n",
      "after have (and cannot be replaced in this context by\n",
      "goes), and cannot occur as the main verb of a clause.\n",
      "\n",
      "  (5)\n",
      "  a.*All the cakes have goes.\n",
      "\n",
      "  b.*He sometimes gone to the cafe.\n",
      "\n",
      "We can easily imagine a tagset in which the four distinct\n",
      "grammatical forms just discussed were all tagged as VB. Although\n",
      "this would be adequate for some purposes, a more fine-grained tagset\n",
      "provides useful information about these forms that can help\n",
      "other processors that try to detect patterns in tag\n",
      "sequences.  The Brown tagset captures these distinctions,\n",
      "as summarized in 7.1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Form\n",
      "Category\n",
      "Tag\n",
      "\n",
      "\n",
      "\n",
      "go\n",
      "base\n",
      "VB\n",
      "\n",
      "goes\n",
      "3rd singular present\n",
      "VBZ\n",
      "\n",
      "gone\n",
      "past participle\n",
      "VBN\n",
      "\n",
      "going\n",
      "gerund\n",
      "VBG\n",
      "\n",
      "went\n",
      "simple past\n",
      "VBD\n",
      "\n",
      "\n",
      "Table 7.1: Some morphosyntactic distinctions in the Brown tagset\n",
      "\n",
      "\n",
      "In addition to this set of verb tags, the various forms of the verb to be\n",
      "have special tags:\n",
      "be/BE, being/BEG, am/BEM, are/BER, is/BEZ, been/BEN, were/BED and\n",
      "was/BEDZ (plus extra tags for negative forms of the verb).   All told,\n",
      "this fine-grained tagging of verbs means that an automatic tagger\n",
      "that uses this tagset is effectively carrying out a limited amount\n",
      "of morphological analysis.\n",
      "Most part-of-speech tagsets make use of the same basic categories,\n",
      "such as noun, verb, adjective, and preposition. However, tagsets\n",
      "differ both in how finely they divide words into categories, and in\n",
      "how they define their categories. For example, is might be tagged\n",
      "simply as a verb in one tagset; but as a distinct form of the lexeme be\n",
      "in another tagset (as in the Brown Corpus).  This variation in tagsets is\n",
      "unavoidable, since part-of-speech tags are used in different ways for\n",
      "different tasks. In other words, there is no one 'right way' to assign\n",
      "tags, only more or less useful ways depending on one's goals.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8   Summary\n",
      "\n",
      "Words can be grouped into classes, such as nouns, verbs, adjectives, and adverbs.\n",
      "These classes are known as lexical categories or parts of speech.\n",
      "Parts of speech are assigned short labels, or tags, such as NN, VB,\n",
      "The process of automatically assigning parts of speech to words in text\n",
      "is called part-of-speech tagging, POS tagging, or just tagging.\n",
      "Automatic tagging is an important step in the NLP pipeline,\n",
      "and is useful in a variety of situations including:\n",
      "predicting the behavior of previously unseen words,\n",
      "analyzing word usage in corpora, and text-to-speech systems.\n",
      "Some linguistic corpora, such as the Brown Corpus, have been POS tagged.\n",
      "A variety of tagging methods are possible, e.g.\n",
      "default tagger, regular expression tagger, unigram tagger and n-gram taggers.\n",
      "These can be combined using a technique known as backoff.\n",
      "Taggers can be trained and evaluated using tagged corpora.\n",
      "Backoff is a method for combining models: when a more specialized\n",
      "model (such as a bigram tagger) cannot assign a tag in a given\n",
      "context, we backoff to a more general model (such as a unigram tagger).\n",
      "Part-of-speech tagging is an important, early example of a sequence\n",
      "classification task in NLP: a classification decision at any one point\n",
      "in the sequence makes use of words and tags in the local context.\n",
      "A dictionary is used to map between arbitrary types of information,\n",
      "such as a string and a number: freq['cat'] = 12.  We create\n",
      "dictionaries using the brace notation: pos = {},\n",
      "pos = {'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}.\n",
      "N-gram taggers can be defined for large values of n, but once\n",
      "n is larger than 3 we usually encounter the sparse data problem;\n",
      "even with a large quantity of training data we only see a tiny\n",
      "fraction of possible contexts.\n",
      "Transformation-based tagging involves learning a series\n",
      "of repair rules of the form \"change tag s to tag\n",
      "t in context c\", where each rule\n",
      "fixes mistakes and possibly introduces a (smaller) number\n",
      "of errors.\n",
      "\n",
      "\n",
      "\n",
      "9   Further Reading\n",
      "Extra materials for this chapter are posted at http://nltk.org/, including links to freely\n",
      "available resources on the web.\n",
      "For more examples of tagging with NLTK, please see the\n",
      "Tagging HOWTO at http://nltk.org/howto.\n",
      "Chapters 4 and 5 of (Jurafsky & Martin, 2008) contain more advanced\n",
      "material on n-grams and part-of-speech tagging.\n",
      "The \"Universal Tagset\" is described by (Petrov, Das, & McDonald, 2012).\n",
      "Other approaches to tagging involve machine learning methods (chap-data-intensive).\n",
      "In 7. we will see a generalization of tagging called chunking in which a\n",
      "contiguous sequence of words is assigned a single tag.\n",
      "For tagset documentation, see\n",
      "nltk.help.upenn_tagset() and nltk.help.brown_tagset().\n",
      "Lexical categories are introduced in linguistics textbooks, including those\n",
      "listed in 1..\n",
      "There are many other kinds of tagging.\n",
      "Words can be tagged with directives to a speech synthesizer,\n",
      "indicating which words should be emphasized.  Words can be tagged with sense\n",
      "numbers, indicating which sense of the word was used.  Words can also\n",
      "be tagged with morphological features.\n",
      "Examples of each of these kinds of tags are shown below.\n",
      "For space reasons, we only show the tag for a single\n",
      "word. Note also that the first two examples use XML-style\n",
      "tags, where elements in angle brackets enclose the word that is\n",
      "tagged.\n",
      "\n",
      "Speech Synthesis Markup Language (W3C SSML):\n",
      "That is a <emphasis>big</emphasis> car!\n",
      "SemCor: Brown Corpus tagged with WordNet senses:\n",
      "Space in any <wf pos=\"NN\" lemma=\"form\" wnsn=\"4\">form</wf>\n",
      "is completely measured by the three dimensions.\n",
      "(Wordnet form/nn sense 4: \"shape, form, configuration,\n",
      "contour, conformation\")\n",
      "Morphological tagging, from the Turin University Italian Treebank:\n",
      "E' italiano , come progetto e realizzazione , il\n",
      "primo (PRIMO ADJ ORDIN M SING) porto turistico dell' Albania .\n",
      "\n",
      "Note that tagging is also performed at higher levels.  Here is an example\n",
      "of dialogue act tagging, from the NPS Chat Corpus (Forsyth & Martell, 2007) included with\n",
      "NLTK.  Each turn of the dialogue is categorized as to its communicative\n",
      "function:\n",
      "\n",
      "Statement  User117 Dude..., I wanted some of that\n",
      "ynQuestion User120 m I missing something?\n",
      "Bye        User117 I'm gonna go fix food, I'll be back later.\n",
      "System     User122 JOIN\n",
      "System     User2   slaps User122 around a bit with a large trout.\n",
      "Statement  User121 18/m pm me if u tryin to chat\n",
      "\n",
      "\n",
      "\n",
      "10   Exercises\n",
      "\n",
      "☼\n",
      "Search the web for \"spoof newspaper headlines\", to find such gems as:\n",
      "British Left Waffles on Falkland Islands, and\n",
      "Juvenile Court to Try Shooting Defendant.\n",
      "Manually tag these headlines to see if knowledge of the part-of-speech\n",
      "tags removes the ambiguity.\n",
      "☼\n",
      "Working with someone else, take turns to pick a word that can be\n",
      "either a noun or a verb (e.g. contest); the opponent has to\n",
      "predict which one is likely to be the most frequent in the Brown corpus; check the\n",
      "opponent's prediction, and tally the score over several turns.\n",
      "☼\n",
      "Tokenize and tag the following sentence:\n",
      "They wind back the clock, while we chase after the wind.\n",
      "What different pronunciations and parts of speech are involved?\n",
      "☼ Review the mappings in 3.1.  Discuss any other\n",
      "examples of mappings you can think of.  What type of information do they map\n",
      "from and to?\n",
      "☼ Using the Python interpreter in interactive mode, experiment with\n",
      "the dictionary examples in this chapter.  Create a dictionary d, and add\n",
      "some entries.  What happens if you try to access a non-existent\n",
      "entry, e.g. d['xyz']?\n",
      "☼ Try deleting an element from a dictionary d, using the syntax\n",
      "del d['abc'].  Check that the item was deleted.\n",
      "☼ Create two dictionaries, d1 and d2, and add some entries to\n",
      "each.  Now issue the command d1.update(d2).  What did this do?\n",
      "What might it be useful for?\n",
      "☼ Create a dictionary e, to represent a single lexical entry\n",
      "for some word of your choice.\n",
      "Define keys like headword, part-of-speech, sense, and\n",
      "example, and assign them suitable values.\n",
      "☼ Satisfy yourself that there are\n",
      "restrictions on the distribution of go and went, in the\n",
      "sense that they cannot be freely interchanged in the kinds of contexts\n",
      "illustrated in (3d) in 7.\n",
      "☼\n",
      "Train a unigram tagger and run it on some new text.\n",
      "Observe that some words are not assigned a tag.  Why not?\n",
      "☼\n",
      "Learn about the affix tagger (type help(nltk.AffixTagger)).\n",
      "Train an affix tagger and run it on some new text.\n",
      "Experiment with different settings for the affix length\n",
      "and the minimum word length.  Discuss your findings.\n",
      "☼\n",
      "Train a bigram tagger with no backoff tagger, and run it on some of the training\n",
      "data.  Next, run it on some new data.\n",
      "What happens to the performance of the tagger?  Why?\n",
      "☼ We can use a dictionary to specify the values to be\n",
      "substituted into a formatting string.  Read Python's library\n",
      "documentation for formatting strings\n",
      "http://docs.python.org/lib/typesseq-strings.html\n",
      "and use this method to display today's date in two\n",
      "different formats.\n",
      "◑ Use sorted() and set() to get a sorted list of tags used in the Brown\n",
      "corpus, removing duplicates.\n",
      "◑ Write programs to process the Brown Corpus and find answers to the following\n",
      "questions:\n",
      "Which nouns are more common in their plural form, rather than their singular\n",
      "form? (Only consider regular plurals, formed with the -s suffix.)\n",
      "Which word has the greatest number of distinct tags.  What are they, and\n",
      "what do they represent?\n",
      "List tags in order of decreasing frequency.  What do the 20 most frequent tags represent?\n",
      "Which tags are nouns most commonly found after?  What do these tags represent?\n",
      "\n",
      "\n",
      "◑ Explore the following issues that arise in connection with the lookup tagger:\n",
      "What happens to the tagger performance for the various\n",
      "model sizes when a backoff tagger is omitted?\n",
      "Consider the curve in 4.2; suggest a\n",
      "good size for a lookup tagger that balances memory and performance.\n",
      "Can you come up with scenarios where it would be preferable to\n",
      "minimize memory usage, or to maximize performance with no regard for memory usage?\n",
      "\n",
      "\n",
      "◑ What is the upper limit of performance for a lookup tagger,\n",
      "assuming no limit to the size of its table?  (Hint: write a program\n",
      "to work out what percentage of tokens of a word are assigned\n",
      "the most likely tag for that word, on average.)\n",
      "◑ Generate some statistics for tagged data to answer the following questions:\n",
      "What proportion of word types are always assigned the same part-of-speech tag?\n",
      "How many words are ambiguous, in the sense that they appear with at least two tags?\n",
      "What percentage of word tokens in the Brown Corpus involve\n",
      "these ambiguous words?\n",
      "\n",
      "\n",
      "◑ The evaluate() method works out how accurately\n",
      "the tagger performs on this text.  For example, if the supplied tagged text\n",
      "was [('the', 'DT'), ('dog', 'NN')] and the tagger produced the output\n",
      "[('the', 'NN'), ('dog', 'NN')], then the score would be 0.5.\n",
      "Let's try to figure out how the evaluation method works:\n",
      "A tagger t takes a list of words as input, and produces a list of tagged words\n",
      "as output.  However, t.evaluate() is given correctly tagged text as its only parameter.\n",
      "What must it do with this input before performing the tagging?\n",
      "Once the tagger has created newly tagged text, how might the evaluate() method\n",
      "go about comparing it with the original tagged text and computing the accuracy score?\n",
      "Now examine the source code to see how the method is implemented.  Inspect\n",
      "nltk.tag.api.__file__ to discover the location of the source code,\n",
      "and open this file using an editor (be sure to use the api.py file and\n",
      "not the compiled api.pyc binary file).\n",
      "\n",
      "\n",
      "◑ Write code to search the Brown Corpus for particular words and phrases\n",
      "according to tags, to answer the following questions:\n",
      "Produce an alphabetically sorted list of the distinct words tagged as MD.\n",
      "Identify words that can be plural nouns or third person singular verbs\n",
      "(e.g. deals, flies).\n",
      "Identify three-word prepositional phrases of the form IN + DET + NN\n",
      "(eg. in the lab).\n",
      "What is the ratio of masculine to feminine pronouns?\n",
      "\n",
      "\n",
      "◑ In 3.1 we saw a table involving frequency counts for\n",
      "the verbs adore, love, like, prefer and\n",
      "preceding qualifiers absolutely and definitely.\n",
      "Investigate the full range of adverbs that appear before these four verbs.\n",
      "◑\n",
      "We defined the regexp_tagger that can be used\n",
      "as a fall-back tagger for unknown words.  This tagger only checks for\n",
      "cardinal numbers.  By testing for particular prefix or suffix strings,\n",
      "it should be possible to guess other tags.  For example,\n",
      "we could tag any word that ends with -s as a plural noun.\n",
      "Define a regular expression tagger (using RegexpTagger())\n",
      "that tests for at least five other patterns in the spelling of words.\n",
      "(Use inline documentation to explain the rules.)\n",
      "◑\n",
      "Consider the regular expression tagger developed in the exercises in\n",
      "the previous section.  Evaluate the tagger using its accuracy() method,\n",
      "and try to come up with ways to improve its performance.  Discuss your findings.\n",
      "How does objective evaluation help in the development process?\n",
      "◑\n",
      "How serious is the sparse data problem?  Investigate the\n",
      "performance of n-gram taggers as n increases from 1 to 6.\n",
      "Tabulate the accuracy score.  Estimate the training data required\n",
      "for these taggers, assuming a vocabulary size of\n",
      "105 and a tagset size of 102.\n",
      "◑ Obtain some tagged data for another language, and train and\n",
      "evaluate a variety of taggers on it.  If the language is\n",
      "morphologically complex, or if there are any orthographic clues\n",
      "(e.g. capitalization) to word classes, consider developing a\n",
      "regular expression tagger for it (ordered after the unigram\n",
      "tagger, and before the default tagger).  How does the accuracy of\n",
      "your tagger(s) compare with the same taggers run on English data?\n",
      "Discuss any issues you encounter in applying these methods to the language.\n",
      "◑ 4.1 plotted a curve showing\n",
      "change in the performance of a lookup tagger as the model size was increased.\n",
      "Plot the performance curve for a unigram tagger, as the amount of training\n",
      "data is varied.\n",
      "◑\n",
      "Inspect the confusion matrix for the bigram tagger t2 defined in 5,\n",
      "and identify one or more sets of tags to collapse.  Define a dictionary to do\n",
      "the mapping, and evaluate the tagger on the simplified data.\n",
      "◑\n",
      "Experiment with taggers using the simplified tagset (or make one of your\n",
      "own by discarding all but the first character of each tag name).\n",
      "Such a tagger has fewer distinctions to make, but much less\n",
      "information on which to base its work.  Discuss your findings.\n",
      "◑\n",
      "Recall the example of a bigram tagger which encountered a word it hadn't\n",
      "seen during training, and tagged the rest of the sentence as None.\n",
      "It is possible for a bigram tagger to fail part way through a sentence\n",
      "even if it contains no unseen words (even if the sentence was used during\n",
      "training).  In what circumstance can this happen?  Can you write a program\n",
      "to find some examples of this?\n",
      "◑\n",
      "Preprocess the Brown News data by replacing low frequency words with UNK,\n",
      "but leaving the tags untouched.  Now train and evaluate a bigram tagger\n",
      "on this data.  How much does this help?  What is the contribution of the unigram\n",
      "tagger and default tagger now?\n",
      "◑\n",
      "Modify the program in 4.1 to use a logarithmic scale on\n",
      "the x-axis, by replacing pylab.plot() with pylab.semilogx().\n",
      "What do you notice about the shape of the resulting plot?  Does the gradient\n",
      "tell you anything?\n",
      "◑\n",
      "Consult the documentation for the Brill tagger demo function,\n",
      "using help(nltk.tag.brill.demo).\n",
      "Experiment with the tagger by setting different values for the parameters.\n",
      "Is there any trade-off between training time (corpus size) and performance?\n",
      "◑ Write code that builds a dictionary of dictionaries of sets.\n",
      "Use it to store the set of POS tags that can follow a given word having\n",
      "a given POS tag, i.e. wordi → tagi\n",
      "→ tagi+1.\n",
      "★ There are 264 distinct words in the Brown Corpus having exactly\n",
      "three possible tags.\n",
      "Print a table with the integers 1..10 in one column, and the\n",
      "number of distinct words in the corpus having 1..10 distinct tags\n",
      "in the other column.\n",
      "For the word with the greatest number of distinct tags, print\n",
      "out sentences from the corpus containing the word, one for each\n",
      "possible tag.\n",
      "\n",
      "\n",
      "★ Write a program to classify contexts involving the word must according\n",
      "to the tag of the following word.  Can this be used to discriminate between the\n",
      "epistemic and deontic uses of must?\n",
      "★\n",
      "Create a regular expression tagger and various unigram and n-gram taggers,\n",
      "incorporating backoff, and train them on part of the Brown corpus.\n",
      "Create three different combinations of the taggers. Test the\n",
      "accuracy of each combined tagger. Which combination works best?\n",
      "Try varying the size of the training corpus. How does it affect\n",
      "your results?\n",
      "\n",
      "\n",
      "★\n",
      "Our approach for tagging an unknown word has been to consider the letters of the word\n",
      "(using RegexpTagger()), or to ignore the word altogether and tag\n",
      "it as a noun (using nltk.DefaultTagger()).  These methods will not do well for texts having\n",
      "new words that are not nouns.\n",
      "Consider the sentence I like to blog on Kim's blog.  If blog is a new\n",
      "word, then looking at the previous tag (TO versus NP$) would probably be helpful.\n",
      "I.e. we need a default tagger that is sensitive to the preceding tag.\n",
      "Create a new kind of unigram tagger that looks at the tag of the previous word,\n",
      "and ignores the current word.  (The best way to do this is to modify the source\n",
      "code for UnigramTagger(), which presumes knowledge of object-oriented\n",
      "programming in Python.)\n",
      "Add this tagger to the sequence of backoff taggers (including ordinary trigram\n",
      "and bigram taggers that look at words), right before the usual default tagger.\n",
      "Evaluate the contribution of this new unigram tagger.\n",
      "\n",
      "\n",
      "★\n",
      "Consider the code in 5 which\n",
      "determines the upper bound for accuracy of a trigram tagger.\n",
      "Review Abney's discussion concerning the impossibility of\n",
      "exact tagging (Church, Young, & Bloothooft, 1996).  Explain why correct tagging of\n",
      "these examples requires access to other kinds of information than\n",
      "just words and tags.  How might you estimate the scale of this problem?\n",
      "★\n",
      "Use some of the estimation techniques in nltk.probability,\n",
      "such as Lidstone or Laplace estimation, to develop a statistical\n",
      "tagger that does a better job than n-gram backoff taggers in cases where\n",
      "contexts encountered during testing were not seen during training.\n",
      "★\n",
      "Inspect the diagnostic files created by the Brill tagger rules.out and\n",
      "errors.out.  Obtain the demonstration code by accessing the source code\n",
      "(at http://www.nltk.org/code)\n",
      "and create your own version of the Brill tagger.\n",
      "Delete some of the rule templates, based on what you learned from inspecting rules.out.\n",
      "Add some new rule templates which employ contexts that might help to\n",
      "correct the errors you saw in errors.out.\n",
      "★\n",
      "Develop an n-gram backoff tagger that permits \"anti-n-grams\" such as\n",
      "[\"the\", \"the\"] to be specified when a tagger is initialized.\n",
      "An anti-ngram is assigned a count of zero and is used to prevent\n",
      "backoff for this n-gram (e.g. to avoid\n",
      "estimating P(the | the) as just P(the)).\n",
      "★\n",
      "Investigate three different ways to define the split between training and\n",
      "testing data when developing a tagger using the Brown Corpus:\n",
      "genre (category), source (fileid), and sentence.\n",
      "Compare their relative performance and discuss which method\n",
      "is the most legitimate.  (You might use n-fold cross validation,\n",
      "discussed in 3, to improve the accuracy of the evaluations.)\n",
      "★\n",
      "Develop your own NgramTagger class that inherits from NLTK's class,\n",
      "and which encapsulates the method of collapsing the vocabulary of\n",
      "the tagged training and testing data that was described in\n",
      "this chapter.  Make sure that the unigram and default backoff taggers\n",
      "have access to the full vocabulary.\n",
      "\n",
      "\n",
      "\n",
      "About this document...\n",
      "UPDATED FOR NLTK 3.0.\n",
      "This is a chapter from Natural Language Processing with Python,\n",
      "by Steven Bird, Ewan Klein and Edward Loper,\n",
      "Copyright © 2019 the authors.\n",
      "It is distributed with the Natural Language Toolkit [http://nltk.org/],\n",
      "Version 3.0, under the terms of the\n",
      "Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\n",
      "[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\n",
      "This document was built on\n",
      "Wed  4 Sep 2019 11:40:48 ACST\n",
      "vectors[5] = [0, 7, 5, 23, 12, 35, 4, 29, 0, 0, 6, 23, 23, 11, 2, 1, 4, 0, 3, 0, 2, 0, 0, 0, 1, 0, 0, 23, 6, 7, 6, 1, 0, 8, 1, 11, 1, 0, 30, 0, 7, 8, 0, 289, 20, 3, 14, 0, 1, 2, 2, 1, 0, 3, 0, 2, 5, 15, 0, 1, 9, 0, 0, 2, 52, 0, 0, 4, 0, 51, 0, 0, 4, 5, 0, 10, 20, 0, 0, 1, 0, 1, 16, 1, 0, 1, 3, 1, 7, 1, 2, 0, 0, 9, 1, 238, 2, 1, 1, 21, 8, 0, 2, 2, 0, 0, 5, 1, 0, 0, 0, 2, 3, 1, 2, 0, 2, 0, 4, 0, 76, 0, 0, 1, 2, 0, 0, 0, 2, 17, 3, 7, 8, 1, 14, 1, 318, 107, 42, 1, 0, 0, 44, 8, 2, 0, 6, 6, 6, 0, 7, 0, 2, 0, 0, 0, 0, 0, 0, 15, 4, 26, 0, 11, 0, 0, 0, 0, 4, 0, 0, 0, 1, 30, 1, 7, 21, 5, 3, 2, 5, 3, 0, 0, 1, 1, 0, 1, 0, 2, 0, 13, 10, 9, 0, 0, 0, 6, 3, 7, 11, 1, 2, 8, 0, 126, 3, 12, 1, 3, 1, 1, 627, 0, 0, 1, 3, 0, 2, 0, 6, 8, 45, 67, 1, 1, 0, 0, 2, 0, 41, 19, 0, 0, 3, 1, 1, 0, 0, 0, 0, 13, 18, 0, 1, 2, 0, 0, 0, 1, 2, 2, 89, 4, 1, 2, 0, 2, 4, 0, 0, 62, 7, 23, 0, 1, 0, 2, 1, 0, 4, 1, 18, 0, 1, 0, 0, 1, 1, 5, 0, 0, 1, 2, 0, 1, 0, 3, 0, 0, 0, 0, 0, 5, 0, 22, 2, 16, 3, 0, 0, 0, 1, 2, 1, 1, 0, 3, 160, 1, 2, 0, 0, 0, 3, 1, 3, 12, 0, 1, 0, 0, 0, 63, 2, 0, 4, 9, 5, 19, 30, 6, 1, 2, 0, 0, 0, 18, 0, 2, 0, 0, 0, 10, 1, 23, 6, 0, 0, 0, 0, 1, 0, 6, 3, 9, 0, 18, 0, 4, 0, 0, 14, 0, 0, 1, 7, 2, 0, 2, 63, 0, 1, 33, 133, 0, 1, 0, 0, 10, 0, 4, 0, 1, 0, 1, 60, 3, 0, 33, 17, 4, 9, 20, 0, 34, 7, 5, 1, 4, 9, 5, 1, 0, 0, 1, 5, 0, 0, 1, 0, 11, 0, 0, 0, 6, 44, 12, 5, 2, 0, 1, 0, 5, 4, 0, 15, 0, 1, 1, 0, 3, 16, 0, 9, 0, 0, 6, 0, 7, 24, 1, 13, 1, 2, 0, 0, 2, 1, 0, 20, 0, 0, 0, 0, 10, 1, 0, 1, 0, 6, 0, 0, 6, 0, 5, 0, 3, 0, 1, 0, 1, 0, 2, 0, 3, 4, 17, 12, 2, 11, 2, 4, 215, 24, 0, 69, 4, 5, 1, 253, 2, 3, 1, 0, 0, 6, 5, 1, 0, 0, 1, 6, 11, 1, 2, 0, 2, 3, 0, 11, 1, 13, 1, 55, 1, 0, 0, 58, 5, 3, 2, 0, 1, 5, 35, 0, 2, 5, 2, 0, 1, 5, 0, 10, 12, 13, 3, 1, 0, 3, 2, 0, 0, 4, 0, 0, 1, 1, 1, 7, 0, 1, 0, 5, 4, 0, 0, 0, 11, 1, 1, 0, 0, 0, 0, 0, 2, 3, 0, 0, 1, 0, 5, 0, 0, 1, 0, 0, 1, 2, 18, 1, 1, 54, 53, 0, 3, 1, 6, 6, 4, 0, 3, 6, 3, 8, 4, 11, 1, 3, 3, 1, 4, 2, 8, 0, 6, 2, 3, 1, 4, 0, 0, 1, 20, 0, 0, 1, 0, 0, 25, 1, 0, 0, 6, 3, 59, 1, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 196, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 6, 9, 0, 0, 3, 16, 9, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 4, 0, 0, 3, 1, 0, 0, 0, 2, 0, 4, 0, 0, 1, 0, 4, 2, 0, 0, 0, 0, 5, 0, 2, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 20, 1, 2, 0, 1, 3, 0, 10, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 226, 83, 33, 26, 15, 0, 0, 2, 1, 1, 1, 3, 0, 0, 0, 0, 0, 3, 1, 0, 14, 2, 0, 0, 0, 0, 0, 0, 6, 0, 0, 6, 0, 1, 3, 1, 0, 2, 0, 3, 2, 0, 2, 1, 0, 0, 0, 0, 3, 11, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 7, 1, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 1, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 3, 3, 16, 0, 14, 13, 6, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 0, 4, 0, 3, 0, 15, 0, 0, 0, 0, 0, 6, 10, 1, 0, 2, 0, 0, 2, 3, 3, 0, 2, 7, 0, 0, 0, 12, 0, 5, 4, 3, 7, 6, 0, 0, 0, 0, 0, 0, 5, 0, 1, 1, 4, 5, 0, 0, 0, 0, 0, 0, 130, 0, 0, 0, 0, 0, 0, 55, 2, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 7, 0, 2, 0, 2, 0, 0, 0, 3, 8, 0, 5, 1, 2, 1, 0, 0, 11, 0, 3, 0, 1, 0, 0, 1, 9, 0, 9, 2, 2, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 1, 4, 6, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 2, 1, 12, 0, 0, 0, 0, 0, 12, 0, 0, 0, 9, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 12, 5, 25, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1, 12, 0, 0, 3, 4, 1, 1, 2, 2, 67, 1, 1, 7, 1, 1, 1, 10, 2, 1, 4, 1, 1, 2, 1, 1, 1, 1, 1, 0, 4, 4, 1, 2, 0, 6, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 4, 1, 0, 2, 1, 2, 1, 1, 0, 5, 1, 0, 1, 1, 0, 21, 6, 0, 6, 0, 0, 0, 0, 5, 18, 35, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 22, 0, 21, 0, 5, 5, 7, 0, 0, 2, 0, 0, 5, 0, 5, 1, 0, 13, 0, 0, 0, 0, 22, 4, 0, 8, 10, 0, 0, 0, 0, 10, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 3, 2, 2, 0, 0, 0, 3, 10, 1, 0, 1, 4, 0, 0, 1, 2, 3, 0, 14, 0, 0, 0, 0, 1, 0, 0, 6, 0, 0, 0, 0, 0, 2, 8, 1, 0, 27, 0, 0, 4, 2, 0, 5, 23, 0, 2, 0, 0, 8, 1, 1, 1, 7, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 3, 0, 42, 0, 0, 7, 5, 5, 2, 1, 0, 3, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 0, 0, 2, 1, 0, 0, 0, 0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 7, 6, 0, 2, 0, 0, 13, 0, 3, 0, 0, 1, 16, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 8, 0, 8, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 1, 1, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 19, 1, 6, 13, 4, 1, 2, 0, 1, 0, 6, 0, 1, 0, 0, 10, 4, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 10, 1, 1, 17, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 5, 0, 1, 15, 0, 5, 1, 0, 1, 5, 3, 0, 0, 5, 0, 0, 0, 26, 0, 0, 11, 3, 0, 0, 1, 0, 0, 19, 9, 0, 546, 3, 0, 0, 3, 0, 17, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 0, 0, 0, 0, 1, 0, 1, 0, 0, 4, 0, 25, 1, 0, 4, 4, 0, 0, 3, 0, 1, 1, 0, 0, 3, 0, 6, 0, 0, 1, 0, 0, 0, 7, 0, 5, 0, 0, 0, 0, 0, 0, 0, 4, 8, 0, 1, 0, 2, 2, 0, 0, 1, 185, 0, 3, 4, 9, 3, 9, 4, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 52, 1, 7, 16, 0, 133, 56, 0, 0, 0, 0, 0, 0, 0, 7, 6, 2, 0, 0, 0, 1, 0, 0, 0, 0, 4, 1, 0, 0, 1, 0, 0, 3, 45, 0, 4, 0, 1, 1, 0, 0, 1, 0, 0, 4, 2, 2, 3, 1, 0, 0, 1, 0, 0, 0, 4, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 5, 2, 0, 2, 3, 4, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 2, 3, 0, 0, 9, 0, 0, 0, 0, 0, 3, 0, 2, 2, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 4, 6, 5, 1, 1, 0, 1, 1, 0, 1, 2, 4, 0, 0, 0, 0, 6, 2, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 7, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 18, 0, 0, 0, 1, 4, 0, 1, 1, 0, 0, 1, 0, 6, 1, 1, 4, 1, 3, 0, 8, 1, 1, 0, 1, 0, 14, 2, 2, 0, 0, 0, 5, 10, 0, 0, 0, 10, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 6, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 1, 1, 0, 3, 14, 3, 2, 0, 37, 2, 0, 42, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 40, 12, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 76, 0, 0, 0, 0, 0, 0, 0, 7, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 41, 1, 0, 1, 0, 2, 0, 4, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 1, 4, 1, 1, 2, 2, 0, 0, 5, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 19, 3, 0, 0, 2, 2, 0, 2, 0, 0, 0, 1, 0, 2, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 11, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 1, 4, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 10, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 5, 0, 0, 2, 0, 0, 0, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 1, 15, 5, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 4, 0, 1, 54, 1, 0, 0, 4, 0, 0, 2, 0, 0, 0, 0, 0, 81, 0, 2, 0, 0, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 1, 0, 0, 1, 0, 88, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 10, 0, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 3, 0, 0, 3, 0, 1, 0, 2, 0, 0, 2, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 1, 1, 0, 0, 0, 0, 3, 1, 2, 0, 10, 1, 2, 12, 0, 2, 0, 10, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 10, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 5, 1, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 1, 0, 1, 1, 0, 1, 1, 0, 0, 2, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 41, 7, 3, 3, 5, 0, 0, 4, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 14, 7, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 34, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 9, 0, 1, 0, 0, 0, 2, 2, 0, 0, 0, 0, 46, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 18, 0, 0, 0, 0, 4, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 14, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 2, 5, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 15, 0, 2, 0, 0, 0, 0, 10, 1, 0, 22, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 7, 5, 0, 0, 1, 0, 3, 0, 3, 0, 0, 0, 0, 0, 1, 0, 10, 0, 1, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1, 0, 0, 16, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 2, 0, 0, 3, 1, 0, 2, 0, 7, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 10, 0, 2, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 20, 9, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 3, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 2, 0, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 1, 0, 12, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 20, 20, 14, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 7, 0, 0, 0, 25, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 26, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 9, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 39, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 35, 1, 2, 9, 134, 18, 1, 1, 3, 1, 1, 1, 7, 4, 2, 13, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 5, 2, 7, 1, 1, 1, 1, 1, 2, 1, 4, 4, 2, 1, 44, 11, 1, 2, 14, 1, 4, 3, 1, 1, 4, 2, 1, 10, 3, 1, 1, 1, 2, 3, 1, 1, 1, 1, 26, 1, 1, 13, 11, 6, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 3, 6, 7, 1, 28, 5, 1, 2, 1, 6, 1, 1, 1, 1, 1, 1, 7, 3, 3, 2, 2, 1, 1, 1, 1, 1, 1, 1, 7, 7, 5, 2, 1, 1, 1, 2, 1, 2, 4, 2, 2, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 1, 2, 3, 1, 1, 1, 6, 3, 1, 1, 1, 1, 1, 2, 7, 1, 1, 3, 5, 1, 3, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 3, 7, 3, 2, 2, 2, 3, 1, 2, 1, 1, 6, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 3, 10, 16, 2, 1, 2, 1, 1, 1, 3, 2, 8, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 22, 1, 1, 1, 1, 20, 2, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 7, 1, 3, 1, 1, 1, 1, 1, 1, 4, 4, 11, 6, 1, 1, 1, 2, 2, 2, 3, 5, 1, 1, 1, 3, 2, 1, 2, 1, 3, 1, 1, 1, 2, 1, 4, 3, 2, 2, 1, 1, 1, 1, 9, 2, 3, 6, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 5, 2, 1, 1, 1, 1, 1, 1, 2, 2, 23, 5, 5, 2, 2, 3, 2, 5, 3, 1, 1, 1, 1, 1, 1, 5, 4, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 4, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 2, 1, 2, 1, 1, 2, 1, 1, 3, 2, 2, 1, 2, 2, 2, 6, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 3, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 3, 5, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "docs[6] = 6. Learning to Classify Text\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6. Learning to Classify Text\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Detecting patterns is a central part of Natural Language Processing.  Words ending in\n",
      "-ed tend to be past tense verbs (5.).  Frequent use of\n",
      "will is indicative of news text (3).  These observable\n",
      "patterns — word structure and word frequency — happen to\n",
      "correlate with particular aspects of meaning, such as tense and topic.\n",
      "But how did we know where to start looking, which aspects of form to\n",
      "associate with which aspects of meaning?\n",
      "The goal of this chapter is to answer the following questions:\n",
      "\n",
      "How can we identify particular features of language data that\n",
      "are salient for classifying it?\n",
      "How can we construct models of language that can\n",
      "be used to perform language processing tasks automatically?\n",
      "What can we learn about language from these models?\n",
      "\n",
      "Along the way we will study some important machine learning\n",
      "techniques, including decision trees, naive Bayes' classifiers,\n",
      "and maximum entropy classifiers.  We will gloss over the mathematical and\n",
      "statistical underpinnings of these techniques, focusing instead on how\n",
      "and when to use them (see the Further Readings section for more\n",
      "technical background).  Before looking at these methods, we first need\n",
      "to appreciate the broad scope of this topic.\n",
      "\n",
      "1   Supervised Classification\n",
      "Classification is the task of choosing the correct class\n",
      "label for a given input.  In basic classification tasks, each\n",
      "input is considered in isolation from all other inputs, and the set of\n",
      "labels is defined in advance.  Some examples of classification tasks\n",
      "are:\n",
      "\n",
      "Deciding whether an email is spam or not.\n",
      "Deciding what the topic of a news article is, from a fixed list of\n",
      "topic areas such as \"sports,\" \"technology,\" and \"politics.\"\n",
      "Deciding whether a given occurrence of the word bank is used to\n",
      "refer to a river bank, a financial institution, the act of tilting\n",
      "to the side, or the act of depositing something in a financial\n",
      "institution.\n",
      "\n",
      "The basic classification task has a number of interesting variants.\n",
      "For example, in multi-class classification, each instance may be\n",
      "assigned multiple labels; in open-class classification, the set of\n",
      "labels is not defined in advance; and in sequence classification, a\n",
      "list of inputs are jointly classified.\n",
      "A classifier is called supervised if it is built based on\n",
      "training corpora containing the correct label for each input.  The\n",
      "framework used by supervised classification is shown in\n",
      "1.1.\n",
      "\n",
      "\n",
      "Figure 1.1: Supervised Classification.  (a) During training, a feature\n",
      "extractor is used to convert each input value to a feature set.\n",
      "These feature sets, which capture the basic information about\n",
      "each input that should be used to classify it, are discussed in\n",
      "the next section.\n",
      "Pairs of feature sets and labels are fed into the machine learning\n",
      "algorithm to generate a model.  (b) During prediction, the same\n",
      "feature extractor is used to convert unseen inputs to feature sets.\n",
      "These feature sets are then fed into the model, which generates\n",
      "predicted labels.\n",
      "\n",
      "In the rest of this section, we will look at how classifiers can be\n",
      "employed to solve a wide variety of tasks.  Our discussion is not intended\n",
      "to be comprehensive, but to give a representative sample of tasks that\n",
      "can be performed with the help of text classifiers.\n",
      "\n",
      "1.1   Gender Identification\n",
      "In 4 we saw that male and female names\n",
      "have some distinctive characteristics.  Names ending in a,\n",
      "e and i are likely to be female, while names ending in\n",
      "k, o, r, s and t are likely to be male.\n",
      "Let's build a classifier to model these differences more precisely.\n",
      "The first step in creating a classifier is deciding what\n",
      "features of the input are relevant, and how to encode\n",
      "those features.  For this example, we'll start by just looking at the\n",
      "final letter of a given name.  The following feature extractor\n",
      "function builds a dictionary containing relevant information about a\n",
      "given name:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def gender_features(word):\n",
      "...     return {'last_letter': word[-1]}\n",
      ">>> gender_features('Shrek')\n",
      "{'last_letter': 'k'}\n",
      "\n",
      "\n",
      "\n",
      "The returned dictionary, known as a feature set, maps from\n",
      "feature names to their values.  Feature names are case-sensitive\n",
      "strings that typically provide a short human-readable description of\n",
      "the feature, as in the example 'last_letter'.  Feature values are values with simple types, such as\n",
      "booleans, numbers, and strings.\n",
      "\n",
      "Note\n",
      "Most classification methods require that features be encoded using\n",
      "simple value types, such as booleans, numbers, and strings.  But\n",
      "note that just because a feature has a simple type, this does not\n",
      "necessarily mean that the feature's value is simple to express or\n",
      "compute. Indeed, it is even possible to use very complex and\n",
      "informative values, such as the output of a second supervised\n",
      "classifier, as features.\n",
      "\n",
      "Now that we've defined a feature extractor, we need to prepare\n",
      "a list of examples and corresponding\n",
      "class labels.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import names\n",
      ">>> labeled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
      "... [(name, 'female') for name in names.words('female.txt')])\n",
      ">>> import random\n",
      ">>> random.shuffle(labeled_names)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next, we use the feature extractor to process the names data, and\n",
      "divide the resulting list of feature sets into a training set\n",
      "and a test set.  The training set is used to train a new\n",
      "\"naive Bayes\" classifier.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\n",
      ">>> train_set, test_set = featuresets[500:], featuresets[:500]\n",
      ">>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "\n",
      "\n",
      "\n",
      "We will learn more about the naive Bayes classifier later in the\n",
      "chapter.  For now, let's just test it out on some names that did not\n",
      "appear in its training data:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> classifier.classify(gender_features('Neo'))\n",
      "'male'\n",
      ">>> classifier.classify(gender_features('Trinity'))\n",
      "'female'\n",
      "\n",
      "\n",
      "\n",
      "Observe that these character names from The Matrix are correctly\n",
      "classified.  Although this science fiction movie is set in 2199, it\n",
      "still conforms with our expectations about names and genders.  We can\n",
      "systematically evaluate the classifier on a much larger quantity of\n",
      "unseen data:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(nltk.classify.accuracy(classifier, test_set))\n",
      "0.77\n",
      "\n",
      "\n",
      "\n",
      "Finally, we can examine the classifier to determine which features it\n",
      "found most effective for distinguishing the names' genders:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> classifier.show_most_informative_features(5)\n",
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     33.2 : 1.0\n",
      "             last_letter = 'k'              male : female =     32.6 : 1.0\n",
      "             last_letter = 'p'              male : female =     19.7 : 1.0\n",
      "             last_letter = 'v'              male : female =     18.6 : 1.0\n",
      "             last_letter = 'f'              male : female =     17.3 : 1.0\n",
      "\n",
      "\n",
      "\n",
      "This listing shows that the names in the training set that end in \"a\"\n",
      "are female 33 times more often than they are male, but names that end\n",
      "in \"k\" are male 32 times more often than they are female.  These\n",
      "ratios are known as likelihood ratios, and can be useful for\n",
      "comparing different feature-outcome relationships.\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Modify the gender_features() function to provide the\n",
      "classifier with features encoding the length of the name, its first\n",
      "letter, and any other features that seem like they might be\n",
      "informative.  Retrain the classifier with these new features, and\n",
      "test its accuracy.\n",
      "\n",
      "When working with large corpora, constructing a single list\n",
      "that contains the features of every instance can use up a large\n",
      "amount of memory.  In these cases, use the function\n",
      "nltk.classify.apply_features, which returns an object that acts\n",
      "like a list but does not store all the feature sets in memory:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.classify import apply_features\n",
      ">>> train_set = apply_features(gender_features, labeled_names[500:])\n",
      ">>> test_set = apply_features(gender_features, labeled_names[:500])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.2   Choosing The Right Features\n",
      "Selecting relevant features and deciding how to encode them for a\n",
      "learning method can have an enormous impact on the learning method's\n",
      "ability to extract a good model.  Much of the interesting work in\n",
      "building a classifier is deciding what features might be relevant, and\n",
      "how we can represent them.  Although it's often possible to get decent\n",
      "performance by using a fairly simple and obvious set of features,\n",
      "there are usually significant gains to be had by using carefully\n",
      "constructed features based on a thorough understanding of the task at\n",
      "hand.\n",
      "Typically, feature extractors are built through a process of\n",
      "trial-and-error, guided by intuitions about what information is\n",
      "relevant to the problem.  It's common to start with a\n",
      "\"kitchen sink\" approach, including all the features that you can think\n",
      "of, and then checking to see which features actually are\n",
      "helpful.  We take this approach for name gender features in\n",
      "1.2.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def gender_features2(name):\n",
      "    features = {}\n",
      "    features[\"first_letter\"] = name[0].lower()\n",
      "    features[\"last_letter\"] = name[-1].lower()\n",
      "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
      "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
      "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
      "    return features\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> gender_features2('John') \n",
      "{'count(j)': 1, 'has(d)': False, 'count(b)': 0, ...}\n",
      "\n",
      "\n",
      "Example 1.2 (code_gender_features_overfitting.py): Figure 1.2: A Feature Extractor that Overfits Gender Features.\n",
      "The feature sets returned by this feature extractor contain a\n",
      "large number of specific features, leading to overfitting for\n",
      "the relatively small Names Corpus.\n",
      "\n",
      "However, there are usually limits to the number of features\n",
      "that you should use with a given learning algorithm — if you provide\n",
      "too many features, then the algorithm will have a higher chance of\n",
      "relying on idiosyncrasies of your training data that don't generalize\n",
      "well to new examples.  This problem is known as overfitting, and\n",
      "can be especially problematic when working with small training sets.  For\n",
      "example, if we train a naive Bayes classifier using the feature\n",
      "extractor shown in 1.2, it will overfit\n",
      "the relatively small training set, resulting in a system whose accuracy\n",
      "is about 1% lower than the accuracy of a classifier that only\n",
      "pays attention to the final letter of each name:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> featuresets = [(gender_features2(n), gender) for (n, gender) in labeled_names]\n",
      ">>> train_set, test_set = featuresets[500:], featuresets[:500]\n",
      ">>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      ">>> print(nltk.classify.accuracy(classifier, test_set))\n",
      "0.768\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Once an initial set of features has been chosen, a very productive\n",
      "method for refining the feature set is error analysis.  First,\n",
      "we select a development set, containing the corpus data for\n",
      "creating the model.  This development set is then subdivided\n",
      "into the training set and the dev-test set.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> train_names = labeled_names[1500:]\n",
      ">>> devtest_names = labeled_names[500:1500]\n",
      ">>> test_names = labeled_names[:500]\n",
      "\n",
      "\n",
      "\n",
      "The training set is used to train the model, and the dev-test set is\n",
      "used to perform error analysis.  The test set serves in our final\n",
      "evaluation of the system.  For reasons discussed below, it is\n",
      "important that we employ a separate dev-test set for error analysis,\n",
      "rather than just using the test set.  The division of the corpus data\n",
      "into different subsets is shown in 1.3.\n",
      "\n",
      "\n",
      "Figure 1.3: Organization of corpus data for training supervised classifiers.\n",
      "The corpus data is divided into two sets: the development set,\n",
      "and the test set.  The development set is often further subdivided\n",
      "into a training set and a dev-test set.\n",
      "\n",
      "Having divided the corpus into appropriate datasets, we train a model\n",
      "using the training set , and then run it on the\n",
      "dev-test set .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> train_set = [(gender_features(n), gender) for (n, gender) in train_names]\n",
      ">>> devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
      ">>> test_set = [(gender_features(n), gender) for (n, gender) in test_names]\n",
      ">>> classifier = nltk.NaiveBayesClassifier.train(train_set) \n",
      ">>> print(nltk.classify.accuracy(classifier, devtest_set)) \n",
      "0.75\n",
      "\n",
      "\n",
      "\n",
      "Using the dev-test set, we can generate a list of the errors that the\n",
      "classifier makes when predicting name genders:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> errors = []\n",
      ">>> for (name, tag) in devtest_names:\n",
      "...     guess = classifier.classify(gender_features(name))\n",
      "...     if guess != tag:\n",
      "...         errors.append( (tag, guess, name) )\n",
      "\n",
      "\n",
      "\n",
      "We can then examine individual error cases where the model predicted\n",
      "the wrong label, and try to determine what additional pieces of\n",
      "information would allow it to make the right decision (or which\n",
      "existing pieces of information are tricking it into making the wrong\n",
      "decision).  The feature set can then be adjusted accordingly.  The\n",
      "names classifier that we have built generates about 100 errors on the\n",
      "dev-test corpus:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> for (tag, guess, name) in sorted(errors):\n",
      "...     print('correct={:<8} guess={:<8s} name={:<30}'.format(tag, guess, name))\n",
      "correct=female   guess=male     name=Abigail\n",
      "  ...\n",
      "correct=female   guess=male     name=Cindelyn\n",
      "  ...\n",
      "correct=female   guess=male     name=Katheryn\n",
      "correct=female   guess=male     name=Kathryn\n",
      "  ...\n",
      "correct=male     guess=female   name=Aldrich\n",
      "  ...\n",
      "correct=male     guess=female   name=Mitch\n",
      "  ...\n",
      "correct=male     guess=female   name=Rich\n",
      "  ...\n",
      "\n",
      "\n",
      "\n",
      "Looking through this list of errors makes it clear that some suffixes\n",
      "that are more than one letter can be indicative of name genders.  For\n",
      "example, names ending in yn appear to be predominantly female,\n",
      "despite the fact that names ending in n tend to be male; and names\n",
      "ending in ch are usually male, even though names that end in h\n",
      "tend to be female.  We therefore\n",
      "adjust our feature extractor to include features for two-letter\n",
      "suffixes:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def gender_features(word):\n",
      "...     return {'suffix1': word[-1:],\n",
      "...             'suffix2': word[-2:]}\n",
      "\n",
      "\n",
      "\n",
      "Rebuilding the classifier with the new feature extractor, we see that\n",
      "the performance on the dev-test dataset improves by almost 2\n",
      "percentage points (from 76.5% to 78.2%):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> train_set = [(gender_features(n), gender) for (n, gender) in train_names]\n",
      ">>> devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
      ">>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      ">>> print(nltk.classify.accuracy(classifier, devtest_set))\n",
      "0.782\n",
      "\n",
      "\n",
      "\n",
      "This error analysis procedure can then be repeated, checking for\n",
      "patterns in the errors that are made by the newly improved classifier.\n",
      "Each time the error analysis procedure is repeated, we should select a\n",
      "different dev-test/training split, to ensure that the classifier\n",
      "does not start to reflect idiosyncrasies in the dev-test set.\n",
      "But once we've used the dev-test set to help us develop the\n",
      "model, we can no longer trust that it will give us an accurate idea of\n",
      "how well the model would perform on new data.  It is therefore\n",
      "important to keep the test set separate, and unused, until our model\n",
      "development is complete.  At that point, we can use the test set to\n",
      "evaluate how well our model will perform on new input values.\n",
      "\n",
      "\n",
      "1.3   Document Classification\n",
      "\n",
      "In 1, we saw several examples of\n",
      "corpora where documents have been labeled with categories.  Using\n",
      "these corpora, we can build classifiers that will automatically tag\n",
      "new documents with appropriate category labels.  First, we\n",
      "construct a list of documents, labeled with the appropriate\n",
      "categories.  For this example, we've chosen the Movie Reviews Corpus,\n",
      "which categorizes each review as positive or negative.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import movie_reviews\n",
      ">>> documents = [(list(movie_reviews.words(fileid)), category)\n",
      "...              for category in movie_reviews.categories()\n",
      "...              for fileid in movie_reviews.fileids(category)]\n",
      ">>> random.shuffle(documents)\n",
      "\n",
      "\n",
      "\n",
      "Next, we define a feature extractor for documents, so the classifier\n",
      "will know which aspects of the data it should pay attention to\n",
      "(1.4).  For document topic identification, we can\n",
      "define a feature for each word, indicating whether the document\n",
      "contains that word.  To limit the number of features that the\n",
      "classifier needs to process, we begin by constructing a list of the\n",
      "2000 most frequent words in the overall corpus\n",
      ".  We can then define a feature extractor\n",
      " that simply checks whether each of these\n",
      "words is present in a given document.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
      "word_features = list(all_words)[:2000] \n",
      "\n",
      "def document_features(document): \n",
      "    document_words = set(document) \n",
      "    features = {}\n",
      "    for word in word_features:\n",
      "        features['contains({})'.format(word)] = (word in document_words)\n",
      "    return features\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(document_features(movie_reviews.words('pos/cv957_8737.txt'))) \n",
      "{'contains(waste)': False, 'contains(lot)': False, ...}\n",
      "\n",
      "\n",
      "Example 1.4 (code_document_classify_fd.py): Figure 1.4: A feature extractor for document classification, whose\n",
      "features indicate whether or not individual words are present\n",
      "in a given document.\n",
      "\n",
      "\n",
      "Note\n",
      "The reason that we compute the set of all words in a document in\n",
      ", rather than just checking if\n",
      "word in document, is\n",
      "that checking whether a word occurs in a set is much faster than\n",
      "checking whether it occurs in a list (4.7).\n",
      "\n",
      "Now that we've defined our feature extractor, we can use it to train a\n",
      "classifier to label new movie reviews (1.5).  To\n",
      "check how reliable the resulting classifier is, we compute its\n",
      "accuracy on the test set .  And once again,\n",
      "we can use show_most_informative_features() to find out which\n",
      "features the classifier found to be most informative\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
      "train_set, test_set = featuresets[100:], featuresets[:100]\n",
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(nltk.classify.accuracy(classifier, test_set)) \n",
      "0.81\n",
      ">>> classifier.show_most_informative_features(5) \n",
      "Most Informative Features\n",
      "   contains(outstanding) = True              pos : neg    =     11.1 : 1.0\n",
      "        contains(seagal) = True              neg : pos    =      7.7 : 1.0\n",
      "   contains(wonderfully) = True              pos : neg    =      6.8 : 1.0\n",
      "         contains(damon) = True              pos : neg    =      5.9 : 1.0\n",
      "        contains(wasted) = True              neg : pos    =      5.8 : 1.0\n",
      "\n",
      "\n",
      "Example 1.5 (code_document_classify_use.py): Figure 1.5: Training and testing a classifier for document classification.\n",
      "\n",
      "Apparently in this corpus, a review that mentions \"Seagal\" is almost 8\n",
      "times more likely to be negative than positive, while a review that\n",
      "mentions \"Damon\" is about 6 times more likely to be positive.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.4   Part-of-Speech Tagging\n",
      "\n",
      "In 5. we built a regular expression tagger that chooses a\n",
      "part-of-speech tag for a word by looking at the internal make-up of\n",
      "the word.  However, this regular expression tagger had to be\n",
      "hand-crafted.  Instead, we can train a classifier to work out which\n",
      "suffixes are most informative.  Let's begin by finding out what the\n",
      "most common suffixes are:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import brown\n",
      ">>> suffix_fdist = nltk.FreqDist()\n",
      ">>> for word in brown.words():\n",
      "...     word = word.lower()\n",
      "...     suffix_fdist[word[-1:]] += 1\n",
      "...     suffix_fdist[word[-2:]] += 1\n",
      "...     suffix_fdist[word[-3:]] += 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n",
      ">>> print(common_suffixes)\n",
      "['e', ',', '.', 's', 'd', 't', 'he', 'n', 'a', 'of', 'the',\n",
      " 'y', 'r', 'to', 'in', 'f', 'o', 'ed', 'nd', 'is', 'on', 'l',\n",
      " 'g', 'and', 'ng', 'er', 'as', 'ing', 'h', 'at', 'es', 'or',\n",
      " 're', 'it', '``', 'an', \"''\", 'm', ';', 'i', 'ly', 'ion', ...]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Next, we'll define a feature extractor function which checks a given\n",
      "word for these suffixes:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def pos_features(word):\n",
      "...     features = {}\n",
      "...     for suffix in common_suffixes:\n",
      "...         features['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
      "...     return features\n",
      "\n",
      "\n",
      "\n",
      "Feature extraction functions behave like tinted glasses, highlighting\n",
      "some of the properties (colors) in our data and making it impossible\n",
      "to see other properties.  The classifier will rely exclusively on\n",
      "these highlighted properties when determining how to label inputs.  In\n",
      "this case, the classifier will make its decisions based only on\n",
      "information about which of the common suffixes (if any) a given word\n",
      "has.\n",
      "\n",
      "\n",
      "Now that we've defined our feature extractor, we can use it to\n",
      "train a new \"decision tree\" classifier (to be discussed in\n",
      "4):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tagged_words = brown.tagged_words(categories='news')\n",
      ">>> featuresets = [(pos_features(n), g) for (n,g) in tagged_words]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> size = int(len(featuresets) * 0.1)\n",
      ">>> train_set, test_set = featuresets[size:], featuresets[:size]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
      ">>> nltk.classify.accuracy(classifier, test_set)\n",
      "0.62705121829935351\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> classifier.classify(pos_features('cats'))\n",
      "'NNS'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "One nice feature of decision tree models is that they are often fairly\n",
      "easy to interpret — we can even instruct NLTK to print them\n",
      "out as pseudocode:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(classifier.pseudocode(depth=4))\n",
      "if endswith(,) == True: return ','\n",
      "if endswith(,) == False:\n",
      "  if endswith(the) == True: return 'AT'\n",
      "  if endswith(the) == False:\n",
      "    if endswith(s) == True:\n",
      "      if endswith(is) == True: return 'BEZ'\n",
      "      if endswith(is) == False: return 'VBZ'\n",
      "    if endswith(s) == False:\n",
      "      if endswith(.) == True: return '.'\n",
      "      if endswith(.) == False: return 'NN'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Here, we can see that the classifier begins by checking whether a word\n",
      "ends with a comma — if so, then it will receive the special tag\n",
      "\",\".  Next, the classifier checks if the word ends in \"the\",\n",
      "in which case it's almost certainly a determiner.  This \"suffix\" gets\n",
      "used early by the decision tree because the word \"the\" is so common.\n",
      "Continuing on, the classifier checks if the word ends in \"s\".  If so,\n",
      "then it's most likely to receive the verb tag VBZ (unless it's\n",
      "the word \"is\", which has a special tag BEZ), and if not,\n",
      "then it's most likely a noun (unless it's the punctuation mark \".\").\n",
      "The actual classifier contains further nested if-then statements below\n",
      "the ones shown here, but the depth=4 argument just displays the\n",
      "top portion of the decision tree.\n",
      "\n",
      "\n",
      "\n",
      "1.5   Exploiting Context\n",
      "\n",
      "By augmenting the feature extraction function, we could modify this\n",
      "part-of-speech tagger to leverage a variety of other word-internal\n",
      "features, such as the length of the word, the number of syllables it\n",
      "contains, or its prefix.  However, as long as the feature extractor\n",
      "just looks at the target word, we have no way to add features that\n",
      "depend on the context that the word appears in.  But contextual\n",
      "features often provide powerful clues about the correct tag — for\n",
      "example, when tagging the word \"fly,\" knowing that the previous word\n",
      "is \"a\" will allow us to determine that it is functioning as a noun, not\n",
      "a verb.\n",
      "In order to accommodate features that depend on a word's context, we\n",
      "must revise the pattern that we used to define our feature extractor.\n",
      "Instead of just passing in the word to be tagged, we will pass in a\n",
      "complete (untagged) sentence, along with the index of the target word.\n",
      "This approach is demonstrated in 1.6, which employs a\n",
      "context-dependent feature extractor to define a part of speech tag\n",
      "classifier.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def pos_features(sentence, i): \n",
      "    features = {\"suffix(1)\": sentence[i][-1:],\n",
      "                \"suffix(2)\": sentence[i][-2:],\n",
      "                \"suffix(3)\": sentence[i][-3:]}\n",
      "    if i == 0:\n",
      "        features[\"prev-word\"] = \"<START>\"\n",
      "    else:\n",
      "        features[\"prev-word\"] = sentence[i-1]\n",
      "    return features\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> pos_features(brown.sents()[0], 8)\n",
      "{'suffix(3)': 'ion', 'prev-word': 'an', 'suffix(2)': 'on', 'suffix(1)': 'n'}\n",
      "\n",
      ">>> tagged_sents = brown.tagged_sents(categories='news')\n",
      ">>> featuresets = []\n",
      ">>> for tagged_sent in tagged_sents:\n",
      "...     untagged_sent = nltk.tag.untag(tagged_sent)\n",
      "...     for i, (word, tag) in enumerate(tagged_sent):\n",
      "...         featuresets.append( (pos_features(untagged_sent, i), tag) )\n",
      "\n",
      ">>> size = int(len(featuresets) * 0.1)\n",
      ">>> train_set, test_set = featuresets[size:], featuresets[:size]\n",
      ">>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "\n",
      ">>> nltk.classify.accuracy(classifier, test_set)\n",
      "0.78915962207856782\n",
      "\n",
      "\n",
      "Example 1.6 (code_suffix_pos_tag.py): Figure 1.6: A part-of-speech classifier whose feature detector\n",
      "examines the context in which a word appears in order to\n",
      "determine which part of speech tag should be assigned.  In\n",
      "particular, the identity of the previous word is included as a\n",
      "feature.\n",
      "\n",
      "\n",
      "\n",
      "It is clear that exploiting contextual features improves the performance\n",
      "of our part-of-speech tagger.  For example, the classifier learns\n",
      "that a word is likely to be a noun if it comes immediately after the\n",
      "word \"large\" or the word \"gubernatorial\".  However, it is unable to\n",
      "learn the generalization that a word is probably a noun if it follows\n",
      "an adjective, because it doesn't have access to the previous word's\n",
      "part-of-speech tag.  In general, simple classifiers always treat each\n",
      "input as independent from all other inputs.  In many contexts, this\n",
      "makes perfect sense.  For example, decisions about whether names tend\n",
      "to be male or female can be made on a case-by-case basis.  However,\n",
      "there are often cases, such as part-of-speech tagging, where we are\n",
      "interested in solving classification problems that are closely related\n",
      "to one another.\n",
      "\n",
      "\n",
      "1.6   Sequence Classification\n",
      "In order to capture the dependencies between related classification\n",
      "tasks, we can use joint classifier models, which choose an\n",
      "appropriate labeling for a collection of related inputs.  In the case\n",
      "of part-of-speech tagging, a variety of different sequence\n",
      "classifier models can be used to jointly choose part-of-speech\n",
      "tags for all the words in a given sentence.\n",
      "\n",
      "One sequence classification strategy, known as consecutive\n",
      "classification or greedy sequence classification, is to\n",
      "find the most likely class label for the first input,\n",
      "then to use that answer to help find the best label for the next\n",
      "input.  The process can then be repeated until all of the inputs have\n",
      "been labeled.  This is the approach that was taken by the bigram\n",
      "tagger from 5, which began by choosing a\n",
      "part-of-speech tag for the first word in the sentence, and then chose\n",
      "the tag for each subsequent word based on the word itself and the\n",
      "predicted tag for the previous word.\n",
      "This strategy is demonstrated in 1.7.\n",
      "First, we must\n",
      "augment our feature extractor function to take a history\n",
      "argument, which provides a list of the tags that we've predicted for\n",
      "the sentence so far .\n",
      "Each tag in history corresponds with a word in sentence.  But\n",
      "note that history will only contain tags for words we've already\n",
      "classified, that is, words to the left of the target word.  Thus, while it is\n",
      "possible to look at some features of words to the right\n",
      "of the target word, it is not possible to look at the tags for those\n",
      "words (since we haven't generated them yet).\n",
      "Having defined a feature extractor, we can proceed to build our\n",
      "sequence classifier .  During training, we use\n",
      "the annotated tags to\n",
      "provide the appropriate history to the feature extractor, but when\n",
      "tagging new sentences, we generate the history list based on the\n",
      "output of the tagger itself.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " def pos_features(sentence, i, history): \n",
      "     features = {\"suffix(1)\": sentence[i][-1:],\n",
      "                 \"suffix(2)\": sentence[i][-2:],\n",
      "                 \"suffix(3)\": sentence[i][-3:]}\n",
      "     if i == 0:\n",
      "         features[\"prev-word\"] = \"<START>\"\n",
      "         features[\"prev-tag\"] = \"<START>\"\n",
      "     else:\n",
      "         features[\"prev-word\"] = sentence[i-1]\n",
      "         features[\"prev-tag\"] = history[i-1]\n",
      "     return features\n",
      "\n",
      "class ConsecutivePosTagger(nltk.TaggerI): \n",
      "\n",
      "    def __init__(self, train_sents):\n",
      "        train_set = []\n",
      "        for tagged_sent in train_sents:\n",
      "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
      "            history = []\n",
      "            for i, (word, tag) in enumerate(tagged_sent):\n",
      "                featureset = pos_features(untagged_sent, i, history)\n",
      "                train_set.append( (featureset, tag) )\n",
      "                history.append(tag)\n",
      "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "\n",
      "    def tag(self, sentence):\n",
      "        history = []\n",
      "        for i, word in enumerate(sentence):\n",
      "            featureset = pos_features(sentence, i, history)\n",
      "            tag = self.classifier.classify(featureset)\n",
      "            history.append(tag)\n",
      "        return zip(sentence, history)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tagged_sents = brown.tagged_sents(categories='news')\n",
      ">>> size = int(len(tagged_sents) * 0.1)\n",
      ">>> train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n",
      ">>> tagger = ConsecutivePosTagger(train_sents)\n",
      ">>> print(tagger.evaluate(test_sents))\n",
      "0.79796012981\n",
      "\n",
      "\n",
      "Example 1.7 (code_consecutive_pos_tagger.py): Figure 1.7: Part of Speech Tagging with a Consecutive Classifier\n",
      "\n",
      "\n",
      "\n",
      "1.7   Other Methods for Sequence Classification\n",
      "One shortcoming of this approach is that we commit to every decision\n",
      "that we make.  For example, if we decide to label a word as a noun,\n",
      "but later find evidence that it should have been a verb, there's no\n",
      "way to go back and fix our mistake.  One solution to this problem is\n",
      "to adopt a transformational strategy instead.  Transformational joint\n",
      "classifiers work by creating an initial assignment of labels for the\n",
      "inputs, and then iteratively refining that assignment in an attempt to\n",
      "repair inconsistencies between related inputs.  The Brill tagger,\n",
      "described in (1), is a good example of this strategy.\n",
      "Another solution is to assign scores to all of the possible\n",
      "sequences of part-of-speech tags, and to choose the sequence\n",
      "whose overall score is highest. This is the approach taken by\n",
      "Hidden Markov Models.\n",
      "Hidden Markov Models are similar to consecutive classifiers in\n",
      "that they look at both the inputs and the history of predicted\n",
      "tags. However, rather than simply finding the single best tag\n",
      "for a given word, they generate a probability distribution over\n",
      "tags. These probabilities are then combined to calculate\n",
      "probability scores for tag sequences, and the tag sequence with\n",
      "the highest probability is chosen. Unfortunately, the number of\n",
      "possible tag sequences is quite large. Given a tag set with 30\n",
      "tags, there are about 600 trillion (3010) ways to label\n",
      "a 10-word sentence.  In order to avoid considering all these possible\n",
      "sequences separately, Hidden Markov Models require that the\n",
      "feature extractor only look at the most recent tag (or the most\n",
      "recent n tags, where n is fairly small). Given that\n",
      "restriction, it is possible to use dynamic programming (4.7)\n",
      "to efficiently find the most likely tag sequence. In particular,\n",
      "for each consecutive word index i,\n",
      "a score is computed for each possible current and previous tag.\n",
      "This same basic approach is taken by\n",
      "two more advanced models, called Maximum Entropy Markov Models and\n",
      "Linear-Chain Conditional Random Field Models;\n",
      "but different algorithms are used to find scores for tag sequences.\n",
      "\n",
      "\n",
      "\n",
      "2   Further Examples of Supervised Classification\n",
      "\n",
      "2.1   Sentence Segmentation\n",
      "Sentence segmentation can be viewed as a classification task for\n",
      "punctuation: whenever we encounter a symbol that could possibly end a\n",
      "sentence, such as a period or a question mark, we have to decide\n",
      "whether it terminates the preceding sentence.\n",
      "The first step is to obtain some data that has already been segmented\n",
      "into sentences and convert it into a form that is suitable for\n",
      "extracting features:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sents = nltk.corpus.treebank_raw.sents()\n",
      ">>> tokens = []\n",
      ">>> boundaries = set()\n",
      ">>> offset = 0\n",
      ">>> for sent in sents:\n",
      "...     tokens.extend(sent)\n",
      "...     offset += len(sent)\n",
      "...     boundaries.add(offset-1)\n",
      "\n",
      "\n",
      "\n",
      "Here, tokens is a merged list of tokens from the individual\n",
      "sentences, and boundaries is a set containing the indexes of all\n",
      "sentence-boundary tokens.  Next, we need to specify the features of\n",
      "the data that will be used in order to decide whether punctuation\n",
      "indicates a sentence-boundary:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def punct_features(tokens, i):\n",
      "...     return {'next-word-capitalized': tokens[i+1][0].isupper(),\n",
      "...             'prev-word': tokens[i-1].lower(),\n",
      "...             'punct': tokens[i],\n",
      "...             'prev-word-is-one-char': len(tokens[i-1]) == 1}\n",
      "\n",
      "\n",
      "\n",
      "Based on this feature extractor, we can create a list of labeled\n",
      "featuresets by selecting all the punctuation tokens, and tagging\n",
      "whether they are boundary tokens or not:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> featuresets = [(punct_features(tokens, i), (i in boundaries))\n",
      "...                for i in range(1, len(tokens)-1)\n",
      "...                if tokens[i] in '.?!']\n",
      "\n",
      "\n",
      "\n",
      "Using these featuresets, we can train and evaluate a\n",
      "punctuation classifier:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> size = int(len(featuresets) * 0.1)\n",
      ">>> train_set, test_set = featuresets[size:], featuresets[:size]\n",
      ">>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      ">>> nltk.classify.accuracy(classifier, test_set)\n",
      "0.936026936026936\n",
      "\n",
      "\n",
      "\n",
      "To use this classifier to perform sentence segmentation, we simply\n",
      "check each punctuation mark to see whether it's labeled as a boundary;\n",
      "and divide the list of words at the boundary marks.  The listing\n",
      "in 2.1 shows how this can be done.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def segment_sentences(words):\n",
      "    start = 0\n",
      "    sents = []\n",
      "    for i, word in enumerate(words):\n",
      "        if word in '.?!' and classifier.classify(punct_features(words, i)) == True:\n",
      "            sents.append(words[start:i+1])\n",
      "            start = i+1\n",
      "    if start < len(words):\n",
      "        sents.append(words[start:])\n",
      "    return sents\n",
      "\n",
      "\n",
      "Example 2.1 (code_classification_based_segmenter.py): Figure 2.1: Classification Based Sentence Segmenter\n",
      "\n",
      "\n",
      "\n",
      "2.2   Identifying Dialogue Act Types\n",
      "When processing dialogue, it can be useful to think of\n",
      "utterances as a type of action performed by the speaker.  This\n",
      "interpretation is most straightforward for performative statements\n",
      "such as \"I forgive you\" or \"I bet you can't climb that hill.\"  But\n",
      "greetings, questions, answers, assertions, and clarifications can all\n",
      "be thought of as types of speech-based actions.  Recognizing the\n",
      "dialogue acts underlying the utterances in a dialogue can be an\n",
      "important first step in understanding the conversation.\n",
      "The NPS Chat Corpus, which was demonstrated in\n",
      "1, consists of over 10,000 posts from\n",
      "instant messaging sessions.  These posts have all been labeled with\n",
      "one of 15 dialogue act types, such as \"Statement,\" \"Emotion,\"\n",
      "\"ynQuestion\", and \"Continuer.\"  We can therefore use this data to\n",
      "build a classifier that can identify the dialogue act types for new\n",
      "instant messaging posts.  The first step is to extract the basic\n",
      "messaging data.  We will call xml_posts() to get a data structure\n",
      "representing the XML annotation for each post:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
      "\n",
      "\n",
      "\n",
      "Next, we'll define a simple feature extractor that checks what words\n",
      "the post contains:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def dialogue_act_features(post):\n",
      "...     features = {}\n",
      "...     for word in nltk.word_tokenize(post):\n",
      "...         features['contains({})'.format(word.lower())] = True\n",
      "...     return features\n",
      "\n",
      "\n",
      "\n",
      "Finally, we construct the training and testing data by applying the\n",
      "feature extractor to each post (using post.get('class') to get\n",
      "a post's dialogue act type), and create a new classifier:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> featuresets = [(dialogue_act_features(post.text), post.get('class'))\n",
      "...                for post in posts]\n",
      ">>> size = int(len(featuresets) * 0.1)\n",
      ">>> train_set, test_set = featuresets[size:], featuresets[:size]\n",
      ">>> classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      ">>> print(nltk.classify.accuracy(classifier, test_set))\n",
      "0.67\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.3   Recognizing Textual Entailment\n",
      "Recognizing textual entailment (RTE) is the task of determining\n",
      "whether a given piece of text T entails another text called the\n",
      "\"hypothesis\" (as already discussed in 5).\n",
      "To date, there have been four RTE Challenges, where\n",
      "shared development and test data is made available to competing teams.\n",
      "Here are a couple of examples of text/hypothesis pairs from the\n",
      "Challenge 3 development dataset. The label True indicates that the\n",
      "entailment holds, and False, that it fails to hold.\n",
      "\n",
      "Challenge 3, Pair 34 (True)\n",
      "\n",
      "T: Parviz Davudi was representing Iran at a meeting of the Shanghai\n",
      "Co-operation Organisation (SCO), the fledgling association that\n",
      "binds Russia, China and four former Soviet republics of central\n",
      "Asia together to fight terrorism.\n",
      "H: China is a member of SCO.\n",
      "\n",
      "Challenge 3, Pair 81 (False)\n",
      "\n",
      "T: According to NC Articles of Organization, the members of LLC\n",
      "company are H. Nelson Beavers, III, H. Chester Beavers and Jennie\n",
      "Beavers Stewart.\n",
      "H: Jennie Beavers Stewart is a share-holder of Carolina Analytical\n",
      "Laboratory.\n",
      "\n",
      "\n",
      "It should be emphasized that the relationship between text and\n",
      "hypothesis is not intended to be logical entailment, but rather\n",
      "whether a human would conclude that the text provides reasonable\n",
      "evidence for taking the hypothesis to be true.\n",
      "We can treat RTE as a classification task, in which we try to\n",
      "predict the True/False label for each pair. Although it seems\n",
      "likely that successful approaches to this task will involve a\n",
      "combination of parsing, semantics and real world\n",
      "knowledge, many early attempts at RTE achieved reasonably good results\n",
      "with shallow analysis, based on similarity between the text and\n",
      "hypothesis at the word level. In the ideal case, we would expect that if\n",
      "there is an entailment, then all the information expressed by the hypothesis\n",
      "should also be present in the text. Conversely, if there is information\n",
      "found in the hypothesis that is absent from the text, then there\n",
      "will be no entailment.\n",
      "In our RTE feature detector (2.2), we let words\n",
      "(i.e., word types) serve as proxies for information, and\n",
      "our features count the degree of word overlap, and the degree to which\n",
      "there are words in the hypothesis but not in the text (captured by the\n",
      "method hyp_extra()). Not all words are equally important —\n",
      "Named Entity mentions such as the names of people, organizations and\n",
      "places are likely to be more significant, which motivates us to\n",
      "extract distinct information for words  and nes (Named\n",
      "Entities). In addition, some high frequency function words are\n",
      "filtered out as \"stopwords\".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[xx]give some intro to RTEFeatureExtractor??\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def rte_features(rtepair):\n",
      "    extractor = nltk.RTEFeatureExtractor(rtepair)\n",
      "    features = {}\n",
      "    features['word_overlap'] = len(extractor.overlap('word'))\n",
      "    features['word_hyp_extra'] = len(extractor.hyp_extra('word'))\n",
      "    features['ne_overlap'] = len(extractor.overlap('ne'))\n",
      "    features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))\n",
      "    return features\n",
      "\n",
      "\n",
      "Example 2.2 (code_rte_features.py): Figure 2.2: \"Recognizing Text Entailment\" Feature Extractor.  The\n",
      "RTEFeatureExtractor class builds a bag\n",
      "of words for both the text and the hypothesis after throwing\n",
      "away some stopwords, then calculates overlap and difference.\n",
      "\n",
      "To illustrate the content of these features, we examine some\n",
      "attributes of the text/hypothesis Pair 34 shown earlier:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]\n",
      ">>> extractor = nltk.RTEFeatureExtractor(rtepair)\n",
      ">>> print(extractor.text_words)\n",
      "{'Russia', 'Organisation', 'Shanghai', 'Asia', 'four', 'at',\n",
      "'operation', 'SCO', ...}\n",
      ">>> print(extractor.hyp_words)\n",
      "{'member', 'SCO', 'China'}\n",
      ">>> print(extractor.overlap('word'))\n",
      "set()\n",
      ">>> print(extractor.overlap('ne'))\n",
      "{'SCO', 'China'}\n",
      ">>> print(extractor.hyp_extra('word'))\n",
      "{'member'}\n",
      "\n",
      "\n",
      "\n",
      "These features indicate that all important words in the hypothesis are\n",
      "contained in the text, and thus there is some evidence for labeling this\n",
      "as True.\n",
      "The module nltk.classify.rte_classify reaches just over 58%\n",
      "accuracy on the combined RTE test data using methods like these. Although\n",
      "this figure is not very impressive, it requires significant effort, and more\n",
      "linguistic processing, to achieve much better results.\n",
      "\n",
      "\n",
      "2.4   Scaling Up to Large Datasets\n",
      "Python provides an excellent environment for performing basic text\n",
      "processing and feature extraction.  However, it is not able to perform\n",
      "the numerically intensive calculations required by machine learning\n",
      "methods nearly as quickly as lower-level languages such as C.  Thus,\n",
      "if you attempt to use the pure-Python machine learning implementations\n",
      "(such as nltk.NaiveBayesClassifier) on large datasets, you may\n",
      "find that the learning algorithm takes an unreasonable amount of time\n",
      "and memory to complete.\n",
      "If you plan to train classifiers with large amounts of training data\n",
      "or a large number of features, we recommend that you explore\n",
      "NLTK's facilities for interfacing with external machine learning\n",
      "packages.  Once these packages have been installed, NLTK can\n",
      "transparently invoke them (via system calls) to train classifier\n",
      "models significantly faster than the pure-Python classifier\n",
      "implementations.  See the NLTK webpage for a list of recommended\n",
      "machine learning packages that are supported by NLTK.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3   Evaluation\n",
      "In order to decide whether a classification model is accurately\n",
      "capturing a pattern, we must evaluate that model.  The result of this\n",
      "evaluation is important for deciding how trustworthy the model is, and\n",
      "for what purposes we can use it.  Evaluation can also be an effective tool\n",
      "for guiding us in making future improvements to the model.\n",
      "\n",
      "3.1   The Test Set\n",
      "Most evaluation techniques calculate a score for a model by comparing\n",
      "the labels that it generates for the inputs in a test set\n",
      "(or evaluation set)\n",
      "with the correct labels for those inputs.  This test set\n",
      "typically has the same format as the training set.  However, it is\n",
      "very important that the test set be distinct from the training\n",
      "corpus: if we simply re-used the training set as the test\n",
      "set, then a model that simply memorized its input, without learning\n",
      "how to generalize to new examples, would receive misleadingly high\n",
      "scores.\n",
      "When building the test set, there is often a trade-off between\n",
      "the amount of data available for testing and the amount available\n",
      "for training.  For classification tasks that have a small\n",
      "number of well-balanced labels and a diverse test set, a\n",
      "meaningful evaluation can be performed with as few as 100 evaluation\n",
      "instances.  But if a classification task has a large number of labels,\n",
      "or includes very infrequent labels, then the size of the test\n",
      "set should be chosen to ensure that the least frequent label occurs at\n",
      "least 50 times.  Additionally, if the test set contains many\n",
      "closely related instances — such as instances drawn from a single\n",
      "document — then the size of the test set should be increased to\n",
      "ensure that this lack of diversity does not skew the evaluation\n",
      "results.  When large amounts of annotated data are available, it is\n",
      "common to err on the side of safety by using 10% of the overall data\n",
      "for evaluation.\n",
      "Another consideration when choosing the test set is the degree\n",
      "of similarity between instances in the test set and those in the\n",
      "development set.  The more similar these two datasets are, the less\n",
      "confident we can be that evaluation results will generalize to other\n",
      "datasets.  For example, consider the part-of-speech tagging task.  At\n",
      "one extreme, we could create the training set and test set by\n",
      "randomly assigning sentences from a data source that reflects a single\n",
      "genre (news):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> import random\n",
      ">>> from nltk.corpus import brown\n",
      ">>> tagged_sents = list(brown.tagged_sents(categories='news'))\n",
      ">>> random.shuffle(tagged_sents)\n",
      ">>> size = int(len(tagged_sents) * 0.1)\n",
      ">>> train_set, test_set = tagged_sents[size:], tagged_sents[:size]\n",
      "\n",
      "\n",
      "\n",
      "In this case, our test set will be very similar to our training\n",
      "set.  The training set and test set are taken from the same\n",
      "genre, and so we cannot be confident that evaluation results would\n",
      "generalize to other genres.  What's worse, because of the call to\n",
      "random.shuffle(), the test set contains sentences that are\n",
      "taken from the same documents that were used for training.  If there\n",
      "is any consistent pattern within a document — say, if a given word\n",
      "appears with a particular part-of-speech tag especially frequently — then\n",
      "that difference will be reflected in both the development set and the\n",
      "test set.  A somewhat better approach is to ensure that\n",
      "the training set and test set are taken from different documents:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> file_ids = brown.fileids(categories='news')\n",
      ">>> size = int(len(file_ids) * 0.1)\n",
      ">>> train_set = brown.tagged_sents(file_ids[size:])\n",
      ">>> test_set = brown.tagged_sents(file_ids[:size])\n",
      "\n",
      "\n",
      "\n",
      "If we want to perform a more stringent evaluation, we can draw the\n",
      "test set from documents that are less closely related to those\n",
      "in the training set:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> train_set = brown.tagged_sents(categories='news')\n",
      ">>> test_set = brown.tagged_sents(categories='fiction')\n",
      "\n",
      "\n",
      "\n",
      "If we build a classifier that performs well on this test set,\n",
      "then we can be confident that it has the power to generalize well\n",
      "beyond the data that it was trained on.\n",
      "\n",
      "\n",
      "3.2   Accuracy\n",
      "The simplest metric that can be used to evaluate a classifier,\n",
      "accuracy, measures the percentage of inputs in the test\n",
      "set that the classifier correctly labeled.  For example, a name gender\n",
      "classifier that predicts the correct name 60 times in a test\n",
      "set containing 80 names would have an accuracy of 60/80 = 75%.  The\n",
      "function nltk.classify.accuracy() will calculate the\n",
      "accuracy of a classifier model on a given test set:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> classifier = nltk.NaiveBayesClassifier.train(train_set) \n",
      ">>> print('Accuracy: {:4.2f}'.format(nltk.classify.accuracy(classifier, test_set))) \n",
      "0.75\n",
      "\n",
      "\n",
      "\n",
      "When interpreting the accuracy score of a classifier, it is important\n",
      "to take into consideration the frequencies of the individual class\n",
      "labels in the test set.  For example, consider a classifier that\n",
      "determines the correct word sense for each occurrence of the word\n",
      "bank.  If we evaluate this classifier on financial newswire text,\n",
      "then we may find that the financial-institution sense appears 19\n",
      "times out of 20.  In that case, an accuracy of 95% would hardly be\n",
      "impressive, since we could achieve that accuracy with a model that\n",
      "always returns the financial-institution sense.  However, if we\n",
      "instead evaluate the classifier on a more balanced corpus, where the\n",
      "most frequent word sense has a frequency of 40%, then a 95% accuracy\n",
      "score would be a much more positive result.  (A similar issue arises\n",
      "when measuring inter-annotator agreement in\n",
      "2.)\n",
      "\n",
      "\n",
      "3.3   Precision and Recall\n",
      "Another instance where accuracy scores can be misleading is in\n",
      "\"search\" tasks, such as information retrieval, where we are attempting\n",
      "to find documents that are relevant to a particular task.  Since the\n",
      "number of irrelevant documents far outweighs the number of relevant\n",
      "documents, the accuracy score for a model that labels every document\n",
      "as irrelevant would be very close to 100%.\n",
      "\n",
      "\n",
      "Figure 3.1: True and False Positives and Negatives\n",
      "\n",
      "It is therefore conventional to employ a different set of measures for\n",
      "search tasks, based on the number of items in each of the four\n",
      "categories shown in 3.1:\n",
      "\n",
      "True positives are relevant items that we correctly identified\n",
      "as relevant.\n",
      "True negatives are irrelevant items that we correctly identified\n",
      "as irrelevant.\n",
      "False positives (or Type I errors) are irrelevant items\n",
      "that we incorrectly identified as relevant.\n",
      "False negatives (or Type II errors) are relevant items\n",
      "that we incorrectly identified as irrelevant.\n",
      "\n",
      "Given these four numbers, we can define the following metrics:\n",
      "\n",
      "Precision, which indicates how many of the items that we\n",
      "identified were relevant, is TP/(TP+FP).\n",
      "Recall, which indicates how many of the relevant items that we\n",
      "identified, is TP/(TP+FN).\n",
      "The F-Measure (or F-Score), which combines the precision\n",
      "and recall to give a single score, is defined to be the harmonic\n",
      "mean of the precision and recall:\n",
      "(2 × Precision × Recall) / (Precision + Recall).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.4   Confusion Matrices\n",
      "\n",
      "When performing classification tasks with three or more labels, it can\n",
      "be informative to subdivide the errors made by the model based on\n",
      "which types of mistake it made.  A confusion matrix is a table\n",
      "where each cell [i,j] indicates how often label j was\n",
      "predicted when the correct label was i.  Thus, the diagonal\n",
      "entries (i.e., cells |ii|) indicate labels that were\n",
      "correctly predicted, and the off-diagonal entries indicate errors.  In\n",
      "the following example, we generate a confusion matrix for the bigram\n",
      "tagger developed in 4:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def tag_list(tagged_sents):\n",
      "...     return [tag for sent in tagged_sents for (word, tag) in sent]\n",
      ">>> def apply_tagger(tagger, corpus):\n",
      "...     return [tagger.tag(nltk.tag.untag(sent)) for sent in corpus]\n",
      ">>> gold = tag_list(brown.tagged_sents(categories='editorial'))\n",
      ">>> test = tag_list(apply_tagger(t2, brown.tagged_sents(categories='editorial')))\n",
      ">>> cm = nltk.ConfusionMatrix(gold, test)\n",
      ">>> print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))\n",
      "    |                                         N                      |\n",
      "    |      N      I      A      J             N             V      N |\n",
      "    |      N      N      T      J      .      S      ,      B      P |\n",
      "----+----------------------------------------------------------------+\n",
      " NN | <11.8%>  0.0%      .   0.2%      .   0.0%      .   0.3%   0.0% |\n",
      " IN |   0.0%  <9.0%>     .      .      .   0.0%      .      .      . |\n",
      " AT |      .      .  <8.6%>     .      .      .      .      .      . |\n",
      " JJ |   1.7%      .      .  <3.9%>     .      .      .   0.0%   0.0% |\n",
      "  . |      .      .      .      .  <4.8%>     .      .      .      . |\n",
      "NNS |   1.5%      .      .      .      .  <3.2%>     .      .   0.0% |\n",
      "  , |      .      .      .      .      .      .  <4.4%>     .      . |\n",
      " VB |   0.9%      .      .   0.0%      .      .      .  <2.4%>     . |\n",
      " NP |   1.0%      .      .   0.0%      .      .      .      .  <1.8%>|\n",
      "----+----------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The confusion matrix indicates that common errors include a\n",
      "substitution of NN for JJ (for 1.6% of words), and of NN for NNS (for\n",
      "1.5% of words).  Note that periods (.) indicate cells\n",
      "whose value is 0, and that the diagonal entries — which correspond to\n",
      "correct classifications — are marked with angle brackets.\n",
      ".. XXX explain use of \"reference\" in the legend above.\n",
      "\n",
      "\n",
      "3.5   Cross-Validation\n",
      "In order to evaluate our models, we must reserve a portion of the\n",
      "annotated data for the test set.  As we already mentioned,\n",
      "if the test set is too small, then\n",
      "our evaluation may not be accurate.  However, making the test set\n",
      "larger usually means making the training set smaller, which can have a\n",
      "significant impact on performance if a limited amount of annotated\n",
      "data is available.\n",
      "One solution to this problem is to perform multiple evaluations on\n",
      "different test sets, then to combine the scores from those\n",
      "evaluations, a technique known as cross-validation.  In\n",
      "particular, we subdivide the original corpus into N\n",
      "subsets called folds.  For each of these folds, we train a model using all\n",
      "of the data except the data in that fold, and then test that\n",
      "model on the fold.  Even though the individual folds might\n",
      "be too small to give accurate evaluation scores on their own, the\n",
      "combined evaluation score is based on a large amount of data, and is\n",
      "therefore quite reliable.\n",
      "A second, and equally important, advantage of using cross-validation\n",
      "is that it allows us to examine how widely the performance varies\n",
      "across different training sets.  If we get very similar scores for all\n",
      "N training sets, then we can be fairly confident that the\n",
      "score is accurate.  On the other hand, if scores vary widely across\n",
      "the N training sets, then we should probably be skeptical\n",
      "about the accuracy of the evaluation score.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4   Decision Trees\n",
      "In the next three sections, we'll take a closer look at three machine\n",
      "learning methods that can be used to automatically build\n",
      "classification models: decision trees, naive Bayes classifiers, and\n",
      "Maximum Entropy classifiers.  As we've seen, it's possible to treat these\n",
      "learning methods as black boxes, simply training models and using them\n",
      "for prediction without understanding how they work.  But there's a lot\n",
      "to be learned from taking a closer look at how these learning methods\n",
      "select models based on the data in a training set.  An\n",
      "understanding of these methods can help guide our selection of\n",
      "appropriate features, and especially our decisions about how those\n",
      "features should be encoded.  And an understanding of the generated\n",
      "models can allow us to extract information about which features\n",
      "are most informative, and how those features relate to one another.\n",
      "\n",
      "\n",
      "A decision tree is a simple flowchart that selects\n",
      "labels for input values.  This flowchart consists of decision\n",
      "nodes, which check feature values, and leaf nodes, which\n",
      "assign labels.  To choose the label for an input value, we begin at\n",
      "the flowchart's initial decision node, known as its root node.\n",
      "This node contains a condition that checks one of the input value's\n",
      "features, and selects a branch based on that feature's value.\n",
      "Following the branch that describes our input value, we arrive at a\n",
      "new decision node, with a new condition on the input value's features.\n",
      "We continue following the branch selected by each node's condition,\n",
      "until we arrive at a leaf node which provides a label for the input\n",
      "value.  4.1 shows an example decision tree model for\n",
      "the name gender task.\n",
      "\n",
      "\n",
      "Figure 4.1: Decision Tree model for the name gender task.  Note that tree\n",
      "diagrams are conventionally drawn \"upside down,\" with the root at the\n",
      "top, and the leaves at the bottom.\n",
      "\n",
      "\n",
      "\n",
      "Once we have a decision tree, it is straightforward to\n",
      "use it to assign labels to new input values.  What's less straightforward\n",
      "is how we can build a decision tree that models a given\n",
      "training set.  But before we look at the learning algorithm for\n",
      "building decision trees, we'll consider a simpler task: picking the\n",
      "best \"decision stump\" for a corpus.  A decision stump is a\n",
      "decision tree with a single node that decides how to classify inputs\n",
      "based on a single feature.  It contains one leaf for each possible\n",
      "feature value, specifying the class label that should be assigned to\n",
      "inputs whose features have that value.  In order to build a decision\n",
      "stump, we must first decide which feature should be used.  The\n",
      "simplest method is to just build a decision stump for each possible\n",
      "feature, and see which one achieves the highest accuracy on the\n",
      "training data, although there are other alternatives that we will discuss below.\n",
      "Once we've picked a feature, we can build the decision stump by assigning a\n",
      "label to each leaf based on the most frequent label for the selected\n",
      "examples in the training set (i.e., the examples where the selected\n",
      "feature has that value).\n",
      "\n",
      "\n",
      "Given the algorithm for choosing decision stumps, the algorithm for\n",
      "growing larger decision trees is straightforward.  We\n",
      "begin by selecting the overall best decision stump for the\n",
      "classification task.  We\n",
      "then check the accuracy of each of the leaves on the training set.\n",
      "Leaves that do not achieve sufficient accuracy are then\n",
      "replaced by new decision stumps, trained on the subset of the training\n",
      "corpus that is selected by the path to the leaf.  For example, we\n",
      "could grow the decision tree in 4.1 by replacing the\n",
      "leftmost leaf with a new decision stump, trained on the subset of the\n",
      "training set names that do not start with a \"k\" or end with a vowel\n",
      "or an \"l.\"\n",
      "\n",
      "4.1   Entropy and Information Gain\n",
      "As was mentioned before, there are several methods for identifying\n",
      "the most informative feature for a decision stump.  One\n",
      "popular alternative, called information gain, measures how\n",
      "much more organized the input values become when we divide them up\n",
      "using a given feature.  To measure how disorganized the original set\n",
      "of input values are, we calculate entropy of their labels, which will\n",
      "be high if the input values have highly varied labels, and low if many\n",
      "input values all have the same label.  In particular, entropy is\n",
      "defined as the sum of the probability of each label times the log\n",
      "probability of that same label:\n",
      "\n",
      "  (1)H = −Σl |in| labelsP(l) × log2P(l).\n",
      "\n",
      "\n",
      "Figure 4.2: The entropy of labels in the name gender prediction task, as a\n",
      "function of the percentage of names in a given set that are male.\n",
      "\n",
      "For example, 4.2 shows how the entropy of labels in\n",
      "the name gender prediction task depends on the ratio of male to female\n",
      "names.  Note that if most input values have the same label (e.g., if\n",
      "P(male) is near 0 or near 1), then entropy is low.  In particular,\n",
      "labels that have low frequency do not contribute much to the entropy\n",
      "(since P(l) is small), and labels with high frequency also do\n",
      "not contribute much to the entropy (since log2P(l) is small).  On the other hand, if the input values have\n",
      "a wide variety of labels, then there are many labels with a \"medium\"\n",
      "frequency, where neither P(l) nor log2P(l) is small, so the entropy is high.\n",
      "4.3 demonstrates how to calculate the entropy\n",
      "of a list of labels.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "import math\n",
      "def entropy(labels):\n",
      "    freqdist = nltk.FreqDist(labels)\n",
      "    probs = [freqdist.freq(l) for l in freqdist]\n",
      "    return -sum(p * math.log(p,2) for p in probs)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(entropy(['male', 'male', 'male', 'male'])) \n",
      "0.0\n",
      ">>> print(entropy(['male', 'female', 'male', 'male']))\n",
      "0.811...\n",
      ">>> print(entropy(['female', 'male', 'female', 'male']))\n",
      "1.0\n",
      ">>> print(entropy(['female', 'female', 'male', 'female']))\n",
      "0.811...\n",
      ">>> print(entropy(['female', 'female', 'female', 'female'])) \n",
      "0.0\n",
      "\n",
      "\n",
      "Example 4.3 (code_entropy.py): Figure 4.3: Calculating the Entropy of a List of Labels\n",
      "\n",
      "\n",
      "Once we have calculated the entropy of the original set of input\n",
      "values' labels, we can determine how much more organized the labels\n",
      "become once we apply the decision stump.  To do so, we calculate the\n",
      "entropy for each of the decision stump's leaves, and take the average\n",
      "of those leaf entropy values (weighted by the number of samples in\n",
      "each leaf).  The information gain is then equal to the original\n",
      "entropy minus this new, reduced entropy.  The higher the information\n",
      "gain, the better job the decision stump does of dividing the input\n",
      "values into coherent groups, so we can build decision trees by\n",
      "selecting the decision stumps with the highest information gain.\n",
      "Another consideration for decision trees is efficiency.  The simple\n",
      "algorithm for selecting decision stumps described above must construct\n",
      "a candidate decision stump for every possible feature, and this\n",
      "process must be repeated for every node in the constructed decision\n",
      "tree.  A number of algorithms have been developed to cut down on the\n",
      "training time by storing and reusing information about previously\n",
      "evaluated examples.\n",
      "\n",
      "\n",
      "Decision trees have a number of useful qualities.  To begin with,\n",
      "they're simple to understand, and easy to interpret.  This is\n",
      "especially true near the top of the decision tree, where it is usually\n",
      "possible for the learning algorithm to find very useful features.\n",
      "Decision trees are especially well suited to cases where many\n",
      "hierarchical categorical distinctions can be made.  For example,\n",
      "decision trees can be very effective at capturing phylogeny trees.\n",
      "However, decision trees also have a few disadvantages.  One problem is\n",
      "that, since each branch in the decision tree splits the training data,\n",
      "the amount of training data available to train nodes lower in the tree\n",
      "can become quite small.  As a result, these lower decision nodes may\n",
      "\n",
      "overfit the training set, learning patterns that reflect\n",
      "idiosyncrasies of the training set rather than linguistically significant\n",
      "patterns in\n",
      "the underlying problem.  One solution to this problem is to stop\n",
      "dividing nodes once the amount of training data becomes too small.\n",
      "Another solution is to grow a full decision tree, but then to\n",
      "prune decision nodes that do not improve performance on a\n",
      "dev-test.\n",
      "A second problem with decision trees is that they force features to be\n",
      "checked in a specific order, even when features may act relatively\n",
      "independently of one another.  For example, when classifying documents\n",
      "into topics (such as sports, automotive, or murder mystery), features\n",
      "such as hasword(football) are highly indicative of a specific\n",
      "label, regardless of what other the feature values are.  Since there\n",
      "is limited space near the top of the decision tree, most of these\n",
      "features will need to be repeated on many different branches in the\n",
      "tree.  And since the number of branches increases exponentially as we\n",
      "go down the tree, the amount of repetition can be very large.\n",
      "A related problem is that decision trees are not good at making use of\n",
      "features that are weak predictors of the correct label.  Since these\n",
      "features make relatively small incremental improvements, they tend to\n",
      "occur very low in the decision tree.  But by the time the decision\n",
      "tree learner has descended far enough to use these features, there is\n",
      "not enough training data left to reliably determine what effect they\n",
      "should have.  If we could instead look at the effect of these features\n",
      "across the entire training set, then we might be able to make some\n",
      "conclusions about how they should affect the choice of label.\n",
      "The fact that decision trees require that features be checked in a\n",
      "specific order limits their ability to exploit features that are\n",
      "relatively independent of one another.  The naive Bayes classification\n",
      "method, which we'll discuss next, overcomes this limitation by\n",
      "allowing all features to act \"in parallel.\"\n",
      "\n",
      "\n",
      "\n",
      "5   Naive Bayes Classifiers\n",
      "In naive Bayes classifiers, every feature gets a say in\n",
      "determining which label should be assigned to a given input value.  To\n",
      "choose a label for an input value, the naive Bayes classifier begins\n",
      "by calculating the prior probability of each label, which is\n",
      "determined by checking frequency of each label in the training set.\n",
      "The contribution from each feature is then combined with this prior\n",
      "probability, to arrive at a likelihood estimate for each label.  The\n",
      "label whose likelihood estimate is the highest is then assigned to the\n",
      "input value.  5.1 illustrates this process.\n",
      "\n",
      "\n",
      "\n",
      "Figure 5.1: An abstract illustration of the procedure used by the naive Bayes\n",
      "classifier to choose the topic for a document.  In the training\n",
      "corpus, most documents are automotive, so the classifier starts out\n",
      "at a point closer to the \"automotive\" label.  But it then\n",
      "considers the effect of each feature.  In this example, the input\n",
      "document contains the word \"dark,\" which is a weak indicator for\n",
      "murder mysteries, but it also contains the word \"football,\" which\n",
      "is a strong indicator for sports documents.  After every feature\n",
      "has made its contribution, the classifier checks which label it is\n",
      "closest to, and assigns that label to the input.\n",
      "\n",
      "Individual features make their contribution to the overall decision by\n",
      "\"voting against\" labels that don't occur with that feature very often.\n",
      "In particular, the likelihood score for each label is reduced by\n",
      "multiplying it by the probability that an input value with that label\n",
      "would have the feature.  For example, if the word run occurs in 12%\n",
      "of the sports documents, 10% of the murder mystery documents, and 2%\n",
      "of the automotive documents, then the likelihood score for the sports\n",
      "label will be multiplied by 0.12; the likelihood score for the murder\n",
      "mystery label will be multiplied by 0.1, and the likelihood score for\n",
      "the automotive label will be multiplied by 0.02.  The overall effect\n",
      "will be to reduce the score of the murder mystery label slightly more\n",
      "than the score of the sports label, and to significantly reduce the\n",
      "automotive label with respect to the other two labels.  This\n",
      "process is illustrated in 5.2 and\n",
      "5.3.\n",
      "\n",
      "\n",
      "Figure 5.2: Calculating label likelihoods with naive Bayes.  Naive Bayes begins\n",
      "by calculating the prior probability of each label, based on how\n",
      "frequently each label occurs in the training data.  Every feature\n",
      "then contributes to the likelihood estimate for each label, by\n",
      "multiplying it by the probability that input values with that label\n",
      "will have that feature.  The resulting likelihood score can be\n",
      "thought of as an estimate of the probability that a randomly\n",
      "selected value from the training set would have both the given\n",
      "label and the set of features, assuming that the feature\n",
      "probabilities are all independent.\n",
      "\n",
      "\n",
      "5.1   Underlying Probabilistic Model\n",
      "Another way of understanding the naive Bayes classifier is that it\n",
      "chooses the most likely label for an input, under the assumption that\n",
      "every input value is generated by first choosing a class label for\n",
      "that input value, and then generating each feature, entirely\n",
      "independent of every other feature.  Of course, this assumption is\n",
      "unrealistic; features are often highly dependent on one another.  We'll\n",
      "return to some of the consequences of this assumption at the end of\n",
      "this section.  This simplifying assumption, known as the\n",
      "naive Bayes assumption (or independence assumption)\n",
      "makes it much\n",
      "easier to combine the contributions of the different features, since\n",
      "we don't need to worry about how they should interact with one\n",
      "another.\n",
      "\n",
      "\n",
      "Figure 5.3: A Bayesian Network Graph illustrating the generative process\n",
      "that is assumed by the naive Bayes classifier.  To generate a\n",
      "labeled input, the model first chooses a label for the input,\n",
      "then it generates each of the input's features based on that label.\n",
      "Every feature is assumed to be entirely independent of every other\n",
      "feature, given the label.\n",
      "\n",
      "Based on this assumption, we can calculate an expression for\n",
      "P(label|features), the probability that an input will have a\n",
      "particular label given that it has a particular set of features.  To\n",
      "choose a label for a new input, we can then simply pick the label\n",
      "l that maximizes P(l|features).\n",
      "To begin, we note that P(label|features) is equal to the\n",
      "probability that an input has a particular label and the specified\n",
      "set of features, divided by the probability that it has the specified\n",
      "set of features:\n",
      "\n",
      "  (2)P(label|features) = P(features, label)/P(features)\n",
      "Next, we note that P(features) will be the same for every\n",
      "choice of label, so if we are simply interested in finding the most\n",
      "likely label, it suffices to calculate P(features, label),\n",
      "which we'll call the label likelihood.\n",
      "\n",
      "Note\n",
      "If we want to generate a probability estimate for each\n",
      "label, rather than just choosing the most likely label, then the\n",
      "easiest way to compute P(features) is to simply calculate the sum\n",
      "over labels of P(features, label):\n",
      "\n",
      "  (3)P(features) =\n",
      "Σl in| labels P(features, label)\n",
      "\n",
      "The label likelihood can be expanded out as the probability of the\n",
      "label times the probability of the features given the label:\n",
      "\n",
      "  (4)P(features, label) = P(label) × P(features|label)\n",
      "Furthermore, since the features are all independent of one another\n",
      "(given the label), we can separate out the probability of each\n",
      "individual feature:\n",
      "\n",
      "  (5)P(features, label) = P(label) × Prodf in| featuresP(f|label)`\n",
      "This is exactly the equation we discussed above for calculating the\n",
      "label likelihood: P(label) is the prior probability for a\n",
      "given label, and each P(f|label) is the contribution of a single\n",
      "feature to the label likelihood.\n",
      "\n",
      "\n",
      "5.2   Zero Counts and Smoothing\n",
      "The simplest way to calculate P(f|label), the contribution of a\n",
      "feature f toward the label likelihood for a label label, is to\n",
      "take the percentage of training instances with the given label that\n",
      "also have the given feature:\n",
      "\n",
      "  (6)P(f|label) = count(f, label) / count(label)\n",
      "However, this simple approach can become problematic when a feature\n",
      "never occurs with a given label in the training set.  In this\n",
      "case, our calculated value for P(f|label) will be zero, which will\n",
      "cause the label likelihood for the given label to be zero.  Thus, the\n",
      "input will never be assigned this label, regardless of how well the\n",
      "other features fit the label.\n",
      "The basic problem here is with our calculation of P(f|label), the\n",
      "probability that an input will have a feature, given a label.  In\n",
      "particular, just because we haven't seen a feature/label combination\n",
      "occur in the training set, doesn't mean it's impossible for that\n",
      "combination to occur.  For example, we may not have seen any murder\n",
      "mystery documents that contained the word \"football,\" but we wouldn't\n",
      "want to conclude that it's completely impossible for such documents to\n",
      "exist.\n",
      "Thus, although count(f,label)/count(label) is a good estimate for\n",
      "P(f|label) when count(f, label) is relatively high, this\n",
      "estimate becomes less reliable when count(f) becomes smaller.\n",
      "Therefore, when building naive Bayes models, we usually employ\n",
      "more sophisticated techniques, known as smoothing techniques,\n",
      "for calculating P(f|label), the probability of a feature given a\n",
      "label.  For example, the Expected Likelihood Estimation for the\n",
      "probability of a feature given a label basically adds 0.5 to each\n",
      "count(f,label) value, and the Heldout Estimation uses a heldout\n",
      "corpus to calculate the relationship between feature frequencies and\n",
      "feature probabilities.  The nltk.probability module provides support\n",
      "for a wide variety of smoothing techniques.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5.3   Non-Binary Features\n",
      "We have assumed here that each feature is binary, i.e.\n",
      "that each input either has a feature or does not.  Label-valued\n",
      "features (e.g., a color feature which could be red, green, blue,\n",
      "white, or orange) can be converted to binary features by replacing\n",
      "them with binary features such as \"color-is-red\".  Numeric features can be\n",
      "converted to binary features by binning, which replaces them with\n",
      "features such as \"4<x<6\".\n",
      "Another alternative is to use regression methods to model the\n",
      "probabilities of numeric features.  For example, if we assume that the\n",
      "height feature has a bell curve distribution, then we could estimate\n",
      "P(height|label) by finding the mean and variance of the heights of the\n",
      "inputs with each label.  In this case, P(f=v|label) would not\n",
      "be a fixed value, but would vary depending on the value of v.\n",
      "\n",
      "\n",
      "5.4   The Naivete of Independence\n",
      "The reason that naive Bayes classifiers are called \"naive\" is that\n",
      "it's unreasonable to assume that all features are independent of one\n",
      "another (given the label).  In particular, almost all real-world\n",
      "problems contain features with varying degrees of dependence on one\n",
      "another.  If we had to avoid any features that were dependent on one\n",
      "another, it would be very difficult to construct good feature sets\n",
      "that provide the required information to the machine learning\n",
      "algorithm.\n",
      "So what happens when we ignore the independence assumption, and use\n",
      "the naive Bayes classifier with features that are not independent?\n",
      "One problem that arises is that the classifier can end up\n",
      "\"double-counting\" the effect of highly correlated features, pushing\n",
      "the classifier closer to a given label than is justified.\n",
      "To see how this can occur, consider a name gender classifier that\n",
      "contains two identical features, f1 and f2.  In other words, f2 is an exact copy of\n",
      "f1, and contains no new information.\n",
      "When the classifier is considering an input, it will include the\n",
      "contribution of both f1 and f2 when\n",
      "deciding which label to choose.  Thus, the information content of\n",
      "these two features will be given more weight than it deserves.\n",
      "Of course, we don't usually build naive Bayes classifiers that contain\n",
      "two identical features.  However, we do build classifiers that contain\n",
      "features which are dependent on one another.  For example, the\n",
      "features ends-with(a) and ends-with(vowel) are dependent on\n",
      "one another, because if an input value has the first feature, then it\n",
      "must also have the second feature.  For features like these, the\n",
      "duplicated information may be given more weight than is justified by\n",
      "the training set.\n",
      "\n",
      "\n",
      "5.5   The Cause of Double-Counting\n",
      "The reason for the double-counting problem is that\n",
      "during training, feature contributions are computed separately;\n",
      "but when using the classifier to choose labels for new inputs, those\n",
      "feature contributions are combined.  One solution, therefore, is to\n",
      "consider the possible interactions between feature contributions\n",
      "during training.  We could then use those interactions to adjust the\n",
      "contributions that individual features make.\n",
      "To make this more precise, we can rewrite the equation used to\n",
      "calculate the likelihood of a label, separating out the\n",
      "contribution made by each feature (or label):\n",
      "\n",
      "\n",
      "  (7)P(features, label) = w[label] × Prodf |in| features w[f, label]\n",
      "Here, w[label] is the \"starting score\" for a given label, and\n",
      "w[f, label] is the contribution made by a given feature\n",
      "towards a label's likelihood.  We call these values w[label]\n",
      "and w[f, label] the parameters or weights for the\n",
      "model.  Using the naive Bayes algorithm, we set each of these\n",
      "parameters independently:\n",
      "\n",
      "  (8)w[label] = P(label)\n",
      "\n",
      "  (9)w[f, label] = P(f|label)\n",
      "However, in the next section, we'll look at a classifier that\n",
      "considers the possible interactions between these parameters when\n",
      "choosing their values.\n",
      "\n",
      "\n",
      "\n",
      "6   Maximum Entropy Classifiers\n",
      "The Maximum Entropy classifier uses a model that is very\n",
      "similar to the model employed by the naive Bayes classifier.  But rather\n",
      "than using probabilities to set the model's parameters, it uses search\n",
      "techniques to find a set of parameters that will maximize the\n",
      "performance of the classifier.  In particular, it looks for the set of\n",
      "parameters that maximizes the total likelihood of the training\n",
      "corpus, which is defined as:\n",
      "\n",
      "  (10)P(features) =\n",
      "Σx |in| corpus P(label(x)|features(x))\n",
      "Where P(label|features), the probability that an input whose\n",
      "features are features will have class label label, is defined as:\n",
      "\n",
      "  (11)P(label|features) = P(label, features) /\n",
      "Σlabel P(label, features)\n",
      "Because of the potentially complex interactions between the effects of\n",
      "related features, there is no way to directly calculate the model\n",
      "parameters that maximize the likelihood of the training set.\n",
      "Therefore, Maximum Entropy classifiers choose the model parameters\n",
      "using iterative optimization techniques, which initialize the\n",
      "model's parameters to random values, and then repeatedly refine those\n",
      "parameters to bring them closer to the optimal solution.  These\n",
      "iterative optimization techniques guarantee that each refinement of\n",
      "the parameters will bring them closer to the optimal values, but do\n",
      "not necessarily provide a means of determining when those optimal\n",
      "values have been reached.  Because the parameters for Maximum Entropy\n",
      "classifiers are selected using iterative optimization techniques, they\n",
      "can take a long time to learn.  This is especially true when the size\n",
      "of the training set, the number of features, and the number of\n",
      "labels are all large.\n",
      "\n",
      "Note\n",
      "Some iterative optimization techniques are much faster than\n",
      "others.  When training Maximum Entropy models, avoid the use of\n",
      "Generalized Iterative Scaling (GIS) or Improved Iterative Scaling\n",
      "(IIS), which are both considerably slower than the Conjugate\n",
      "Gradient (CG) and the BFGS optimization methods.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6.1   The Maximum Entropy Model\n",
      "The Maximum Entropy classifier model is a generalization of the model\n",
      "used by the naive Bayes classifier.  Like the naive Bayes model, the\n",
      "Maximum Entropy classifier calculates the likelihood of each label for\n",
      "a given input value by multiplying together the parameters that are\n",
      "applicable for the input value and label.  The naive Bayes classifier\n",
      "model defines a parameter for each label, specifying its prior\n",
      "probability, and a parameter for each (feature, label) pair,\n",
      "specifying the contribution of individual features towards a label's\n",
      "likelihood.\n",
      "In contrast, the Maximum Entropy classifier model leaves it up to the\n",
      "user to decide what combinations of labels and features should receive\n",
      "their own parameters.  In particular, it is possible to use a single\n",
      "parameter to associate a feature with more than one label; or to\n",
      "associate more than one feature with a given label.  This will\n",
      "sometimes allow the model to \"generalize\" over some of the\n",
      "differences between related labels or features.\n",
      "Each combination of labels and features that receives its own\n",
      "parameter is called a joint-feature.  Note that joint-features\n",
      "are properties of labeled values, whereas (simple) features are\n",
      "properties of unlabeled values.\n",
      "\n",
      "Note\n",
      "In literature that describes and discusses Maximum Entropy\n",
      "models, the term \"features\" often refers to\n",
      "joint-features; the term \"contexts\" refers to what\n",
      "we have been calling (simple) features.\n",
      "\n",
      "Typically, the joint-features that are used to construct Maximum\n",
      "Entropy models exactly mirror those that are used by the naive Bayes\n",
      "model.  In particular, a joint-feature is defined for each label,\n",
      "corresponding to w[label], and for each combination of\n",
      "(simple) feature and label, corresponding to w[f,label].\n",
      "Given the joint-features for a Maximum Entropy model, the score\n",
      "assigned to a label for a given input is simply the product of the\n",
      "parameters associated with the joint-features that apply to that input\n",
      "and label:\n",
      "\n",
      "\n",
      "  (12)P(input, label) = Prodjoint-features(input,label) w[joint-feature]\n",
      "\n",
      "\n",
      "6.2   Maximizing Entropy\n",
      "The intuition that motivates Maximum Entropy classification is that we\n",
      "should build a model that captures the frequencies of individual\n",
      "joint-features, without making any unwarranted assumptions.\n",
      "An example will help to illustrate this principle.\n",
      "Suppose we are assigned the task of picking the correct word sense for\n",
      "a given word, from a list of ten possible senses (labeled A-J).  At\n",
      "first, we are not told anything more about the word or the senses.\n",
      "There are many probability distributions that we could choose for the\n",
      "ten senses, such as:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "H\n",
      "I\n",
      "J\n",
      "\n",
      "\n",
      "\n",
      "(i)\n",
      "10%\n",
      "10%\n",
      "10%\n",
      "10%\n",
      "10%\n",
      "10%\n",
      "10%\n",
      "10%\n",
      "10%\n",
      "10%\n",
      "\n",
      "(ii)\n",
      "5%\n",
      "15%\n",
      "0%\n",
      "30%\n",
      "0%\n",
      "8%\n",
      "12%\n",
      "0%\n",
      "6%\n",
      "24%\n",
      "\n",
      "(iii)\n",
      "0%\n",
      "100%\n",
      "0%\n",
      "0%\n",
      "0%\n",
      "0%\n",
      "0%\n",
      "0%\n",
      "0%\n",
      "0%\n",
      "\n",
      "\n",
      "Table 6.1\n",
      "\n",
      "Although any of these distributions might be correct, we are likely\n",
      "to choose distribution (i), because without any more information,\n",
      "there is no reason to believe that any word sense is more likely than\n",
      "any other.  On the other hand, distributions (ii) and (iii) reflect\n",
      "assumptions that are not supported by what we know.\n",
      "One way to capture this intuition that distribution (i) is more \"fair\"\n",
      "than the other two is to invoke the concept of entropy.  In the\n",
      "discussion of decision trees, we described entropy as a measure of how\n",
      "\"disorganized\" a set of labels was.  In particular, if a single label\n",
      "dominates then entropy is low, but if the labels are more evenly\n",
      "distributed then entropy is high.  In our example, we chose\n",
      "distribution (i) because its label probabilities are evenly\n",
      "distributed — in other words, because its entropy is high.  In\n",
      "general, the Maximum Entropy principle states that, among the\n",
      "distributions that are consistent with what we know, we should choose\n",
      "the distribution whose entropy is highest.\n",
      "Next, suppose that we are told that sense A appears 55% of the time.\n",
      "Once again, there are many distributions that are consistent with this\n",
      "new piece of information, such as:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "H\n",
      "I\n",
      "J\n",
      "\n",
      "\n",
      "\n",
      "(iv)\n",
      "55%\n",
      "45%\n",
      "0%\n",
      "0%\n",
      "0%\n",
      "0%\n",
      "0%\n",
      "0%\n",
      "0%\n",
      "0%\n",
      "\n",
      "(v)\n",
      "55%\n",
      "5%\n",
      "5%\n",
      "5%\n",
      "5%\n",
      "5%\n",
      "5%\n",
      "5%\n",
      "5%\n",
      "5%\n",
      "\n",
      "(vi)\n",
      "55%\n",
      "3%\n",
      "1%\n",
      "2%\n",
      "9%\n",
      "5%\n",
      "0%\n",
      "25%\n",
      "0%\n",
      "0%\n",
      "\n",
      "\n",
      "Table 6.2\n",
      "\n",
      "But again, we will likely choose the distribution that makes the\n",
      "fewest unwarranted assumptions — in this case, distribution (v).\n",
      "Finally, suppose that we are told that the word \"up\" appears in the\n",
      "nearby context 10% of the time, and that when it does appear in the\n",
      "context there's an 80% chance that sense A or C will be used.  In\n",
      "this case, we will have a harder time coming up with an appropriate\n",
      "distribution by hand; however, we can verify that the following\n",
      "distribution looks appropriate:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "H\n",
      "I\n",
      "J\n",
      "\n",
      "\n",
      "\n",
      "(vii)\n",
      "+up\n",
      "5.1%\n",
      "0.25%\n",
      "2.9%\n",
      "0.25%\n",
      "0.25%\n",
      "0.25%\n",
      "0.25%\n",
      "0.25%\n",
      "0.25%\n",
      "0.25%\n",
      "\n",
      "` `\n",
      "-up\n",
      "49.9%\n",
      "4.46%\n",
      "4.46%\n",
      "4.46%\n",
      "4.46%\n",
      "4.46%\n",
      "4.46%\n",
      "4.46%\n",
      "4.46%\n",
      "4.46%\n",
      "\n",
      "\n",
      "Table 6.3\n",
      "\n",
      "In particular, the distribution is consistent with what we know: if we\n",
      "add up the probabilities in column A, we get 55%; if we add up the\n",
      "probabilities of row 1, we get 10%; and if we add up the boxes for\n",
      "senses A and C in the +up row, we get 8% (or 80% of the +up cases).\n",
      "Furthermore, the remaining probabilities appear to be \"evenly\n",
      "distributed.\"\n",
      "Throughout this example, we have restricted ourselves to distributions\n",
      "that are consistent with what we know; among these, we chose the\n",
      "distribution with the highest entropy.  This is exactly what the\n",
      "Maximum Entropy classifier does as well.  In particular, for each\n",
      "joint-feature, the Maximum Entropy model calculates the \"empirical\n",
      "frequency\" of that feature — i.e., the frequency with which it occurs\n",
      "in the training set.  It then searches for the distribution which\n",
      "maximizes entropy, while still predicting the correct frequency for\n",
      "each joint-feature.\n",
      "\n",
      "\n",
      "6.3   Generative vs Conditional Classifiers\n",
      "\n",
      "An important difference between the naive Bayes classifier and the\n",
      "Maximum Entropy classifier concerns the type of questions they can be\n",
      "used to answer.  The naive Bayes classifier is an example of a\n",
      "generative classifier, which builds a model that predicts\n",
      "P(input, label), the joint probability of a (input,\n",
      "label) pair.  As a result, generative models can be used to\n",
      "answer the following questions:\n",
      "\n",
      "What is the most likely label for a given input?\n",
      "How likely is a given label for a given input?\n",
      "What is the most likely input value?\n",
      "How likely is a given input value?\n",
      "How likely is a given input value with a given label?\n",
      "What is the most likely label for an input that might have one\n",
      "of two values (but we don't know which)?\n",
      "\n",
      "\n",
      "The Maximum Entropy classifier, on the other hand, is an example of a\n",
      "conditional classifier.  Conditional classifiers build models\n",
      "that predict P(label|input) — the probability of a label\n",
      "given the input value.  Thus, conditional models can still be used\n",
      "to answer questions 1 and 2.  However, conditional models can not be\n",
      "used to answer the remaining questions 3-6.\n",
      "In general, generative models are strictly more powerful than\n",
      "conditional models, since we can calculate the conditional probability\n",
      "P(label|input) from the joint probability P(input,\n",
      "label), but not vice versa.\n",
      "However, this additional power comes at a price.  Because the model is\n",
      "more powerful, it has more \"free parameters\" which need to be learned.\n",
      "However, the size of the training set is fixed.  Thus, when using a\n",
      "more powerful model, we end up with less data that can be used to\n",
      "train each parameter's value, making it harder to find the best\n",
      "parameter values.  As a result, a generative model may not do as good\n",
      "a job at answering questions 1 and 2 as a conditional model, since the\n",
      "conditional model can focus its efforts on those two questions.\n",
      "However, if we do need answers to questions like 3-6, then we have no\n",
      "choice but to use a generative model.\n",
      "The difference between a generative model and a conditional model is\n",
      "analogous to the difference between a topographical map and a picture of\n",
      "a skyline.  Although the topographical map can be used to answer a wider\n",
      "variety of questions, it is significantly more difficult to generate\n",
      "an accurate topographical map than it is to generate an accurate skyline.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7   Modeling Linguistic Patterns\n",
      "Classifiers can help us to understand the linguistic patterns that\n",
      "occur in natural language, by allowing us to create explicit\n",
      "models that capture those patterns.  Typically, these models are\n",
      "using supervised classification techniques, but it is also possible to\n",
      "build analytically motivated models.  Either way, these explicit\n",
      "models serve two important purposes: they help us to understand\n",
      "linguistic patterns, and they can be used to make predictions about\n",
      "new language data.\n",
      "The extent to which explicit models can give us insights into\n",
      "linguistic patterns depends largely on what kind of model is used.\n",
      "Some models, such as decision trees, are relatively transparent, and\n",
      "give us direct information about which factors are important in making\n",
      "decisions and about which factors are related to one another.  Other\n",
      "models, such as multi-level neural networks, are much more opaque.\n",
      "Although it can be possible to gain insight by studying them, it\n",
      "typically takes a lot more work.\n",
      "But all explicit models can make predictions about new \"unseen\"\n",
      "language data that was not included in the corpus used to build the\n",
      "model.  These predictions can be evaluated to assess the accuracy of\n",
      "the model.  Once a model is deemed sufficiently accurate, it can then\n",
      "be used to automatically predict information about new language\n",
      "data.  These predictive models can be combined into systems that\n",
      "perform many useful language processing tasks, such as document\n",
      "classification, automatic translation, and question answering.\n",
      "\n",
      "7.1   What do models tell us?\n",
      "It's important to understand what we can learn about language from an\n",
      "automatically constructed model.  One important consideration when\n",
      "dealing with models of language is the distinction between descriptive\n",
      "models and explanatory models.  Descriptive models capture patterns in\n",
      "the data but they don't provide any information about why the data\n",
      "contains those patterns.  For example, as we saw in 3.1,\n",
      "the synonyms absolutely and definitely are not\n",
      "interchangeable: we say absolutely adore not definitely\n",
      "adore, and definitely prefer not absolutely prefer.\n",
      "In contrast, explanatory models attempt to capture properties and\n",
      "relationships that cause the linguistic patterns.  For example, we\n",
      "might introduce the abstract concept of \"polar verb\", as one that\n",
      "has an extreme meaning, and categorize some verb like\n",
      "adore and detest as polar.  Our explanatory model would\n",
      "contain the constraint that absolutely can only combine with\n",
      "polar verbs, and definitely can only combine with non-polar\n",
      "verbs.  In summary, descriptive models provide information about\n",
      "correlations in the data, while explanatory models go further to\n",
      "postulate causal relationships.\n",
      "Most models that are automatically constructed from a corpus are\n",
      "descriptive models; in other words, they can tell us what features are\n",
      "relevant to a given pattern or construction, but they can't\n",
      "necessarily tell us how those features and patterns relate to one\n",
      "another.  If our goal is to understand the linguistic patterns, then\n",
      "we can use this information about which features are related as a\n",
      "starting point for further experiments designed to tease apart the\n",
      "relationships between features and patterns.  On the other hand, if\n",
      "we're just interested in using the model to make predictions (e.g., as\n",
      "part of a language processing system), then we can use the model to\n",
      "make predictions about new data without worrying about the details\n",
      "of underlying causal relationships.\n",
      "\n",
      "\n",
      "\n",
      "8   Summary\n",
      "\n",
      "Modeling the linguistic data found in corpora can help us to\n",
      "understand linguistic patterns, and can be used to make predictions\n",
      "about new language data.\n",
      "Supervised classifiers use labeled training corpora to build models\n",
      "that predict the label of an input based on specific features of\n",
      "that input.\n",
      "Supervised classifiers can perform a wide variety of\n",
      "NLP tasks, including document classification, part-of-speech\n",
      "tagging, sentence segmentation, dialogue act type identification,\n",
      "and determining entailment relations, and many other tasks.\n",
      "When training a supervised classifier, you should split your corpus\n",
      "into three datasets: a training set for building the\n",
      "classifier model; a dev-test set for helping select\n",
      "and tune the model's features; and a test set for\n",
      "evaluating the final model's performance.\n",
      "When evaluating a supervised classifier, it is important that you\n",
      "use fresh data, that was not included in the training or dev-test\n",
      "set.  Otherwise, your evaluation results may be unrealistically\n",
      "optimistic.\n",
      "Decision trees are automatically constructed tree-structured\n",
      "flowcharts that are used to assign labels to input values based on\n",
      "their features.  Although they're easy to interpret, they are not\n",
      "very good at handling cases where feature values interact in\n",
      "determining the proper label.\n",
      "In naive Bayes classifiers, each feature independently contributes\n",
      "to the decision of which label should be used.  This allows feature\n",
      "values to interact, but can be problematic when two or more features\n",
      "are highly correlated with one another.\n",
      "Maximum Entropy classifiers use a basic model that is similar to the\n",
      "model used by naive Bayes; however, they employ iterative optimization\n",
      "to find the set of feature weights that maximizes the probability of\n",
      "the training set.\n",
      "Most of the models that are automatically constructed from a corpus\n",
      "are descriptive — they let us know which features are relevant\n",
      "to a given patterns or construction, but they don't give any\n",
      "information about causal relationships between those features and\n",
      "patterns.\n",
      "\n",
      "\n",
      "\n",
      "9   Further Reading\n",
      "Please consult http://nltk.org/ for further materials on this chapter and on how to\n",
      "install external machine learning packages, such as Weka, Mallet,\n",
      "TADM, and MEGAM.\n",
      "For more examples of classification and machine learning with NLTK,\n",
      "please see the classification HOWTOs at http://nltk.org/howto.\n",
      "For a general introduction to machine learning, we recommend\n",
      "(Alpaydin, 2004).  For a more mathematically intense introduction to\n",
      "the theory of machine learning, see (Hastie, Tibshirani, & Friedman, 2009).  Excellent books on\n",
      "using machine learning techniques for NLP include (Abney, 2008),\n",
      "(Daelemans & Bosch, 2005), (Feldman & Sanger, 2007), (Segaran, 2007), (Weiss et al, 2004).  For\n",
      "more on smoothing techniques for language problems, see\n",
      "(Manning & Schutze, 1999).  For more on sequence modeling, and especially\n",
      "hidden Markov models, see (Manning & Schutze, 1999) or (Jurafsky & Martin, 2008).\n",
      "Chapter 13 of (Manning, Raghavan, & Schutze, 2008) discusses the use of naive Bayes for\n",
      "classifying texts.\n",
      "Many of the machine learning algorithms discussed in this chapter are\n",
      "numerically intensive, and as a result, they will run slowly when\n",
      "coded naively in Python.  For information on increasing the efficiency\n",
      "of numerically intensive algorithms in Python, see (Kiusalaas, 2005).\n",
      "The classification techniques described in this chapter can be applied\n",
      "to a very wide variety of problems.  For example, (Agirre & Edmonds, 2007) uses\n",
      "classifiers to perform word-sense disambiguation; and (Melamed, 2001)\n",
      "uses classifiers to create parallel texts.  Recent textbooks that\n",
      "cover text classification include (Manning, Raghavan, & Schutze, 2008) and (Croft, Metzler, & Strohman, 2009).\n",
      "Much of the current research in the application of machine learning\n",
      "techniques to NLP problems is driven by government-sponsored\n",
      "\"challenges,\" where a set of research organizations are all provided\n",
      "with the same development corpus, and asked to build a system; and the\n",
      "resulting systems are compared based on a reserved test set.  Examples\n",
      "of these challenge competitions include CoNLL Shared Tasks, the ACE\n",
      "competitions, the Recognizing Textual Entailment competitions,\n",
      "and the AQUAINT competitions.  Consult http://nltk.org/ for a\n",
      "list of pointers to the webpages for these challenges.\n",
      "\n",
      "\n",
      "\n",
      "10   Exercises\n",
      "\n",
      "☼ Read up on one of the language technologies mentioned in this section, such as\n",
      "word sense disambiguation, semantic role labeling, question answering, machine translation,\n",
      "named entity detection.\n",
      "Find out what type and quantity of annotated data is required for developing such systems.\n",
      "Why do you think a large amount of data is required?\n",
      "\n",
      "☼ Using any of the three classifiers described in this\n",
      "chapter, and any features you can think of, build the best name\n",
      "gender classifier you can.  Begin by splitting the Names Corpus\n",
      "into three subsets: 500 words for the test set, 500 words for the\n",
      "dev-test set, and the remaining 6900 words for the training set.\n",
      "Then, starting with the example name gender classifier, make\n",
      "incremental improvements.  Use the dev-test set to check your\n",
      "progress.  Once you are satisfied with your classifier, check its\n",
      "final performance on the test set.  How does the performance on\n",
      "the test set compare to the performance on the dev-test set?\n",
      "Is this what you'd expect?\n",
      "\n",
      "☼ The Senseval 2 Corpus contains data intended to train\n",
      "word-sense disambiguation classifiers.  It contains data for\n",
      "four words: hard, interest, line, and serve.  Choose one of these\n",
      "four words, and load the corresponding data:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import senseval\n",
      ">>> instances = senseval.instances('hard.pos')\n",
      ">>> size = int(len(instances) * 0.1)\n",
      ">>> train_set, test_set = instances[size:], instances[:size]\n",
      "\n",
      "\n",
      "\n",
      "Using this dataset, build a classifier that predicts the correct\n",
      "sense tag for a given instance.  See the corpus HOWTO at\n",
      "http://nltk.org/howto for information on using the instance objects\n",
      "returned by the Senseval 2 Corpus.\n",
      "\n",
      "☼\n",
      "Using the movie review document classifier discussed in this\n",
      "chapter, generate a list of the 30 features that the classifier\n",
      "finds to be most informative.  Can you explain why these particular\n",
      "features are informative?  Do you find any of them surprising?\n",
      "\n",
      "☼\n",
      "Select one of the classification tasks described in this chapter,\n",
      "such as name gender detection, document classification,\n",
      "part-of-speech tagging, or dialog act classification.  Using the\n",
      "same training and test data, and the same feature extractor,\n",
      "build three classifiers for the task: a decision tree, a naive\n",
      "Bayes classifier, and a Maximum Entropy classifier.  Compare\n",
      "the performance of the three classifiers on your selected task.\n",
      "How do you think that your results might be different if you used\n",
      "a different feature extractor?\n",
      "\n",
      "☼\n",
      "The synonyms strong and powerful pattern\n",
      "differently (try combining them with chip and sales).\n",
      "What features are relevant in this distinction?\n",
      "Build a classifier that predicts when each word should be used.\n",
      "\n",
      "◑\n",
      "The dialog act classifier assigns labels to individual posts,\n",
      "without considering the context in which the post is found.\n",
      "However, dialog acts are highly dependent on context, and some\n",
      "sequences of dialog act are much more likely than others.  For\n",
      "example, a ynQuestion dialog act is much more likely to be\n",
      "answered by a yanswer than by a greeting.  Make use of this\n",
      "fact to build a consecutive classifier for labeling dialog acts.\n",
      "Be sure to consider what features might be useful.  See the code\n",
      "for the consecutive classifier for part-of-speech tags in\n",
      "1.7 to get some ideas.\n",
      "\n",
      "◑\n",
      "Word features can be very useful for performing document\n",
      "classification, since the words that appear in a document give a\n",
      "strong indication about what its semantic content is.  However,\n",
      "many words occur very infrequently, and some of the most\n",
      "informative words in a document may never have occurred in our\n",
      "training data.  One solution is to make use of a lexicon,\n",
      "which describes how different words relate to one another.  Using\n",
      "WordNet lexicon, augment the movie review document classifier\n",
      "presented in this chapter to use features that generalize the words\n",
      "that appear in a document, making it more likely that they will\n",
      "match words found in the training data.\n",
      "\n",
      "★\n",
      "The PP Attachment Corpus is a corpus describing\n",
      "prepositional phrase attachment decisions.  Each instance in the\n",
      "corpus is encoded as a PPAttachment object:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import ppattach\n",
      ">>> ppattach.attachments('training')\n",
      "[PPAttachment(sent='0', verb='join', noun1='board',\n",
      "              prep='as', noun2='director', attachment='V'),\n",
      " PPAttachment(sent='1', verb='is', noun1='chairman',\n",
      "              prep='of', noun2='N.V.', attachment='N'),\n",
      " ...]\n",
      ">>> inst = ppattach.attachments('training')[1]\n",
      ">>> (inst.noun1, inst.prep, inst.noun2)\n",
      "('chairman', 'of', 'N.V.')\n",
      "\n",
      "\n",
      "\n",
      "Select only the instances where inst.attachment is N:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nattach = [inst for inst in ppattach.attachments('training')\n",
      "...            if inst.attachment == 'N']\n",
      "\n",
      "\n",
      "\n",
      "Using this sub-corpus, build a classifier that attempts to predict\n",
      "which preposition is used to connect a given pair of nouns.  For\n",
      "example, given the pair of nouns \"team\" and \"researchers,\" the\n",
      "classifier should predict the preposition \"of\".  See the corpus\n",
      "HOWTO at http://nltk.org/howto for more information on using the PP\n",
      "attachment corpus.\n",
      "\n",
      "★ Suppose you wanted to automatically generate a prose description of a scene,\n",
      "and already had a word to uniquely describe each entity, such as the jar,\n",
      "and simply wanted to decide whether to use in or on in relating\n",
      "various items, e.g. the book is in the cupboard vs the book is on the shelf.\n",
      "Explore this issue by looking at corpus data; writing programs as needed.\n",
      "\n",
      "\n",
      "\n",
      "  (13)\n",
      "  a.in the car versus on the train\n",
      "\n",
      "  b.in town versus on campus\n",
      "\n",
      "  c.in the picture versus on the screen\n",
      "\n",
      "  d.in Macbeth versus on Letterman\n",
      "\n",
      "\n",
      "\n",
      "About this document...\n",
      "UPDATED FOR NLTK 3.0.\n",
      "This is a chapter from Natural Language Processing with Python,\n",
      "by Steven Bird, Ewan Klein and Edward Loper,\n",
      "Copyright © 2019 the authors.\n",
      "It is distributed with the Natural Language Toolkit [http://nltk.org/],\n",
      "Version 3.0, under the terms of the\n",
      "Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\n",
      "[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\n",
      "This document was built on\n",
      "Wed  4 Sep 2019 11:40:48 ACST\n",
      "\n",
      "\n",
      "\n",
      "Docutils System Messages\n",
      "\n",
      "System Message: ERROR/3 (ch06.rst2, line 1264); backlink\n",
      "Undefined substitution referenced: \"ii\".\n",
      "vectors[6] = [0, 3, 4, 19, 8, 100, 6, 43, 0, 0, 2, 20, 8, 0, 0, 0, 2, 0, 1, 1, 0, 0, 1, 0, 2, 0, 4, 0, 9, 18, 3, 1, 5, 13, 1, 1, 0, 0, 44, 2, 10, 14, 3, 148, 13, 2, 14, 1, 0, 0, 6, 3, 2, 2, 1, 2, 9, 7, 0, 2, 22, 0, 0, 0, 57, 0, 0, 1, 1, 23, 0, 0, 0, 4, 0, 1, 33, 0, 0, 16, 2, 2, 18, 0, 0, 0, 1, 0, 6, 0, 0, 0, 0, 0, 0, 145, 0, 1, 0, 23, 2, 0, 2, 1, 0, 0, 5, 6, 0, 2, 1, 11, 1, 1, 2, 0, 0, 0, 0, 0, 53, 0, 0, 5, 0, 0, 0, 0, 1, 6, 2, 1, 1, 0, 7, 1, 408, 60, 31, 1, 0, 0, 58, 0, 0, 0, 7, 7, 7, 0, 19, 1, 0, 0, 0, 0, 0, 0, 0, 3, 2, 10, 0, 16, 0, 0, 0, 0, 5, 0, 0, 0, 1, 18, 0, 8, 28, 1, 1, 2, 1, 1, 0, 4, 0, 10, 0, 1, 0, 0, 1, 16, 8, 67, 0, 3, 1, 3, 5, 0, 1, 0, 2, 5, 0, 128, 2, 11, 1, 0, 5, 2, 187, 0, 0, 2, 1, 0, 1, 1, 8, 4, 17, 25, 0, 0, 0, 0, 1, 2, 33, 12, 0, 16, 298, 1, 0, 0, 1, 0, 1, 3, 8, 0, 2, 5, 0, 0, 0, 2, 1, 14, 115, 1, 0, 1, 0, 0, 0, 2, 0, 63, 66, 2, 2, 0, 0, 0, 9, 1, 10, 6, 8, 0, 0, 1, 0, 1, 0, 9, 0, 0, 0, 0, 2, 0, 0, 3, 0, 2, 0, 6, 0, 1, 0, 14, 2, 6, 0, 0, 1, 0, 2, 2, 0, 2, 0, 6, 20, 1, 3, 0, 0, 0, 15, 0, 4, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 8, 9, 12, 17, 1, 1, 0, 3, 7, 140, 3, 2, 0, 0, 0, 15, 0, 119, 3, 1, 0, 3, 0, 5, 0, 10, 11, 34, 0, 4, 0, 2, 4, 2, 10, 0, 0, 3, 4, 2, 0, 0, 45, 0, 1, 41, 207, 0, 7, 0, 3, 1, 0, 1, 0, 0, 0, 0, 10, 45, 3, 43, 14, 14, 8, 13, 0, 34, 0, 6, 0, 3, 14, 18, 5, 0, 0, 0, 4, 0, 0, 0, 0, 1, 1, 0, 0, 0, 40, 12, 5, 0, 1, 2, 0, 1, 27, 0, 3, 0, 0, 1, 0, 6, 16, 0, 13, 0, 0, 13, 0, 1, 8, 0, 6, 3, 1, 0, 0, 0, 2, 0, 2, 4, 2, 0, 0, 2, 2, 0, 7, 1, 5, 3, 0, 7, 0, 0, 1, 1, 1, 5, 2, 0, 0, 1, 0, 0, 2, 8, 0, 3, 24, 6, 1, 129, 0, 0, 47, 3, 1, 12, 24, 0, 1, 0, 0, 0, 9, 0, 0, 1, 0, 3, 4, 3, 0, 0, 4, 0, 11, 1, 17, 1, 3, 2, 13, 0, 2, 2, 15, 2, 0, 2, 1, 0, 17, 14, 5, 2, 1, 2, 0, 1, 15, 4, 13, 9, 5, 3, 3, 1, 1, 0, 1, 1, 8, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 28, 8, 13, 2, 3, 21, 20, 1, 6, 1, 2, 17, 1, 0, 0, 13, 2, 10, 3, 22, 0, 0, 1, 5, 0, 3, 24, 0, 6, 0, 3, 1, 0, 1, 0, 4, 21, 0, 0, 0, 17, 0, 111, 0, 0, 0, 8, 3, 52, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 12, 0, 163, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 2, 24, 1, 1, 0, 0, 0, 25, 0, 0, 0, 0, 1, 0, 0, 1, 0, 4, 40, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 14, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 1, 0, 0, 0, 14, 2, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 56, 35, 0, 0, 1, 0, 0, 24, 0, 0, 67, 42, 23, 54, 32, 31, 0, 5, 0, 0, 2, 3, 0, 0, 1, 1, 1, 0, 0, 0, 7, 20, 2, 6, 6, 1, 0, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 7, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 7, 1, 5, 0, 0, 5, 0, 0, 0, 3, 1, 0, 0, 6, 1, 0, 2, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 6, 13, 3, 9, 0, 16, 22, 5, 3, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 6, 2, 0, 0, 17, 1, 0, 0, 0, 0, 9, 11, 0, 0, 0, 10, 0, 12, 0, 0, 0, 0, 7, 0, 0, 0, 20, 0, 4, 0, 1, 2, 86, 3, 0, 0, 1, 0, 0, 2, 2, 3, 0, 2, 9, 0, 0, 0, 0, 2, 6, 38, 0, 0, 2, 0, 0, 1, 11, 1, 0, 0, 0, 0, 5, 1, 1, 0, 0, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 13, 1, 1, 0, 0, 1, 0, 3, 10, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 3, 0, 0, 1, 0, 0, 0, 0, 0, 2, 9, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 11, 0, 0, 0, 0, 2, 5, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 58, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 10, 5, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 17, 0, 0, 3, 48, 1, 1, 2, 4, 55, 1, 1, 7, 1, 1, 1, 2, 2, 1, 2, 2, 1, 5, 1, 1, 2, 1, 1, 8, 0, 1, 3, 1, 4, 1, 3, 3, 3, 1, 1, 3, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 1, 0, 0, 16, 0, 0, 3, 0, 0, 1, 0, 13, 36, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 4, 0, 1, 2, 5, 0, 0, 0, 0, 0, 13, 0, 1, 0, 0, 10, 0, 0, 0, 0, 5, 10, 0, 10, 9, 0, 6, 0, 0, 24, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 20, 0, 0, 1, 12, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 6, 11, 1, 2, 4, 0, 0, 0, 1, 7, 1, 2, 28, 0, 0, 1, 3, 0, 3, 8, 6, 0, 0, 2, 2, 0, 0, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 3, 1, 0, 0, 0, 0, 5, 2, 0, 0, 12, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 0, 0, 0, 0, 49, 2, 0, 3, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 0, 0, 0, 2, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 11, 7, 1, 22, 6, 0, 6, 0, 0, 0, 0, 0, 0, 3, 1, 24, 0, 0, 6, 0, 0, 0, 6, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 3, 11, 0, 3, 0, 0, 9, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 14, 0, 1, 2, 0, 0, 4, 2, 0, 1, 16, 0, 0, 0, 15, 1, 0, 2, 6, 0, 1, 0, 0, 1, 1, 15, 0, 79, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 10, 0, 2, 5, 1, 0, 5, 14, 6, 112, 0, 0, 1, 0, 4, 4, 0, 7, 0, 2, 1, 0, 0, 0, 8, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 8, 0, 0, 4, 2, 0, 1, 0, 3, 2, 6, 0, 0, 3, 0, 0, 2, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 1, 19, 0, 179, 8, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 2, 0, 0, 0, 0, 0, 1, 7, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 44, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 21, 0, 0, 1, 0, 0, 13, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 2, 0, 2, 0, 1, 4, 0, 0, 3, 0, 1, 0, 1, 1, 0, 0, 0, 8, 10, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 15, 0, 0, 0, 0, 1, 1, 1, 2, 7, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 4, 0, 0, 0, 0, 1, 6, 0, 0, 2, 0, 1, 0, 1, 1, 2, 17, 0, 1, 0, 0, 7, 2, 12, 0, 1, 5, 0, 0, 0, 0, 1, 0, 6, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 1, 0, 1, 0, 0, 6, 13, 1, 1, 0, 11, 7, 0, 52, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 3, 7, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 2, 4, 0, 0, 4, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 8, 0, 0, 3, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 13, 0, 4, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 2, 3, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 15, 0, 1, 0, 0, 9, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 14, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 13, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 0, 0, 6, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 1, 0, 11, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 13, 0, 0, 0, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 3, 6, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 11, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 10, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 2, 4, 0, 0, 0, 1, 1, 0, 1, 0, 4, 0, 0, 0, 30, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 4, 0, 0, 0, 2, 0, 1, 0, 2, 0, 6, 15, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0, 3, 1, 0, 0, 0, 0, 11, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 4, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 3, 9, 0, 5, 7, 0, 4, 3, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 6, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 11, 0, 1, 0, 2, 0, 2, 0, 0, 0, 3, 0, 0, 3, 0, 1, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 2, 0, 0, 1, 0, 17, 0, 0, 0, 0, 1, 29, 0, 0, 0, 0, 35, 33, 0, 1, 0, 0, 3, 0, 1, 10, 8, 0, 0, 0, 0, 1, 6, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 9, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 10, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 3, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 7, 0, 0, 0, 3, 0, 77, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 10, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 3, 7, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 6, 0, 0, 0, 26, 0, 0, 0, 0, 2, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 0, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 4, 1, 0, 0, 1, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 216, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 26, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 11, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 2, 0, 1, 2, 2, 1, 0, 3, 2, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 7, 0, 7, 0, 2, 0, 8, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 8, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 1, 0, 21, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 7, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 8, 4, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 6, 0, 0, 0, 0, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 0, 0, 4, 2, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 3, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 8, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 6, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 1, 0, 0, 1, 0, 14, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 3, 0, 0, 5, 3, 1, 1, 1, 1, 5, 4, 1, 1, 2, 41, 1, 1, 15, 1, 1, 2, 1, 9, 1, 28, 27, 21, 11, 1, 1, 1, 1, 1, 1, 1, 3, 3, 23, 1, 4, 1, 2, 1, 1, 1, 1, 3, 1, 1, 2, 8, 3, 1, 1, 2, 7, 1, 3, 3, 2, 1, 1, 2, 2, 3, 2, 4, 2, 4, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 3, 1, 1, 1, 5, 1, 1, 1, 2, 2, 3, 2, 3, 1, 1, 1, 2, 1, 5, 2, 1, 2, 1, 1, 1, 5, 3, 3, 1, 1, 2, 9, 1, 1, 3, 1, 1, 1, 1, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 6, 2, 6, 6, 9, 1, 4, 3, 1, 1, 2, 1, 1, 8, 15, 2, 1, 2, 1, 4, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 5, 1, 1, 2, 1, 1, 2, 1, 1, 1, 4, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 4, 2, 4, 1, 1, 4, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 4, 1, 1, 4, 1, 4, 1, 1, 1, 1, 1, 6, 1, 4, 2, 1, 1, 1, 2, 3, 1, 3, 2, 2, 1, 1, 1, 1, 2, 8, 1, 1, 1, 1, 2, 1, 4, 2, 3, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 3, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 6, 6, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 3, 1, 1, 2, 1, 1, 3, 1, 8, 4, 2, 1, 2, 1, 1, 2, 4, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 2, 4, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 4, 1, 4, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 3, 4, 3, 3, 2, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "docs[7] = 7. Extracting Information from Text\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7. Extracting Information from Text\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For any given question, it's likely that someone has written the\n",
      "answer down somewhere.  The amount of natural language text that is\n",
      "available in electronic form is truly staggering, and is increasing\n",
      "every day.  However, the complexity of natural language can make it\n",
      "very difficult to access the information in that text.\n",
      "The state of the art in NLP is still a long way from being\n",
      "able to build general-purpose representations of meaning from unrestricted text.\n",
      "If we instead focus our efforts on a limited set of questions or\n",
      "\"entity relations,\" such as \"where are different facilities located,\"\n",
      "or \"who is employed by what company,\" we can make significant progress.\n",
      "The goal of this chapter is to answer the following questions:\n",
      "\n",
      "How can we build a system that extracts structured data, such as\n",
      "tables, from unstructured text?\n",
      "What are some robust methods for identifying the entities and\n",
      "relationships described in a text?\n",
      "Which corpora are appropriate for this work, and how do we use\n",
      "them for training and evaluating our models?\n",
      "\n",
      "Along the way, we'll apply techniques from the last two chapters to\n",
      "the problems of chunking and named-entity recognition.\n",
      "\n",
      "1   Information Extraction\n",
      "Information comes in many shapes and sizes. One important form is\n",
      "structured data, where there is a regular and predictable\n",
      "organization of entities and relationships. For example,\n",
      "we might be interested in the\n",
      "relation between companies and locations. Given a particular company,\n",
      "we would like to be able to identify the locations where it does\n",
      "business; conversely, given a location, we would like to discover\n",
      "which companies do business in that location. If our data is in tabular\n",
      "form, such as the example in 1.1, then\n",
      "answering these queries is straightforward.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OrgName\n",
      "LocationName\n",
      "\n",
      "\n",
      "\n",
      "Omnicom\n",
      "New York\n",
      "\n",
      "DDB Needham\n",
      "New York\n",
      "\n",
      "Kaplan Thaler Group\n",
      "New York\n",
      "\n",
      "BBDO South\n",
      "Atlanta\n",
      "\n",
      "Georgia-Pacific\n",
      "Atlanta\n",
      "\n",
      "\n",
      "Table 1.1: Locations data\n",
      "\n",
      "\n",
      "If this location data was stored in Python as a list of tuples\n",
      "(entity, relation, entity), then the question\n",
      "\"Which organizations operate in Atlanta?\" could be\n",
      "translated as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> locs = [('Omnicom', 'IN', 'New York'),\n",
      "...         ('DDB Needham', 'IN', 'New York'),\n",
      "...         ('Kaplan Thaler Group', 'IN', 'New York'),\n",
      "...         ('BBDO South', 'IN', 'Atlanta'),\n",
      "...         ('Georgia-Pacific', 'IN', 'Atlanta')]\n",
      ">>> query = [e1 for (e1, rel, e2) in locs if e2=='Atlanta']\n",
      ">>> print(query)\n",
      "['BBDO South', 'Georgia-Pacific']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OrgName\n",
      "\n",
      "\n",
      "\n",
      "BBDO South\n",
      "\n",
      "Georgia-Pacific\n",
      "\n",
      "\n",
      "Table 1.2: Companies that operate in Atlanta\n",
      "\n",
      "\n",
      "Things are more tricky if we try to get similar information out of\n",
      "text. For example, consider the\n",
      "following snippet (from nltk.corpus.ieer, for fileid NYT19980315.0085).\n",
      "\n",
      "  (1)The fourth Wells account moving to another agency is the packaged\n",
      "paper-products division of Georgia-Pacific Corp., which arrived at\n",
      "Wells only last fall. Like Hertz and the History Channel, it is\n",
      "also leaving for an Omnicom-owned agency, the BBDO South unit of\n",
      "BBDO Worldwide.  BBDO South in Atlanta, which handles corporate\n",
      "advertising for Georgia-Pacific, will assume additional duties for\n",
      "brands like Angel Soft toilet tissue and Sparkle paper towels,\n",
      "said Ken Haldin, a spokesman for Georgia-Pacific in Atlanta.\n",
      "If you read through (1), you will glean the information required\n",
      "to answer the example question.\n",
      "But how do we get a machine to understand enough\n",
      "about (1) to return the answers in 1.2?  This is\n",
      "obviously a much harder task. Unlike 1.1, (1)\n",
      "contains no structure that links organization names with\n",
      "location names.\n",
      "One approach to this problem involves building a very general\n",
      "representation of meaning (10.).\n",
      "In this chapter we take a different approach,\n",
      "deciding in advance that we will only look for very specific kinds of\n",
      "information in text, such as the relation between organizations and\n",
      "locations.  Rather than trying to\n",
      "use text like (1) to answer the question directly,\n",
      "we first convert the unstructured\n",
      "data of natural language sentences into the structured data of\n",
      "1.1. Then we reap the benefits of powerful query\n",
      "tools such as SQL. This method of getting meaning from text is\n",
      "called Information Extraction.\n",
      "Information Extraction has many applications, including\n",
      "business intelligence, resume harvesting, media analysis, sentiment detection,\n",
      "patent search, and email scanning. A\n",
      "particularly important area of current research involves the attempt\n",
      "to extract structured data out of electronically-available scientific\n",
      "literature, especially in the domain of biology and medicine.\n",
      "\n",
      "1.1   Information Extraction Architecture\n",
      "1.1 shows the architecture for a simple information\n",
      "extraction system.  It begins by processing a document using several\n",
      "of the procedures discussed in 3 and 5.: first,\n",
      "the raw text of the document is split into sentences using a sentence\n",
      "segmenter, and each sentence is further subdivided into words using a\n",
      "tokenizer.  Next, each sentence is tagged with part-of-speech tags,\n",
      "which will prove very helpful in the next step, named entity\n",
      "detection.  In this step, we search for mentions of potentially\n",
      "interesting entities in each sentence.  Finally, we use relation\n",
      "detection to search for likely relations between different\n",
      "entities in the text.\n",
      "\n",
      "\n",
      "Figure 1.1: Simple Pipeline Architecture for an Information Extraction System.\n",
      "This system takes the raw text of a document as its input, and\n",
      "generates a list of (entity, relation, entity) tuples as its\n",
      "output.  For example, given a document that indicates that the\n",
      "company Georgia-Pacific is located in Atlanta, it might generate\n",
      "the tuple ([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']).\n",
      "\n",
      "To perform the first three tasks, we can define a simple function that\n",
      "simply connects together NLTK's default sentence segmenter\n",
      ", word tokenizer , and part-of-speech tagger\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def ie_preprocess(document):\n",
      "...    sentences = nltk.sent_tokenize(document) \n",
      "...    sentences = [nltk.word_tokenize(sent) for sent in sentences] \n",
      "...    sentences = [nltk.pos_tag(sent) for sent in sentences] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Remember that our program samples assume you\n",
      "begin your interactive session or your program with: import nltk, re, pprint\n",
      "\n",
      "Next, in named entity detection, we segment and label the\n",
      "entities that might participate in interesting relations with one\n",
      "another.  Typically, these will be definite noun phrases such as the\n",
      "knights who say \"ni\", or proper names such as Monty Python.\n",
      "In some tasks it is useful to also consider indefinite nouns or noun\n",
      "chunks, such as every student or cats,\n",
      "and these do not necessarily refer to\n",
      "entities in the same way as definite NPs and proper names.\n",
      "Finally, in relation extraction, we search for specific patterns\n",
      "between pairs of entities that occur near one another in the text, and\n",
      "use those patterns to build tuples recording the relationships\n",
      "between the entities.\n",
      "\n",
      "\n",
      "\n",
      "2   Chunking\n",
      "The basic technique we will use for entity detection is\n",
      "chunking, which segments and labels multi-token sequences as\n",
      "illustrated in 2.1.  The smaller boxes show the\n",
      "word-level tokenization and part-of-speech tagging, while the large\n",
      "boxes show higher-level chunking.  Each of these larger boxes is\n",
      "called a chunk.\n",
      "Like tokenization, which omits whitespace,\n",
      "chunking usually selects a subset of the tokens.\n",
      "Also like tokenization, the pieces produced by a chunker do not overlap\n",
      "in the source text.\n",
      "\n",
      "\n",
      "Figure 2.1: Segmentation and Labeling at both the Token and Chunk Levels\n",
      "\n",
      "In this section, we will explore chunking in some depth, beginning\n",
      "with the definition and representation of chunks.  We will see regular\n",
      "expression and n-gram approaches to chunking, and will develop and\n",
      "evaluate chunkers using the CoNLL-2000 chunking corpus. We will then return in\n",
      "(5) and 6\n",
      "to the tasks of named entity recognition and relation extraction.\n",
      "\n",
      "2.1   Noun Phrase Chunking\n",
      "We will begin by considering the task of noun phrase chunking,\n",
      "or NP-chunking, where we search for chunks corresponding to\n",
      "individual noun phrases.  For example, here is some Wall Street\n",
      "Journal text with NP-chunks marked using brackets:\n",
      "\n",
      "  (2)[ The/DT market/NN ] for/IN [ system-management/NN software/NN ]\n",
      "for/IN [ Digital/NNP ] [ 's/POS hardware/NN ] is/VBZ fragmented/JJ\n",
      "enough/RB that/IN [ a/DT giant/NN ] such/JJ as/IN [ Computer/NNP\n",
      "Associates/NNPS ] should/MD do/VB well/RB there/RB ./.\n",
      "As we can see, NP-chunks are often smaller pieces than complete\n",
      "noun phrases.  For example, the market for system-management software\n",
      "for Digital's hardware is a single noun phrase (containing two\n",
      "nested noun phrases), but it is captured in NP-chunks by the\n",
      "simpler chunk the market.  One of the motivations for this\n",
      "difference is that NP-chunks are defined so as not to contain\n",
      "other NP-chunks.  Consequently, any\n",
      "prepositional phrases or subordinate clauses that modify a nominal\n",
      "will not be included in the corresponding NP-chunk, since they\n",
      "almost certainly contain further noun phrases.\n",
      "One of the most useful sources of information for NP-chunking is\n",
      "part-of-speech tags.  This is one of the motivations for\n",
      "performing part-of-speech tagging in our information extraction\n",
      "system.  We demonstrate this approach using an example sentence that\n",
      "has been part-of-speech tagged in 2.2.  In order to create an\n",
      "NP-chunker, we will first define a chunk grammar, consisting of rules\n",
      "that indicate how sentences should be chunked.  In this case, we will\n",
      "define a simple grammar with a single regular-expression rule\n",
      ".  This rule says that an NP chunk should be formed\n",
      "whenever the chunker finds an optional determiner (DT) followed by any\n",
      "number of adjectives (JJ) and then a noun (NN).  Using this grammar,\n",
      "we create a chunk parser , and test it on our example\n",
      "sentence .  The result is a tree, which we can either\n",
      "print , or display graphically .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), \n",
      "... (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
      "\n",
      ">>> grammar = \"NP: {<DT>?<JJ>*<NN>}\" \n",
      "\n",
      ">>> cp = nltk.RegexpParser(grammar) \n",
      ">>> result = cp.parse(sentence) \n",
      ">>> print(result) \n",
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n",
      ">>> result.draw() \n",
      "\n",
      "\n",
      "Example 2.2 (code_chunkex.py): Figure 2.2: Example of a Simple Regular Expression Based NP Chunker.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.2   Tag Patterns\n",
      "The rules that make up a chunk grammar use tag patterns to\n",
      "describe sequences of tagged words.\n",
      "A tag pattern is a sequence of part-of-speech tags delimited\n",
      "using angle brackets, e.g. <DT>?<JJ>*<NN>.  Tag patterns are\n",
      "similar to regular expression patterns (3.4).\n",
      "Now, consider the following noun phrases from the Wall Street Journal:\n",
      "\n",
      "another/DT sharp/JJ dive/NN\n",
      "trade/NN figures/NNS\n",
      "any/DT new/JJ policy/NN measures/NNS\n",
      "earlier/JJR stages/NNS\n",
      "Panamanian/JJ dictator/NN Manuel/NNP Noriega/NNP\n",
      "\n",
      "We can match these noun phrases using a slight refinement of the first tag pattern\n",
      "above, i.e. <DT>?<JJ.*>*<NN.*>+.  This will chunk any sequence\n",
      "of tokens beginning with an optional determiner, followed by\n",
      "zero or more adjectives of any type (including relative\n",
      "adjectives like earlier/JJR), followed by one or more nouns of any\n",
      "type.  However, it is easy to find many more complicated examples which\n",
      "this rule will not cover:\n",
      "\n",
      "his/PRP$ Mansion/NNP House/NNP speech/NN\n",
      "the/DT price/NN cutting/VBG\n",
      "3/CD %/NN to/TO 4/CD %/NN\n",
      "more/JJR than/IN 10/CD %/NN\n",
      "the/DT fastest/JJS developing/VBG trends/NNS\n",
      "'s/POS skill/NN\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Try to come up with tag patterns to cover these cases.\n",
      "Test them using the graphical interface\n",
      "nltk.app.chunkparser().  Continue to refine your\n",
      "tag patterns with the help of the feedback given by this tool.\n",
      "\n",
      "\n",
      "\n",
      "2.3   Chunking with Regular Expressions\n",
      "To find the chunk structure for a given sentence, the RegexpParser\n",
      "chunker begins with a flat structure in which no tokens are\n",
      "chunked.  The chunking rules are applied in turn,\n",
      "successively updating the\n",
      "chunk structure.  Once all of the rules have been invoked, the\n",
      "resulting chunk structure is returned.\n",
      "2.3 shows a\n",
      "simple chunk grammar consisting of two rules.  The first rule\n",
      "matches an optional determiner or possessive pronoun,\n",
      "zero or more adjectives, then a noun.\n",
      "The second rule matches one or more proper nouns.\n",
      "We also define an example sentence to be chunked ,\n",
      "and run the chunker on this input .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "grammar = r\"\"\"\n",
      "  NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
      "      {<NNP>+}                # chunk sequences of proper nouns\n",
      "\"\"\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"), \n",
      "                 (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(cp.parse(sentence)) \n",
      "(S\n",
      "  (NP Rapunzel/NNP)\n",
      "  let/VBD\n",
      "  down/RP\n",
      "  (NP her/PP$ long/JJ golden/JJ hair/NN))\n",
      "\n",
      "\n",
      "Example 2.3 (code_chunker1.py): Figure 2.3: Simple Noun Phrase Chunker\n",
      "\n",
      "\n",
      "Note\n",
      "The $ symbol is a special character in regular\n",
      "expressions, and must be backslash escaped\n",
      "in order to match the tag PP$.\n",
      "\n",
      "If a tag pattern matches at overlapping locations, the leftmost\n",
      "match takes precedence.  For example, if we apply a rule that matches\n",
      "two consecutive nouns to a text containing three consecutive nouns,\n",
      "then only the first two nouns will be chunked:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nouns = [(\"money\", \"NN\"), (\"market\", \"NN\"), (\"fund\", \"NN\")]\n",
      ">>> grammar = \"NP: {<NN><NN>}  # Chunk two consecutive nouns\"\n",
      ">>> cp = nltk.RegexpParser(grammar)\n",
      ">>> print(cp.parse(nouns))\n",
      "(S (NP money/NN market/NN) fund/NN)\n",
      "\n",
      "\n",
      "\n",
      "Once we have created the chunk for money market, we have\n",
      "removed the context that would have permitted fund to be\n",
      "included in a chunk.  This issue would have been avoided with\n",
      "a more permissive chunk rule, e.g. NP: {<NN>+}.\n",
      "\n",
      "Note\n",
      "We have added a comment to each of our chunk rules.\n",
      "These are optional; when they are present, the chunker\n",
      "prints these comments as part of its tracing output.\n",
      "\n",
      "\n",
      "\n",
      "2.4   Exploring Text Corpora\n",
      "In 2 we saw how we could interrogate\n",
      "a tagged corpus to extract phrases matching a particular\n",
      "sequence of part-of-speech tags.  We can do the same work\n",
      "more easily with a chunker, as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> cp = nltk.RegexpParser('CHUNK: {<V.*> <TO> <V.*>}')\n",
      ">>> brown = nltk.corpus.brown\n",
      ">>> for sent in brown.tagged_sents():\n",
      "...     tree = cp.parse(sent)\n",
      "...     for subtree in tree.subtrees():\n",
      "...         if subtree.label() == 'CHUNK': print(subtree)\n",
      "...\n",
      "(CHUNK combined/VBN to/TO achieve/VB)\n",
      "(CHUNK continue/VB to/TO place/VB)\n",
      "(CHUNK serve/VB to/TO protect/VB)\n",
      "(CHUNK wanted/VBD to/TO wait/VB)\n",
      "(CHUNK allowed/VBN to/TO place/VB)\n",
      "(CHUNK expected/VBN to/TO become/VB)\n",
      "...\n",
      "(CHUNK seems/VBZ to/TO overtake/VB)\n",
      "(CHUNK want/VB to/TO buy/VB)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Encapsulate the above example inside a function find_chunks()\n",
      "that takes a chunk string like \"CHUNK: {<V.*> <TO> <V.*>}\" as an argument.\n",
      "Use it to search the corpus for several other patterns, such as four\n",
      "or more nouns in a row, e.g. \"NOUNS: {<N.*>{4,}}\"\n",
      "\n",
      "\n",
      "\n",
      "2.5   Chinking\n",
      "Sometimes it is easier to define what we want to exclude from\n",
      "a chunk.  We can define a chink to be a sequence\n",
      "of tokens that is not included in a chunk.\n",
      "In the following example, barked/VBD at/IN is a chink:\n",
      "\n",
      "[ the/DT little/JJ yellow/JJ dog/NN ] barked/VBD at/IN [ the/DT cat/NN ]\n",
      "\n",
      "Chinking is the process of removing a sequence of tokens from a\n",
      "chunk.  If the matching sequence of tokens spans an entire chunk, then the\n",
      "whole chunk is removed; if the sequence of tokens appears in the\n",
      "middle of the chunk, these tokens are removed, leaving two chunks\n",
      "where there was only one before.  If the sequence is at the periphery\n",
      "of the chunk, these tokens are removed, and a smaller chunk remains.\n",
      "These three possibilities are illustrated in 2.1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "` `\n",
      "Entire chunk\n",
      "Middle of a chunk\n",
      "End of a chunk\n",
      "\n",
      "\n",
      "\n",
      "Input\n",
      "[a/DT little/JJ\n",
      "dog/NN]\n",
      "[a/DT little/JJ\n",
      "dog/NN]\n",
      "[a/DT little/JJ\n",
      "dog/NN]\n",
      "\n",
      "Operation\n",
      "Chink \"DT JJ NN\"\n",
      "Chink \"JJ\"\n",
      "Chink \"NN\"\n",
      "\n",
      "Pattern\n",
      "}DT JJ NN{\n",
      "}JJ{\n",
      "}NN{\n",
      "\n",
      "Output\n",
      "a/DT little/JJ\n",
      "dog/NN\n",
      "[a/DT] little/JJ\n",
      "[dog/NN]\n",
      "[a/DT little/JJ]\n",
      "dog/NN\n",
      "\n",
      "\n",
      "Table 2.1: Three chinking rules applied to the same chunk\n",
      "\n",
      "\n",
      "In 2.4, we put the entire sentence into a single chunk,\n",
      "then excise the chinks.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "grammar = r\"\"\"\n",
      "  NP:\n",
      "    {<.*>+}          # Chunk everything\n",
      "    }<VBD|IN>+{      # Chink sequences of VBD and IN\n",
      "  \"\"\"\n",
      "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
      "       (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(cp.parse(sentence))\n",
      " (S\n",
      "   (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "   barked/VBD\n",
      "   at/IN\n",
      "   (NP the/DT cat/NN))\n",
      "\n",
      "\n",
      "Example 2.4 (code_chinker.py): Figure 2.4: Simple Chinker\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.6   Representing Chunks: Tags vs Trees\n",
      "As befits their intermediate status between tagging and parsing (8.),\n",
      "chunk structures can be represented using either tags or trees.  The most\n",
      "widespread file representation uses IOB tags.  In this\n",
      "scheme, each token is tagged with one of three special chunk tags,\n",
      "I (inside), O (outside), or B (begin).  A token is tagged\n",
      "as B if it marks the beginning of a chunk.  Subsequent tokens\n",
      "within the chunk are tagged I.  All other tokens are tagged O.\n",
      "The B and I tags are suffixed with the chunk type,\n",
      "e.g. B-NP, I-NP.  Of course, it is not necessary to specify a\n",
      "chunk type for tokens that appear outside a chunk, so these are just\n",
      "labeled O. An example of this scheme is shown in 2.5.\n",
      "\n",
      "\n",
      "Figure 2.5: Tag Representation of Chunk Structures\n",
      "\n",
      "IOB tags have become the standard way to represent chunk structures in\n",
      "files, and we will also be using this format.  Here is\n",
      "how the information in 2.5 would appear in a file:\n",
      "\n",
      "We PRP B-NP\n",
      "saw VBD O\n",
      "the DT B-NP\n",
      "yellow JJ I-NP\n",
      "dog NN I-NP\n",
      "\n",
      "In this representation there is one token per line, each with\n",
      "its part-of-speech tag and chunk tag.  This format permits us\n",
      "to represent more than one chunk type, so long as the chunks do not overlap.\n",
      "As we saw earlier, chunk structures can also be represented using\n",
      "trees.  These have the benefit that each chunk is a constituent that\n",
      "can be manipulated directly.  An example is shown in 2.6.\n",
      "\n",
      "\n",
      "Figure 2.6: Tree Representation of Chunk Structures\n",
      "\n",
      "\n",
      "Note\n",
      "NLTK uses trees for its internal representation of chunks, but\n",
      "provides methods for reading and writing such trees to the IOB format.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3   Developing and Evaluating Chunkers\n",
      "Now you have a taste of what chunking does, but we haven't\n",
      "explained how to evaluate chunkers.\n",
      "As usual, this requires a suitably annotated corpus.\n",
      "We begin by looking at the mechanics of converting IOB format into an\n",
      "NLTK tree, then at how this is done on a larger scale using a\n",
      "chunked corpus.  We will see how to score\n",
      "the accuracy of a chunker relative to a corpus,\n",
      "then look at some more data-driven ways to search for NP chunks.\n",
      "Our focus throughout will be on expanding the coverage of a chunker.\n",
      "\n",
      "\n",
      "3.1   Reading IOB Format and the CoNLL 2000 Corpus\n",
      "Using the corpus module we can load Wall Street Journal\n",
      "text that has been tagged then chunked using the IOB notation.  The\n",
      "chunk categories provided in this corpus are NP, VP and PP.  As we\n",
      "have seen, each sentence is represented using multiple lines, as shown\n",
      "below:\n",
      "\n",
      "he PRP B-NP\n",
      "accepted VBD B-VP\n",
      "the DT B-NP\n",
      "position NN I-NP\n",
      "...\n",
      "\n",
      "A conversion function chunk.conllstr2tree() builds a tree\n",
      "representation from one of these multi-line strings.  Moreover, it\n",
      "permits us to choose any subset of the three chunk types to use,\n",
      "here just for NP chunks:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = '''\n",
      "... he PRP B-NP\n",
      "... accepted VBD B-VP\n",
      "... the DT B-NP\n",
      "... position NN I-NP\n",
      "... of IN B-PP\n",
      "... vice NN B-NP\n",
      "... chairman NN I-NP\n",
      "... of IN B-PP\n",
      "... Carlyle NNP B-NP\n",
      "... Group NNP I-NP\n",
      "... , , O\n",
      "... a DT B-NP\n",
      "... merchant NN I-NP\n",
      "... banking NN I-NP\n",
      "... concern NN I-NP\n",
      "... . . O\n",
      "... '''\n",
      ">>> nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We can use the NLTK corpus module to access a larger amount of chunked\n",
      "text.  The CoNLL 2000 corpus contains 270k words of Wall Street\n",
      "Journal text, divided into \"train\" and \"test\" portions, annotated with\n",
      "part-of-speech tags and chunk tags in the IOB format.  We can access\n",
      "the data using nltk.corpus.conll2000.  Here is an\n",
      "example that reads the 100th sentence of the \"train\" portion of the corpus:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import conll2000\n",
      ">>> print(conll2000.chunked_sents('train.txt')[99])\n",
      "(S\n",
      "  (PP Over/IN)\n",
      "  (NP a/DT cup/NN)\n",
      "  (PP of/IN)\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  (VP told/VBD)\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n",
      "\n",
      "\n",
      "\n",
      "As you can see, the CoNLL 2000 corpus contains three chunk types:\n",
      "NP chunks, which we have already seen; VP chunks such as\n",
      "has already delivered; and PP chunks such as because of.\n",
      "Since we are only interested in the NP chunks right now, we can use the\n",
      "chunk_types argument to select them:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99])\n",
      "(S\n",
      "  Over/IN\n",
      "  (NP a/DT cup/NN)\n",
      "  of/IN\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  told/VBD\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.2   Simple Evaluation and Baselines\n",
      "Now that we can access a chunked corpus, we can evaluate chunkers.\n",
      "We start off by establishing a baseline for the trivial chunk parser\n",
      "cp that creates no chunks:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import conll2000\n",
      ">>> cp = nltk.RegexpParser(\"\")\n",
      ">>> test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
      ">>> print(cp.evaluate(test_sents))\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  43.4%\n",
      "    Precision:      0.0%\n",
      "    Recall:         0.0%\n",
      "    F-Measure:      0.0%\n",
      "\n",
      "\n",
      "\n",
      "The IOB tag accuracy indicates that more than a third of the words are\n",
      "tagged with O, i.e. not in an NP chunk.  However, since our\n",
      "tagger did not find any chunks, its precision, recall, and f-measure\n",
      "are all zero.  Now let's try a naive regular expression chunker that\n",
      "looks for tags beginning with letters that are characteristic of noun phrase tags\n",
      "(e.g. CD, DT, and JJ).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> grammar = r\"NP: {<[CDJNP].*>+}\"\n",
      ">>> cp = nltk.RegexpParser(grammar)\n",
      ">>> print(cp.evaluate(test_sents))\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%\n",
      "    Precision:     70.6%\n",
      "    Recall:        67.8%\n",
      "    F-Measure:     69.2%\n",
      "\n",
      "\n",
      "\n",
      "As you can see, this approach achieves decent results.  However, we\n",
      "can improve on it by adopting a more data-driven approach, where we\n",
      "use the training corpus to find the chunk tag (I, O, or B)\n",
      "that is most likely for each part-of-speech tag.  In other words, we\n",
      "can build a chunker using a unigram tagger (4).\n",
      "But rather than trying to determine the correct part-of-speech tag for\n",
      "each word, we are trying to determine the correct chunk tag, given\n",
      "each word's part-of-speech tag.\n",
      "In 3.1, we define the UnigramChunker class, which\n",
      "uses a unigram tagger to label sentences with chunk tags.  Most of the\n",
      "code in this class is simply used to convert back and forth between\n",
      "the chunk tree representation used by NLTK's ChunkParserI\n",
      "interface, and the IOB representation used by the embedded tagger.\n",
      "The class defines two methods: a constructor\n",
      " which is called when we build a new\n",
      "UnigramChunker; and the parse method \n",
      "which is used to chunk new sentences.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "class UnigramChunker(nltk.ChunkParserI):\n",
      "    def __init__(self, train_sents): \n",
      "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
      "                      for sent in train_sents]\n",
      "        self.tagger = nltk.UnigramTagger(train_data) \n",
      "\n",
      "    def parse(self, sentence): \n",
      "        pos_tags = [pos for (word,pos) in sentence]\n",
      "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
      "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
      "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
      "                     in zip(sentence, chunktags)]\n",
      "        return nltk.chunk.conlltags2tree(conlltags)\n",
      "\n",
      "\n",
      "Example 3.1 (code_unigram_chunker.py): Figure 3.1: Noun Phrase Chunking with a Unigram Tagger\n",
      "\n",
      "The constructor  expects a list of\n",
      "training sentences, which will be in the form of chunk trees.  It\n",
      "first converts training data to a form that is suitable for training the\n",
      "tagger, using tree2conlltags to map each chunk tree to a list of\n",
      "word,tag,chunk triples.  It then uses that converted training data\n",
      "to train a unigram tagger, and stores it in self.tagger for later\n",
      "use.\n",
      "The parse method  takes a tagged sentence\n",
      "as its input, and begins by extracting the part-of-speech tags from\n",
      "that sentence.  It then tags the part-of-speech tags with IOB chunk\n",
      "tags, using the tagger self.tagger that was trained in the\n",
      "constructor.  Next, it extracts the chunk tags, and combines them with\n",
      "the original sentence, to yield conlltags.  Finally, it uses\n",
      "conlltags2tree to convert the result back into a chunk tree.\n",
      "Now that we have UnigramChunker, we can train it using the CoNLL\n",
      "2000 corpus, and test its resulting performance:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
      ">>> train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
      ">>> unigram_chunker = UnigramChunker(train_sents)\n",
      ">>> print(unigram_chunker.evaluate(test_sents))\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%\n",
      "    Precision:     79.9%\n",
      "    Recall:        86.8%\n",
      "    F-Measure:     83.2%\n",
      "\n",
      "\n",
      "\n",
      "This chunker does reasonably well, achieving an overall f-measure\n",
      "score of 83%.  Let's take a look at what it's learned, by using its\n",
      "unigram tagger to assign a tag to each of the part-of-speech tags that\n",
      "appear in the corpus:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> postags = sorted(set(pos for sent in train_sents\n",
      "...                      for (word,pos) in sent.leaves()))\n",
      ">>> print(unigram_chunker.tagger.tag(postags))\n",
      "[('#', 'B-NP'), ('$', 'B-NP'), (\"''\", 'O'), ('(', 'O'), (')', 'O'),\n",
      " (',', 'O'), ('.', 'O'), (':', 'O'), ('CC', 'O'), ('CD', 'I-NP'),\n",
      " ('DT', 'B-NP'), ('EX', 'B-NP'), ('FW', 'I-NP'), ('IN', 'O'),\n",
      " ('JJ', 'I-NP'), ('JJR', 'B-NP'), ('JJS', 'I-NP'), ('MD', 'O'),\n",
      " ('NN', 'I-NP'), ('NNP', 'I-NP'), ('NNPS', 'I-NP'), ('NNS', 'I-NP'),\n",
      " ('PDT', 'B-NP'), ('POS', 'B-NP'), ('PRP', 'B-NP'), ('PRP$', 'B-NP'),\n",
      " ('RB', 'O'), ('RBR', 'O'), ('RBS', 'B-NP'), ('RP', 'O'), ('SYM', 'O'),\n",
      " ('TO', 'O'), ('UH', 'O'), ('VB', 'O'), ('VBD', 'O'), ('VBG', 'O'),\n",
      " ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'B-NP'),\n",
      " ('WP', 'B-NP'), ('WP$', 'B-NP'), ('WRB', 'O'), ('``', 'O')]\n",
      "\n",
      "\n",
      "\n",
      "It has discovered that most punctuation marks occur outside of NP\n",
      "chunks, with the exception of # and $, both of which are used\n",
      "as currency markers.  It has also found that determiners (DT) and\n",
      "possessives (PRP$ and WP$) occur at the beginnings of NP chunks,\n",
      "while noun types (NN, NNP, NNPS, NNS) mostly occur\n",
      "inside of NP chunks.\n",
      "\n",
      "Having built a unigram chunker, it is quite easy to build a bigram\n",
      "chunker: we simply change the class name to BigramChunker, and\n",
      "modify line  in 3.1\n",
      "to construct a BigramTagger rather than a UnigramTagger.\n",
      "The resulting chunker has slightly higher performance than the unigram chunker:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> bigram_chunker = BigramChunker(train_sents)\n",
      ">>> print(bigram_chunker.evaluate(test_sents))\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%\n",
      "    Precision:     82.3%\n",
      "    Recall:        86.8%\n",
      "    F-Measure:     84.5%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.3   Training Classifier-Based Chunkers\n",
      "Both the regular-expression based chunkers and the n-gram chunkers\n",
      "decide what chunks to create entirely based on part-of-speech tags.\n",
      "However, sometimes part-of-speech tags\n",
      "are insufficient to determine how a sentence should be chunked.\n",
      "For example, consider the following two statements:\n",
      "\n",
      "  (3)\n",
      "  a.Joey/NN sold/VBD the/DT farmer/NN rice/NN ./.\n",
      "\n",
      "  b.Nick/NN broke/VBD my/DT computer/NN monitor/NN ./.\n",
      "\n",
      "These two sentences have the same part-of-speech tags,\n",
      "yet they are chunked differently.  In the first sentence,\n",
      "the farmer and rice are separate chunks, while the\n",
      "corresponding material in the second sentence,\n",
      "the computer monitor, is a single chunk.  Clearly, we need to make\n",
      "use of information about the content of the words, in addition to just\n",
      "their part-of-speech tags, if we wish to maximize chunking\n",
      "performance.\n",
      "One way that we can incorporate information about the content of words\n",
      "is to use a classifier-based tagger to chunk the sentence.  Like the\n",
      "n-gram chunker considered in the previous section, this\n",
      "classifier-based chunker will work by assigning IOB tags to the words\n",
      "in a sentence, and then converting those tags to chunks.  For the\n",
      "classifier-based tagger itself, we will use the same approach that we\n",
      "used in 1 to build a part-of-speech tagger.\n",
      "The basic code for the classifier-based NP chunker is shown in\n",
      "3.2.  It consists of two classes.  The first\n",
      "class  is almost identical to the\n",
      "ConsecutivePosTagger class from 1.5.\n",
      "The only two differences are that it calls a different feature\n",
      "extractor  and that it uses a MaxentClassifier rather\n",
      "than a NaiveBayesClassifier .  The second class\n",
      " is basically a wrapper around the tagger class that\n",
      "turns it into a chunker.  During training, this second class maps the\n",
      "chunk trees in the training corpus into tag sequences; in the\n",
      "parse() method, it converts the tag sequence provided by the\n",
      "tagger back into a chunk tree.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "class ConsecutiveNPChunkTagger(nltk.TaggerI): \n",
      "\n",
      "    def __init__(self, train_sents):\n",
      "        train_set = []\n",
      "        for tagged_sent in train_sents:\n",
      "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
      "            history = []\n",
      "            for i, (word, tag) in enumerate(tagged_sent):\n",
      "                featureset = npchunk_features(untagged_sent, i, history) \n",
      "                train_set.append( (featureset, tag) )\n",
      "                history.append(tag)\n",
      "        self.classifier = nltk.MaxentClassifier.train( \n",
      "            train_set, algorithm='megam', trace=0)\n",
      "\n",
      "    def tag(self, sentence):\n",
      "        history = []\n",
      "        for i, word in enumerate(sentence):\n",
      "            featureset = npchunk_features(sentence, i, history)\n",
      "            tag = self.classifier.classify(featureset)\n",
      "            history.append(tag)\n",
      "        return zip(sentence, history)\n",
      "\n",
      "class ConsecutiveNPChunker(nltk.ChunkParserI): \n",
      "    def __init__(self, train_sents):\n",
      "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
      "                         nltk.chunk.tree2conlltags(sent)]\n",
      "                        for sent in train_sents]\n",
      "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
      "\n",
      "    def parse(self, sentence):\n",
      "        tagged_sents = self.tagger.tag(sentence)\n",
      "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
      "        return nltk.chunk.conlltags2tree(conlltags)\n",
      "\n",
      "\n",
      "Example 3.2 (code_classifier_chunker.py): Figure 3.2: Noun Phrase Chunking with a Consecutive Classifier\n",
      "\n",
      "\n",
      "The only piece left to fill in is the feature extractor.  We begin by\n",
      "defining a simple feature extractor which just provides the\n",
      "part-of-speech tag of the current token.  Using this feature extractor, our\n",
      "classifier-based chunker is very similar to the unigram chunker, as is\n",
      "reflected in its performance:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def npchunk_features(sentence, i, history):\n",
      "...     word, pos = sentence[i]\n",
      "...     return {\"pos\": pos}\n",
      ">>> chunker = ConsecutiveNPChunker(train_sents)\n",
      ">>> print(chunker.evaluate(test_sents))\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%\n",
      "    Precision:     79.9%\n",
      "    Recall:        86.7%\n",
      "    F-Measure:     83.2%\n",
      "\n",
      "\n",
      "\n",
      "We can also add a feature for the previous part-of-speech tag.  Adding this\n",
      "feature allows the classifier to model interactions between adjacent\n",
      "tags, and results in a chunker that is closely related to the bigram\n",
      "chunker.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def npchunk_features(sentence, i, history):\n",
      "...     word, pos = sentence[i]\n",
      "...     if i == 0:\n",
      "...         prevword, prevpos = \"<START>\", \"<START>\"\n",
      "...     else:\n",
      "...         prevword, prevpos = sentence[i-1]\n",
      "...     return {\"pos\": pos, \"prevpos\": prevpos}\n",
      ">>> chunker = ConsecutiveNPChunker(train_sents)\n",
      ">>> print(chunker.evaluate(test_sents))\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.6%\n",
      "    Precision:     81.9%\n",
      "    Recall:        87.2%\n",
      "    F-Measure:     84.5%\n",
      "\n",
      "\n",
      "\n",
      "Next, we'll try adding a feature for the current word, since we\n",
      "hypothesized that word content should be useful for chunking.  We find\n",
      "that this feature does indeed improve the chunker's performance,\n",
      "by about 1.5 percentage points (which corresponds to about a 10%\n",
      "reduction in the error rate).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def npchunk_features(sentence, i, history):\n",
      "...     word, pos = sentence[i]\n",
      "...     if i == 0:\n",
      "...         prevword, prevpos = \"<START>\", \"<START>\"\n",
      "...     else:\n",
      "...         prevword, prevpos = sentence[i-1]\n",
      "...     return {\"pos\": pos, \"word\": word, \"prevpos\": prevpos}\n",
      ">>> chunker = ConsecutiveNPChunker(train_sents)\n",
      ">>> print(chunker.evaluate(test_sents))\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  94.5%\n",
      "    Precision:     84.2%\n",
      "    Recall:        89.4%\n",
      "    F-Measure:     86.7%\n",
      "\n",
      "\n",
      "\n",
      "Finally, we can try extending the feature extractor with a variety of\n",
      "additional features, such as lookahead features ,\n",
      "paired features , and complex contextual features\n",
      ".  This last feature, called tags-since-dt, creates a\n",
      "string describing the set of all part-of-speech tags that have been\n",
      "encountered since the most recent determiner, or since the beginning\n",
      "of the sentence if there is no determiner before index i.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def npchunk_features(sentence, i, history):\n",
      "...     word, pos = sentence[i]\n",
      "...     if i == 0:\n",
      "...         prevword, prevpos = \"<START>\", \"<START>\"\n",
      "...     else:\n",
      "...         prevword, prevpos = sentence[i-1]\n",
      "...     if i == len(sentence)-1:\n",
      "...         nextword, nextpos = \"<END>\", \"<END>\"\n",
      "...     else:\n",
      "...         nextword, nextpos = sentence[i+1]\n",
      "...     return {\"pos\": pos,\n",
      "...             \"word\": word,\n",
      "...             \"prevpos\": prevpos,\n",
      "...             \"nextpos\": nextpos, \n",
      "...             \"prevpos+pos\": \"%s+%s\" % (prevpos, pos),  \n",
      "...             \"pos+nextpos\": \"%s+%s\" % (pos, nextpos),\n",
      "...             \"tags-since-dt\": tags_since_dt(sentence, i)}  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def tags_since_dt(sentence, i):\n",
      "...     tags = set()\n",
      "...     for word, pos in sentence[:i]:\n",
      "...         if pos == 'DT':\n",
      "...             tags = set()\n",
      "...         else:\n",
      "...             tags.add(pos)\n",
      "...     return '+'.join(sorted(tags))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> chunker = ConsecutiveNPChunker(train_sents)\n",
      ">>> print(chunker.evaluate(test_sents))\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  96.0%\n",
      "    Precision:     88.6%\n",
      "    Recall:        91.0%\n",
      "    F-Measure:     89.8%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Try adding different features to the feature extractor function\n",
      "npchunk_features, and see if you can further improve the\n",
      "performance of the NP chunker.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4   Recursion in Linguistic Structure\n",
      "\n",
      "4.1   Building Nested Structure with Cascaded Chunkers\n",
      "So far, our chunk structures have been relatively flat.  Trees consist\n",
      "of tagged tokens, optionally grouped under a chunk node such as\n",
      "NP.  However, it is possible to build chunk structures of\n",
      "arbitrary depth, simply by creating a multi-stage chunk grammar\n",
      "containing recursive rules.  4.1 has\n",
      "patterns for noun phrases, prepositional phrases, verb phrases, and\n",
      "sentences.\n",
      "This is a four-stage chunk grammar, and can be used to create\n",
      "structures having a depth of at most four.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "grammar = r\"\"\"\n",
      "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
      "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
      "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
      "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
      "  \"\"\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),\n",
      "    (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(cp.parse(sentence))\n",
      "(S\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n",
      "\n",
      "\n",
      "Example 4.1 (code_cascaded_chunker.py): Figure 4.1: A Chunker that Handles NP, PP, VP and S\n",
      "\n",
      "Unfortunately this result misses the VP headed by saw.  It has\n",
      "other shortcomings too.  Let's see what happens when we apply this\n",
      "chunker to a sentence having deeper nesting.  Notice that it fails to\n",
      "identify the VP chunk starting at .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sentence = [(\"John\", \"NNP\"), (\"thinks\", \"VBZ\"), (\"Mary\", \"NN\"),\n",
      "...     (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"sit\", \"VB\"),\n",
      "...     (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
      ">>> print(cp.parse(sentence))\n",
      "(S\n",
      "  (NP John/NNP)\n",
      "  thinks/VBZ\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD # [_saw-vbd]\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NN)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n",
      "\n",
      "\n",
      "\n",
      "The solution to these problems is to get the chunker to loop over its\n",
      "patterns: after trying all of them, it repeats the process.\n",
      "We add an optional second argument loop to specify the number\n",
      "of times the set of patterns should be run:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> cp = nltk.RegexpParser(grammar, loop=2)\n",
      ">>> print(cp.parse(sentence))\n",
      "(S\n",
      "  (NP John/NNP)\n",
      "  thinks/VBZ\n",
      "  (CLAUSE\n",
      "    (NP Mary/NN)\n",
      "    (VP\n",
      "      saw/VBD\n",
      "      (CLAUSE\n",
      "        (NP the/DT cat/NN)\n",
      "        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "This cascading process enables us to create deep structures.  However,\n",
      "creating and debugging a cascade is difficult, and there comes\n",
      "a point where it is more effective to do full parsing (see 8.).\n",
      "Also, the cascading process can only produce trees of fixed depth\n",
      "(no deeper than the number of stages in the cascade), and this is\n",
      "insufficient for complete syntactic analysis.\n",
      "\n",
      "\n",
      "\n",
      "4.2   Trees\n",
      "A tree is a set of connected labeled nodes, each reachable\n",
      "by a unique path from a distinguished root node.  Here's an\n",
      "example of a tree (note that they are standardly drawn upside-down):\n",
      "\n",
      "  (4)\n",
      "We use a 'family' metaphor to talk about the\n",
      "relationships of nodes in a tree: for example, S is the\n",
      "parent of VP; conversely VP is a child\n",
      "of S.  Also, since NP and VP are both\n",
      "children of S, they are also siblings.\n",
      "For convenience, there is also a text format for specifying\n",
      "trees:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "(S\n",
      "   (NP Alice)\n",
      "   (VP\n",
      "      (V chased)\n",
      "      (NP\n",
      "         (Det the)\n",
      "         (N rabbit))))\n",
      "\n",
      "\n",
      "\n",
      "Although we will focus on syntactic trees, trees can be used to encode\n",
      "any homogeneous hierarchical structure that spans a sequence\n",
      "of linguistic forms (e.g. morphological structure, discourse structure).\n",
      "In the general case, leaves and node values do not have to be strings.\n",
      "In NLTK, we create a tree by giving a node label and a list of children:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tree1 = nltk.Tree('NP', ['Alice'])\n",
      ">>> print(tree1)\n",
      "(NP Alice)\n",
      ">>> tree2 = nltk.Tree('NP', ['the', 'rabbit'])\n",
      ">>> print(tree2)\n",
      "(NP the rabbit)\n",
      "\n",
      "\n",
      "\n",
      "We can incorporate these into successively larger trees as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tree3 = nltk.Tree('VP', ['chased', tree2])\n",
      ">>> tree4 = nltk.Tree('S', [tree1, tree3])\n",
      ">>> print(tree4)\n",
      "(S (NP Alice) (VP chased (NP the rabbit)))\n",
      "\n",
      "\n",
      "\n",
      "Here are some of the methods available for tree objects:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(tree4[1])\n",
      "(VP chased (NP the rabbit))\n",
      ">>> tree4[1].label()\n",
      "'VP'\n",
      ">>> tree4.leaves()\n",
      "['Alice', 'chased', 'the', 'rabbit']\n",
      ">>> tree4[1][1][1]\n",
      "'rabbit'\n",
      "\n",
      "\n",
      "\n",
      "The bracketed representation for complex trees can be difficult to read.\n",
      "In these cases, the draw method can be very useful.\n",
      "It opens a new window, containing a graphical representation\n",
      "of the tree.  The tree display window allows you to zoom in and out,\n",
      "to collapse and expand subtrees, and to print the graphical\n",
      "representation to a postscript file (for inclusion in a document).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tree3.draw()                           \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4.3   Tree Traversal\n",
      "It is standard to use a recursive function to traverse a tree.\n",
      "The listing in 4.2 demonstrates this.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def traverse(t):\n",
      "    try:\n",
      "        t.label()\n",
      "    except AttributeError:\n",
      "        print(t, end=\" \")\n",
      "    else:\n",
      "        # Now we know that t.node is defined\n",
      "        print('(', t.label(), end=\" \")\n",
      "        for child in t:\n",
      "            traverse(child)\n",
      "        print(')', end=\" \")\n",
      "\n",
      " >>> t = nltk.Tree('(S (NP Alice) (VP chased (NP the rabbit)))')\n",
      " >>> traverse(t)\n",
      " ( S ( NP Alice ) ( VP chased ( NP the rabbit ) ) )\n",
      "\n",
      "\n",
      "Example 4.2 (code_traverse.py): Figure 4.2: A Recursive Function to Traverse a Tree\n",
      "\n",
      "\n",
      "Note\n",
      "We have used a technique called duck typing to detect that t\n",
      "is a tree (i.e. t.label() is defined).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5   Named Entity Recognition\n",
      "At the start of this chapter, we briefly introduced named entities\n",
      "(NEs). Named entities are definite noun phrases that\n",
      "refer to specific types of individuals, such as organizations, persons,\n",
      "dates, and so on. 5.1 lists some of the more commonly used\n",
      "types of NEs. These should be self-explanatory, except for \"Facility\":\n",
      "human-made artifacts in the domains of architecture and civil\n",
      "engineering; and \"GPE\": geo-political entities such as city, state/province, and country.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NE Type\n",
      "Examples\n",
      "\n",
      "\n",
      "\n",
      "ORGANIZATION\n",
      "Georgia-Pacific Corp., WHO\n",
      "\n",
      "PERSON\n",
      "Eddy Bonte, President Obama\n",
      "\n",
      "LOCATION\n",
      "Murray River, Mount Everest\n",
      "\n",
      "DATE\n",
      "June, 2008-06-29\n",
      "\n",
      "TIME\n",
      "two fifty a m, 1:30 p.m.\n",
      "\n",
      "MONEY\n",
      "175 million Canadian Dollars, GBP 10.40\n",
      "\n",
      "PERCENT\n",
      "twenty pct, 18.75 %\n",
      "\n",
      "FACILITY\n",
      "Washington Monument, Stonehenge\n",
      "\n",
      "GPE\n",
      "South East Asia, Midlothian\n",
      "\n",
      "\n",
      "Table 5.1: Commonly Used Types of Named Entity\n",
      "\n",
      "\n",
      "The goal of a named entity recognition (NER) system is to identify all\n",
      "textual mentions of the named entities. This can be broken down into\n",
      "two sub-tasks: identifying the boundaries of the NE, and identifying its\n",
      "type.\n",
      "While named entity recognition is frequently a prelude to identifying\n",
      "relations in Information Extraction, it can also contribute to other\n",
      "tasks.  For example, in Question Answering (QA), we try to improve the\n",
      "precision of Information Retrieval by recovering not whole pages, but\n",
      "just those parts which contain an answer to the user's question. Most\n",
      "QA systems take the documents returned by standard Information\n",
      "Retrieval, and then attempt to isolate the minimal text snippet in the\n",
      "document containing the answer. Now suppose the question was Who was\n",
      "the first President of the US?, and one of the documents that was\n",
      "retrieved contained the following passage:\n",
      "\n",
      "  (5)The Washington Monument is the most prominent structure in\n",
      "Washington, D.C. and one of the city's early attractions.  It was\n",
      "built in honor of George Washington, who led the country to\n",
      "independence and then became its first President.\n",
      "Analysis of the question leads us to expect that an answer should be\n",
      "of the form  X was the first President of the US, where X\n",
      "is not only a noun phrase, but also refers to a named entity of type\n",
      "PERSON. This should allow us to ignore the first sentence in the\n",
      "passage.  While it contains two occurrences of Washington,\n",
      "named entity recognition should tell us that neither of them\n",
      "has the correct type.\n",
      "How do we go about identifying named entities?  One option would be to\n",
      "look up each word in an appropriate list of names.\n",
      "For example, in the case of locations, we could use a gazetteer,\n",
      "or geographical dictionary, such as the Alexandria Gazetteer or the\n",
      "Getty Gazetteer.  However, doing this\n",
      "blindly runs into problems, as shown in 5.1.\n",
      "\n",
      "\n",
      "Figure 5.1: Location Detection by Simple Lookup for a News Story: Looking up every\n",
      "word in a gazetteer is error-prone; case distinctions may help, but\n",
      "these are not always present.\n",
      "\n",
      "Observe that the gazetteer has good coverage of locations in many countries,\n",
      "and incorrectly finds locations like Sanchez in the Dominican Republic\n",
      "and On in Vietnam.\n",
      "Of course we could omit such locations from the gazetteer, but then we won't\n",
      "be able to identify them when they do appear in a document.\n",
      "It gets even harder in the case of names for people or organizations.\n",
      "Any list of such names will probably have poor coverage. New organizations\n",
      "come into existence every day, so if we are trying to deal\n",
      "with contemporary newswire or blog entries, it is unlikely that\n",
      "we will be able to recognize many of the entities using gazetteer lookup.\n",
      "Another major source of difficulty is caused by the fact that many\n",
      "named entity terms are ambiguous. Thus\n",
      "May and North are likely to be parts of named entities for DATE\n",
      "and LOCATION, respectively, but could both be part of a PERSON;\n",
      "conversely Christian Dior looks like a PERSON but is more\n",
      "likely to be of type ORGANIZATION. A term like Yankee will be\n",
      "ordinary modifier in some contexts, but will be marked as an entity of\n",
      "type ORGANIZATION in the phrase Yankee infielders.\n",
      "Further challenges are posed by multi-word names like\n",
      "Stanford University, and by names that contain other names\n",
      "such as Cecil H. Green Library and Escondido Village Conference\n",
      "Service Center. In named entity recognition, therefore, we need\n",
      "to be able to identify the beginning and end of multi-token\n",
      "sequences.\n",
      "Named entity recognition is a task that is well-suited to the type of\n",
      "classifier-based approach that we saw for noun phrase chunking.  In\n",
      "particular, we can build a tagger that labels each word in a sentence\n",
      "using the IOB format, where chunks are labeled by their appropriate type.\n",
      "Here is part of the CONLL 2002 (conll2002) Dutch training data:\n",
      "\n",
      "Eddy N B-PER\n",
      "Bonte N I-PER\n",
      "is V O\n",
      "woordvoerder N O\n",
      "van Prep O\n",
      "diezelfde Pron O\n",
      "Hogeschool N B-ORG\n",
      ". Punc O\n",
      "\n",
      "In this representation, there is one token per line, each with its\n",
      "part-of-speech tag and its named entity tag.  Based on this training\n",
      "corpus, we can construct a tagger that can be used to label new\n",
      "sentences; and use the nltk.chunk.conlltags2tree() function to\n",
      "convert the tag sequences into a chunk tree.\n",
      "NLTK provides a classifier that has already been trained to recognize named entities,\n",
      "accessed with the function nltk.ne_chunk().  If we set the\n",
      "parameter binary=True , then named entities are just\n",
      "tagged as NE; otherwise, the classifier adds category labels such\n",
      "as PERSON, ORGANIZATION, and GPE.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent = nltk.corpus.treebank.tagged_sents()[22]\n",
      ">>> print(nltk.ne_chunk(sent, binary=True)) \n",
      "(S\n",
      "  The/DT\n",
      "  (NE U.S./NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  ...\n",
      "  according/VBG\n",
      "  to/TO\n",
      "  (NE Brooke/NNP T./NNP Mossman/NNP)\n",
      "  ...)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(nltk.ne_chunk(sent)) \n",
      "(S\n",
      "  The/DT\n",
      "  (GPE U.S./NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  ...\n",
      "  according/VBG\n",
      "  to/TO\n",
      "  (PERSON Brooke/NNP T./NNP Mossman/NNP)\n",
      "  ...)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6   Relation Extraction\n",
      "\n",
      "Once named entities have been identified in a text, we then want to extract\n",
      "the relations that exist between them. As indicated earlier, we will\n",
      "typically be looking for relations between specified types of\n",
      "named entity. One way of approaching this task is to initially look for all\n",
      "triples of the form (X, α, Y), where X and Y are named entities\n",
      "of the required types, and α is the string of words that\n",
      "intervenes between X and Y. We can then use regular expressions to\n",
      "pull out just those instances of α that express the relation\n",
      "that we are looking for. The following example searches for strings\n",
      "that contain the word in. The special regular expression\n",
      "(?!\\b.+ing\\b) is a negative lookahead assertion that allows us to\n",
      "disregard strings such as success in supervising the transition\n",
      "of, where in is followed by a gerund.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
      ">>> for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
      "...     for rel in nltk.sem.extract_rels('ORG', 'LOC', doc,\n",
      "...                                      corpus='ieer', pattern = IN):\n",
      "...         print(nltk.sem.rtuple(rel))\n",
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n",
      "\n",
      "\n",
      "\n",
      "Searching for the keyword in works reasonably well,\n",
      "though it will also retrieve false positives such as [ORG: House\n",
      "Transportation Committee] , secured the most money in the [LOC: New\n",
      "York]; there is unlikely to be simple string-based method of\n",
      "excluding filler strings such as this.\n",
      "\n",
      "As shown above, the conll2002 Dutch corpus contains not just named entity\n",
      "annotation but also part-of-speech tags. This allows us to devise\n",
      "patterns that are sensitive to these tags, as shown in the next\n",
      "example. The method clause() prints out the relations in a\n",
      "clausal form, where the binary relation symbol is specified as the\n",
      "value of parameter relsym .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import conll2002\n",
      ">>> vnv = \"\"\"\n",
      "... (\n",
      "... is/V|    # 3rd sing present and\n",
      "... was/V|   # past forms of the verb zijn ('be')\n",
      "... werd/V|  # and also present\n",
      "... wordt/V  # past of worden ('become)\n",
      "... )\n",
      "... .*       # followed by anything\n",
      "... van/Prep # followed by van ('of')\n",
      "... \"\"\"\n",
      ">>> VAN = re.compile(vnv, re.VERBOSE)\n",
      ">>> for doc in conll2002.chunked_sents('ned.train'):\n",
      "...     for rel in nltk.sem.extract_rels('PER', 'ORG', doc,\n",
      "...                                    corpus='conll2002', pattern=VAN):\n",
      "...         print(nltk.sem.clause(rel, relsym=\"VAN\")) \n",
      "VAN(\"cornet_d'elzius\", 'buitenlandse_handel')\n",
      "VAN('johan_rottiers', 'kardinaal_van_roey_instituut')\n",
      "VAN('annie_lennox', 'eurythmics')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn: Replace the last line , by\n",
      "print(nltk.rtuple(rel, lcon=True, rcon=True)). This will show you\n",
      "the actual words that intervene between the two NEs and\n",
      "also their left and right context, within a default 10-word\n",
      "window. With the help of a Dutch dictionary, you might be able to\n",
      "figure out why the result VAN('annie_lennox', 'eurythmics') is\n",
      "a false hit.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7   Summary\n",
      "\n",
      "Information extraction systems search large bodies of unrestricted\n",
      "text for specific types of entities and relations, and use them to\n",
      "populate well-organized databases.  These databases can then be used\n",
      "to find answers for specific questions.\n",
      "The typical architecture for an information extraction system begins\n",
      "by segmenting, tokenizing, and part-of-speech tagging the text.\n",
      "The resulting data is then searched for specific types of entity.\n",
      "Finally, the information extraction system looks at entities that\n",
      "are mentioned near one another in the text, and tries to determine\n",
      "whether specific relationships hold between those entities.\n",
      "Entity recognition is often performed using chunkers, which\n",
      "segment multi-token sequences, and label them with the appropriate\n",
      "entity type.  Common entity types include ORGANIZATION, PERSON,\n",
      "LOCATION, DATE, TIME, MONEY, and GPE (geo-political entity).\n",
      "Chunkers can be constructed using rule-based systems, such as the\n",
      "RegexpParser class provided by NLTK; or using machine learning\n",
      "techniques, such as the ConsecutiveNPChunker presented in this\n",
      "chapter.  In either case, part-of-speech tags are often a very\n",
      "important feature when searching for chunks.\n",
      "Although chunkers are specialized to create relatively flat\n",
      "data structures, where no two chunks are allowed to overlap,\n",
      "they can be cascaded together to build nested structures.\n",
      "Relation extraction can be performed using either rule-based\n",
      "systems which typically look for specific patterns in the\n",
      "text that connect entities and the intervening words; or using\n",
      "machine-learning systems which typically attempt to learn\n",
      "such patterns automatically from a training corpus.\n",
      "\n",
      "\n",
      "\n",
      "8   Further Reading\n",
      "Extra materials for this chapter are posted at http://nltk.org/,\n",
      "including links to freely available resources on the web.\n",
      "For more examples of chunking with NLTK, please see the\n",
      "Chunking HOWTO at http://nltk.org/howto.\n",
      "The popularity of chunking is due in great part to pioneering work by\n",
      "Abney e.g., (Church, Young, & Bloothooft, 1996). Abney's Cass chunker is described in\n",
      "http://www.vinartus.net/spa/97a.pdf.\n",
      "The word chink initially meant a sequence of stopwords,\n",
      "according to a 1975 paper by Ross and Tukey (Church, Young, & Bloothooft, 1996).\n",
      "The IOB format (or sometimes  BIO Format) was developed for\n",
      "NP chunking by (Ramshaw & Marcus, 1995), and was used for the shared NP\n",
      "bracketing task run by the Conference on Natural Language Learning\n",
      "(CoNLL) in 1999.  The same format was\n",
      "adopted by CoNLL 2000 for annotating a section of Wall Street\n",
      "Journal text as part of a shared task on NP chunking.\n",
      "Section 13.5 of (Jurafsky & Martin, 2008) contains a discussion of chunking.\n",
      "Chapter 22 covers information extraction, including named entity recognition.\n",
      "For information about text mining in biology and medicine, see\n",
      "(Ananiadou & McNaught, 2006).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9   Exercises\n",
      "\n",
      "☼ The IOB format categorizes tagged tokens as I,\n",
      "O and B.  Why are three tags necessary?  What\n",
      "problem would be caused if we used I and O tags\n",
      "exclusively?\n",
      "☼ Write a tag pattern to match noun phrases containing plural head nouns,\n",
      "e.g. \"many/JJ researchers/NNS\", \"two/CD weeks/NNS\", \"both/DT new/JJ positions/NNS\".\n",
      "Try to do this by generalizing the tag pattern that handled singular\n",
      "noun phrases.\n",
      "☼\n",
      "Pick one of the three chunk types in the CoNLL corpus.\n",
      "Inspect the CoNLL corpus and try to observe any patterns in the POS tag sequences\n",
      "that make up this kind of chunk.  Develop a simple chunker using\n",
      "the regular expression chunker nltk.RegexpParser.\n",
      "Discuss any tag sequences that are difficult to chunk reliably.\n",
      "☼\n",
      "An early definition of chunk was the material that occurs between chinks.\n",
      "Develop a chunker that starts by putting the whole sentence in a single\n",
      "chunk, and then does the rest of its work solely by chinking.\n",
      "Determine which tags (or tag sequences) are most likely to make up chinks\n",
      "with the help of your own utility program.  Compare the performance and\n",
      "simplicity of this approach relative to a chunker based entirely on\n",
      "chunk rules.\n",
      "◑ Write a tag pattern to cover noun phrases that contain gerunds,\n",
      "e.g. \"the/DT receiving/VBG end/NN\", \"assistant/NN managing/VBG editor/NN\".\n",
      "Add these patterns to the grammar, one per line.  Test your work using\n",
      "some tagged sentences of your own devising.\n",
      "◑ Write one or more tag patterns to handle coordinated noun phrases,\n",
      "e.g. \"July/NNP and/CC August/NNP\",\n",
      "\"all/DT your/PRP$ managers/NNS and/CC supervisors/NNS\",\n",
      "\"company/NN courts/NNS and/CC adjudicators/NNS\".\n",
      "◑ Carry out the following evaluation tasks for\n",
      "any of the chunkers you have developed earlier.\n",
      "(Note that most chunking corpora contain some internal\n",
      "inconsistencies, such that any reasonable rule-based approach\n",
      "will produce errors.)\n",
      "Evaluate your chunker on 100 sentences from a chunked corpus,\n",
      "and report the precision, recall and F-measure.\n",
      "Use the chunkscore.missed() and chunkscore.incorrect()\n",
      "methods to identify the errors made by your chunker.  Discuss.\n",
      "Compare the performance of your chunker to the baseline chunker\n",
      "discussed in the evaluation section of this chapter.\n",
      "\n",
      "\n",
      "◑\n",
      "Develop a chunker for one of the chunk types in the CoNLL corpus using a\n",
      "regular-expression based chunk grammar RegexpChunk.  Use any\n",
      "combination of rules for chunking, chinking, merging or splitting.\n",
      "◑ Sometimes a word is incorrectly tagged, e.g. the head noun in\n",
      "\"12/CD or/CC so/RB cases/VBZ\".  Instead of requiring manual correction of\n",
      "tagger output, good chunkers are able to work with the erroneous\n",
      "output of taggers.  Look for other examples of correctly chunked\n",
      "noun phrases with incorrect tags.\n",
      "◑\n",
      "The bigram chunker scores about 90% accuracy.\n",
      "Study its errors and try to work out why it doesn't get 100% accuracy.\n",
      "Experiment with trigram chunking.  Are you able to improve the performance any more?\n",
      "★\n",
      "Apply the n-gram and Brill tagging methods to IOB chunk tagging.\n",
      "Instead of assigning POS tags to words, here we will assign IOB tags\n",
      "to the POS tags.  E.g. if the tag DT (determiner) often occurs\n",
      "at the start of a chunk, it will be tagged B (begin).  Evaluate\n",
      "the performance of these chunking methods relative to the regular\n",
      "expression chunking methods covered in this chapter.\n",
      "★\n",
      "We saw in 5. that it is possible to establish\n",
      "an upper limit to tagging performance by looking for ambiguous n-grams,\n",
      "n-grams that are tagged in more than one possible way in the training data.\n",
      "Apply the same method to determine an upper bound on the performance\n",
      "of an n-gram chunker.\n",
      "★\n",
      "Pick one of the three chunk types in the CoNLL corpus.  Write functions\n",
      "to do the following tasks for your chosen type:\n",
      "List all the tag sequences that occur with each instance of this chunk type.\n",
      "Count the frequency of each tag sequence, and produce a ranked list in\n",
      "order of decreasing frequency; each line should consist of an integer (the frequency)\n",
      "and the tag sequence.\n",
      "Inspect the high-frequency tag sequences.  Use these as the basis for\n",
      "developing a better chunker.\n",
      "\n",
      "\n",
      "★\n",
      "The baseline chunker presented in the evaluation section tends to\n",
      "create larger chunks than it should.  For example, the\n",
      "phrase:\n",
      "[every/DT time/NN] [she/PRP] sees/VBZ [a/DT newspaper/NN]\n",
      "contains two consecutive chunks, and our baseline chunker will\n",
      "incorrectly combine the first two: [every/DT time/NN she/PRP].\n",
      "Write a program that finds which of these chunk-internal tags\n",
      "typically occur at the start of a chunk, then\n",
      "devise one or more rules that will split up these chunks.\n",
      "Combine these with the existing baseline chunker and\n",
      "re-evaluate it, to see if you have discovered an improved baseline.\n",
      "★\n",
      "Develop an NP chunker that converts POS-tagged text into a list of\n",
      "tuples, where each tuple consists of a verb followed by a sequence of\n",
      "noun phrases and prepositions,\n",
      "e.g. the little cat sat on the mat becomes ('sat', 'on', 'NP')...\n",
      "★\n",
      "The Penn Treebank contains a section of tagged Wall Street Journal text\n",
      "that has been chunked into noun phrases.  The format uses square brackets,\n",
      "and we have encountered it several times during this chapter.\n",
      "The Treebank corpus can be accessed using:\n",
      "for sent in nltk.corpus.treebank_chunk.chunked_sents(fileid).  These are flat trees,\n",
      "just as we got using nltk.corpus.conll2000.chunked_sents().\n",
      "The functions nltk.tree.pprint() and nltk.chunk.tree2conllstr()\n",
      "can be used to create Treebank and IOB strings from a tree.\n",
      "Write functions chunk2brackets() and chunk2iob() that take a single\n",
      "chunk tree as their sole argument, and return the required multi-line string\n",
      "representation.\n",
      "Write command-line conversion utilities bracket2iob.py and iob2bracket.py\n",
      "that take a file in Treebank or CoNLL format (resp) and convert it to the other\n",
      "format.  (Obtain some raw Treebank or CoNLL data from the NLTK Corpora, save it\n",
      "to a file, and then use for line in open(filename) to access it from Python.)\n",
      "\n",
      "\n",
      "★\n",
      "An n-gram chunker can use information other than the current\n",
      "part-of-speech tag and the n-1 previous chunk tags.\n",
      "Investigate other models of the context, such as\n",
      "the n-1 previous part-of-speech tags, or some combination of\n",
      "previous chunk tags along with previous and following part-of-speech tags.\n",
      "★\n",
      "Consider the way an n-gram tagger uses recent tags to inform its tagging choice.\n",
      "Now observe how a chunker may re-use this sequence information.  For example,\n",
      "both tasks will make use of the information that nouns tend to follow adjectives\n",
      "(in English).  It would appear that the same information is being maintained in\n",
      "two places.  Is this likely to become a problem as the size of the rule sets grows?\n",
      "If so, speculate about any ways that this problem might be addressed.\n",
      "\n",
      "\n",
      "\n",
      "About this document...\n",
      "UPDATED FOR NLTK 3.0.\n",
      "This is a chapter from Natural Language Processing with Python,\n",
      "by Steven Bird, Ewan Klein and Edward Loper,\n",
      "Copyright © 2019 the authors.\n",
      "It is distributed with the Natural Language Toolkit [http://nltk.org/],\n",
      "Version 3.0, under the terms of the\n",
      "Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\n",
      "[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\n",
      "This document was built on\n",
      "Wed  4 Sep 2019 11:40:48 ACST\n",
      "vectors[7] = [0, 0, 6, 6, 2, 89, 0, 16, 0, 0, 1, 17, 14, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 22, 9, 0, 1, 0, 0, 0, 4, 2, 3, 0, 30, 0, 5, 13, 0, 42, 4, 2, 5, 1, 0, 2, 0, 2, 0, 0, 0, 8, 0, 4, 0, 0, 19, 0, 0, 1, 37, 0, 0, 0, 0, 36, 0, 10, 1, 9, 0, 5, 30, 0, 2, 3, 0, 4, 13, 1, 1, 0, 3, 0, 3, 1, 1, 1, 0, 1, 0, 218, 2, 0, 1, 14, 4, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 1, 0, 2, 0, 0, 0, 1, 1, 43, 0, 0, 6, 0, 0, 0, 0, 1, 4, 2, 3, 3, 1, 5, 1, 278, 66, 18, 0, 0, 2, 18, 0, 1, 0, 5, 5, 22, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 12, 2, 10, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 1, 17, 0, 1, 0, 0, 2, 0, 1, 4, 0, 0, 1, 0, 0, 0, 7, 0, 11, 0, 0, 0, 3, 1, 7, 4, 0, 0, 6, 0, 160, 2, 11, 1, 0, 0, 4, 150, 0, 0, 0, 5, 1, 1, 0, 2, 3, 12, 11, 1, 4, 0, 0, 0, 2, 36, 5, 0, 0, 17, 0, 1, 0, 0, 0, 1, 1, 4, 0, 1, 3, 0, 0, 0, 2, 3, 11, 56, 0, 0, 0, 0, 2, 0, 0, 0, 2, 7, 0, 0, 0, 0, 1, 0, 0, 10, 0, 18, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 5, 0, 0, 5, 2, 0, 0, 1, 0, 4, 0, 12, 0, 10, 3, 0, 2, 0, 1, 0, 0, 0, 0, 4, 258, 0, 3, 0, 1, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 22, 3, 15, 14, 12, 4, 0, 0, 2, 2, 0, 9, 0, 2, 1, 1, 0, 2, 1, 3, 0, 0, 0, 0, 0, 2, 1, 15, 1, 14, 0, 5, 0, 3, 12, 1, 3, 0, 1, 1, 5, 1, 1, 0, 27, 8, 4, 22, 130, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 8, 0, 16, 19, 6, 6, 10, 10, 13, 77, 2, 2, 16, 3, 7, 5, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 28, 4, 0, 1, 0, 1, 0, 0, 3, 0, 13, 0, 1, 0, 0, 0, 15, 0, 11, 0, 0, 8, 0, 3, 10, 2, 4, 2, 0, 0, 0, 0, 1, 0, 6, 0, 4, 1, 0, 8, 4, 0, 1, 0, 3, 0, 0, 7, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 11, 4, 6, 5, 36, 0, 2, 80, 39, 0, 89, 2, 2, 0, 60, 2, 0, 0, 1, 0, 1, 1, 3, 0, 0, 2, 5, 1, 0, 0, 52, 0, 3, 0, 29, 1, 11, 1, 21, 0, 5, 0, 16, 3, 1, 5, 0, 0, 2, 6, 1, 1, 0, 6, 0, 0, 1, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 4, 0, 3, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 0, 11, 2, 14, 1, 1, 38, 32, 2, 1, 0, 2, 7, 1, 0, 0, 6, 2, 11, 1, 2, 1, 0, 1, 1, 1, 1, 11, 0, 3, 1, 0, 0, 1, 1, 0, 2, 3, 0, 0, 0, 0, 0, 16, 0, 2, 0, 9, 8, 47, 0, 30, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 27, 2, 13, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 1, 8, 2, 1, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 2, 1, 0, 0, 0, 5, 0, 0, 2, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 0, 1, 0, 0, 1, 0, 9, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 101, 17, 9, 0, 1, 0, 0, 1, 0, 0, 0, 46, 0, 0, 1, 0, 0, 0, 31, 149, 14, 26, 0, 0, 0, 0, 0, 4, 0, 0, 0, 4, 0, 11, 10, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 2, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 7, 0, 8, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 0, 0, 2, 6, 3, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 4, 0, 0, 0, 3, 0, 2, 0, 0, 3, 4, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 9, 0, 0, 0, 0, 0, 0, 16, 0, 5, 2, 0, 1, 0, 4, 0, 0, 0, 0, 0, 3, 1, 1, 0, 0, 2, 4, 0, 7, 1, 0, 0, 0, 0, 0, 3, 0, 3, 1, 0, 0, 0, 0, 21, 0, 2, 0, 0, 0, 0, 0, 6, 2, 0, 3, 0, 3, 0, 22, 1, 0, 1, 0, 0, 0, 0, 6, 0, 0, 0, 0, 2, 3, 0, 0, 2, 0, 1, 0, 0, 0, 1, 7, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 2, 8, 0, 0, 0, 9, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 3, 0, 7, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 6, 9, 2, 0, 5, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 6, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 0, 1, 0, 13, 1, 1, 2, 1, 66, 1, 1, 2, 1, 1, 1, 3, 2, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 0, 1, 0, 1, 0, 5, 31, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 9, 0, 0, 4, 0, 1, 0, 0, 0, 28, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 51, 0, 0, 1, 1, 0, 0, 0, 0, 0, 9, 0, 1, 0, 0, 4, 0, 0, 0, 0, 5, 10, 0, 3, 1, 0, 0, 0, 0, 6, 6, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 3, 8, 0, 4, 0, 0, 0, 1, 5, 0, 0, 0, 6, 0, 0, 0, 0, 1, 0, 7, 0, 0, 0, 0, 1, 1, 3, 1, 2, 1, 0, 0, 0, 0, 3, 0, 0, 2, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 5, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 6, 6, 6, 0, 0, 0, 0, 13, 0, 0, 2, 2, 0, 1, 0, 0, 3, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 2, 1, 3, 0, 0, 3, 0, 0, 0, 0, 1, 128, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 11, 0, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 1, 0, 0, 0, 2, 0, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 8, 6, 17, 1, 0, 0, 0, 0, 0, 0, 1, 3, 27, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 5, 0, 0, 0, 2, 1, 0, 0, 0, 23, 0, 0, 0, 7, 0, 1, 0, 0, 0, 2, 23, 0, 73, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 10, 8, 1, 0, 1, 0, 12, 38, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 7, 1, 13, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 1, 0, 0, 1, 0, 0, 0, 47, 0, 0, 2, 1, 1, 5, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 9, 0, 89, 5, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 1, 30, 0, 3, 0, 4, 0, 0, 0, 0, 0, 0, 3, 0, 3, 2, 1, 0, 0, 0, 0, 0, 0, 6, 0, 0, 36, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 1, 2, 1, 0, 1, 0, 0, 0, 0, 0, 1, 3, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 2, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 6, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 11, 4, 3, 9, 0, 1, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 0, 0, 0, 0, 5, 0, 0, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 11, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 3, 0, 0, 3, 12, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 2, 0, 0, 0, 0, 5, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 4, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 45, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 4, 2, 0, 0, 2, 0, 2, 0, 2, 0, 0, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 8, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 1, 3, 0, 1, 0, 1, 0, 1, 0, 1, 2, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 5, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 13, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 14, 1, 13, 0, 3, 0, 0, 0, 6, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 1, 0, 5, 0, 0, 0, 7, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 37, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 4, 0, 0, 3, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 8, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 3, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 97, 41, 0, 0, 0, 0, 0, 0, 0, 12, 1, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 13, 2, 0, 13, 0, 0, 7, 1, 0, 0, 0, 0, 0, 4, 2, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 9, 0, 30, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 5, 0, 0, 0, 2, 0, 0, 0, 0, 0, 6, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 1, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 2, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 2, 1, 4, 3, 3, 3, 3, 8, 8, 11, 11, 1, 2, 2, 6, 2, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 58, 13, 2, 1, 3, 1, 1, 6, 6, 12, 3, 3, 1, 20, 12, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 4, 1, 4, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 5, 10, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 25, 1, 1, 1, 1, 25, 2, 1, 1, 1, 1, 6, 1, 8, 6, 2, 1, 1, 8, 1, 1, 1, 5, 3, 3, 2, 3, 2, 2, 2, 4, 5, 4, 1, 3, 2, 1, 2, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 2, 7, 6, 1, 1, 6, 14, 1, 1, 1, 2, 2, 6, 2, 2, 1, 1, 1, 2, 1, 1, 5, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 3, 3, 3, 6, 1, 1, 1, 1, 1, 1, 5, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 10, 1, 1, 1, 3, 2, 2, 2, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "docs[8] = 8. Analyzing Sentence Structure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8. Analyzing Sentence Structure\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Earlier chapters focused on words: how to identify them,\n",
      "analyze their structure, assign them to lexical categories,\n",
      "and access their meanings.\n",
      "We have also seen how to identify patterns in word sequences or n-grams.\n",
      "However, these methods only scratch the surface of the complex constraints\n",
      "that govern sentences.\n",
      "We need a way to deal with the ambiguity that natural language is famous for.\n",
      "We also need to be able to cope with the fact that there are an unlimited number\n",
      "of possible sentences, and we can only write finite programs to analyze their\n",
      "structures and discover their meanings.\n",
      "The goal of this chapter is to answer the following questions:\n",
      "\n",
      "How can we use a formal grammar to describe the structure of an unlimited set of sentences?\n",
      "How do we represent the structure of sentences using syntax trees?\n",
      "How do parsers analyze a sentence and automatically build a syntax tree?\n",
      "\n",
      "Along the way, we will cover the fundamentals of English syntax, and\n",
      "see that there are systematic aspects of meaning that are much easier\n",
      "to capture once we have identified the structure of sentences.\n",
      "\n",
      "1   Some Grammatical Dilemmas\n",
      "\n",
      "1.1   Linguistic Data and Unlimited Possibilities\n",
      "Previous chapters have shown you how to process and analyse text\n",
      "corpora, and we have stressed the challenges for NLP in dealing with\n",
      "the vast amount of electronic language data that is growing\n",
      "daily. Let's consider this data more closely, and make the thought\n",
      "experiment that we have a gigantic corpus consisting of everything\n",
      "that has been either uttered or written in English over, say, the last\n",
      "50 years. Would we be justified in calling this corpus \"the language\n",
      "of modern English\"? There are a number of reasons why we might answer\n",
      "No. Recall that in 3, we asked you to search\n",
      "the web for instances of the pattern the of.  Although it is\n",
      "easy to find examples on the web containing this word sequence, such as\n",
      "New man at the of IMG\n",
      "(http://www.telegraph.co.uk/sport/2387900/New-man-at-the-of-IMG.html),\n",
      "speakers of English will say that most such examples are errors, and\n",
      "therefore not part of English after all.\n",
      "Accordingly, we can argue\n",
      "that the \"modern English\" is not equivalent to the very big\n",
      "set of word sequences in our imaginary corpus. Speakers\n",
      "of English can make judgements about these sequences, and will reject\n",
      "some of them as being ungrammatical.\n",
      "Equally, it is easy to compose a new sentence and have speakers agree that it is perfectly\n",
      "good English.  For example, sentences have an interesting property\n",
      "that they can be embedded inside larger sentences.  Consider the\n",
      "following sentences:\n",
      "\n",
      "  (1)\n",
      "  a.Usain Bolt broke the 100m record\n",
      "\n",
      "  b.The Jamaica Observer reported that Usain Bolt broke the 100m record\n",
      "\n",
      "  c.Andre said The Jamaica Observer reported that Usain Bolt broke the 100m record\n",
      "\n",
      "  d.I think Andre said the Jamaica Observer reported that Usain Bolt broke the 100m record\n",
      "\n",
      "If we replaced whole sentences with the symbol S, we would see patterns like\n",
      "Andre said S and I think S.  These are templates for taking a sentence\n",
      "and constructing a bigger sentence.  There are other templates we can use, like\n",
      "S but S, and S when S.  With a bit of ingenuity we can\n",
      "construct some really long sentences using these templates.\n",
      "Here's an impressive example from a Winnie the Pooh story by A.A. Milne,\n",
      "In which Piglet is Entirely Surrounded by Water:\n",
      "\n",
      "[You can imagine Piglet's joy when at last the ship came in sight of\n",
      "him.] In after-years he liked to think that he had been in Very\n",
      "Great Danger during the Terrible Flood, but the only danger he had\n",
      "really been in was the last half-hour of his imprisonment, when\n",
      "Owl, who had just flown up, sat on a branch of his tree to comfort\n",
      "him, and told him a very long story about an aunt who had once laid\n",
      "a seagull's egg by mistake, and the story went on and on, rather\n",
      "like this sentence, until Piglet who was listening out of his\n",
      "window without much hope, went to sleep quietly and naturally,\n",
      "slipping slowly out of the window towards the water until he was\n",
      "only hanging on by his toes, at which moment, luckily, a sudden\n",
      "loud squawk from Owl, which was really part of the story, being\n",
      "what his aunt said, woke the Piglet up and just gave him time to\n",
      "jerk himself back into safety and say, \"How interesting, and did\n",
      "she?\" when — well, you can imagine his joy when at last he saw\n",
      "the good ship, Brain of Pooh (Captain, C. Robin; 1st Mate, P. Bear)\n",
      "coming over the sea to rescue him...\n",
      "This long sentence actually has a simple structure that begins\n",
      "S but S when S.  We can see from this example that language\n",
      "provides us with constructions which seem to allow us to extend\n",
      "sentences indefinitely.  It is also striking that\n",
      "we can understand sentences of arbitrary length\n",
      "that we've never heard before:  it's not hard to concoct an\n",
      "entirely novel sentence, one that has probably never been used before\n",
      "in the history of the language, yet all speakers of the language\n",
      "will understand it.\n",
      "The purpose of a grammar is to give an explicit description of a\n",
      "language. But the way in which we think of a grammar is closely\n",
      "intertwined with what we consider to be a language. Is it a\n",
      "large but finite set of observed utterances and written texts? Is it\n",
      "something more abstract like the implicit knowledge that competent\n",
      "speakers have about grammatical sentences? Or is it some combination\n",
      "of the two? We won't take a stand on this issue, but instead will\n",
      "introduce the main approaches.\n",
      "In this chapter, we will adopt the formal framework\n",
      "of \"generative grammar\", in which\n",
      "a \"language\" is considered to be nothing more than an\n",
      "enormous collection of all grammatical sentences, and a\n",
      "grammar is a formal notation that can be used for \"generating\" the\n",
      "members of this set.  Grammars use recursive productions\n",
      "of the form S → S and S, as we will explore in\n",
      "3.  In 10. we will extend this,\n",
      "to automatically build up the meaning of a sentence out of the meanings\n",
      "of its parts.\n",
      "\n",
      "\n",
      "1.2   Ubiquitous Ambiguity\n",
      "A well-known example of ambiguity is shown in (2),\n",
      "from the Groucho Marx movie, Animal Crackers (1930):\n",
      "\n",
      "\n",
      "  (2)While hunting in Africa, I shot an elephant in my pajamas.\n",
      "How he got into my pajamas, I don't know.\n",
      "Let's take a closer look at the ambiguity in the phrase:\n",
      "I shot an elephant in my pajamas.  First we\n",
      "need to define a simple grammar:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
      "... S -> NP VP\n",
      "... PP -> P NP\n",
      "... NP -> Det N | Det N PP | 'I'\n",
      "... VP -> V NP | VP PP\n",
      "... Det -> 'an' | 'my'\n",
      "... N -> 'elephant' | 'pajamas'\n",
      "... V -> 'shot'\n",
      "... P -> 'in'\n",
      "... \"\"\")\n",
      "\n",
      "\n",
      "\n",
      "This grammar permits the sentence to be analyzed in two ways,\n",
      "depending on whether the prepositional phrase in my pajamas\n",
      "describes the elephant or the shooting event.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
      ">>> parser = nltk.ChartParser(groucho_grammar)\n",
      ">>> for tree in parser.parse(sent):\n",
      "...     print(tree)\n",
      "...\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n",
      "\n",
      "\n",
      "\n",
      "The program produces two bracketed structures, which we can depict as\n",
      "trees, as shown in (3b):\n",
      "\n",
      "  (3)\n",
      "  a.\n",
      "\n",
      "  b.\n",
      "\n",
      "Notice that there's no ambiguity concerning the meaning of any of the words;\n",
      "e.g. the word shot doesn't refer to the act of using a gun in the first sentence,\n",
      "and using a camera in the second sentence.\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Consider the following sentences and see if you can think of two quite different\n",
      "interpretations: Fighting animals could be dangerous.\n",
      "Visiting relatives can be tiresome.  Is ambiguity of the individual\n",
      "words to blame?  If not, what is the cause of the ambiguity?\n",
      "\n",
      "This chapter presents grammars and parsing, as the formal and\n",
      "computational methods for investigating and modeling the linguistic\n",
      "phenomena we have been discussing.\n",
      "As we shall see, patterns of well-formedness and ill-formedness in a\n",
      "sequence of words can be understood with respect to the\n",
      "phrase structure and dependencies.  We can develop formal\n",
      "models of these structures using grammars and parsers.\n",
      "As before, a key motivation is natural language understanding.  How\n",
      "much more of the meaning of a text can we access when we can reliably\n",
      "recognize the linguistic structures it contains?  Having read in a\n",
      "text, can a program \"understand\" it enough to be able to answer simple\n",
      "questions about \"what happened\" or \"who did what to whom\"?  Also as\n",
      "before, we will develop simple programs to process annotated corpora\n",
      "and perform useful tasks.\n",
      "\n",
      "\n",
      "\n",
      "2   What's the Use of Syntax?\n",
      "\n",
      "2.1   Beyond n-grams\n",
      "We gave an example in 2. of how to use\n",
      "the frequency information in bigrams to generate text that seems\n",
      "perfectly acceptable for small sequences of words but rapidly\n",
      "degenerates into nonsense. Here's another pair of examples that we created by\n",
      "computing the bigrams over the text of a childrens' story, The\n",
      "Adventures of Buster Brown (http://www.gutenberg.org/files/22816/22816.txt):\n",
      "\n",
      "  (4)\n",
      "  a.He roared with me the pail slip down his back\n",
      "\n",
      "  b.The worst part and clumsy looking for whoever heard light\n",
      "\n",
      "You intuitively know that these sequences are \"word-salad\", but you\n",
      "probably find it hard to pin down what's wrong with them. One\n",
      "benefit of studying grammar is that it provides a conceptual framework\n",
      "and vocabulary for spelling out these intuitions. Let's take a closer look\n",
      "at the sequence the worst part and clumsy looking. This looks like a coordinate\n",
      "structure, where two phrases are joined by a coordinating\n",
      "conjunction such as and, but or or. Here's an\n",
      "informal (and simplified) statement of how coordination works\n",
      "syntactically:\n",
      "Coordinate Structure:\n",
      "\n",
      "If v1 and v2 are both phrases of grammatical\n",
      "category X, then v1 and v2 is also a\n",
      "phrase of category  X.\n",
      "Here are a couple of examples. In the first, two NPs (noun\n",
      "phrases) have been conjoined to make an NP, while in the second,\n",
      "two APs (adjective phrases) have been conjoined to make an\n",
      "AP.\n",
      "\n",
      "  (5)\n",
      "  a.The book's ending was (NP the worst part and the best part) for me.\n",
      "\n",
      "  b.On land they are (AP slow and clumsy looking).\n",
      "\n",
      "What we can't do is conjoin an NP and an AP, which is\n",
      "why the worst part and clumsy looking is ungrammatical.\n",
      "Before we can formalize these ideas, we need to\n",
      "understand the concept of constituent structure.\n",
      "Constituent structure is based on the observation that words combine\n",
      "with other words to form units. The evidence that a sequence of words\n",
      "forms such a unit is given by substitutability — that is, a\n",
      "sequence of words in a well-formed sentence can be replaced by a\n",
      "shorter sequence without rendering the sentence ill-formed. To clarify\n",
      "this idea, consider the following sentence:\n",
      "\n",
      "  (6)The little bear saw the fine fat trout in the brook.\n",
      "The fact that we can substitute He for The little bear\n",
      "indicates that the latter sequence is a unit. By contrast, we cannot\n",
      "replace  little bear saw in the same way.\n",
      "\n",
      "\n",
      "  (7)\n",
      "  a.He saw the fine fat trout in the brook.\n",
      "\n",
      "  b.*The he the fine fat trout in the brook.\n",
      "\n",
      "In 2.1, we systematically substitute longer sequences\n",
      "by shorter ones in a way which preserves grammaticality. Each sequence\n",
      "that forms a unit can in fact be replaced by a single word, and we end\n",
      "up with just two elements.\n",
      "\n",
      "\n",
      "Figure 2.1: Substitution of Word Sequences: working from the top row, we can replace\n",
      "particular sequences of words (e.g. the brook) with individual\n",
      "words (e.g. it); repeating this process we arrive at a grammatical\n",
      "two-word sentence.\n",
      "\n",
      "In 2.2, we have added\n",
      "grammatical category labels to the words we saw in the earlier figure.\n",
      "The labels NP, VP, and PP stand for noun phrase,\n",
      "verb phrase and prepositional phrase respectively.\n",
      "\n",
      "\n",
      "Figure 2.2: Substitution of Word Sequences Plus Grammatical Categories:\n",
      "This diagram reproduces 2.1 along with grammatical\n",
      "categories corresponding to noun phrases (NP), verb phrases (VP),\n",
      "prepositional phrases (PP), and nominals (Nom).\n",
      "\n",
      "If we now strip out the words apart from the topmost row, add an\n",
      "S node, and flip the figure over, we end up with a standard\n",
      "phrase structure tree, shown in (8).\n",
      "Each node in this tree (including the words) is called\n",
      "a constituent.  The immediate constituents of\n",
      "S are NP and VP.\n",
      "\n",
      "  (8)\n",
      "As we will see in the next section, a grammar specifies how the sentence\n",
      "can be subdivided into its immediate constituents, and how these can be further\n",
      "subdivided until we reach the level of individual words.\n",
      "\n",
      "Note\n",
      "As we saw in 1, sentences can have arbitrary length.\n",
      "Consequently, phrase structure trees can have arbitrary depth.\n",
      "The cascaded chunk parsers we saw in 4\n",
      "can only produce structures of bounded depth, so chunking methods\n",
      "aren't applicable here.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3   Context Free Grammar\n",
      "\n",
      "3.1   A Simple Grammar\n",
      "\n",
      "Let's start off by looking at a simple context-free grammar.  By\n",
      "convention, the left-hand-side of the first production is the\n",
      "start-symbol of the grammar, typically S, and all\n",
      "well-formed trees must have this symbol as their root label. In\n",
      "NLTK, context-free grammars are defined in the nltk.grammar\n",
      "module.  In 3.1 we define a grammar and show how to parse a\n",
      "simple sentence admitted by the grammar.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
      "  S -> NP VP\n",
      "  VP -> V NP | V NP PP\n",
      "  PP -> P NP\n",
      "  V -> \"saw\" | \"ate\" | \"walked\"\n",
      "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
      "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
      "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
      "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
      "  \"\"\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent = \"Mary saw Bob\".split()\n",
      ">>> rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
      ">>> for tree in rd_parser.parse(sent):\n",
      "...      print(tree)\n",
      "(S (NP Mary) (VP (V saw) (NP Bob)))\n",
      "\n",
      "\n",
      "Example 3.1 (code_cfg1.py): Figure 3.1: A Simple Context-Free Grammar\n",
      "\n",
      "The grammar in 3.1 contains productions involving various syntactic categories,\n",
      "as laid out in 3.1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Symbol\n",
      "Meaning\n",
      "Example\n",
      "\n",
      "\n",
      "\n",
      "S\n",
      "sentence\n",
      "the man walked\n",
      "\n",
      "NP\n",
      "noun phrase\n",
      "a dog\n",
      "\n",
      "VP\n",
      "verb phrase\n",
      "saw a park\n",
      "\n",
      "PP\n",
      "prepositional phrase\n",
      "with a telescope\n",
      "\n",
      "Det\n",
      "determiner\n",
      "the\n",
      "\n",
      "N\n",
      "noun\n",
      "dog\n",
      "\n",
      "V\n",
      "verb\n",
      "walked\n",
      "\n",
      "P\n",
      "preposition\n",
      "in\n",
      "\n",
      "\n",
      "Table 3.1: Syntactic Categories\n",
      "\n",
      "\n",
      "A production like VP -> V NP | V NP PP has a disjunction on the\n",
      "righthand side, shown by the | and is an abbreviation for the two productions\n",
      "VP -> V NP and VP -> V NP PP.\n",
      "\n",
      "\n",
      "Figure 3.2: Recursive Descent Parser Demo: This tool allows you to watch the operation of\n",
      "a recursive descent parser as it grows the parse tree and matches it against\n",
      "the input words.\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Try developing a simple grammar of your own, using the\n",
      "recursive descent parser application, nltk.app.rdparser(),\n",
      "shown in 3.2.\n",
      "It comes already loaded with a sample grammar, but you can\n",
      "edit this as you please (using the Edit menu).\n",
      "Change the grammar, and the sentence to be parsed, and\n",
      "run the parser using the autostep button.\n",
      "\n",
      "If we parse the sentence The dog saw a man in the park using\n",
      "the grammar shown in 3.1, we end up with two trees, similar to\n",
      "those we saw for (3b):\n",
      "\n",
      "  (9)\n",
      "  a.\n",
      "\n",
      "  b.\n",
      "\n",
      "Since our grammar licenses two trees for this sentence, the sentence is\n",
      "said to be structurally ambiguous.  The ambiguity in question is called\n",
      "a prepositional phrase attachment ambiguity, as we saw earlier in this chapter.\n",
      "As you may recall, it is an ambiguity about attachment since the\n",
      "PP in the park needs to be attached to one of two places\n",
      "in the tree: either as a child of VP or else as a child of NP.\n",
      "When the PP is attached to VP, the intended interpretation\n",
      "is that the seeing event happened\n",
      "in the park.  However, if the PP is attached to NP,\n",
      "then it was the man who was in the park, and the agent of the seeing (the dog)\n",
      "might have been sitting on the balcony of an apartment overlooking the\n",
      "park.\n",
      "\n",
      "\n",
      "3.2   Writing Your Own Grammars\n",
      "If you are interested in experimenting with writing CFGs, you will\n",
      "find it helpful to create and edit your grammar in a text\n",
      "file, say mygrammar.cfg. You can then load it into NLTK and\n",
      "parse with it as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> grammar1 = nltk.data.load('file:mygrammar.cfg')\n",
      ">>> sent = \"Mary saw Bob\".split()\n",
      ">>> rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
      ">>> for tree in rd_parser.parse(sent):\n",
      "...      print(tree)\n",
      "\n",
      "\n",
      "\n",
      "Make sure that you put a .cfg suffix on the filename, and that\n",
      "there are no spaces in the string 'file:mygrammar.cfg'. If the\n",
      "command print(tree) produces no output, this is probably because\n",
      "your sentence sent is not admitted by your grammar. In this case,\n",
      "call the parser with tracing set to be on: rd_parser =\n",
      "nltk.RecursiveDescentParser(grammar1, trace=2). You can also check\n",
      "what productions are currently in the grammar with the command for p\n",
      "in grammar1.productions(): print(p).\n",
      "When you write CFGs for parsing in NLTK, you cannot combine\n",
      "grammatical categories with lexical items on the righthand side of the\n",
      "same production. Thus, a production such as PP -> 'of' NP is disallowed. In\n",
      "addition, you are not permitted to place multi-word lexical items on the\n",
      "righthand side of a production. So rather than writing NP -> 'New\n",
      "York', you have to resort to something like NP -> 'New_York'\n",
      "instead.\n",
      "\n",
      "\n",
      "3.3   Recursion in Syntactic Structure\n",
      "A grammar is said to be recursive if a category occurring on the left hand\n",
      "side of a production also appears on\n",
      "the righthand side of a production, as illustrated in 3.3.\n",
      "The production Nom -> Adj Nom (where Nom is the\n",
      "category of nominals) involves direct recursion on the category\n",
      "Nom, whereas indirect recursion on S arises from the\n",
      "combination of two productions, namely S -> NP VP and VP -> V S.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "grammar2 = nltk.CFG.fromstring(\"\"\"\n",
      "  S  -> NP VP\n",
      "  NP -> Det Nom | PropN\n",
      "  Nom -> Adj Nom | N\n",
      "  VP -> V Adj | V NP | V S | V NP PP\n",
      "  PP -> P NP\n",
      "  PropN -> 'Buster' | 'Chatterer' | 'Joe'\n",
      "  Det -> 'the' | 'a'\n",
      "  N -> 'bear' | 'squirrel' | 'tree' | 'fish' | 'log'\n",
      "  Adj  -> 'angry' | 'frightened' |  'little' | 'tall'\n",
      "  V ->  'chased'  | 'saw' | 'said' | 'thought' | 'was' | 'put'\n",
      "  P -> 'on'\n",
      "  \"\"\")\n",
      "\n",
      "\n",
      "Example 3.3 (code_cfg2.py): Figure 3.3: A Recursive Context-Free Grammar\n",
      "\n",
      "To see how recursion arises from this grammar, consider the following\n",
      "trees.  (10a) involves nested nominal phrases,\n",
      "while (10b) contains nested sentences.\n",
      "\n",
      "  (10)\n",
      "  a.\n",
      "\n",
      "  b.\n",
      "\n",
      "We've only illustrated two levels of recursion here, but there's\n",
      "no upper limit on the depth.  You can experiment with parsing\n",
      "sentences that involve more deeply nested structures.\n",
      "Beware that the RecursiveDescentParser is unable to handle\n",
      "left-recursive productions of the form X -> X Y; we will\n",
      "return to this in 4.\n",
      "\n",
      "\n",
      "\n",
      "4   Parsing With Context Free Grammar\n",
      "\n",
      "A parser processes input sentences according to the\n",
      "productions of a grammar, and builds one or more\n",
      "constituent structures that conform to the grammar.\n",
      "A grammar is a declarative specification of well-formedness —\n",
      "it is actually just a string, not a program.\n",
      "A parser is a procedural interpretation of the grammar.\n",
      "It searches through the space of trees licensed by a grammar\n",
      "to find one that has the required sentence along its fringe.\n",
      "\n",
      "A parser permits a grammar to be evaluated against\n",
      "a collection of test sentences, helping linguists\n",
      "to discover mistakes in their grammatical analysis.\n",
      "A parser can serve as a model of psycholinguistic processing,\n",
      "helping to explain the difficulties that humans have with processing\n",
      "certain syntactic constructions.\n",
      "Many natural language applications involve parsing at some point;\n",
      "for example, we would expect the natural language questions\n",
      "submitted to a question-answering system to undergo parsing as an initial step.\n",
      "In this section we see two simple parsing algorithms,\n",
      "a top-down method called recursive descent parsing,\n",
      "and a bottom-up method called shift-reduce parsing.\n",
      "We also see some more sophisticated algorithms,\n",
      "a top-down method with bottom-up filtering called\n",
      "left-corner parsing, and a dynamic programming\n",
      "technique called chart parsing.\n",
      "\n",
      "4.1   Recursive Descent Parsing\n",
      "The simplest kind of parser interprets a grammar as a specification\n",
      "of how to break a high-level goal into several lower-level subgoals.\n",
      "The top-level goal is to find an S.  The S → NP VP\n",
      "production permits the parser to replace this goal with two subgoals:\n",
      "find an NP, then find a VP.  Each of these subgoals can be\n",
      "replaced in turn by sub-sub-goals, using productions that have NP\n",
      "and VP on their left-hand side.  Eventually, this expansion\n",
      "process leads to subgoals such as: find the word telescope.  Such\n",
      "subgoals can be directly compared against the input sequence, and\n",
      "succeed if the next word is matched.  If there is no match the parser\n",
      "must back up and try a different alternative.\n",
      "The recursive descent parser builds a parse tree during the above\n",
      "process.  With the initial goal (find an S), the S root node\n",
      "is created.  As the above process recursively expands its goals using\n",
      "the productions of the grammar, the parse tree is extended downwards\n",
      "(hence the name recursive descent).  We can see this in action using\n",
      "the graphical demonstration nltk.app.rdparser().\n",
      "Six stages of the execution of this parser are shown in 4.1.\n",
      "\n",
      "\n",
      "Figure 4.1: Six Stages of a Recursive Descent Parser: the parser begins with a\n",
      "tree consisting of the node S; at each stage it consults the grammar\n",
      "to find a production that can be used to enlarge the tree; when\n",
      "a lexical production is encountered, its word is compared against the input;\n",
      "after a complete parse has been found, the parser backtracks to look for\n",
      "more parses.\n",
      "\n",
      "During this process, the parser is often forced to choose between several\n",
      "possible productions.  For example, in going from step 3 to step 4, it\n",
      "tries to find productions with N on the left-hand side.  The\n",
      "first of these is N → man.  When this does not work\n",
      "it backtracks, and tries other N productions in order, until it\n",
      "gets to N → dog, which matches the next word in the\n",
      "input sentence.  Much later, as shown in step 5, it finds a complete\n",
      "parse.  This is a tree that covers the entire sentence, without any\n",
      "dangling edges.  Once a parse has been found, we can get the parser to\n",
      "look for additional parses.  Again it will backtrack and explore other\n",
      "choices of production in case any of them result in a parse.\n",
      "NLTK provides a recursive descent parser:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
      ">>> sent = 'Mary saw a dog'.split()\n",
      ">>> for tree in rd_parser.parse(sent):\n",
      "...     print(tree)\n",
      "(S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "RecursiveDescentParser() takes an optional parameter trace.\n",
      "If trace is greater than zero, then the parser will report the steps\n",
      "that it takes as it parses a text.\n",
      "\n",
      "Recursive descent parsing has three key shortcomings.  First,\n",
      "left-recursive productions like NP -> NP PP send it\n",
      "into an infinite loop.  Second, the parser wastes a lot of time\n",
      "considering words and structures that do not correspond to the input\n",
      "sentence.  Third, the backtracking process may discard parsed\n",
      "constituents that will need to be rebuilt again later.  For example,\n",
      "backtracking over VP -> V NP will discard the subtree\n",
      "created for the NP.  If the parser then proceeds with\n",
      "VP -> V NP PP, then the NP subtree must be created all\n",
      "over again.\n",
      "Recursive descent parsing is a kind of top-down parsing.\n",
      "Top-down parsers use a grammar to predict what the input will be,\n",
      "before inspecting the input!  However, since the input is available to\n",
      "the parser all along, it would be more sensible to consider the input\n",
      "sentence from the very beginning.  This approach is called\n",
      "bottom-up parsing, and we will see an example in the next section.\n",
      "\n",
      "\n",
      "4.2   Shift-Reduce Parsing\n",
      "A simple kind of bottom-up parser is the shift-reduce parser.\n",
      "In common with all bottom-up parsers, a shift-reduce\n",
      "parser tries to find sequences of words and phrases that correspond\n",
      "to the right hand side of a grammar production, and replace them\n",
      "with the left-hand side, until the whole sentence is reduced to\n",
      "an S.\n",
      "\n",
      "The shift-reduce parser repeatedly pushes the next input word onto a\n",
      "stack (4.1); this is the shift operation.\n",
      "If the top n items on the stack match\n",
      "the n items on the right hand side of some production,\n",
      "then they are all popped off the stack, and the item on the left-hand\n",
      "side of the production is pushed on the stack.  This replacement of\n",
      "the top n items with a single item is the reduce operation.\n",
      "This operation may only be applied to the top of the stack;\n",
      "reducing items lower in the stack must be done before later items are\n",
      "pushed onto the stack.  The parser finishes when all the input is\n",
      "consumed and there is only one item remaining on the stack, a parse\n",
      "tree with an S node as its root.\n",
      "The shift-reduce parser builds a parse tree during the above process.\n",
      "Each time it pops n items off the stack it combines them into\n",
      "a partial parse tree, and pushes this back on the stack.\n",
      "We can see the shift-reduce parsing algorithm in action using the\n",
      "graphical demonstration nltk.app.srparser().\n",
      "Six stages of the execution of this parser are shown in 4.2.\n",
      "\n",
      "\n",
      "Figure 4.2: Six Stages of a Shift-Reduce Parser: the parser begins by shifting the\n",
      "first input word onto its stack; once the top items on the stack match\n",
      "the right hand side of a grammar production, they can be replaced with\n",
      "the left hand side of that production; the parser succeeds once all input\n",
      "is consumed and one S item remains on the stack.\n",
      "\n",
      "NLTK provides ShiftReduceParser(), a simple\n",
      "implementation of a shift-reduce parser.  This parser does not\n",
      "implement any backtracking, so it is not guaranteed to find a parse\n",
      "for a text, even if one exists.  Furthermore, it will only find at\n",
      "most one parse, even if more parses exist.  We can provide an\n",
      "optional trace parameter that controls how verbosely the\n",
      "parser reports the steps that it takes as it parses a text:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sr_parser = nltk.ShiftReduceParser(grammar1)\n",
      ">>> sent = 'Mary saw a dog'.split()\n",
      ">>> for tree in sr_parser.parse(sent):\n",
      "...     print(tree)\n",
      "  (S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Run the above parser in tracing mode to see the sequence of shift and reduce\n",
      "operations, using sr_parse = nltk.ShiftReduceParser(grammar1, trace=2)\n",
      "\n",
      "A shift-reduce parser can reach a dead end and fail to find any parse,\n",
      "even if the input sentence is well-formed according to the grammar.\n",
      "When this happens, no input remains, and the stack contains items\n",
      "which cannot be reduced to an S.  The problem arises because\n",
      "there are choices made earlier that cannot be undone by the parser\n",
      "(although users of the graphical demonstration can undo their choices).\n",
      "There are two kinds of choices to be made by the parser:\n",
      "(a) which reduction to do when more than one is possible\n",
      "(b) whether to shift or reduce when either action is possible.\n",
      "A shift-reduce parser may be extended to implement policies for resolving such\n",
      "conflicts.  For example, it may address shift-reduce conflicts by\n",
      "shifting only when no reductions are possible, and it may address\n",
      "reduce-reduce conflicts by favoring the reduction operation that removes\n",
      "the most items from the stack.  (A generalization of shift-reduce\n",
      "parser, a \"lookahead LR parser\", is commonly used in programming\n",
      "language compilers.)\n",
      "The advantage of shift-reduce parsers over recursive descent parsers\n",
      "is that they only build structure that corresponds to the words in the\n",
      "input.  Furthermore, they only build each sub-structure once,\n",
      "e.g. NP(Det(the), N(man)) is only built and pushed onto the stack\n",
      "a single time, regardless of whether it will later be used by the\n",
      "VP -> V NP PP reduction or the NP -> NP PP reduction.\n",
      "\n",
      "\n",
      "4.3   The Left-Corner Parser\n",
      "One of the problems with the recursive descent parser is that it\n",
      "goes into an infinite loop when it encounters a left-recursive production.\n",
      "This is because it applies the grammar\n",
      "productions blindly, without considering the actual input sentence.\n",
      "A left-corner parser is a hybrid between the bottom-up and top-down\n",
      "approaches we have seen.\n",
      "Grammar grammar1 allows us to produce the following parse of John saw\n",
      "Mary:\n",
      "\n",
      "  (11)\n",
      "Recall that the grammar (defined in 3.3) has the following productions for expanding NP:\n",
      "\n",
      "  (12)\n",
      "  a.NP -> Det N\n",
      "\n",
      "  b.NP -> Det N PP\n",
      "\n",
      "  c.NP -> \"John\" | \"Mary\" | \"Bob\"\n",
      "\n",
      "\n",
      "Suppose we ask you to first look at tree (11), and then decide\n",
      "which of the NP productions you'd want a recursive descent parser to\n",
      "apply first — obviously, (12c) is the right choice! How do you\n",
      "know that it would be pointless to apply (12a) or (12b) instead? Because\n",
      "neither of these productions will derive a sequence whose first word is\n",
      "John.  That is, we can easily tell that in a successful\n",
      "parse of John saw Mary, the parser has to expand NP in\n",
      "such a way that NP derives the sequence John α. More\n",
      "generally, we say that a category B is a left-corner of\n",
      "a tree rooted in A if  A ⇒*\n",
      "B α.\n",
      "\n",
      "  (13)\n",
      "A left-corner parser is a top-down parser with bottom-up filtering.\n",
      "Unlike an ordinary recursive descent parser, it does not get trapped\n",
      "in left recursive productions.\n",
      "Before starting its work, a left-corner parser preprocesses the\n",
      "context-free grammar to build a table where each row contains two\n",
      "cells, the first holding a non-terminal, and the second holding the\n",
      "collection of possible left corners of that non-terminal. 4.1\n",
      "illustrates this for the grammar from grammar2.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Category\n",
      "Left-Corners (pre-terminals)\n",
      "\n",
      "\n",
      "\n",
      "S\n",
      "NP\n",
      "\n",
      "NP\n",
      "Det, PropN\n",
      "\n",
      "VP\n",
      "V\n",
      "\n",
      "PP\n",
      "P\n",
      "\n",
      "\n",
      "Table 4.1: Left-Corners in grammar2\n",
      "\n",
      "\n",
      "Each time a production is considered by the parser, it checks that the\n",
      "next input word is compatible with at least one of the pre-terminal\n",
      "categories in the left-corner table.\n",
      "\n",
      "\n",
      "\n",
      "4.4   Well-Formed Substring Tables\n",
      "The simple parsers discussed above suffer from limitations in\n",
      "both completeness and efficiency. In order to remedy these, we will\n",
      "apply the algorithm design technique of dynamic programming to\n",
      "the parsing problem.  As we saw in 4.7,\n",
      "dynamic programming stores intermediate results and re-uses them when\n",
      "appropriate, achieving significant efficiency gains. This technique\n",
      "can be applied to syntactic parsing, allowing us to store\n",
      "partial solutions to the parsing task and then look them up as\n",
      "necessary in order to efficiently arrive at a complete solution.\n",
      "This approach to parsing is known as chart parsing.  We introduce\n",
      "the main idea in this section; see the online materials available for\n",
      "this chapter for more implementation details.\n",
      "Dynamic programming allows us to build the PP in my pajamas\n",
      "just once.  The first time we build it we save it in a table, then we look it\n",
      "up when we need to use it as a subconstituent of either the object NP or\n",
      "the higher VP. This table is known as a\n",
      "well-formed substring table, or WFST for short.\n",
      "(The term \"substring\" refers to a contiguous sequence of words within a sentence.)\n",
      "We will show how to construct the WFST bottom-up so as to systematically record\n",
      "what syntactic constituents have been found.\n",
      "Let's set our input to be the sentence in (2).  The numerically specified\n",
      "spans of the WFST are reminiscent of Python's slice notation (3.2).  Another\n",
      "way to think about the data structure is shown in 4.3, a data\n",
      "structure known as a chart.\n",
      "\n",
      "\n",
      "Figure 4.3: The Chart Data Structure: words are the edge labels of a linear graph structure.\n",
      "\n",
      "In a WFST, we record the position of the words\n",
      "by filling in cells in a triangular matrix:\n",
      "the vertical axis will denote the start position of a substring,\n",
      "while the horizontal axis will denote the end position\n",
      "(thus shot will appear in the cell with coordinates (1, 2)).\n",
      "To simplify this presentation, we will assume each word has a unique\n",
      "lexical category, and we will store this (not the word) in the matrix.\n",
      "So cell (1, 2) will contain the entry V.\n",
      "More generally, if our input string is\n",
      "a0a1 ... an, and our grammar\n",
      "contains a production of the form A → ai, then we add A to\n",
      "the cell (i, `i`+1).\n",
      "\n",
      "System Message: WARNING/2 (ch08.rst2, line 900); backlink\n",
      "Inline interpreted text or phrase reference start-string without end-string.\n",
      "So, for every word in text, we can look up in our grammar what\n",
      "category it belongs to.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> text = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
      ">>> groucho_grammar.productions(rhs=text[1])\n",
      "[V -> 'shot']\n",
      "\n",
      "\n",
      "\n",
      "For our WFST, we create an (n-1) × (n-1) matrix\n",
      "as a list of lists in Python, and initialize it\n",
      "with the lexical categories of each token, in the init_wfst()\n",
      "function in 4.4.  We also define a utility function display()\n",
      "to pretty-print the WFST for us.\n",
      "As expected, there is a V in cell (1, 2).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def init_wfst(tokens, grammar):\n",
      "    numtokens = len(tokens)\n",
      "    wfst = [[None for i in range(numtokens+1)] for j in range(numtokens+1)]\n",
      "    for i in range(numtokens):\n",
      "        productions = grammar.productions(rhs=tokens[i])\n",
      "        wfst[i][i+1] = productions[0].lhs()\n",
      "    return wfst\n",
      "\n",
      "def complete_wfst(wfst, tokens, grammar, trace=False):\n",
      "    index = dict((p.rhs(), p.lhs()) for p in grammar.productions())\n",
      "    numtokens = len(tokens)\n",
      "    for span in range(2, numtokens+1):\n",
      "        for start in range(numtokens+1-span):\n",
      "            end = start + span\n",
      "            for mid in range(start+1, end):\n",
      "                nt1, nt2 = wfst[start][mid], wfst[mid][end]\n",
      "                if nt1 and nt2 and (nt1,nt2) in index:\n",
      "                    wfst[start][end] = index[(nt1,nt2)]\n",
      "                    if trace:\n",
      "                        print(\"[%s] %3s [%s] %3s [%s] ==> [%s] %3s [%s]\" % \\\n",
      "                        (start, nt1, mid, nt2, end, start, index[(nt1,nt2)], end))\n",
      "    return wfst\n",
      "\n",
      "def display(wfst, tokens):\n",
      "    print('\\nWFST ' + ' '.join((\"%-4d\" % i) for i in range(1, len(wfst))))\n",
      "    for i in range(len(wfst)-1):\n",
      "        print(\"%d   \" % i, end=\" \")\n",
      "        for j in range(1, len(wfst)):\n",
      "            print(\"%-4s\" % (wfst[i][j] or '.'), end=\" \")\n",
      "        print()\n",
      ">>> tokens = \"I shot an elephant in my pajamas\".split()\n",
      ">>> wfst0 = init_wfst(tokens, groucho_grammar)\n",
      ">>> display(wfst0, tokens)\n",
      "WFST 1    2    3    4    5    6    7\n",
      "0    NP   .    .    .    .    .    .\n",
      "1    .    V    .    .    .    .    .\n",
      "2    .    .    Det  .    .    .    .\n",
      "3    .    .    .    N    .    .    .\n",
      "4    .    .    .    .    P    .    .\n",
      "5    .    .    .    .    .    Det  .\n",
      "6    .    .    .    .    .    .    N\n",
      ">>> wfst1 = complete_wfst(wfst0, tokens, groucho_grammar)\n",
      ">>> display(wfst1, tokens)\n",
      "WFST 1    2    3    4    5    6    7\n",
      "0    NP   .    .    S    .    .    S\n",
      "1    .    V    .    VP   .    .    VP\n",
      "2    .    .    Det  NP   .    .    .\n",
      "3    .    .    .    N    .    .    .\n",
      "4    .    .    .    .    P    .    PP\n",
      "5    .    .    .    .    .    Det  NP\n",
      "6    .    .    .    .    .    .    N\n",
      "\n",
      "\n",
      "Example 4.4 (code_wfst.py): Figure 4.4: Acceptor Using Well-Formed Substring Table\n",
      "\n",
      "Returning to our tabular representation, given that we have Det\n",
      "in cell (2, 3) for the word an, and N in cell (3, 4) for the\n",
      "word elephant, what should we put into cell (2, 4) for an elephant?\n",
      "We need to find a production of the form A → Det N.\n",
      "Consulting the grammar, we know that we can enter NP in cell (2, 4).\n",
      "\n",
      "More generally, we can enter A in (i, j) if there\n",
      "is a production A → B C, and we find\n",
      "nonterminal B in (i, k) and C in (k, j).\n",
      "The program in 4.4 uses this rule to complete the WFST.\n",
      "By setting trace to True when calling the function complete_wfst(),\n",
      "we see tracing output that shows the WFST being constructed:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> wfst1 = complete_wfst(wfst0, tokens, groucho_grammar, trace=True)\n",
      "[2] Det [3]   N [4] ==> [2]  NP [4]\n",
      "[5] Det [6]   N [7] ==> [5]  NP [7]\n",
      "[1]   V [2]  NP [4] ==> [1]  VP [4]\n",
      "[4]   P [5]  NP [7] ==> [4]  PP [7]\n",
      "[0]  NP [1]  VP [4] ==> [0]   S [4]\n",
      "[1]  VP [4]  PP [7] ==> [1]  VP [7]\n",
      "[0]  NP [1]  VP [7] ==> [0]   S [7]\n",
      "\n",
      "\n",
      "\n",
      "For example, this says that since we found Det at\n",
      "wfst[2][3] and N at wfst[3][4], we can add NP to\n",
      "wfst[2][4].\n",
      "\n",
      "Note\n",
      "To help us easily retrieve productions by their right hand\n",
      "sides, we create an index for the grammar.\n",
      "This is an example of a space-time trade-off: we do a reverse lookup\n",
      "on the grammar, instead of having to check through the entire list of\n",
      "productions each time we want to look up via the right hand side.\n",
      "\n",
      "\n",
      "\n",
      "Figure 4.5: The Chart Data Structure: non-terminals are represented as extra edges in the chart.\n",
      "\n",
      "We conclude that there is a parse for the whole input string once\n",
      "we have constructed an S node in cell (0, 7), showing that we\n",
      "have found a sentence that covers the whole input.  The final state of\n",
      "the WFST is depicted in 4.5.\n",
      "Notice that we have not used any built-in parsing functions here.\n",
      "We've implemented a complete, primitive chart parser from the ground up!\n",
      "\n",
      "WFST's have several shortcomings.\n",
      "First, as you can see, the WFST is not itself a parse tree, so the technique is\n",
      "strictly speaking recognizing that a sentence is admitted by a\n",
      "grammar, rather than parsing it.\n",
      "Second, it requires every non-lexical grammar production to be binary.\n",
      "Although it is possible to convert an arbitrary CFG into this form,\n",
      "we would prefer to use an approach without such a requirement.\n",
      "Third, as a bottom-up approach it is potentially wasteful, being\n",
      "able to propose constituents in locations that would not be licensed by\n",
      "the grammar.\n",
      "\n",
      "Finally, the WFST did not represent the structural ambiguity in\n",
      "the sentence (i.e. the two verb phrase readings).  The VP\n",
      "in cell (1, 7) was actually entered twice, once for a V NP\n",
      "reading, and once for a VP PP reading.  These are different\n",
      "hypotheses, and the second overwrote the first (as it happens this didn't\n",
      "matter since the left hand side was the same.)\n",
      "Chart parsers use a slighly richer data structure and some interesting\n",
      "algorithms to solve these problems (see the Further Reading section at\n",
      "the end of this chapter for details).\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Try out the interactive chart parser application nltk.app.chartparser().\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5   Dependencies and Dependency Grammar\n",
      "Phrase structure grammar is concerned with how words and sequences of\n",
      "words combine to form constituents. A distinct and complementary\n",
      "approach, dependency grammar, focusses instead on how words\n",
      "relate to other words. Dependency is a binary asymmetric relation that\n",
      "holds between a head and its dependents.\n",
      "The head of a sentence is usually taken to be the tensed verb, and every other word is\n",
      "either dependent on the sentence head, or connects to it through a path of\n",
      "dependencies.\n",
      "\n",
      "A dependency representation is a labeled directed graph, where the\n",
      "nodes are the lexical items and the labeled arcs represent dependency\n",
      "relations from heads to dependents.  5.1 illustrates a\n",
      "dependency graph, where arrows point from heads to their dependents.\n",
      "\n",
      "\n",
      "Figure 5.1: Dependency Structure: arrows point from heads to their dependents;\n",
      "labels indicate the grammatical function of the dependent as\n",
      "subject, object or modifier.\n",
      "\n",
      "The arcs in 5.1 are labeled with the grammatical\n",
      "function that holds between a dependent and its head. For example,\n",
      "I is the SBJ (subject) of shot (which is the head of\n",
      "the whole sentence), and in is an NMOD (noun modifier of\n",
      "elephant). In contrast to phrase structure grammar, therefore,\n",
      "dependency grammars can be used to directly express grammatical\n",
      "functions as a type of dependency.\n",
      "Here's one way of encoding a\n",
      "dependency grammar in NLTK — note that it only captures bare\n",
      "dependency information without specifying the type of dependency:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> groucho_dep_grammar = nltk.DependencyGrammar.fromstring(\"\"\"\n",
      "... 'shot' -> 'I' | 'elephant' | 'in'\n",
      "... 'elephant' -> 'an' | 'in'\n",
      "... 'in' -> 'pajamas'\n",
      "... 'pajamas' -> 'my'\n",
      "... \"\"\")\n",
      ">>> print(groucho_dep_grammar)\n",
      "Dependency grammar with 7 productions\n",
      "  'shot' -> 'I'\n",
      "  'shot' -> 'elephant'\n",
      "  'shot' -> 'in'\n",
      "  'elephant' -> 'an'\n",
      "  'elephant' -> 'in'\n",
      "  'in' -> 'pajamas'\n",
      "  'pajamas' -> 'my'\n",
      "\n",
      "\n",
      "\n",
      "A dependency graph is projective if, when all the words are\n",
      "written in linear order, the edges can be drawn above the words\n",
      "without crossing. This is equivalent to saying that a word and all its\n",
      "descendents (dependents and dependents of its dependents, etc.) form a\n",
      "contiguous sequence of words within the sentence. 5.1 is\n",
      "projective, and we can parse many sentences in English using a\n",
      "projective dependency parser. The next example shows how\n",
      "groucho_dep_grammar provides an alternative approach to capturing\n",
      "the attachment ambiguity that we examined earlier with phrase\n",
      "structure grammar.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> pdp = nltk.ProjectiveDependencyParser(groucho_dep_grammar)\n",
      ">>> sent = 'I shot an elephant in my pajamas'.split()\n",
      ">>> trees = pdp.parse(sent)\n",
      ">>> for tree in trees:\n",
      "...     print(tree)\n",
      "(shot I (elephant an (in (pajamas my))))\n",
      "(shot I (elephant an) (in (pajamas my)))\n",
      "\n",
      "\n",
      "\n",
      "These bracketed dependency structures can also be displayed as trees,\n",
      "where dependents are shown as children of their heads.\n",
      "\n",
      "  (14)\n",
      "In languages with more flexible word order than English,\n",
      "non-projective dependencies are more frequent.\n",
      "\n",
      "\n",
      "Various criteria have been proposed for deciding what is the head H and\n",
      "what is the dependent D in a construction C. Some of the most important\n",
      "are the following:\n",
      "\n",
      "H determines the distribution class of C; or alternatively, the\n",
      "external syntactic properties of C are due to H.\n",
      "H determines the semantic type of C.\n",
      "H is obligatory while D may be optional.\n",
      "H selects D and determines whether it is obligatory or\n",
      "optional.\n",
      "The morphological form of D is determined by H (e.g. agreement\n",
      "or case government).\n",
      "\n",
      "When we say in a phrase structure grammar that the immediate\n",
      "constituents of a PP are P and NP, we are implicitly\n",
      "appealing to the head / dependent distinction. A prepositional phrase\n",
      "is a phrase whose head is a preposition; moreover, the NP is a\n",
      "dependent of P.  The same distinction carries over to the other\n",
      "types of phrase that we have discussed. The key point to note here is\n",
      "that although phrase structure grammars seem very different from\n",
      "dependency grammars, they implicitly embody a recognition of\n",
      "dependency relations. While CFGs are not intended to directly capture\n",
      "dependencies, more recent linguistic frameworks have increasingly\n",
      "adopted formalisms which combine aspects of both approaches.\n",
      "\n",
      "5.1   Valency and the Lexicon\n",
      "Let us take a closer look at verbs and their dependents.\n",
      "The grammar in 3.3 correctly generates examples like\n",
      "(15d).\n",
      "\n",
      "  (15)\n",
      "  a.The squirrel was frightened.\n",
      "\n",
      "  b.Chatterer saw the bear.\n",
      "\n",
      "  c.Chatterer thought Buster was angry.\n",
      "\n",
      "  d.Joe put the fish on the log.\n",
      "\n",
      "\n",
      "These possibilities correspond to the following productions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "VP -> V Adj\n",
      "was\n",
      "\n",
      "VP -> V NP\n",
      "saw\n",
      "\n",
      "VP -> V S\n",
      "thought\n",
      "\n",
      "VP -> V NP PP\n",
      "put\n",
      "\n",
      "\n",
      "Table 5.1: VP productions and their lexical heads\n",
      "\n",
      "\n",
      "\n",
      "That is, was can occur with a following Adj, saw can occur with a\n",
      "following NP, thought can occur with a following S and put can\n",
      "occur with a following NP and PP. The dependents Adj, NP, PP and\n",
      "S are often called complements of the respective verbs and there are strong\n",
      "constraints on what verbs can occur with what complements. By contrast with\n",
      "(15d), the word sequences in (16d) are ill-formed:\n",
      "\n",
      "  (16)\n",
      "  a.*The squirrel was Buster was angry.\n",
      "\n",
      "  b.*Chatterer saw frightened.\n",
      "\n",
      "  c.*Chatterer thought the bear.\n",
      "\n",
      "  d.*Joe put on the log.\n",
      "\n",
      "\n",
      "Note\n",
      "With a little imagination, it is possible to\n",
      "invent contexts in which unusual combinations of verbs and\n",
      "complements are interpretable. However, we assume that the above\n",
      "examples are to be interpreted in neutral contexts.\n",
      "\n",
      "\n",
      "In the tradition of dependency grammar, the verbs in 5.1 are said\n",
      "to have different valencies. Valency restrictions are not just\n",
      "applicable to verbs, but also to the other classes of heads.\n",
      "\n",
      "Within frameworks based on phrase structure grammar, various\n",
      "techniques have been proposed for excluding the\n",
      "ungrammatical examples in (16d). In a CFG, we need some way of constraining\n",
      "grammar productions which expand VP so that verbs only co-occur\n",
      "with their correct complements. We can do this by dividing the class of\n",
      "verbs into \"subcategories\", each of which is associated with a\n",
      "different set of complements. For example, transitive verbs such\n",
      "as chased and saw require a following NP\n",
      "object complement; that is, they are subcategorized for NP\n",
      "direct objects. If we introduce a new category label for transitive verbs, namely\n",
      "TV (for Transitive Verb), then we can use it in the following productions:\n",
      "\n",
      "VP -> TV NP\n",
      "TV -> 'chased' | 'saw'\n",
      "\n",
      "Now *Joe thought the bear is excluded since we haven't listed\n",
      "thought as a TV, but Chatterer saw the bear is still allowed.\n",
      "5.2 provides more examples of labels for verb subcategories.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Symbol\n",
      "Meaning\n",
      "Example\n",
      "\n",
      "\n",
      "\n",
      "IV\n",
      "intransitive verb\n",
      "barked\n",
      "\n",
      "TV\n",
      "transitive verb\n",
      "saw a man\n",
      "\n",
      "DatV\n",
      "dative verb\n",
      "gave a dog to a man\n",
      "\n",
      "SV\n",
      "sentential verb\n",
      "said that a dog barked\n",
      "\n",
      "\n",
      "Table 5.2: Verb Subcategories\n",
      "\n",
      "\n",
      "Valency is a property of lexical items, and we will discuss it further\n",
      "in 9..\n",
      "Complements are often contrasted with modifiers (or adjuncts),\n",
      "although both are kinds of dependent. Prepositional phrases,\n",
      "adjectives and adverbs typically function as modifiers. Unlike\n",
      "complements, modifiers are optional, can often be iterated, and are\n",
      "not selected for by heads in the same way as complements. For\n",
      "example, the adverb really can be added as a modifer to all the\n",
      "sentence in (17d):\n",
      "\n",
      "  (17)\n",
      "  a.The squirrel really was frightened.\n",
      "\n",
      "  b.Chatterer really saw the bear.\n",
      "\n",
      "  c.Chatterer really thought Buster was angry.\n",
      "\n",
      "  d.Joe really put the fish on the log.\n",
      "\n",
      "The structural ambiguity of PP attachment, which we have\n",
      "illustrated in both phrase structure and dependency grammars,\n",
      "corresponds semantically to an ambiguity in the scope of the modifier.\n",
      "\n",
      "\n",
      "5.2   Scaling Up\n",
      "So far, we have only considered \"toy grammars,\" small grammars that\n",
      "illustrate the key aspects of parsing.  But there is an obvious\n",
      "question as to whether the approach can be scaled up to cover\n",
      "large corpora of natural languages. How hard would it be to construct\n",
      "such a set of productions by hand? In general, the answer is: very\n",
      "hard. Even if we allow ourselves to use various formal devices that\n",
      "give much more succinct representations of grammar productions, it is still extremely\n",
      "difficult to keep control of the complex interactions between the many\n",
      "productions required to cover the major constructions of a\n",
      "language. In other words, it is hard to modularize grammars so that\n",
      "one portion can be developed independently of the other parts. This in\n",
      "turn means that it is difficult to distribute the task of grammar\n",
      "writing across a team of linguists. Another difficulty is that as the\n",
      "grammar expands to cover a wider and wider range of constructions,\n",
      "there is a corresponding increase in the number of analyses which are\n",
      "admitted for any one sentence. In other words, ambiguity increases\n",
      "with coverage.\n",
      "Despite these problems, some large collaborative\n",
      "projects have achieved interesting and impressive results in\n",
      "developing rule-based grammars for several languages. Examples are the\n",
      "Lexical Functional Grammar (LFG) Pargram project,\n",
      "the Head-Driven Phrase Structure Grammar (HPSG) LinGO Matrix framework,\n",
      "and the Lexicalized Tree Adjoining Grammar XTAG Project.\n",
      "\n",
      "\n",
      "\n",
      "6   Grammar Development\n",
      "Parsing builds trees over sentences, according to a phrase\n",
      "structure grammar.  Now, all the examples we gave above\n",
      "only involved toy grammars containing a handful of productions.\n",
      "What happens if we try to scale up this approach to deal\n",
      "with realistic corpora of language?  In this section we will\n",
      "see how to access treebanks, and look at the challenge of developing\n",
      "broad-coverage grammars.\n",
      "\n",
      "6.1   Treebanks and Grammars\n",
      "The corpus module defines the treebank corpus reader,\n",
      "which contains a 10% sample of the Penn Treebank corpus.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import treebank\n",
      ">>> t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
      ">>> print(t)\n",
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR\n",
      "        (IN as)\n",
      "        (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n",
      "\n",
      "\n",
      "\n",
      "We can use this data to help develop a grammar.\n",
      "For example, the program in 6.1\n",
      "uses a simple filter to find verbs that take sentential complements.\n",
      "Assuming we already have a production of the form VP -> Vs S,\n",
      "this information enables us to identify particular verbs\n",
      "that would be included in the expansion of Vs.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def filter(tree):\n",
      "    child_nodes = [child.label() for child in tree\n",
      "                   if isinstance(child, nltk.Tree)]\n",
      "    return  (tree.label() == 'VP') and ('S' in child_nodes)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import treebank\n",
      ">>> [subtree for tree in treebank.parsed_sents()\n",
      "...          for subtree in tree.subtrees(filter)]\n",
      " [Tree('VP', [Tree('VBN', ['named']), Tree('S', [Tree('NP-SBJ', ...]), ...]), ...]\n",
      "\n",
      "\n",
      "Example 6.1 (code_sentential_complement.py): Figure 6.1: Searching a Treebank to find Sentential Complements\n",
      "\n",
      "The Prepositional Phrase Attachment Corpus, nltk.corpus.ppattach\n",
      "is another source of information about the valency of particular verbs.\n",
      "Here we illustrate a technique for mining this corpus.\n",
      "It finds pairs of prepositional phrases where the preposition\n",
      "and noun are fixed, but where the choice of verb determines whether\n",
      "the prepositional phrase is attached to the VP or to the NP.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from collections import defaultdict\n",
      ">>> entries = nltk.corpus.ppattach.attachments('training')\n",
      ">>> table = defaultdict(lambda: defaultdict(set))\n",
      ">>> for entry in entries:\n",
      "...     key = entry.noun1 + '-' + entry.prep + '-' + entry.noun2\n",
      "...     table[key][entry.attachment].add(entry.verb)\n",
      "...\n",
      ">>> for key in sorted(table):\n",
      "...     if len(table[key]) > 1:\n",
      "...         print(key, 'N:', sorted(table[key]['N']), 'V:', sorted(table[key]['V']))\n",
      "\n",
      "\n",
      "\n",
      "Amongst the output lines of this program we find\n",
      "offer-from-group N: ['rejected'] V: ['received'],\n",
      "which indicates that received expects a separate\n",
      "PP complement attached to the VP, while rejected does not.\n",
      "As before, we can use this information to help construct the grammar.\n",
      "The NLTK corpus collection includes data from the PE08\n",
      "Cross-Framework and Cross Domain Parser Evaluation Shared Task.\n",
      "A collection of larger grammars has been prepared for the purpose of\n",
      "comparing different parsers, which can be obtained by downloading\n",
      "the large_grammars package\n",
      "(e.g. python -m nltk.downloader large_grammars).\n",
      "The NLTK corpus collection also includes a sample from the Sinica Treebank Corpus,\n",
      "consisting of 10,000 parsed sentences drawn from the\n",
      "Academia Sinica Balanced Corpus of Modern Chinese.\n",
      "Let's load and display one of the trees in this corpus.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nltk.corpus.sinica_treebank.parsed_sents()[3450].draw()               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6.2   Pernicious Ambiguity\n",
      "Unfortunately, as the coverage of\n",
      "the grammar increases and the length of the input sentences grows, the\n",
      "number of parse trees grows rapidly.  In fact, it grows at an\n",
      "astronomical rate.\n",
      "Let's explore this issue with the help of a simple example.\n",
      "The word\n",
      "fish is both a noun and a verb.  We can make up the sentence\n",
      "fish fish fish, meaning fish like to fish for other fish.\n",
      "(Try this with police if you prefer something more sensible.)\n",
      "Here is a toy grammar for the \"fish\" sentences.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> grammar = nltk.CFG.fromstring(\"\"\"\n",
      "... S -> NP V NP\n",
      "... NP -> NP Sbar\n",
      "... Sbar -> NP V\n",
      "... NP -> 'fish'\n",
      "... V -> 'fish'\n",
      "... \"\"\")\n",
      "\n",
      "\n",
      "\n",
      "Now we can try parsing a longer sentence, fish fish fish fish\n",
      "fish, which amongst other things, means 'fish that other fish\n",
      "fish are in the habit of fishing fish themselves'. We use the NLTK\n",
      "chart parser, which was mentioned earlier in this chapter.  This\n",
      "sentence has two readings.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tokens = [\"fish\"] * 5\n",
      ">>> cp = nltk.ChartParser(grammar)\n",
      ">>> for tree in cp.parse(tokens):\n",
      "...     print(tree)\n",
      "(S (NP fish) (V fish) (NP (NP fish) (Sbar (NP fish) (V fish))))\n",
      "(S (NP (NP fish) (Sbar (NP fish) (V fish))) (V fish) (NP fish))\n",
      "\n",
      "\n",
      "\n",
      "As the length of this sentence goes up (3, 5, 7, ...) we get the\n",
      "following numbers of parse trees:\n",
      "1; 2; 5; 14; 42; 132; 429; 1,430; 4,862; 16,796; 58,786; 208,012; ...\n",
      "(These are the Catalan numbers, which we saw in an exercise\n",
      "in 4).\n",
      "The last of these is for a sentence of length 23, the average length\n",
      "of sentences in the  WSJ section of Penn Treebank.  For a sentence\n",
      "of length 50 there would be over 1012 parses, and this\n",
      "is only half the length of the Piglet sentence\n",
      "(1),\n",
      "which young children process effortlessly.\n",
      "No practical NLP system could construct millions of trees for a\n",
      "sentence and choose the appropriate one in the context.\n",
      "It's clear that humans don't do this either!\n",
      "Note that the problem is not with our choice of example.\n",
      "(Church & Patil, 1982) point out that the syntactic ambiguity of PP\n",
      "attachment in sentences like (18) also grows in proportion to the Catalan\n",
      "numbers.\n",
      "\n",
      "  (18)Put the block in the box on the table.\n",
      "So much for structural ambiguity; what about lexical ambiguity?\n",
      "As soon as we try to construct a broad-coverage grammar, we\n",
      "are forced to make lexical entries highly ambiguous for their part of\n",
      "speech.  In a toy grammar, a is only a determiner, dog is\n",
      "only a noun, and runs is only a verb.  However, in a\n",
      "broad-coverage grammar, a is also a noun (e.g. part a),\n",
      "dog is also a verb (meaning to follow closely), and runs\n",
      "is also a noun (e.g. ski runs).  In fact, all words can be\n",
      "referred to by name: e.g. the verb 'ate' is spelled with three\n",
      "letters; in speech we do not need to supply quotation marks.\n",
      "Furthermore, it is possible to verb most nouns.  Thus a parser for a\n",
      "broad-coverage grammar will be overwhelmed with ambiguity.  Even\n",
      "complete gibberish will often have a reading, e.g. the a are of\n",
      "I.  As (Klavans & Resnik, 1996) has pointed out, this is not word salad but a\n",
      "grammatical noun phrase, in which are is a noun meaning a\n",
      "hundredth of a hectare (or 100 sq m), and a and I are\n",
      "nouns designating coordinates, as shown in 6.2.\n",
      "\n",
      "\n",
      "Figure 6.2: \"The a are of I\": a schematic drawing of 27 paddocks, each being\n",
      "one \"are\" in size, and each identified using coordinates;\n",
      "the top left cell is the a \"are\" of column I (after Abney).\n",
      "\n",
      "\n",
      "Even though this phrase is unlikely, it is still grammatical and\n",
      "a broad-coverage parser should be able to construct a parse tree\n",
      "for it.  Similarly, sentences that seem to be\n",
      "unambiguous, such as John saw Mary, turn out to have other\n",
      "readings we would not have anticipated  (as Abney explains).  This\n",
      "ambiguity is unavoidable, and leads to horrendous inefficiency in\n",
      "parsing seemingly innocuous sentences.\n",
      "The solution to these problems is provided by\n",
      "probabilistic parsing, which allows us to rank\n",
      "the parses of an ambiguous sentence on the basis of evidence from corpora.\n",
      "\n",
      "\n",
      "6.3   Weighted Grammar\n",
      "\n",
      "As we have just seen, dealing with ambiguity\n",
      "is a key challenge in developing broad coverage parsers.\n",
      "Chart parsers improve the efficiency of computing multiple\n",
      "parses of the same sentences, but they are still overwhelmed by\n",
      "the sheer number of possible parses.  Weighted grammars and\n",
      "probabilistic parsing algorithms have provided an effective\n",
      "solution to these problems.\n",
      "Before looking at these, we need to understand why the notion of\n",
      "grammaticality could be gradient.  Considering the verb give.\n",
      "This verb requires both a direct object (the thing being given)\n",
      "and an indirect object (the recipient).\n",
      "These complements can be given in either order, as\n",
      "illustrated in (19).  In the \"prepositional dative\" form in\n",
      "(19a), the direct object appears first, followed\n",
      "by a prepositional phrase containing the indirect object.\n",
      "\n",
      "  (19)\n",
      "  a.Kim gave a bone to the dog\n",
      "\n",
      "  b.Kim gave the dog a bone\n",
      "\n",
      "In the \"double object\" form in (19b),\n",
      "the indirect object appears first, followed by the direct object.\n",
      "In the above case, either order is acceptable.  However, if\n",
      "the indirect object is a pronoun, there is a strong preference for\n",
      "the double object construction:\n",
      "\n",
      "  (20)\n",
      "  a.Kim gives the heebie-jeebies to me (*prepositional dative)\n",
      "\n",
      "  b.Kim gives me the heebie-jeebies (double object)\n",
      "\n",
      "Using the Penn Treebank sample, we can examine all instances of\n",
      "prepositional dative and double object constructions involving\n",
      "give, as shown in 6.3.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "def give(t):\n",
      "    return t.label() == 'VP' and len(t) > 2 and t[1].label() == 'NP'\\\n",
      "           and (t[2].label() == 'PP-DTV' or t[2].label() == 'NP')\\\n",
      "           and ('give' in t[0].leaves() or 'gave' in t[0].leaves())\n",
      "def sent(t):\n",
      "    return ' '.join(token for token in t.leaves() if token[0] not in '*-0')\n",
      "def print_node(t, width):\n",
      "        output = \"%s %s: %s / %s: %s\" %\\\n",
      "            (sent(t[0]), t[1].label(), sent(t[1]), t[2].label(), sent(t[2]))\n",
      "        if len(output) > width:\n",
      "            output = output[:width] + \"...\"\n",
      "        print(output)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> for tree in nltk.corpus.treebank.parsed_sents():\n",
      "...     for t in tree.subtrees(give):\n",
      "...         print_node(t, 72)\n",
      "gave NP: the chefs / NP: a standing ovation\n",
      "give NP: advertisers / NP: discounts for maintaining or increasing ad sp...\n",
      "give NP: it / PP-DTV: to the politicians\n",
      "gave NP: them / NP: similar help\n",
      "give NP: them / NP:\n",
      "give NP: only French history questions / PP-DTV: to students in a Europe...\n",
      "give NP: federal judges / NP: a raise\n",
      "give NP: consumers / NP: the straight scoop on the U.S. waste crisis\n",
      "gave NP: Mitsui / NP: access to a high-tech medical product\n",
      "give NP: Mitsubishi / NP: a window on the U.S. glass industry\n",
      "give NP: much thought / PP-DTV: to the rates she was receiving , nor to ...\n",
      "give NP: your Foster Savings Institution / NP: the gift of hope and free...\n",
      "give NP: market operators / NP: the authority to suspend trading in futu...\n",
      "gave NP: quick approval / PP-DTV: to $ 3.18 billion in supplemental appr...\n",
      "give NP: the Transportation Department / NP: up to 50 days to review any...\n",
      "give NP: the president / NP: such power\n",
      "give NP: me / NP: the heebie-jeebies\n",
      "give NP: holders / NP: the right , but not the obligation , to buy a cal...\n",
      "gave NP: Mr. Thomas / NP: only a `` qualified '' rating , rather than ``...\n",
      "give NP: the president / NP: line-item veto power\n",
      "\n",
      "\n",
      "Example 6.3 (code_give.py): Figure 6.3: Usage of Give and Gave in the Penn Treebank sample\n",
      "\n",
      "We can observe a strong tendency for the shortest complement to appear\n",
      "first.  However, this does not account for a form like\n",
      "give NP: federal judges / NP: a raise, where animacy may\n",
      "play a role.  In fact there turn out to be a large number of contributing\n",
      "factors, as surveyed by (Bresnan & Hay, 2006).\n",
      "Such preferences can be represented in a weighted grammar.\n",
      "A probabilistic context free grammar (or PCFG) is a context free\n",
      "grammar that associates a probability with each of its productions.\n",
      "It generates the same set of parses for a text that the corresponding\n",
      "context free grammar does, and assigns a probability to each parse.\n",
      "The probability of a parse generated by a PCFG is simply the product\n",
      "of the probabilities of the productions used to generate it.\n",
      "The simplest way to define a PCFG is to load it from a specially\n",
      "formatted string consisting of a sequence of weighted productions,\n",
      "where weights appear in brackets, as shown in 6.4.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "grammar = nltk.PCFG.fromstring(\"\"\"\n",
      "    S    -> NP VP              [1.0]\n",
      "    VP   -> TV NP              [0.4]\n",
      "    VP   -> IV                 [0.3]\n",
      "    VP   -> DatV NP NP         [0.3]\n",
      "    TV   -> 'saw'              [1.0]\n",
      "    IV   -> 'ate'              [1.0]\n",
      "    DatV -> 'gave'             [1.0]\n",
      "    NP   -> 'telescopes'       [0.8]\n",
      "    NP   -> 'Jack'             [0.2]\n",
      "    \"\"\")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(grammar)\n",
      "Grammar with 9 productions (start state = S)\n",
      "    S -> NP VP [1.0]\n",
      "    VP -> TV NP [0.4]\n",
      "    VP -> IV [0.3]\n",
      "    VP -> DatV NP NP [0.3]\n",
      "    TV -> 'saw' [1.0]\n",
      "    IV -> 'ate' [1.0]\n",
      "    DatV -> 'gave' [1.0]\n",
      "    NP -> 'telescopes' [0.8]\n",
      "    NP -> 'Jack' [0.2]\n",
      "\n",
      "\n",
      "Example 6.4 (code_pcfg1.py): Figure 6.4: Defining a Probabilistic Context Free Grammar (PCFG)\n",
      "\n",
      "It is sometimes convenient to combine multiple productions into a single line,\n",
      "e.g. VP -> TV NP [0.4] | IV [0.3] | DatV NP NP [0.3].\n",
      "In order to ensure that the trees generated by the grammar form a\n",
      "probability distribution, PCFG grammars impose the constraint\n",
      "that all productions with a given left-hand side must have\n",
      "probabilities that sum to one.\n",
      "The grammar in 6.4 obeys this constraint: for S,\n",
      "there is only one production, with a probability of 1.0; for VP,\n",
      "0.4+0.3+0.3=1.0; and for NP, 0.8+0.2=1.0.\n",
      "The parse tree returned by parse() includes probabilities:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> viterbi_parser = nltk.ViterbiParser(grammar)\n",
      ">>> for tree in viterbi_parser.parse(['Jack', 'saw', 'telescopes']):\n",
      "...     print(tree)\n",
      "(S (NP Jack) (VP (TV saw) (NP telescopes))) (p=0.064)\n",
      "\n",
      "\n",
      "\n",
      "Now that parse trees are assigned probabilities, it no longer matters\n",
      "that there may be a huge number of possible parses for a given sentence.\n",
      "A parser will be responsible for finding the most likely parses.\n",
      "\n",
      "\n",
      "\n",
      "7   Summary\n",
      "\n",
      "Sentences have internal organization\n",
      "that can be represented using a tree. Notable features of constituent\n",
      "structure are: recursion, heads, complements and modifiers.\n",
      "A grammar is a compact characterization of a potentially infinite set of sentences;\n",
      "we say that a tree is well-formed according to a grammar, or that a grammar licenses a tree.\n",
      "A grammar is a formal model for describing whether a given phrase can be\n",
      "assigned a particular constituent or dependency structure.\n",
      "Given a set of syntactic categories, a context-free grammar\n",
      "uses a set of productions to say how a phrase of some category A can\n",
      "be analyzed into a sequence of smaller parts α1\n",
      "... αn.\n",
      "A dependency grammar uses productions to specify what the dependents\n",
      "are of a given lexical head.\n",
      "Syntactic ambiguity arises when one sentence has more than one syntactic analysis\n",
      "(e.g. prepositional phrase attachment ambiguity).\n",
      "A parser is a procedure for finding one or more trees corresponding to a grammatically\n",
      "well-formed sentence.\n",
      "A simple top-down parser is the recursive descent parser, which recursively\n",
      "expands the start symbol (usually S) with the help of the grammar\n",
      "productions, and tries to match the input sentence.  This parser cannot\n",
      "handle left-recursive productions (e.g., productions such as NP -> NP PP).\n",
      "It is inefficient in the way it blindly expands\n",
      "categories without checking whether they are compatible with the input string, and\n",
      "in repeatedly expanding the same non-terminals and discarding the results.\n",
      "A simple bottom-up parser is the shift-reduce parser, which shifts input onto\n",
      "a stack and tries to match the items at the top of the stack with the right\n",
      "hand side of grammar productions.  This parser is not guaranteed to find\n",
      "a valid parse for the input even if one exists, and builds substructure without\n",
      "checking whether it is globally consistent with the grammar.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8   Further Reading\n",
      "Extra materials for this chapter are posted at http://nltk.org/, including links to freely\n",
      "available resources on the web.\n",
      "For more examples of parsing with NLTK, please see the\n",
      "Parsing HOWTO at http://nltk.org/howto.\n",
      "There are many introductory books on syntax. (O'Grady et al, 2004) is a\n",
      "general introduction to linguistics, while (Radford, 1988) provides a\n",
      "gentle introduction to transformational grammar, and can be\n",
      "recommended for its coverage of transformational approaches to\n",
      "unbounded dependency constructions.  The most widely used\n",
      "term in linguistics for formal grammar is generative grammar,\n",
      "though it has nothing to do with generation (Chomsky, 1965).\n",
      "The framework of X-bar Syntax is\n",
      "due to (Jacobs & Rosenbaum, 1970), and is explored at greater length in (Jackendoff, 1977)\n",
      "(The primes we use replace Chomsky's typographically more demanding horizontal bars.)\n",
      "(Burton-Roberts, 1997) is a practically oriented textbook on how to\n",
      "analyze constituency in English, with extensive exemplification and\n",
      "exercises. (Huddleston & Pullum, 2002) provides an up-to-date and comprehensive analysis of\n",
      "syntactic phenomena in English.\n",
      "Chapter 12 of (Jurafsky & Martin, 2008) covers formal grammars of English;\n",
      "Sections 13.1-3 cover simple parsing algorithms and techniques\n",
      "for dealing with ambiguity;\n",
      "Chapter 14 covers statistical parsing;\n",
      "Chapter 16 covers the Chomsky hierarchy and the formal complexity\n",
      "of natural language.\n",
      "(Levin, 1993) has categorized English verbs into fine-grained classes,\n",
      "according to their syntactic properties.\n",
      "There are several ongoing efforts to build large-scale rule-based grammars,\n",
      "e.g. the LFG Pargram project http://www2.parc.com/istl/groups/nltt/pargram/,\n",
      "the HPSG LinGO Matrix framework http://www.delph-in.net/matrix/\n",
      "and the XTAG Project http://www.cis.upenn.edu/~xtag/.\n",
      "\n",
      "\n",
      "9   Exercises\n",
      "\n",
      "☼ Can you come up with grammatical sentences that have probably never\n",
      "been uttered before?  (Take turns with a partner.)  What does this tell you\n",
      "about human language?\n",
      "\n",
      "☼ Recall Strunk and White's prohibition against sentence-initial\n",
      "however used to mean \"although\".\n",
      "Do a web search for however used at the start of the sentence.\n",
      "How widely used is this construction?\n",
      "\n",
      "☼ Consider the sentence Kim arrived or Dana left and everyone cheered.\n",
      "Write down the parenthesized forms to show the relative scope of and\n",
      "and or.  Generate tree structures corresponding to both of these interpretations.\n",
      "\n",
      "☼ The Tree class implements a variety of other useful methods.\n",
      "See the Tree help documentation for more details, i.e. import\n",
      "the Tree class and then type help(Tree).\n",
      "\n",
      "☼ In this exercise you will manually construct some parse trees.\n",
      "\n",
      "Write code to produce two trees, one for each reading of the phrase\n",
      "old men and women\n",
      "Encode any of the trees presented in this chapter as a labeled\n",
      "bracketing and use nltk.Tree() to check that it is well-formed.\n",
      "Now use draw() to display the tree.\n",
      "As in (a) above, draw a tree for The woman saw a man last Thursday.\n",
      "\n",
      "\n",
      "☼ Write a recursive function to traverse a tree and return the\n",
      "depth of the tree, such that a tree with a single node would have\n",
      "depth zero.  (Hint: the depth of a subtree is the maximum depth\n",
      "of its children, plus one.)\n",
      "\n",
      "☼ Analyze the A.A. Milne sentence about Piglet, by underlining all\n",
      "of the sentences it contains then replacing these with S\n",
      "(e.g. the first sentence becomes S when:lx` S).\n",
      "Draw a tree structure for this \"compressed\" sentence.  What are\n",
      "the main syntactic constructions used for building such a long\n",
      "sentence?\n",
      "\n",
      "☼ In the recursive descent parser demo, experiment with changing the\n",
      "sentence to be parsed by selecting Edit Text in the Edit menu.\n",
      "\n",
      "☼ Can the grammar in grammar1 be used to describe sentences that are\n",
      "more than 20 words in length?\n",
      "\n",
      "☼ Use the graphical chart-parser interface to experiment with\n",
      "different rule invocation strategies. Come up with your own strategy\n",
      "that you can execute manually using the graphical interface. Describe\n",
      "the steps, and report any efficiency improvements it has (e.g. in terms\n",
      "of the size of the resulting chart). Do these improvements depend on\n",
      "the structure of the grammar? What do you think of the prospects for\n",
      "significant performance boosts from cleverer rule invocation\n",
      "strategies?\n",
      "\n",
      "☼ With pen and paper, manually trace the execution of a recursive descent\n",
      "parser and a shift-reduce parser, for a CFG you have already seen, or one\n",
      "of your own devising.\n",
      "\n",
      "☼ We have seen that a chart parser adds but never removes edges\n",
      "from a chart.  Why?\n",
      "\n",
      "☼ Consider the sequence of words:\n",
      "Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo.\n",
      "This is a grammatically correct sentence, as explained at\n",
      "http://en.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buffalo.\n",
      "Consider the tree diagram presented on this Wikipedia page, and write down a suitable\n",
      "grammar.  Normalize case to lowercase, to simulate the problem that a listener has when hearing\n",
      "this sentence.  Can you find other parses for this sentence?\n",
      "How does the number of parse trees grow as the sentence gets longer?\n",
      "(More examples of these sentences can be found at http://en.wikipedia.org/wiki/List_of_homophonous_phrases).\n",
      "\n",
      "◑ You can modify the grammar in the recursive descent parser demo\n",
      "by selecting Edit Grammar  in the Edit menu. Change\n",
      "the second expansion production, namely NP -> Det N PP, to NP -> NP\n",
      "PP. Using the Step button, try to build a parse tree. What happens?\n",
      "\n",
      "◑ Extend the grammar in grammar2 with productions that expand prepositions as\n",
      "intransitive, transitive and requiring a PP\n",
      "complement. Based on these productions, use the method of the\n",
      "preceding exercise to draw a tree for the sentence Lee ran away home.\n",
      "\n",
      "◑ Pick some common verbs and complete the following tasks:\n",
      "\n",
      "Write a program to find those verbs in the Prepositional Phrase Attachment Corpus\n",
      "nltk.corpus.ppattach.  Find any cases where the same verb\n",
      "exhibits two different attachments, but where the first noun,\n",
      "or second noun, or preposition, stay unchanged (as we saw in\n",
      "our discussion of syntactic ambiguity in 2).\n",
      "Devise CFG grammar productions to cover some of these cases.\n",
      "\n",
      "\n",
      "◑ Write a program to compare the efficiency of a top-down chart parser\n",
      "compared with a recursive descent parser (4).\n",
      "Use the same grammar and input sentences for both.  Compare their performance\n",
      "using the timeit module (see 4.7 for an example of\n",
      "how to do this).\n",
      "\n",
      "◑ Compare the performance of the top-down, bottom-up, and left-corner\n",
      "parsers using the same grammar and three grammatical test\n",
      "sentences. Use timeit to log the amount of time each\n",
      "parser takes on the same sentence.  Write a function that runs all\n",
      "three parsers on all three sentences, and prints a 3-by-3 grid of\n",
      "times, as well as row and column totals. Discuss your findings.\n",
      "\n",
      "◑ Read up on \"garden path\" sentences.  How might the computational\n",
      "work of a parser relate to the difficulty humans have with\n",
      "processing these sentences?\n",
      "http://en.wikipedia.org/wiki/Garden_path_sentence\n",
      "\n",
      "◑ To compare multiple trees in a single window, we can use the\n",
      "draw_trees() method.  Define some trees and try it out:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.draw.tree import draw_trees\n",
      ">>> draw_trees(tree1, tree2, tree3)                    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "◑ Using tree positions, list the subjects of the first 100\n",
      "sentences in the Penn treebank; to make the results easier to view,\n",
      "limit the extracted subjects to subtrees whose height is 2.\n",
      "\n",
      "◑ Inspect the Prepositional Phrase Attachment Corpus\n",
      "and try to suggest some factors that influence PP attachment.\n",
      "\n",
      "◑ In this section we claimed that there are linguistic regularities\n",
      "that cannot be described simply in terms of n-grams.\n",
      "Consider the following sentence, particularly the position of the phrase\n",
      "in his turn.  Does this illustrate a problem for an approach based\n",
      "on n-grams?\n",
      "\n",
      "What was more, the in his turn somewhat youngish Nikolay Parfenovich\n",
      "also turned out to be the only person in the entire world to acquire a\n",
      "sincere liking to our \"discriminated-against\" public procurator.\n",
      "(Dostoevsky: The Brothers Karamazov)\n",
      "\n",
      "\n",
      "◑ Write a recursive function that produces a nested bracketing for\n",
      "a tree, leaving out the leaf nodes, and displaying the non-terminal\n",
      "labels after their subtrees.  So the above example about Pierre\n",
      "Vinken would produce:\n",
      "[[[NNP NNP]NP , [ADJP [CD NNS]NP JJ]ADJP ,]NP-SBJ MD [VB [DT NN]NP [IN [DT JJ NN]NP]PP-CLR [NNP CD]NP-TMP]VP .]S\n",
      "Consecutive categories should be separated by space.\n",
      "\n",
      "◑ Download several electronic books from Project Gutenberg.\n",
      "Write a program to scan these texts for any extremely long sentences.\n",
      "What is the longest sentence you can find?  What syntactic construction(s)\n",
      "are responsible for such long sentences?\n",
      "\n",
      "◑ Modify the functions init_wfst() and complete_wfst() so\n",
      "that the contents of each cell in the WFST is a set of\n",
      "non-terminal symbols rather than a single non-terminal.\n",
      "\n",
      "◑ Consider the algorithm in 4.4.  Can you explain why\n",
      "parsing context-free grammar is proportional to n3, where n\n",
      "is the length of the input sentence.\n",
      "\n",
      "◑ Process each tree of the Treebank corpus sample nltk.corpus.treebank\n",
      "and extract the productions with the help of Tree.productions().  Discard\n",
      "the productions that occur only once.  Productions with the same left hand side,\n",
      "and similar right hand sides can be collapsed, resulting in an equivalent but\n",
      "more compact set of rules.  Write code to output a compact grammar.\n",
      "\n",
      "★ One common way of defining the subject of a sentence S in\n",
      "English is as the noun phrase that is the child of S and\n",
      "the sibling of VP.   Write a function that takes the tree for\n",
      "a sentence and returns the subtree corresponding to the subject of the\n",
      "sentence.  What should it do if the root node of the tree passed to\n",
      "this function is not S, or it lacks a subject?\n",
      "\n",
      "★ Write a function that takes a grammar (such as the one defined in\n",
      "3.1) and returns a random sentence generated by the grammar.\n",
      "(Use grammar.start() to find the start symbol of the grammar;\n",
      "grammar.productions(lhs) to get the list of productions from the grammar\n",
      "that have the specified left-hand side; and production.rhs() to get\n",
      "the right-hand side of a production.)\n",
      "\n",
      "★ Implement a version of the shift-reduce parser using backtracking,\n",
      "so that it finds all possible parses for a sentence, what might be called\n",
      "a \"recursive ascent parser.\"  Consult the Wikipedia entry for backtracking\n",
      "at http://en.wikipedia.org/wiki/Backtracking\n",
      "\n",
      "★\n",
      "As we saw in 7., it is possible\n",
      "to collapse chunks down to their chunk label.  When we do this\n",
      "for sentences involving the word gave, we find patterns\n",
      "such as the following:\n",
      "\n",
      "gave NP\n",
      "gave up NP in NP\n",
      "gave NP up\n",
      "gave NP NP\n",
      "gave NP to NP\n",
      "\n",
      "\n",
      "Use this method to study the complementation patterns of a verb\n",
      "of interest, and write suitable grammar productions.  (This task\n",
      "is sometimes called lexical acquisition.)\n",
      "Identify some English verbs that are near-synonyms, such as the\n",
      "dumped/filled/loaded example from earlier in this chapter.\n",
      "Use the chunking method to study the complementation patterns of\n",
      "these verbs.  Create a grammar to cover these cases.  Can the verbs\n",
      "be freely substituted for each other, or are their constraints?\n",
      "Discuss your findings.\n",
      "\n",
      "\n",
      "★ Develop a left-corner parser based on the\n",
      "recursive descent parser, and inheriting from ParseI.\n",
      "\n",
      "★ Extend NLTK's shift-reduce parser to incorporate backtracking, so\n",
      "that it is guaranteed to find all parses that exist (i.e. it is complete).\n",
      "\n",
      "★ Modify the functions init_wfst() and complete_wfst() so\n",
      "that when a non-terminal symbol is added to a cell in the WFST, it includes\n",
      "a record of the cells from which it was derived. Implement a\n",
      "function that will convert a WFST in this form to a parse tree.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "About this document...\n",
      "UPDATED FOR NLTK 3.0.\n",
      "This is a chapter from Natural Language Processing with Python,\n",
      "by Steven Bird, Ewan Klein and Edward Loper,\n",
      "Copyright © 2019 the authors.\n",
      "It is distributed with the Natural Language Toolkit [http://nltk.org/],\n",
      "Version 3.0, under the terms of the\n",
      "Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\n",
      "[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\n",
      "This document was built on\n",
      "Wed  4 Sep 2019 11:40:48 ACST\n",
      "vectors[8] = [0, 3, 8, 22, 4, 90, 3, 14, 0, 0, 4, 39, 12, 16, 0, 0, 3, 0, 5, 0, 2, 0, 0, 1, 5, 1, 1, 6, 12, 5, 2, 1, 0, 0, 12, 5, 0, 0, 30, 0, 3, 18, 0, 64, 1, 4, 9, 4, 0, 2, 1, 8, 1, 1, 0, 4, 26, 2, 0, 0, 7, 0, 1, 0, 42, 0, 0, 0, 0, 17, 0, 1, 4, 3, 0, 4, 5, 0, 0, 0, 0, 4, 13, 1, 3, 1, 2, 0, 4, 1, 0, 0, 0, 0, 0, 177, 2, 1, 0, 19, 3, 1, 0, 1, 0, 0, 8, 1, 0, 2, 0, 3, 2, 1, 0, 2, 2, 0, 0, 1, 29, 0, 1, 3, 0, 0, 0, 0, 5, 4, 0, 0, 1, 0, 10, 1, 343, 49, 39, 4, 1, 0, 11, 1, 2, 0, 13, 13, 9, 1, 2, 2, 4, 0, 0, 0, 0, 0, 1, 15, 4, 14, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 11, 0, 1, 5, 10, 0, 1, 1, 3, 1, 1, 0, 1, 1, 0, 0, 2, 0, 4, 4, 2, 0, 2, 0, 2, 0, 15, 11, 5, 1, 1, 0, 133, 0, 16, 2, 1, 0, 6, 216, 0, 0, 1, 4, 0, 0, 0, 11, 2, 21, 5, 1, 0, 0, 0, 0, 1, 25, 7, 0, 0, 1, 0, 1, 0, 0, 0, 3, 5, 9, 0, 1, 3, 0, 0, 0, 4, 2, 16, 82, 1, 1, 0, 0, 1, 0, 0, 0, 0, 2, 3, 2, 0, 0, 2, 1, 0, 13, 1, 17, 0, 0, 0, 0, 1, 0, 5, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 11, 1, 5, 9, 0, 2, 0, 2, 1, 0, 0, 0, 3, 51, 0, 1, 1, 1, 1, 8, 1, 1, 2, 0, 2, 0, 0, 0, 12, 3, 1, 47, 1, 0, 0, 3, 7, 1, 0, 1, 1, 1, 15, 0, 7, 0, 0, 0, 5, 0, 3, 0, 0, 0, 1, 0, 1, 0, 3, 1, 6, 0, 18, 0, 9, 1, 0, 8, 1, 3, 1, 5, 3, 0, 0, 66, 2, 0, 64, 150, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 25, 19, 7, 5, 12, 42, 124, 16, 8, 3, 12, 8, 4, 1, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 1, 0, 25, 5, 0, 0, 2, 2, 0, 3, 0, 0, 0, 0, 1, 0, 0, 10, 20, 0, 7, 0, 0, 15, 0, 5, 19, 0, 13, 2, 2, 0, 0, 0, 6, 0, 15, 3, 7, 1, 1, 7, 2, 0, 3, 0, 7, 1, 0, 9, 1, 2, 0, 2, 1, 0, 0, 0, 0, 14, 0, 0, 4, 12, 4, 1, 22, 1, 0, 52, 0, 0, 50, 6, 2, 0, 9, 0, 2, 3, 0, 0, 3, 0, 1, 2, 0, 0, 8, 15, 1, 0, 0, 1, 3, 0, 29, 0, 8, 6, 31, 1, 3, 0, 19, 4, 2, 0, 0, 0, 2, 4, 1, 0, 3, 1, 0, 0, 7, 0, 0, 9, 7, 4, 0, 0, 0, 6, 0, 0, 2, 0, 2, 0, 1, 0, 3, 0, 2, 4, 3, 0, 0, 0, 0, 2, 0, 5, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 7, 0, 5, 1, 1, 0, 14, 0, 6, 0, 0, 12, 2, 36, 0, 1, 2, 10, 1, 0, 0, 3, 3, 17, 1, 2, 0, 1, 1, 2, 2, 1, 9, 0, 5, 1, 0, 3, 0, 0, 0, 5, 11, 0, 1, 0, 0, 0, 48, 1, 1, 0, 6, 2, 57, 0, 5, 0, 6, 0, 12, 0, 6, 0, 0, 0, 6, 0, 0, 100, 0, 0, 0, 1, 1, 0, 0, 0, 1, 4, 0, 1, 0, 6, 0, 0, 0, 4, 22, 11, 0, 0, 0, 2, 8, 2, 0, 0, 0, 1, 0, 0, 2, 0, 2, 9, 0, 0, 0, 0, 0, 7, 4, 2, 16, 0, 1, 2, 1, 0, 0, 5, 0, 0, 4, 10, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 23, 0, 4, 0, 1, 0, 0, 6, 1, 0, 0, 0, 1, 0, 1, 6, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 46, 4, 0, 0, 0, 0, 0, 0, 0, 0, 101, 1, 0, 0, 0, 0, 2, 2, 3, 0, 1, 0, 0, 4, 28, 4, 0, 1, 0, 2, 1, 0, 0, 4, 1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 1, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 5, 0, 0, 10, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 5, 12, 0, 13, 8, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 6, 23, 7, 4, 10, 1, 0, 0, 1, 0, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 0, 2, 5, 0, 0, 0, 13, 1, 8, 3, 0, 0, 30, 1, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 4, 0, 0, 1, 0, 2, 0, 20, 0, 0, 1, 0, 1, 1, 16, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 2, 1, 2, 0, 1, 0, 0, 0, 23, 0, 4, 0, 0, 1, 0, 1, 4, 0, 15, 0, 0, 0, 2, 154, 0, 1, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 5, 0, 0, 1, 0, 1, 0, 0, 3, 0, 14, 1, 0, 2, 0, 5, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 9, 0, 0, 0, 0, 1, 5, 1, 0, 0, 4, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 85, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 2, 9, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 6, 4, 0, 0, 2, 0, 0, 2, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 1, 3, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 2, 1, 1, 2, 1, 98, 1, 1, 4, 1, 1, 1, 3, 4, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 20, 1, 1, 0, 0, 1, 51, 1, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 31, 7, 0, 6, 0, 0, 0, 0, 4, 31, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 4, 0, 1, 4, 1, 0, 0, 1, 0, 0, 5, 1, 1, 0, 0, 5, 0, 0, 0, 0, 8, 3, 0, 16, 5, 0, 0, 1, 0, 4, 11, 2, 0, 0, 0, 0, 0, 2, 0, 0, 1, 4, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 2, 0, 1, 1, 2, 15, 1, 0, 0, 1, 0, 0, 4, 0, 0, 0, 1, 0, 0, 4, 2, 0, 0, 2, 0, 3, 4, 2, 0, 9, 19, 1, 0, 0, 1, 9, 0, 1, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 1, 16, 0, 0, 3, 1, 0, 0, 0, 0, 2, 7, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 4, 0, 2, 0, 0, 22, 3, 0, 1, 0, 0, 4, 0, 0, 0, 0, 0, 189, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 41, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 6, 9, 0, 1, 3, 0, 1, 0, 0, 0, 0, 4, 3, 7, 6, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 3, 2, 2, 1, 0, 3, 17, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 6, 0, 0, 0, 0, 4, 2, 0, 0, 4, 11, 1, 9, 8, 0, 0, 0, 18, 0, 0, 1, 1, 0, 0, 0, 1, 0, 3, 8, 0, 19, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 6, 0, 18, 0, 3, 3, 1, 0, 0, 12, 0, 3, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 5, 0, 7, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 4, 0, 0, 0, 1, 0, 0, 1, 6, 0, 5, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 8, 0, 56, 5, 0, 0, 0, 0, 1, 1, 2, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 6, 0, 8, 2, 0, 0, 0, 0, 0, 0, 0, 3, 3, 1, 1, 0, 3, 0, 0, 0, 4, 0, 0, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 4, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 9, 30, 5, 0, 2, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 3, 4, 1, 2, 0, 2, 2, 0, 0, 0, 2, 1, 6, 0, 2, 0, 0, 0, 0, 7, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 3, 1, 0, 0, 0, 10, 0, 0, 0, 0, 0, 9, 1, 1, 1, 9, 0, 1, 0, 2, 3, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 2, 7, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 1, 0, 0, 0, 4, 0, 1, 1, 0, 48, 4, 0, 52, 0, 0, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 2, 0, 2, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0, 12, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 12, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 43, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 1, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 5, 1, 0, 2, 0, 1, 0, 1, 0, 0, 0, 6, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 5, 1, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 5, 0, 2, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 49, 13, 0, 0, 17, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 0, 4, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 5, 0, 4, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 26, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 1, 3, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 91, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 29, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 8, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 10, 0, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 32, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 18, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 6, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 0, 2, 69, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 6, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 4, 4, 4, 3, 3, 3, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 19, 20, 18, 6, 6, 6, 3, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 4, 4, 1, 1, 2, 5, 1, 2, 1, 1, 1, 1, 3, 4, 2, 2, 1, 1, 1, 4, 11, 5, 7, 7, 7, 6, 1, 4, 21, 2, 1, 2, 1, 5, 1, 1, 3, 3, 1, 1, 1, 5, 4, 3, 8, 5, 4, 4, 4, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 22, 5, 3, 4, 1, 1, 1, 2, 1, 1, 1, 3, 7, 1, 1, 2, 1, 1, 1, 2, 1, 3, 3, 2, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 33, 1, 1, 2, 1, 1, 1, 4, 5, 7, 3, 6, 4, 6, 6, 1, 4, 8, 1, 1, 1, 1, 1, 1, 4, 3, 1, 1, 1, 1, 3, 1, 1, 1, 2, 4, 1, 4, 1, 4, 1, 1, 2, 1, 2, 1, 1, 1, 5, 2, 2, 2, 1, 1, 1, 3, 5, 1, 11, 1, 2, 6, 4, 1, 3, 1, 1, 1, 4, 1, 1, 1, 1, 1, 2, 3, 2, 2, 1, 1, 3, 4, 2, 1, 3, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 5, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "docs[9] = 9. Building Feature Based Grammars\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9. Building Feature Based Grammars\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Natural languages have an extensive range of grammatical constructions which\n",
      "are hard to handle with the simple methods described in 8.. In order to gain\n",
      "more flexibility, we change our treatment of grammatical categories like S,\n",
      "NP and V. In place of atomic labels, we decompose them into structures like\n",
      "dictionaries, where features can take on a range of values.\n",
      "The goal of this chapter is to answer the following questions:\n",
      "\n",
      "How can we extend the framework of context free grammars with features so as to\n",
      "gain more fine-grained control over grammatical categories and productions?\n",
      "What are the main formal properties of feature structures and how do we use them\n",
      "computationally?\n",
      "What kinds of linguistic patterns and grammatical constructions can we now capture\n",
      "with feature based grammars?\n",
      "\n",
      "Along the way, we will cover more topics in English syntax, including phenomena such as\n",
      "agreement, subcategorization, and unbounded dependency constructions.\n",
      "\n",
      "1   Grammatical Features\n",
      "In chap-data-intensive, we described how to build classifiers that rely on detecting features of\n",
      "text.  Such features may be quite simple, such as extracting the last letter of a\n",
      "word, or more complex, such as a part-of-speech tag which has itself been predicted\n",
      "by the classifier.  In this chapter, we will investigate the role of features in\n",
      "building rule-based grammars.  In contrast to feature extractors, which record\n",
      "features that have been automatically detected, we are now going to\n",
      "declare the features of words and phrases. We start off with a\n",
      "very simple example, using dictionaries to store features and their values.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> kim = {'CAT': 'NP', 'ORTH': 'Kim', 'REF': 'k'}\n",
      ">>> chase = {'CAT': 'V', 'ORTH': 'chased', 'REL': 'chase'}\n",
      "\n",
      "\n",
      "\n",
      "The objects kim and chase both have a couple of shared features, CAT\n",
      "(grammatical category) and ORTH (orthography, i.e., spelling). In addition, each\n",
      "has a more semantically-oriented feature: kim['REF'] is intended to give the\n",
      "referent of kim, while chase['REL'] gives the relation expressed by\n",
      "chase.  In the context of rule-based grammars, such pairings of features and\n",
      "values are known as feature structures, and we will shortly see alternative\n",
      "notations for them.\n",
      "Feature structures contain various kinds of information about grammatical\n",
      "entities. The information need not be exhaustive, and we might want to add further\n",
      "properties. For example, in the case of a verb, it is often useful to know what\n",
      "\"semantic role\" is played by the arguments of the verb. In the case of chase,\n",
      "the subject plays the role of \"agent\", while the object has the role of\n",
      "\"patient\". Let's add this information, using 'sbj' and 'obj' as placeholders\n",
      "which will get filled once the verb combines with its grammatical arguments:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> chase['AGT'] = 'sbj'\n",
      ">>> chase['PAT'] = 'obj'\n",
      "\n",
      "\n",
      "\n",
      "If we now process a sentence Kim chased Lee, we want to \"bind\" the verb's agent role to\n",
      "the subject and the patient role to the object. We do this by linking to the\n",
      "REF feature of the relevant NP. In the following example, we make the\n",
      "simple-minded assumption that the NPs immediately to the left and right of the\n",
      "verb are the subject and object respectively. We also add a feature\n",
      "structure for Lee to complete the example.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sent = \"Kim chased Lee\"\n",
      ">>> tokens = sent.split()\n",
      ">>> lee = {'CAT': 'NP', 'ORTH': 'Lee', 'REF': 'l'}\n",
      ">>> def lex2fs(word):\n",
      "...     for fs in [kim, lee, chase]:\n",
      "...         if fs['ORTH'] == word:\n",
      "...             return fs\n",
      ">>> subj, verb, obj = lex2fs(tokens[0]), lex2fs(tokens[1]), lex2fs(tokens[2])\n",
      ">>> verb['AGT'] = subj['REF']\n",
      ">>> verb['PAT'] = obj['REF']\n",
      ">>> for k in ['ORTH', 'REL', 'AGT', 'PAT']:\n",
      "...     print(\"%-5s => %s\" % (k, verb[k]))\n",
      "ORTH  => chased\n",
      "REL   => chase\n",
      "AGT   => k\n",
      "PAT   => l\n",
      "\n",
      "\n",
      "\n",
      "The same approach could be adopted for a different verb, say surprise, though in\n",
      "this case, the subject would play the role of \"source\" (SRC) and the object,\n",
      "the role of \"experiencer\" (EXP):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> surprise = {'CAT': 'V', 'ORTH': 'surprised', 'REL': 'surprise',\n",
      "...             'SRC': 'sbj', 'EXP': 'obj'}\n",
      "\n",
      "\n",
      "\n",
      "Feature structures are pretty powerful, but the way\n",
      "in which we have manipulated them is extremely ad hoc. Our next task in this\n",
      "chapter is to show how the framework of context free grammar and parsing can be\n",
      "expanded to accommodate feature structures, so that we can build analyses like this\n",
      "in a more generic and principled way.\n",
      "We will start off by looking  at the\n",
      "phenomenon of syntactic agreement; we will show how agreement\n",
      "constraints can be expressed elegantly using features, and illustrate\n",
      "their use in a simple grammar.\n",
      "Since feature structures are a general data\n",
      "structure for representing information of any kind, we will briefly\n",
      "look at them from a more formal point of view, and illustrate the support for feature\n",
      "structures offered by NLTK. In the final part of the chapter,\n",
      "we demonstrate that the additional expressiveness of features opens\n",
      "up a wide spectrum of possibilities for describing sophisticated\n",
      "aspects of linguistic structure.\n",
      "\n",
      "1.1   Syntactic Agreement\n",
      "The following examples show pairs of word sequences, the first of which is\n",
      "grammatical and the second not. (We use an asterisk at the start of a\n",
      "word sequence to signal that it is ungrammatical.)\n",
      "\n",
      "  (1)\n",
      "  a.this dog\n",
      "\n",
      "  b.*these dog\n",
      "\n",
      "\n",
      "  (2)\n",
      "  a.these dogs\n",
      "\n",
      "  b.*this dogs\n",
      "\n",
      "In English, nouns are usually marked as being singular\n",
      "or plural. The form of the demonstrative also varies:\n",
      "this (singular) and these (plural).\n",
      "Examples (1b) and (2b) show that there are constraints on\n",
      "the use of demonstratives and nouns within a noun phrase:\n",
      "either both are singular or both are plural. A similar\n",
      "constraint holds between subjects and predicates:\n",
      "\n",
      "  (3)\n",
      "  a.the dog runs\n",
      "\n",
      "  b.*the dog run\n",
      "\n",
      "\n",
      "  (4)\n",
      "  a.the dogs run\n",
      "\n",
      "  b.*the dogs runs\n",
      "\n",
      "\n",
      "Here we can see that morphological properties of the verb co-vary\n",
      "with syntactic properties of the subject noun phrase.  This co-variance is\n",
      "called agreement.\n",
      "If we look further at verb agreement in English, we will see that\n",
      "present tense verbs typically have two inflected forms: one for third person\n",
      "singular, and another for every other combination of person and number,\n",
      "as shown in 1.1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "singular\n",
      "plural\n",
      "\n",
      "1st per\n",
      "I run\n",
      "we run\n",
      "\n",
      "2nd per\n",
      "you run\n",
      "you run\n",
      "\n",
      "3rd per\n",
      "he/she/it\n",
      "runs\n",
      "they run\n",
      "\n",
      "\n",
      "Table 1.1: Agreement Paradigm for English Regular Verbs\n",
      "\n",
      "\n",
      "We can make the role of morphological properties a bit more explicit\n",
      "as illustrated in ex-runs and ex-run. These representations indicate that\n",
      "the verb agrees with its subject in person and number. (We use \"3\" as\n",
      "an abbreviation for 3rd person, \"SG\" for singular and \"PL\" for plural.)\n",
      "\n",
      "System Message: ERROR/3 (ch09.rst2, line 252)\n",
      "Error in \"gloss\" directive: may contain a single table only.\n",
      "\n",
      "System Message: ERROR/3 (ch09.rst2, line 257)\n",
      "Error in \"gloss\" directive: may contain a single table only.\n",
      "Let's see what happens when we encode these agreement constraints in a\n",
      "context-free grammar.  We will begin with the simple CFG in (5).\n",
      "\n",
      "  (5)\n",
      "S   ->   NP VP\n",
      "NP  ->   Det N\n",
      "VP  ->   V\n",
      "\n",
      "Det  ->  'this'\n",
      "N    ->  'dog'\n",
      "V    ->  'runs'\n",
      "\n",
      "\n",
      "Grammar (5) allows us to generate the sentence this dog runs;\n",
      "however, what we really want to do is also generate these dogs\n",
      "run while blocking unwanted sequences like *this dogs run\n",
      "and *these dog runs. The most straightforward approach is to\n",
      "add new non-terminals and productions to the grammar:\n",
      "\n",
      "  (6)\n",
      "S -> NP_SG VP_SG\n",
      "S -> NP_PL VP_PL\n",
      "NP_SG -> Det_SG N_SG\n",
      "NP_PL -> Det_PL N_PL\n",
      "VP_SG -> V_SG\n",
      "VP_PL -> V_PL\n",
      "\n",
      "Det_SG -> 'this'\n",
      "Det_PL -> 'these'\n",
      "N_SG -> 'dog'\n",
      "N_PL -> 'dogs'\n",
      "V_SG -> 'runs'\n",
      "V_PL -> 'run'\n",
      "\n",
      "\n",
      "In place of a single production expanding S, we now have two\n",
      "productions, one covering the sentences involving singular subject\n",
      "NPs and VPs, the other covering sentences with plural\n",
      "subject NPs and VPs. In fact, every production in\n",
      "(5) has two counterparts in (6). With a small grammar,\n",
      "this is not really such a problem, although it is aesthetically\n",
      "unappealing. However, with a larger grammar that covers a reasonable\n",
      "subset of English constructions, the prospect of doubling the grammar\n",
      "size is very unattractive. Let's suppose now that we used the same\n",
      "approach to deal with first, second and third person agreement, for\n",
      "both singular and plural. This would lead to the original grammar\n",
      "being multiplied by a factor of 6, which we definitely want to\n",
      "avoid. Can we do better than this? In the next section we will show\n",
      "that capturing number and person agreement need not come at the cost\n",
      "of \"blowing up\" the number of productions.\n",
      "\n",
      "\n",
      "\n",
      "1.2   Using Attributes and Constraints\n",
      "We spoke informally of linguistic categories having properties; for\n",
      "example, that a noun has the property of being plural. Let's\n",
      "make this explicit:\n",
      "\n",
      "  (7)\n",
      "N[NUM=pl]\n",
      "\n",
      "\n",
      "In (7), we have introduced some new notation which says that the\n",
      "category N has a (grammatical) feature called NUM (short for\n",
      "'number') and that the value of this feature is pl (short for\n",
      "'plural'). We can add similar annotations to other categories, and use\n",
      "them in lexical entries:\n",
      "\n",
      "  (8)\n",
      "Det[NUM=sg] -> 'this'\n",
      "Det[NUM=pl] -> 'these'\n",
      "\n",
      "N[NUM=sg] -> 'dog'\n",
      "N[NUM=pl] -> 'dogs'\n",
      "V[NUM=sg] -> 'runs'\n",
      "V[NUM=pl] -> 'run'\n",
      "\n",
      "\n",
      "Does this help at all? So far, it looks just like a slightly more\n",
      "verbose alternative to what was specified in (6). Things become\n",
      "more interesting when we allow variables over feature values, and use\n",
      "these to state constraints:\n",
      "\n",
      "  (9)\n",
      "S -> NP[NUM=?n] VP[NUM=?n]\n",
      "NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\n",
      "VP[NUM=?n] -> V[NUM=?n]\n",
      "\n",
      "\n",
      "We are using ?n as a variable over values of NUM; it can\n",
      "be instantiated either to sg or pl, within a given production.\n",
      "We can read the first production as saying that whatever\n",
      "value NP takes for the feature NUM,\n",
      "VP must take the same value.\n",
      "In order to understand how these feature constraints work, it's\n",
      "helpful to think about how one would go about building a tree. Lexical\n",
      "productions will admit the following local trees (trees of\n",
      "depth one):\n",
      "\n",
      "  (10)\n",
      "  a.\n",
      "\n",
      "  b.\n",
      "\n",
      "\n",
      "  (11)\n",
      "  a.\n",
      "\n",
      "  b.\n",
      "\n",
      "Now S -> NP[NUM=?n] VP[NUM=?n] says that whatever the NUM\n",
      "values of N and Det are, they have to be the\n",
      "same. Consequently, NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n] will\n",
      "permit (10a) and (11a) to be combined into an NP as shown\n",
      "in (12a) and it will also allow (10b) and (11b) to be\n",
      "combined, as in (12b). By contrast, (13a) and (13b) are\n",
      "prohibited because the roots of their subtrees differ\n",
      "in their values for the NUM feature; this incompatibility of values is\n",
      "indicated informally with a FAIL value at the top node.\n",
      "\n",
      "  (12)\n",
      "  a.\n",
      "\n",
      "  b.\n",
      "\n",
      "\n",
      "  (13)\n",
      "  a.\n",
      "\n",
      "  b.\n",
      "\n",
      "Production VP[NUM=?n] -> V[NUM=?n] says\n",
      "that the NUM value of the head verb has to be the same as the\n",
      "NUM value of the VP parent. Combined with the production for\n",
      "expanding S, we\n",
      "derive the consequence that if the NUM value of the subject head\n",
      "noun is pl, then so is the NUM value of the VP's\n",
      "head verb.\n",
      "\n",
      "  (14)\n",
      "Grammar (8) illustrated lexical productions for determiners like this\n",
      "and these which require a singular or plural head noun\n",
      "respectively. However, other determiners in English are not choosy\n",
      "about the grammatical number of the noun they combine with.\n",
      "One way of describing this would be to add\n",
      "two lexical entries to the grammar, one each for the singular and\n",
      "plural versions of determiner such as the\n",
      "\n",
      "Det[NUM=sg] -> 'the' | 'some' | 'any'\n",
      "Det[NUM=pl] -> 'the' | 'some' | 'any'\n",
      "\n",
      "However, a more elegant solution is to\n",
      "leave the NUM value underspecified and letting it agree\n",
      "in number with whatever noun it combines with. Assigning a variable\n",
      "value to NUM is one way of achieving this result:\n",
      "\n",
      "Det[NUM=?n] -> 'the' | 'some' | 'any'\n",
      "\n",
      "But in fact we can be even more economical, and just omit any\n",
      "specification for NUM in such productions. We only need\n",
      "to explicitly enter a variable value when this constrains another\n",
      "value elsewhere in the same production.\n",
      "The grammar in 1.1 illustrates most of the ideas we have introduced so\n",
      "far in this chapter, plus a couple of new ones.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nltk.data.show_cfg('grammars/book_grammars/feat0.fcfg')\n",
      "% start S\n",
      "# ###################\n",
      "# Grammar Productions\n",
      "# ###################\n",
      "# S expansion productions\n",
      "S -> NP[NUM=?n] VP[NUM=?n]\n",
      "# NP expansion productions\n",
      "NP[NUM=?n] -> N[NUM=?n]\n",
      "NP[NUM=?n] -> PropN[NUM=?n]\n",
      "NP[NUM=?n] -> Det[NUM=?n] N[NUM=?n]\n",
      "NP[NUM=pl] -> N[NUM=pl]\n",
      "# VP expansion productions\n",
      "VP[TENSE=?t, NUM=?n] -> IV[TENSE=?t, NUM=?n]\n",
      "VP[TENSE=?t, NUM=?n] -> TV[TENSE=?t, NUM=?n] NP\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "Det[NUM=sg] -> 'this' | 'every'\n",
      "Det[NUM=pl] -> 'these' | 'all'\n",
      "Det -> 'the' | 'some' | 'several'\n",
      "PropN[NUM=sg]-> 'Kim' | 'Jody'\n",
      "N[NUM=sg] -> 'dog' | 'girl' | 'car' | 'child'\n",
      "N[NUM=pl] -> 'dogs' | 'girls' | 'cars' | 'children'\n",
      "IV[TENSE=pres,  NUM=sg] -> 'disappears' | 'walks'\n",
      "TV[TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
      "IV[TENSE=pres,  NUM=pl] -> 'disappear' | 'walk'\n",
      "TV[TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
      "IV[TENSE=past] -> 'disappeared' | 'walked'\n",
      "TV[TENSE=past] -> 'saw' | 'liked'\n",
      "\n",
      "\n",
      "Example 1.1 (code_feat0cfg.py): Figure 1.1: Example Feature based Grammar\n",
      "\n",
      "Notice that a syntactic category can have more than one feature; for\n",
      "example,\n",
      "V[TENSE=pres, NUM=pl].\n",
      "In general, we can add as many features as we like.\n",
      "A final detail about 1.1 is the statement %start S.\n",
      "This \"directive\" tells the parser to take S as the\n",
      "start symbol for the grammar.\n",
      "In general, when we are trying to develop even a very small grammar,\n",
      "it is convenient to put the productions in a file where they can be edited,\n",
      "tested and revised.  We have saved 1.1 as a file named\n",
      "'feat0.fcfg' in the NLTK data distribution. You can make your own\n",
      "copy of this for further experimentation using nltk.data.load().\n",
      "1.2 illustrates the operation of a chart\n",
      "parser with a feature-based grammar.\n",
      "After tokenizing the input, we import the load_parser function\n",
      " which takes a grammar filename as input and returns a\n",
      "chart parser cp .  Calling the parser's\n",
      "parse() method will iterate over the resulting parse trees;\n",
      "trees will be empty if the grammar fails to parse the input and\n",
      "will contain one or more parse trees, depending on whether the input\n",
      "is syntactically ambiguous or not.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tokens = 'Kim likes children'.split()\n",
      ">>> from nltk import load_parser \n",
      ">>> cp = load_parser('grammars/book_grammars/feat0.fcfg', trace=2)  \n",
      ">>> for tree in cp.parse(tokens):\n",
      "...     print(tree)\n",
      "...\n",
      "|.Kim .like.chil.|\n",
      "Leaf Init Rule:\n",
      "|[----]    .    .| [0:1] 'Kim'\n",
      "|.    [----]    .| [1:2] 'likes'\n",
      "|.    .    [----]| [2:3] 'children'\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] PropN[NUM='sg'] -> 'Kim' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[----]    .    .| [0:1] NP[NUM='sg'] -> PropN[NUM='sg'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[---->    .    .| [0:1] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'sg'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [----]    .| [1:2] TV[NUM='sg', TENSE='pres'] -> 'likes' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    [---->    .| [1:2] VP[NUM=?n, TENSE=?t] -> TV[NUM=?n, TENSE=?t] * NP[] {?n: 'sg', ?t: 'pres'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] N[NUM='pl'] -> 'children' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [----]| [2:3] NP[NUM='pl'] -> N[NUM='pl'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.    .    [---->| [2:3] S[] -> NP[NUM=?n] * VP[NUM=?n] {?n: 'pl'}\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|.    [---------]| [1:3] VP[NUM='sg', TENSE='pres'] -> TV[NUM='sg', TENSE='pres'] NP[] *\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|[==============]| [0:3] S[] -> NP[NUM='sg'] VP[NUM='sg'] *\n",
      "(S[]\n",
      "  (NP[NUM='sg'] (PropN[NUM='sg'] Kim))\n",
      "  (VP[NUM='sg', TENSE='pres']\n",
      "    (TV[NUM='sg', TENSE='pres'] likes)\n",
      "    (NP[NUM='pl'] (N[NUM='pl'] children))))\n",
      "\n",
      "\n",
      "Example 1.2 (code_featurecharttrace.py): Figure 1.2: Trace of Feature based Chart Parser\n",
      "\n",
      "The details of the parsing procedure are not that important for\n",
      "present purposes. However, there is an implementation issue which\n",
      "bears on our earlier discussion of grammar size. One possible approach\n",
      "to parsing productions containing feature constraints is to compile\n",
      "out all admissible values of the features in question so that we end\n",
      "up with a large, fully specified CFG along the lines of (6). By\n",
      "contrast, the parser process illustrated above works directly with the\n",
      "underspecified productions given by the grammar. Feature values \"flow\n",
      "upwards\" from lexical entries, and variable values are then associated\n",
      "with those values, via bindings (i.e., dictionaries) such as {?n:\n",
      "'sg', ?t: 'pres'}.  As the parser assembles information about the\n",
      "nodes of the tree it is building, these variable bindings are used to\n",
      "instantiate values in these nodes; thus the underspecified\n",
      "VP[NUM=?n, TENSE=?t] -> TV[NUM=?n, TENSE=?t] NP[] becomes\n",
      "instantiated as VP[NUM='sg', TENSE='pres'] -> TV[NUM='sg',\n",
      "TENSE='pres'] NP[] by looking up the values of ?n and ?t in\n",
      "the bindings.\n",
      "Finally, we can inspect the resulting parse trees (in this case, a\n",
      "single one).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> for tree in trees: print(tree)\n",
      "(S[]\n",
      "  (NP[NUM='sg'] (PropN[NUM='sg'] Kim))\n",
      "  (VP[NUM='sg', TENSE='pres']\n",
      "    (TV[NUM='sg', TENSE='pres'] likes)\n",
      "    (NP[NUM='pl'] (N[NUM='pl'] children))))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.3   Terminology\n",
      "So far, we have only seen feature values like sg and\n",
      "pl. These simple values are usually called atomic\n",
      "— that is, they can't be decomposed into subparts. A special\n",
      "case of atomic values are boolean values, that is, values that\n",
      "just specify whether a property is true or false. For\n",
      "example, we might want to distinguish auxiliary verbs such as\n",
      "can, may, will and do with the boolean feature\n",
      "AUX. For example, the production V[TENSE=pres, AUX=+] -> 'can'\n",
      "means that can receives the value pres for TENSE and\n",
      "+ or true for AUX. There is a widely adopted\n",
      "convention which abbreviates the representation of boolean\n",
      "features f; instead of AUX=+ or AUX=-, we use +AUX and\n",
      "-AUX respectively. These are just abbreviations, however, and the\n",
      "parser interprets them as though + and - are like any\n",
      "other atomic value. (15) shows some representative productions:\n",
      "\n",
      "  (15)\n",
      "V[TENSE=pres, +AUX] -> 'can'\n",
      "V[TENSE=pres, +AUX] -> 'may'\n",
      "\n",
      "V[TENSE=pres, -AUX] -> 'walks'\n",
      "V[TENSE=pres, -AUX] -> 'likes'\n",
      "\n",
      "\n",
      "We have spoken of attaching \"feature annotations\" to\n",
      "syntactic categories. A more radical approach represents the whole category\n",
      "— that is, the non-terminal symbol plus the annotation —\n",
      "as a bundle of features.  For example, N[NUM=sg] contains part of speech\n",
      "information which can be represented as\n",
      "POS=N.  An alternative notation for this category therefore\n",
      "is [POS=N, NUM=sg].\n",
      "In addition to atomic-valued features,  features may take values that\n",
      "are themselves feature structures. For example, we can group\n",
      "together agreement features (e.g., person, number and gender) as a\n",
      "distinguished part of a category, grouped together as the value of AGR. In this case,\n",
      "we say that AGR has a complex value.  (16) depicts the structure, in a format\n",
      "known as an attribute value matrix (AVM).\n",
      "\n",
      "  (16)\n",
      "[POS = N           ]\n",
      "[                  ]\n",
      "[AGR = [PER = 3   ]]\n",
      "[      [NUM = pl  ]]\n",
      "[      [GND = fem ]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figure 1.3: Rendering a Feature Structure as an Attribute Value Matrix\n",
      "\n",
      "In passing, we should point out that there are alternative approaches\n",
      "for displaying AVMs; 1.3 shows an example.\n",
      "Athough feature structures rendered in the style of (16) are less\n",
      "visually pleasing, we will stick with this format, since it\n",
      "corresponds to the output we will be getting from NLTK.\n",
      "\n",
      "On the topic of representation, we also note that feature structures, like\n",
      "dictionaries, assign no\n",
      "particular significance to the order of features. So (16) is equivalent to:\n",
      "\n",
      "  (17)\n",
      "[AGR = [NUM = pl  ]]\n",
      "[      [PER = 3   ]]\n",
      "[      [GND = fem ]]\n",
      "[                  ]\n",
      "[POS = N           ]\n",
      "\n",
      "\n",
      "Once we have the possibility of using features like AGR, we\n",
      "can refactor a grammar like 1.1 so that agreement features are\n",
      "bundled together. A tiny grammar illustrating this idea is shown in (18).\n",
      "\n",
      "  (18)\n",
      "S                    -> NP[AGR=?n] VP[AGR=?n]\n",
      "NP[AGR=?n]           -> PropN[AGR=?n]\n",
      "VP[TENSE=?t, AGR=?n] -> Cop[TENSE=?t, AGR=?n] Adj\n",
      "\n",
      "Cop[TENSE=pres,  AGR=[NUM=sg, PER=3]] -> 'is'\n",
      "PropN[AGR=[NUM=sg, PER=3]]            -> 'Kim'\n",
      "Adj                                   -> 'happy'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2   Processing  Feature Structures\n",
      "In this section, we will show how feature structures can be\n",
      "constructed and manipulated in NLTK. We will also discuss the\n",
      "fundamental operation of unification, which allows us to combine the\n",
      "information contained in two different feature structures.\n",
      "Feature structures in NLTK are declared with the\n",
      "FeatStruct() constructor. Atomic feature values can be strings or\n",
      "integers.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fs1 = nltk.FeatStruct(TENSE='past', NUM='sg')\n",
      ">>> print(fs1)\n",
      "[ NUM   = 'sg'   ]\n",
      "[ TENSE = 'past' ]\n",
      "\n",
      "\n",
      "\n",
      "A feature structure is actually just a kind of dictionary,\n",
      "and so we access its values by indexing in the usual way.\n",
      "We can use our familiar syntax to assign values to features:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fs1 = nltk.FeatStruct(PER=3, NUM='pl', GND='fem')\n",
      ">>> print(fs1['GND'])\n",
      "fem\n",
      ">>> fs1['CASE'] = 'acc'\n",
      "\n",
      "\n",
      "\n",
      "We can also define feature structures that have complex values, as\n",
      "discussed earlier.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fs2 = nltk.FeatStruct(POS='N', AGR=fs1)\n",
      ">>> print(fs2)\n",
      "[       [ CASE = 'acc' ] ]\n",
      "[ AGR = [ GND  = 'fem' ] ]\n",
      "[       [ NUM  = 'pl'  ] ]\n",
      "[       [ PER  = 3     ] ]\n",
      "[                        ]\n",
      "[ POS = 'N'              ]\n",
      ">>> print(fs2['AGR'])\n",
      "[ CASE = 'acc' ]\n",
      "[ GND  = 'fem' ]\n",
      "[ NUM  = 'pl'  ]\n",
      "[ PER  = 3     ]\n",
      ">>> print(fs2['AGR']['PER'])\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "An alternative method of specifying feature structures is to\n",
      "use a bracketed string consisting of feature-value pairs in the format\n",
      "feature=value, where values may themselves be feature structures:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(nltk.FeatStruct(\"[POS='N', AGR=[PER=3, NUM='pl', GND='fem']]\"))\n",
      "[       [ GND = 'fem' ] ]\n",
      "[ AGR = [ NUM = 'pl'  ] ]\n",
      "[       [ PER = 3     ] ]\n",
      "[                       ]\n",
      "[ POS = 'N'             ]\n",
      "\n",
      "\n",
      "\n",
      "Feature structures are not inherently tied to linguistic objects; they are\n",
      "general purpose structures for representing knowledge. For example, we\n",
      "could encode information about a person in a feature structure:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(nltk.FeatStruct(NAME='Lee', TELNO='01 27 86 42 96', AGE=33))\n",
      "[ AGE   = 33               ]\n",
      "[ NAME  = 'Lee'            ]\n",
      "[ TELNO = '01 27 86 42 96' ]\n",
      "\n",
      "\n",
      "\n",
      "In the next couple of pages, we are going to use examples like this\n",
      "to explore standard operations over feature structures.\n",
      "This will briefly divert us from processing natural language,\n",
      "but we need to lay the groundwork before we can\n",
      "get back to talking about grammars. Hang on tight!\n",
      "It is often helpful to view feature structures as graphs; more\n",
      "specifically, directed acyclic graphs (DAGs).\n",
      "(19) is equivalent to the above AVM.\n",
      "\n",
      "  (19)\n",
      "The feature names appear as labels on the directed arcs, and feature\n",
      "values appear as labels on the nodes that are pointed to by the arcs.\n",
      "Just as before, feature values can be complex:\n",
      "\n",
      "  (20)\n",
      "When we look at such graphs, it is natural to think in terms of\n",
      "paths through the graph. A feature path is a sequence of arcs\n",
      "that can be followed from the root node. We will represent paths as\n",
      "tuples. Thus, ('ADDRESS', 'STREET') is a feature path whose value\n",
      "in (20) is the node labeled 'rue Pascal'.\n",
      "Now let's consider a situation where Lee has a spouse named Kim, and\n",
      "Kim's address is the same as Lee's.\n",
      "We might represent this as (21).\n",
      "\n",
      "  (21)\n",
      "However, rather than repeating the address\n",
      "information in the feature structure, we can \"share\" the same\n",
      "sub-graph between different arcs:\n",
      "\n",
      "  (22)\n",
      "In other words, the value of the path ('ADDRESS') in (22) is\n",
      "identical to the value of the path ('SPOUSE', 'ADDRESS').  DAGs\n",
      "such as (22) are said to involve structure sharing or\n",
      "reentrancy. When two paths have the same value, they are said to\n",
      "be equivalent.\n",
      "In order to indicate reentrancy in our matrix-style representations, we will\n",
      "prefix the first occurrence of a shared feature structure\n",
      "with an integer in parentheses, such as (1).\n",
      "Any later reference to that structure will use the notation\n",
      "->(1), as shown below.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(nltk.FeatStruct(\"\"\"[NAME='Lee', ADDRESS=(1)[NUMBER=74, STREET='rue Pascal'],\n",
      "...                          SPOUSE=[NAME='Kim', ADDRESS->(1)]]\"\"\"))\n",
      "[ ADDRESS = (1) [ NUMBER = 74           ] ]\n",
      "[               [ STREET = 'rue Pascal' ] ]\n",
      "[                                         ]\n",
      "[ NAME    = 'Lee'                         ]\n",
      "[                                         ]\n",
      "[ SPOUSE  = [ ADDRESS -> (1)  ]           ]\n",
      "[           [ NAME    = 'Kim' ]           ]\n",
      "\n",
      "\n",
      "\n",
      "The bracketed integer is sometimes called a tag or a\n",
      "coindex. The choice of integer is not significant.\n",
      "There can be any number of tags within a single feature structure.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(nltk.FeatStruct(\"[A='a', B=(1)[C='c'], D->(1), E->(1)]\"))\n",
      "[ A = 'a'             ]\n",
      "[                     ]\n",
      "[ B = (1) [ C = 'c' ] ]\n",
      "[                     ]\n",
      "[ D -> (1)            ]\n",
      "[ E -> (1)            ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2.1   Subsumption and Unification\n",
      "It is standard to think of feature structures as providing partial\n",
      "information about some object, in the sense that we can order\n",
      "feature structures according to how much information they contain. For example,\n",
      "(23a) has less information than (23b), which in turn has less information than (23c).\n",
      "\n",
      "  (23)\n",
      "  a.\n",
      "[NUMBER = 74]\n",
      "\n",
      "\n",
      "\n",
      "  b.\n",
      "[NUMBER = 74          ]\n",
      "[STREET = 'rue Pascal']\n",
      "\n",
      "\n",
      "\n",
      "  c.\n",
      "[NUMBER = 74          ]\n",
      "[STREET = 'rue Pascal']\n",
      "[CITY = 'Paris'       ]\n",
      "\n",
      "\n",
      "\n",
      "This ordering is called subsumption; FS0 subsumes FS1 if all the\n",
      "information contained in FS0 is also contained in FS1.\n",
      "We use the symbol ⊑ to represent subsumption.\n",
      "When we add the possibility of reentrancy, we need to be more careful\n",
      "about how we describe subsumption: if\n",
      "FS0 ⊑ FS1, then FS1 must have all the\n",
      "paths and reentrancies of FS0. Thus, (20) subsumes\n",
      "(22), since the latter has additional reentrancies. It should\n",
      "be obvious that subsumption only provides a partial ordering on\n",
      "feature structures, since some feature structures are\n",
      "incommensurable. For example, (24) neither subsumes nor is subsumed\n",
      "by (23a).\n",
      "\n",
      "  (24)\n",
      "[TELNO = 01 27 86 42 96]\n",
      "\n",
      "\n",
      "So we have seen that some feature structures carry more information than\n",
      "others. How do we go about adding more information to a given feature structure?\n",
      "For example, we might decide that addresses should\n",
      "consist of not just a street number and a street name, but also a\n",
      "city. That is, we might want to merge  graph (25b) with (25a) to\n",
      "yield (25c).\n",
      "\n",
      "  (25)\n",
      "  a.\n",
      "\n",
      "  b.\n",
      "\n",
      "  c.\n",
      "\n",
      "Merging information from two feature structures is called\n",
      "unification and is supported by the unify() method.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fs1 = nltk.FeatStruct(NUMBER=74, STREET='rue Pascal')\n",
      ">>> fs2 = nltk.FeatStruct(CITY='Paris')\n",
      ">>> print(fs1.unify(fs2))\n",
      "[ CITY   = 'Paris'      ]\n",
      "[ NUMBER = 74           ]\n",
      "[ STREET = 'rue Pascal' ]\n",
      "\n",
      "\n",
      "\n",
      "Unification is formally defined as a (partial) binary operation:\n",
      "FS0 ⊔\n",
      "FS1.\n",
      "Unification is symmetric, so\n",
      "FS0 ⊔\n",
      "FS1 = FS1 ⊔\n",
      "FS0.\n",
      "The same is true in Python:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(fs2.unify(fs1))\n",
      "[ CITY   = 'Paris'      ]\n",
      "[ NUMBER = 74           ]\n",
      "[ STREET = 'rue Pascal' ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If we unify two feature structures which stand in the subsumption\n",
      "relationship, then the result of unification is the most informative of\n",
      "the two:\n",
      "\n",
      "  (26)If FS0 ⊑ FS1,  then FS0\n",
      "⊔ FS1 = FS1\n",
      "For example, the result of unifying (23b) with (23c) is (23c).\n",
      "Unification between FS0 and FS1 will fail if the two feature structures share a path π,\n",
      "but the value of π in FS0 is a distinct\n",
      "atom from the value of π in FS1.\n",
      "This is implemented by setting the result of unification to be None.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fs0 = nltk.FeatStruct(A='a')\n",
      ">>> fs1 = nltk.FeatStruct(A='b')\n",
      ">>> fs2 = fs0.unify(fs1)\n",
      ">>> print(fs2)\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "Now, if we look at how unification interacts with structure-sharing,\n",
      "things become really interesting. First, let's define (21) in Python:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fs0 = nltk.FeatStruct(\"\"\"[NAME=Lee,\n",
      "...                           ADDRESS=[NUMBER=74,\n",
      "...                                    STREET='rue Pascal'],\n",
      "...                           SPOUSE= [NAME=Kim,\n",
      "...                                    ADDRESS=[NUMBER=74,\n",
      "...                                             STREET='rue Pascal']]]\"\"\")\n",
      ">>> print(fs0)\n",
      "[ ADDRESS = [ NUMBER = 74           ]               ]\n",
      "[           [ STREET = 'rue Pascal' ]               ]\n",
      "[                                                   ]\n",
      "[ NAME    = 'Lee'                                   ]\n",
      "[                                                   ]\n",
      "[           [ ADDRESS = [ NUMBER = 74           ] ] ]\n",
      "[ SPOUSE  = [           [ STREET = 'rue Pascal' ] ] ]\n",
      "[           [                                     ] ]\n",
      "[           [ NAME    = 'Kim'                     ] ]\n",
      "\n",
      "\n",
      "\n",
      "What happens when we augment Kim's address with a specification\n",
      "for CITY?  Notice that fs1 needs to include the\n",
      "whole path from the root of the feature structure down to CITY.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fs1 = nltk.FeatStruct(\"[SPOUSE = [ADDRESS = [CITY = Paris]]]\")\n",
      ">>> print(fs1.unify(fs0))\n",
      "[ ADDRESS = [ NUMBER = 74           ]               ]\n",
      "[           [ STREET = 'rue Pascal' ]               ]\n",
      "[                                                   ]\n",
      "[ NAME    = 'Lee'                                   ]\n",
      "[                                                   ]\n",
      "[           [           [ CITY   = 'Paris'      ] ] ]\n",
      "[           [ ADDRESS = [ NUMBER = 74           ] ] ]\n",
      "[ SPOUSE  = [           [ STREET = 'rue Pascal' ] ] ]\n",
      "[           [                                     ] ]\n",
      "[           [ NAME    = 'Kim'                     ] ]\n",
      "\n",
      "\n",
      "\n",
      "By contrast, the result is very different if fs1 is unified with\n",
      "the structure-sharing version fs2 (also shown earlier as the graph\n",
      "(22)):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fs2 = nltk.FeatStruct(\"\"\"[NAME=Lee, ADDRESS=(1)[NUMBER=74, STREET='rue Pascal'],\n",
      "...                           SPOUSE=[NAME=Kim, ADDRESS->(1)]]\"\"\")\n",
      ">>> print(fs1.unify(fs2))\n",
      "[               [ CITY   = 'Paris'      ] ]\n",
      "[ ADDRESS = (1) [ NUMBER = 74           ] ]\n",
      "[               [ STREET = 'rue Pascal' ] ]\n",
      "[                                         ]\n",
      "[ NAME    = 'Lee'                         ]\n",
      "[                                         ]\n",
      "[ SPOUSE  = [ ADDRESS -> (1)  ]           ]\n",
      "[           [ NAME    = 'Kim' ]           ]\n",
      "\n",
      "\n",
      "\n",
      "Rather than just updating what was in effect Kim's \"copy\" of Lee's address,\n",
      "we have now updated both their addresses at the same time. More\n",
      "generally, if a unification adds information to the value of some\n",
      "path π, then that unification simultaneously updates the value\n",
      "of any path that is equivalent to π.\n",
      "\n",
      "As we have already seen, structure sharing can also be stated\n",
      "using variables such as ?x.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fs1 = nltk.FeatStruct(\"[ADDRESS1=[NUMBER=74, STREET='rue Pascal']]\")\n",
      ">>> fs2 = nltk.FeatStruct(\"[ADDRESS1=?x, ADDRESS2=?x]\")\n",
      ">>> print(fs2)\n",
      "[ ADDRESS1 = ?x ]\n",
      "[ ADDRESS2 = ?x ]\n",
      ">>> print(fs2.unify(fs1))\n",
      "[ ADDRESS1 = (1) [ NUMBER = 74           ] ]\n",
      "[                [ STREET = 'rue Pascal' ] ]\n",
      "[                                          ]\n",
      "[ ADDRESS2 -> (1)                          ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3   Extending a Feature based Grammar\n",
      "In this section, we return to feature based grammar and explore\n",
      "a variety of linguistic issues, and demonstrate the benefits\n",
      "of incorporating features into the grammar.\n",
      "\n",
      "3.1   Subcategorization\n",
      "In 8., we augmented our category labels to\n",
      "represent different kinds of verb, and used the labels\n",
      "IV and TV for intransitive and transitive verbs\n",
      "respectively.  This allowed us to write productions like the\n",
      "following:\n",
      "\n",
      "  (27)\n",
      "VP -> IV\n",
      "VP -> TV NP\n",
      "\n",
      "\n",
      "Although we know that IV and TV are two kinds of V,\n",
      "they are just atomic nonterminal symbols from a CFG, as distinct\n",
      "from each other as any other pair of symbols.  This notation doesn't\n",
      "let us say anything about verbs in general, e.g. we cannot say\n",
      "\"All lexical items of category V can be marked for tense\",\n",
      "since walk, say, is an item of category IV, not V.\n",
      "So, can we replace category labels such as TV and IV\n",
      "by V along with a feature that tells us whether\n",
      "the verb combines with a following NP object\n",
      "or whether it can occur without any complement?\n",
      "A simple approach, originally developed for a grammar framework\n",
      "called Generalized Phrase Structure Grammar (GPSG), tries to solve\n",
      "this problem by allowing lexical\n",
      "categories to bear a SUBCAT which tells us what subcategorization\n",
      "class the item belongs to. While GPSG used integer values for\n",
      "SUBCAT, the example below adopts more mnemonic values, namely\n",
      "intrans, trans and clause:\n",
      "\n",
      "  (28)\n",
      "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=intrans, TENSE=?t, NUM=?n]\n",
      "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=trans, TENSE=?t, NUM=?n] NP\n",
      "VP[TENSE=?t, NUM=?n] -> V[SUBCAT=clause, TENSE=?t, NUM=?n] SBar\n",
      "\n",
      "V[SUBCAT=intrans, TENSE=pres, NUM=sg] -> 'disappears' | 'walks'\n",
      "V[SUBCAT=trans, TENSE=pres, NUM=sg] -> 'sees' | 'likes'\n",
      "V[SUBCAT=clause, TENSE=pres, NUM=sg] -> 'says' | 'claims'\n",
      "\n",
      "V[SUBCAT=intrans, TENSE=pres, NUM=pl] -> 'disappear' | 'walk'\n",
      "V[SUBCAT=trans, TENSE=pres, NUM=pl] -> 'see' | 'like'\n",
      "V[SUBCAT=clause, TENSE=pres, NUM=pl] -> 'say' | 'claim'\n",
      "\n",
      "V[SUBCAT=intrans, TENSE=past, NUM=?n] -> 'disappeared' | 'walked'\n",
      "V[SUBCAT=trans, TENSE=past, NUM=?n] -> 'saw' | 'liked'\n",
      "V[SUBCAT=clause, TENSE=past, NUM=?n] -> 'said' | 'claimed'\n",
      "\n",
      "\n",
      "When we see a lexical category like V[SUBCAT=trans], we can\n",
      "interpret the SUBCAT specification as a pointer to a production in\n",
      "which V[SUBCAT=trans] is introduced as the head child in a\n",
      "VP production.  By convention, there is a correspondence between\n",
      "the values of SUBCAT and the productions that introduce lexical\n",
      "heads.  On this approach, SUBCAT can only appear on lexical\n",
      "categories; it makes no sense, for example, to specify a SUBCAT\n",
      "value on VP. As required, walk and like both belong to\n",
      "the category V. Nevertheless, walk will only occur in\n",
      "VPs expanded by a production with the feature SUBCAT=intrans\n",
      "on the right hand side, as opposed to like, which requires a\n",
      "SUBCAT=trans.\n",
      "In our third class of verbs above, we have specified a category\n",
      "SBar. This is a label for subordinate clauses such as the\n",
      "complement of claim in the example You claim that you like\n",
      "children. We require two further productions to analyze such sentences:\n",
      "\n",
      "  (29)\n",
      "SBar -> Comp S\n",
      "Comp -> 'that'\n",
      "\n",
      "\n",
      "The resulting structure is the following.\n",
      "\n",
      "  (30)\n",
      "An alternative treatment of subcategorization, due originally to a framework\n",
      "known as categorial grammar, is represented in feature based frameworks such as PATR\n",
      "and Head-driven Phrase Structure Grammar. Rather than using\n",
      "SUBCAT values as a way of indexing productions, the SUBCAT\n",
      "value directly encodes the valency of a head (the list of\n",
      "arguments that it can combine with). For example, a verb like\n",
      "put that takes NP and PP complements (put the\n",
      "book on the table) might be represented as (31):\n",
      "\n",
      "\n",
      "  (31)\n",
      "V[SUBCAT=<NP, NP, PP>]\n",
      "\n",
      "\n",
      "This says that the verb can combine with three arguments. The\n",
      "leftmost element in the list is the subject NP, while everything\n",
      "else — an NP followed by a PP in this case — comprises the\n",
      "subcategorized-for complements. When a verb like put is combined\n",
      "with appropriate complements, the requirements which are specified in\n",
      "the  SUBCAT are discharged, and only a subject NP is\n",
      "needed. This category, which corresponds to what is traditionally\n",
      "thought of as VP, might be represented as follows.\n",
      "\n",
      "  (32)\n",
      "V[SUBCAT=<NP>]\n",
      "\n",
      "\n",
      "Finally, a sentence is a kind of verbal category that has no\n",
      "requirements for further arguments, and hence has a SUBCAT\n",
      "whose value is the empty list. The tree (33) shows how these\n",
      "category assignments combine in a parse of Kim put the book on the table.\n",
      "\n",
      "  (33)\n",
      "\n",
      "\n",
      "3.2   Heads Revisited\n",
      "\n",
      "We noted in the previous section that by factoring subcategorization\n",
      "information out of the main category label, we could express more\n",
      "generalizations about properties of verbs. Another property of this\n",
      "kind is the following: expressions of category V are heads of\n",
      "phrases of category VP. Similarly,\n",
      "Ns are heads of NPs,\n",
      "As (i.e., adjectives) are heads of APs,  and\n",
      "Ps (i.e., prepositions) are heads of PPs.\n",
      "Not all phrases have heads — for example, it is standard to say that coordinate\n",
      "phrases (e.g., the book and the bell) lack heads —\n",
      "nevertheless, we would like our grammar formalism to express the\n",
      "parent / head-child relation where it holds.\n",
      "At present, V and VP are just atomic symbols, and\n",
      "we need to find a way to relate them using features\n",
      "(as we did earlier to relate IV and TV).\n",
      "X-bar Syntax addresses\n",
      "this issue by abstracting out the notion of phrasal level. It is\n",
      "usual to recognize three such levels. If N represents the\n",
      "lexical level, then N' represents the next level up,\n",
      "corresponding to the more traditional category Nom, while\n",
      "N'' represents the phrasal level, corresponding to the\n",
      "category NP.   (34a) illustrates a\n",
      "representative structure while  (34b) is the more conventional counterpart.\n",
      "\n",
      "  (34)\n",
      "  a.\n",
      "\n",
      "  b.\n",
      "\n",
      "\n",
      "The head of the structure (34a) is N while N'\n",
      "and N'' are called (phrasal) projections of N. N''\n",
      "is the maximal projection, and N is sometimes called the\n",
      "zero projection. One of the central claims of X-bar syntax is\n",
      "that all constituents share a structural similarity. Using X as\n",
      "a variable over N, V, A and P, we say that\n",
      "directly subcategorized complements of a lexical head  X are always\n",
      "placed as siblings of the head, whereas adjuncts are\n",
      "placed as siblings of the intermediate category, X'. Thus, the\n",
      "configuration of the two P'' adjuncts in (35) contrasts with that\n",
      "of the complement P'' in (34a).\n",
      "\n",
      "  (35)\n",
      "The productions in (36) illustrate how bar levels can be encoded\n",
      "using feature structures. The nested structure in (35) is\n",
      "achieved by two applications of the recursive rule expanding N[BAR=1].\n",
      "\n",
      "  (36)\n",
      "S -> N[BAR=2] V[BAR=2]\n",
      "N[BAR=2] -> Det N[BAR=1]\n",
      "N[BAR=1] -> N[BAR=1] P[BAR=2]\n",
      "N[BAR=1] -> N[BAR=0] P[BAR=2]\n",
      "N[BAR=1] -> N[BAR=0]XS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.3   Auxiliary Verbs and Inversion\n",
      "Inverted clauses — where the order of subject and verb is\n",
      "switched — occur in English interrogatives and also after\n",
      "'negative' adverbs:\n",
      "\n",
      "  (37)\n",
      "  a.Do you like children?\n",
      "\n",
      "  b.Can Jody walk?\n",
      "\n",
      "\n",
      "  (38)\n",
      "  a.Rarely do you see Kim.\n",
      "\n",
      "  b.Never have I seen this dog.\n",
      "\n",
      "However, we cannot place just any verb in pre-subject position:\n",
      "\n",
      "  (39)\n",
      "  a.*Like you children?\n",
      "\n",
      "  b.*Walks Jody?\n",
      "\n",
      "\n",
      "  (40)\n",
      "  a.*Rarely see you Kim.\n",
      "\n",
      "  b.*Never saw I this dog.\n",
      "\n",
      "Verbs that can be positioned initially in inverted clauses belong to\n",
      "the class known as auxiliaries, and as well as  do,\n",
      "can and have  include be, will  and\n",
      "shall. One way of capturing such structures is with the\n",
      "following production:\n",
      "\n",
      "  (41)\n",
      "S[+INV] -> V[+AUX] NP VP\n",
      "\n",
      "\n",
      "That is, a clause marked as [+INV] consists of an auxiliary\n",
      "verb followed by a VP. (In a more detailed grammar, we would\n",
      "need to place some constraints on the form of the VP, depending\n",
      "on the choice of auxiliary.) (42) illustrates the structure of an\n",
      "inverted clause.\n",
      "\n",
      "  (42)\n",
      "\n",
      "\n",
      "3.4   Unbounded Dependency Constructions\n",
      "Consider the following contrasts:\n",
      "\n",
      "  (43)\n",
      "  a.You like Jody.\n",
      "\n",
      "  b.*You like.\n",
      "\n",
      "\n",
      "  (44)\n",
      "  a.You put the card into the slot.\n",
      "\n",
      "  b.*You put into the slot.\n",
      "\n",
      "  c.*You put the card.\n",
      "\n",
      "  d.*You put.\n",
      "\n",
      "The verb like requires an NP complement, while\n",
      "put requires both a following NP and PP.\n",
      "(43) and (44) show that these complements are obligatory:\n",
      "omitting them leads to ungrammaticality. Yet there are contexts in\n",
      "which obligatory complements can be omitted, as (45) and (46)\n",
      "illustrate.\n",
      "\n",
      "  (45)\n",
      "  a.Kim knows who you like.\n",
      "\n",
      "  b.This music, you really like.\n",
      "\n",
      "\n",
      "  (46)\n",
      "  a.Which card do you put into the slot?\n",
      "\n",
      "  b.Which slot do you put the card into?\n",
      "\n",
      "That is, an obligatory complement can be omitted if there is an\n",
      "appropriate filler in the sentence, such as the question word\n",
      "who in (45a), the preposed topic this music in (45b), or\n",
      "the wh phrases which card/slot in (46). It is common to\n",
      "say that sentences like (45) – (46) contain gaps where\n",
      "the obligatory complements have been omitted, and these gaps are\n",
      "sometimes made explicit using an underscore:\n",
      "\n",
      "  (47)\n",
      "  a.Which card do you put __ into the slot?\n",
      "\n",
      "  b.Which slot do you put the card into __?\n",
      "\n",
      "So, a gap can occur if it is licensed by a filler. Conversely,\n",
      "fillers can only occur if there is an appropriate gap elsewhere  in\n",
      "the sentence, as shown by the following examples.\n",
      "\n",
      "  (48)\n",
      "  a.*Kim knows who you like Jody.\n",
      "\n",
      "  b.*This music, you really like hip-hop.\n",
      "\n",
      "\n",
      "  (49)\n",
      "  a.*Which card do you put this into the slot?\n",
      "\n",
      "  b.*Which slot do you put the card into this one?\n",
      "\n",
      "The mutual co-occurence between filler and gap is sometimes termed a\n",
      "\"dependency\". One issue of considerable importance in theoretical\n",
      "linguistics has been the nature of the material that can intervene\n",
      "between a filler and the gap that it licenses; in particular, can we\n",
      "simply list a finite set of sequences that separate the two? The answer\n",
      "is No: there is no upper bound on the distance between filler and\n",
      "gap. This fact can be easily illustrated with constructions involving\n",
      "sentential complements, as shown in (50).\n",
      "\n",
      "  (50)\n",
      "  a.Who do you like __?\n",
      "\n",
      "  b.Who do you claim that you like __?\n",
      "\n",
      "  c.Who do you claim that Jody says that you like __?\n",
      "\n",
      "Since we can have indefinitely deep recursion of sentential\n",
      "complements, the gap can be embedded indefinitely far inside the whole\n",
      "sentence. This constellation of properties leads to the notion of an\n",
      "unbounded dependency construction; that is, a filler-gap\n",
      "dependency where there is no upper bound on the distance between\n",
      "filler and gap.\n",
      "A variety of mechanisms have been suggested for handling unbounded\n",
      "dependencies in formal grammars; here we illustrate the approach due to\n",
      "Generalized Phrase Structure Grammar that involves\n",
      "slash categories. A slash category has the form Y/XP;\n",
      "we interpret this as a phrase of category Y that\n",
      "is missing a sub-constituent of category XP. For example,\n",
      "S/NP is an S that is missing an NP. The use of\n",
      "slash categories is illustrated in (51).\n",
      "\n",
      "  (51)\n",
      "The top part of the tree introduces the filler who (treated as\n",
      "an expression of category NP[+wh]) together with a\n",
      "corresponding gap-containing constituent S/NP. The gap information is\n",
      "then \"percolated\" down the tree via the VP/NP category, until it\n",
      "reaches the category NP/NP. At this point, the dependency\n",
      "is discharged by realizing the gap information as the empty string,\n",
      "immediately dominated by NP/NP.\n",
      "Do we need to think of slash categories as a completely new kind of\n",
      "object?  Fortunately, we\n",
      "can accommodate them within our existing feature based framework,\n",
      "by treating slash as a feature, and the category to its right\n",
      "as a value; that is,  S/NP is reducible to S[SLASH=NP]. In practice,\n",
      "this is also how the parser interprets slash categories.\n",
      "The grammar shown in 3.1 illustrates\n",
      "the main principles of slash categories, and also includes productions for\n",
      "inverted clauses. To simplify presentation, we have omitted any\n",
      "specification of tense on the verbs.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nltk.data.show_cfg('grammars/book_grammars/feat1.fcfg')\n",
      "% start S\n",
      "# ###################\n",
      "# Grammar Productions\n",
      "# ###################\n",
      "S[-INV] -> NP VP\n",
      "S[-INV]/?x -> NP VP/?x\n",
      "S[-INV] -> NP S/NP\n",
      "S[-INV] -> Adv[+NEG] S[+INV]\n",
      "S[+INV] -> V[+AUX] NP VP\n",
      "S[+INV]/?x -> V[+AUX] NP VP/?x\n",
      "SBar -> Comp S[-INV]\n",
      "SBar/?x -> Comp S[-INV]/?x\n",
      "VP -> V[SUBCAT=intrans, -AUX]\n",
      "VP -> V[SUBCAT=trans, -AUX] NP\n",
      "VP/?x -> V[SUBCAT=trans, -AUX] NP/?x\n",
      "VP -> V[SUBCAT=clause, -AUX] SBar\n",
      "VP/?x -> V[SUBCAT=clause, -AUX] SBar/?x\n",
      "VP -> V[+AUX] VP\n",
      "VP/?x -> V[+AUX] VP/?x\n",
      "# ###################\n",
      "# Lexical Productions\n",
      "# ###################\n",
      "V[SUBCAT=intrans, -AUX] -> 'walk' | 'sing'\n",
      "V[SUBCAT=trans, -AUX] -> 'see' | 'like'\n",
      "V[SUBCAT=clause, -AUX] -> 'say' | 'claim'\n",
      "V[+AUX] -> 'do' | 'can'\n",
      "NP[-WH] -> 'you' | 'cats'\n",
      "NP[+WH] -> 'who'\n",
      "Adv[+NEG] -> 'rarely' | 'never'\n",
      "NP/NP ->\n",
      "Comp -> 'that'\n",
      "\n",
      "\n",
      "Example 3.1 (code_slashcfg.py): Figure 3.1: Grammar with productions for inverted clauses and\n",
      "long-distance dependencies, making use of slash categories\n",
      "\n",
      "The grammar in 3.1 contains one \"gap-introduction\"\n",
      "production, namely S[-INV] -> NP S/NP.\n",
      "In order to percolate the slash feature correctly, we need to add\n",
      "slashes with variable values to both sides of the arrow in productions\n",
      "that expand S, VP and NP. For example, VP/?x -> V SBar/?x is\n",
      "the slashed version of VP -> V SBar and\n",
      "says that a slash value can be specified on the VP parent of a\n",
      "constituent if the same value is also specified on the SBar\n",
      "child. Finally, NP/NP ->  allows the slash information on NP to\n",
      "be discharged as the empty string.\n",
      "Using 3.1, we can parse the sequence who do you claim that you\n",
      "like\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tokens = 'who do you claim that you like'.split()\n",
      ">>> from nltk import load_parser\n",
      ">>> cp = load_parser('grammars/book_grammars/feat1.fcfg')\n",
      ">>> for tree in cp.parse(tokens):\n",
      "...     print(tree)\n",
      "(S[-INV]\n",
      "  (NP[+WH] who)\n",
      "  (S[+INV]/NP[]\n",
      "    (V[+AUX] do)\n",
      "    (NP[-WH] you)\n",
      "    (VP[]/NP[]\n",
      "      (V[-AUX, SUBCAT='clause'] claim)\n",
      "      (SBar[]/NP[]\n",
      "        (Comp[] that)\n",
      "        (S[-INV]/NP[]\n",
      "          (NP[-WH] you)\n",
      "          (VP[]/NP[] (V[-AUX, SUBCAT='trans'] like) (NP[]/NP[] )))))))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A more readable version of this tree is shown in (52).\n",
      "\n",
      "  (52)\n",
      "The grammar in 3.1 will also allow us to parse sentences\n",
      "without gaps:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tokens = 'you claim that you like cats'.split()\n",
      ">>> for tree in cp.parse(tokens):\n",
      "...     print(tree)\n",
      "(S[-INV]\n",
      "  (NP[-WH] you)\n",
      "  (VP[]\n",
      "    (V[-AUX, SUBCAT='clause'] claim)\n",
      "    (SBar[]\n",
      "      (Comp[] that)\n",
      "      (S[-INV]\n",
      "        (NP[-WH] you)\n",
      "        (VP[] (V[-AUX, SUBCAT='trans'] like) (NP[-WH] cats))))))\n",
      "\n",
      "\n",
      "\n",
      "In addition, it admits inverted sentences which do not involve\n",
      "wh constructions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tokens = 'rarely do you sing'.split()\n",
      ">>> for tree in cp.parse(tokens):\n",
      "...     print(tree)\n",
      "(S[-INV]\n",
      "  (Adv[+NEG] rarely)\n",
      "  (S[+INV]\n",
      "    (V[+AUX] do)\n",
      "    (NP[-WH] you)\n",
      "    (VP[] (V[-AUX, SUBCAT='intrans'] sing))))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.5   Case and Gender in German\n",
      "Compared with English, German has a relatively rich morphology for\n",
      "agreement. For example, the definite article in German varies with\n",
      "case, gender and number, as shown in 3.1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Case\n",
      "Masc\n",
      "Fem\n",
      "Neut\n",
      "Plural\n",
      "\n",
      "Nom\n",
      "der\n",
      "die\n",
      "das\n",
      "die\n",
      "\n",
      "Gen\n",
      "des\n",
      "der\n",
      "des\n",
      "der\n",
      "\n",
      "Dat\n",
      "dem\n",
      "der\n",
      "dem\n",
      "den\n",
      "\n",
      "Acc\n",
      "den\n",
      "die\n",
      "das\n",
      "die\n",
      "\n",
      "\n",
      "Table 3.1: Morphological Paradigm for the German definite Article\n",
      "\n",
      "\n",
      "Subjects in German take the nominative case, and most verbs\n",
      "govern their objects in the accusative case. However, there are\n",
      "exceptions like helfen that govern the dative case:\n",
      "\n",
      "  (53)\n",
      "System Message: ERROR/3 (ch09.rst2, line 1693)\n",
      "Error in \"gloss\" directive: may contain a single table only.\n",
      "\n",
      "System Message: ERROR/3 (ch09.rst2, line 1698)\n",
      "Error in \"gloss\" directive: may contain a single table only.\n",
      "\n",
      "System Message: ERROR/3 (ch09.rst2, line 1702)\n",
      "Error in \"gloss\" directive: may contain a single table only.\n",
      "\n",
      "System Message: ERROR/3 (ch09.rst2, line 1707)\n",
      "Error in \"gloss\" directive: may contain a single table only.\n",
      "\n",
      "The grammar in 3.2 illustrates the interaction of agreement\n",
      "(comprising person, number and gender) with case.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nltk.data.show_cfg('grammars/book_grammars/german.fcfg')\n",
      "% start S\n",
      " # Grammar Productions\n",
      " S -> NP[CASE=nom, AGR=?a] VP[AGR=?a]\n",
      " NP[CASE=?c, AGR=?a] -> PRO[CASE=?c, AGR=?a]\n",
      " NP[CASE=?c, AGR=?a] -> Det[CASE=?c, AGR=?a] N[CASE=?c, AGR=?a]\n",
      " VP[AGR=?a] -> IV[AGR=?a]\n",
      " VP[AGR=?a] -> TV[OBJCASE=?c, AGR=?a] NP[CASE=?c]\n",
      " # Lexical Productions\n",
      " # Singular determiners\n",
      " # masc\n",
      " Det[CASE=nom, AGR=[GND=masc,PER=3,NUM=sg]] -> 'der'\n",
      " Det[CASE=dat, AGR=[GND=masc,PER=3,NUM=sg]] -> 'dem'\n",
      " Det[CASE=acc, AGR=[GND=masc,PER=3,NUM=sg]] -> 'den'\n",
      " # fem\n",
      " Det[CASE=nom, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die'\n",
      " Det[CASE=dat, AGR=[GND=fem,PER=3,NUM=sg]] -> 'der'\n",
      " Det[CASE=acc, AGR=[GND=fem,PER=3,NUM=sg]] -> 'die'\n",
      " # Plural determiners\n",
      " Det[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'die'\n",
      " Det[CASE=dat, AGR=[PER=3,NUM=pl]] -> 'den'\n",
      " Det[CASE=acc, AGR=[PER=3,NUM=pl]] -> 'die'\n",
      " # Nouns\n",
      " N[AGR=[GND=masc,PER=3,NUM=sg]] -> 'Hund'\n",
      " N[CASE=nom, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
      " N[CASE=dat, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunden'\n",
      " N[CASE=acc, AGR=[GND=masc,PER=3,NUM=pl]] -> 'Hunde'\n",
      " N[AGR=[GND=fem,PER=3,NUM=sg]] -> 'Katze'\n",
      " N[AGR=[GND=fem,PER=3,NUM=pl]] -> 'Katzen'\n",
      " # Pronouns\n",
      " PRO[CASE=nom, AGR=[PER=1,NUM=sg]] -> 'ich'\n",
      " PRO[CASE=acc, AGR=[PER=1,NUM=sg]] -> 'mich'\n",
      " PRO[CASE=dat, AGR=[PER=1,NUM=sg]] -> 'mir'\n",
      " PRO[CASE=nom, AGR=[PER=2,NUM=sg]] -> 'du'\n",
      " PRO[CASE=nom, AGR=[PER=3,NUM=sg]] -> 'er' | 'sie' | 'es'\n",
      " PRO[CASE=nom, AGR=[PER=1,NUM=pl]] -> 'wir'\n",
      " PRO[CASE=acc, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
      " PRO[CASE=dat, AGR=[PER=1,NUM=pl]] -> 'uns'\n",
      " PRO[CASE=nom, AGR=[PER=2,NUM=pl]] -> 'ihr'\n",
      " PRO[CASE=nom, AGR=[PER=3,NUM=pl]] -> 'sie'\n",
      " # Verbs\n",
      " IV[AGR=[NUM=sg,PER=1]] -> 'komme'\n",
      " IV[AGR=[NUM=sg,PER=2]] -> 'kommst'\n",
      " IV[AGR=[NUM=sg,PER=3]] -> 'kommt'\n",
      " IV[AGR=[NUM=pl, PER=1]] -> 'kommen'\n",
      " IV[AGR=[NUM=pl, PER=2]] -> 'kommt'\n",
      " IV[AGR=[NUM=pl, PER=3]] -> 'kommen'\n",
      " TV[OBJCASE=acc, AGR=[NUM=sg,PER=1]] -> 'sehe' | 'mag'\n",
      " TV[OBJCASE=acc, AGR=[NUM=sg,PER=2]] -> 'siehst' | 'magst'\n",
      " TV[OBJCASE=acc, AGR=[NUM=sg,PER=3]] -> 'sieht' | 'mag'\n",
      " TV[OBJCASE=dat, AGR=[NUM=sg,PER=1]] -> 'folge' | 'helfe'\n",
      " TV[OBJCASE=dat, AGR=[NUM=sg,PER=2]] -> 'folgst' | 'hilfst'\n",
      " TV[OBJCASE=dat, AGR=[NUM=sg,PER=3]] -> 'folgt' | 'hilft'\n",
      " TV[OBJCASE=acc, AGR=[NUM=pl,PER=1]] -> 'sehen' | 'moegen'\n",
      " TV[OBJCASE=acc, AGR=[NUM=pl,PER=2]] -> 'sieht' | 'moegt'\n",
      " TV[OBJCASE=acc, AGR=[NUM=pl,PER=3]] -> 'sehen' | 'moegen'\n",
      " TV[OBJCASE=dat, AGR=[NUM=pl,PER=1]] -> 'folgen' | 'helfen'\n",
      " TV[OBJCASE=dat, AGR=[NUM=pl,PER=2]] -> 'folgt' | 'helft'\n",
      " TV[OBJCASE=dat, AGR=[NUM=pl,PER=3]] -> 'folgen' | 'helfen'\n",
      "\n",
      "\n",
      "Example 3.2 (code_germancfg.py): Figure 3.2: Example Feature based Grammar\n",
      "\n",
      "As you can see, the feature objcase is used to specify the case that\n",
      "a verb governs on its object. The next example illustrates the parse\n",
      "tree for a sentence containing a verb which governs dative case.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tokens = 'ich folge den Katzen'.split()\n",
      ">>> cp = load_parser('grammars/book_grammars/german.fcfg')\n",
      ">>> for tree in cp.parse(tokens):\n",
      "...     print(tree)\n",
      "(S[]\n",
      "  (NP[AGR=[NUM='sg', PER=1], CASE='nom']\n",
      "    (PRO[AGR=[NUM='sg', PER=1], CASE='nom'] ich))\n",
      "  (VP[AGR=[NUM='sg', PER=1]]\n",
      "    (TV[AGR=[NUM='sg', PER=1], OBJCASE='dat'] folge)\n",
      "    (NP[AGR=[GND='fem', NUM='pl', PER=3], CASE='dat']\n",
      "      (Det[AGR=[NUM='pl', PER=3], CASE='dat'] den)\n",
      "      (N[AGR=[GND='fem', NUM='pl', PER=3]] Katzen))))\n",
      "\n",
      "\n",
      "\n",
      "In developing grammars, excluding ungrammatical word sequences is often as\n",
      "challenging as parsing grammatical ones. In order to get an idea\n",
      "where and why a sequence fails to parse, setting the trace\n",
      "parameter of the load_parser() method can be crucial. Consider the\n",
      "following parse failure:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> tokens = 'ich folge den Katze'.split()\n",
      ">>> cp = load_parser('grammars/book_grammars/german.fcfg', trace=2)\n",
      ">>> for tree in cp.parse(tokens):\n",
      "...     print(tree)\n",
      "|.ich.fol.den.Kat.|\n",
      "Leaf Init Rule:\n",
      "|[---]   .   .   .| [0:1] 'ich'\n",
      "|.   [---]   .   .| [1:2] 'folge'\n",
      "|.   .   [---]   .| [2:3] 'den'\n",
      "|.   .   .   [---]| [3:4] 'Katze'\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[---]   .   .   .| [0:1] PRO[AGR=[NUM='sg', PER=1], CASE='nom']\n",
      "                          -> 'ich' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[---]   .   .   .| [0:1] NP[AGR=[NUM='sg', PER=1], CASE='nom'] -> PRO[AGR=[NUM='sg', PER=1], CASE='nom'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[--->   .   .   .| [0:1] S[] -> NP[AGR=?a, CASE='nom'] * VP[AGR=?a] {?a: [NUM='sg', PER=1]}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   [---]   .   .| [1:2] TV[AGR=[NUM='sg', PER=1], OBJCASE='dat'] -> 'folge' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   [--->   .   .| [1:2] VP[AGR=?a] -> TV[AGR=?a, OBJCASE=?c] * NP[CASE=?c] {?a: [NUM='sg', PER=1], ?c: 'dat'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   .   [---]   .| [2:3] Det[AGR=[GND='masc', NUM='sg', PER=3], CASE='acc'] -> 'den' *\n",
      "|.   .   [---]   .| [2:3] Det[AGR=[NUM='pl', PER=3], CASE='dat'] -> 'den' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   .   [--->   .| [2:3] NP[AGR=?a, CASE=?c] -> Det[AGR=?a, CASE=?c] * N[AGR=?a, CASE=?c] {?a: [NUM='pl', PER=3], ?c: 'dat'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   .   [--->   .| [2:3] NP[AGR=?a, CASE=?c] -> Det[AGR=?a, CASE=?c] * N[AGR=?a, CASE=?c] {?a: [GND='masc', NUM='sg', PER=3], ?c: 'acc'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|.   .   .   [---]| [3:4] N[AGR=[GND='fem', NUM='sg', PER=3]] -> 'Katze' *\n",
      "\n",
      "\n",
      "\n",
      "The last two Scanner lines in the trace show that den is recognized as\n",
      "admitting two possible categories: Det[AGR=[GND='masc', NUM='sg',\n",
      "PER=3], CASE='acc'] and Det[AGR=[NUM='pl', PER=3], CASE='dat'].\n",
      "We know from the grammar in 3.2 that Katze has category\n",
      "N[AGR=[GND=fem, NUM=sg, PER=3]]. Thus there is no binding for the\n",
      "variable ?a in production NP[CASE=?c, AGR=?a] -> Det[CASE=?c,\n",
      "AGR=?a] N[CASE=?c, AGR=?a] which will satisfy these constraints, since the\n",
      "AGR value of Katze will not unify with either of the AGR\n",
      "values of  den, that is, with either [GND='masc', NUM='sg',\n",
      "PER=3] or [NUM='pl', PER=3].\n",
      "\n",
      "\n",
      "\n",
      "4   Summary\n",
      "\n",
      "The traditional categories of context-free grammar are atomic\n",
      "symbols. An important motivation for feature structures is to capture\n",
      "fine-grained distinctions that would otherwise require a massive\n",
      "multiplication of atomic categories.\n",
      "By using variables over feature values, we can express constraints\n",
      "in grammar productions that allow the realization of different feature\n",
      "specifications to be inter-dependent.\n",
      "Typically we specify fixed values of features at the lexical level\n",
      "and constrain the values of features in phrases to unify with the\n",
      "corresponding values in their children.\n",
      "Feature values are either atomic or complex. A particular sub-case of\n",
      "atomic value is the Boolean value, represented by convention as\n",
      "[+/- f].\n",
      "Two features can share a value (either atomic or\n",
      "complex). Structures with shared values are said to be\n",
      "re-entrant. Shared values are represented by numerical indexes (or\n",
      "tags) in AVMs.\n",
      "A path in a feature structure is a tuple of features\n",
      "corresponding to the labels on  a sequence of arcs from the root of the graph\n",
      "representation.\n",
      "Two paths are equivalent if they share a value.\n",
      "Feature structures are partially ordered by subsumption.\n",
      "FS0 subsumes FS1 when\n",
      "all the information contained in\n",
      "FS0 is also present in\n",
      "FS1.\n",
      "The unification of two structures FS0 and\n",
      "FS1, if successful, is the feature\n",
      "structure FS2 that contains the combined\n",
      "information of both FS0 and FS1.\n",
      "If unification adds information to a path π in FS, then it also\n",
      "adds information to every path π' equivalent to π.\n",
      "We can use feature structures to build succinct analyses of a wide\n",
      "variety of linguistic phenomena, including verb subcategorization,\n",
      "inversion constructions, unbounded dependency constructions and case government.\n",
      "\n",
      "\n",
      "\n",
      "5   Further Reading\n",
      "Please consult http://nltk.org/ for further materials on this chapter, including\n",
      "feature structures, feature grammars, and grammar test suites.\n",
      "X-bar Syntax: (Jacobs & Rosenbaum, 1970), (Jackendoff, 1977)\n",
      "(The primes we use replace Chomsky's typographically more demanding horizontal bars.)\n",
      "For an excellent introduction to the phenomenon of agreement, see\n",
      "(Corbett, 2006).\n",
      "The earliest use of features in theoretical linguistics was designed\n",
      "to capture phonological properties of phonemes. For example, a sound\n",
      "like /b/ might be decomposed into the structure [+labial, +voice]. An important motivation was to capture\n",
      "generalizations across classes of segments; for example, that /n/ gets\n",
      "realized as /m/ preceding any +labial consonant.\n",
      "Within Chomskyan grammar, it was standard to use atomic features for\n",
      "phenomena like agreement, and also to capture generalizations across\n",
      "syntactic categories, by analogy with phonology.\n",
      "A radical expansion of the use of features in theoretical syntax was\n",
      "advocated by Generalized Phrase Structure Grammar (GPSG;\n",
      "(Gazdar, Klein, & and, 1985)), particularly in the use of features with complex values.\n",
      "Coming more from the perspective of computational linguistics,\n",
      "(Dahl & Saint-Dizier, 1985) proposed that functional aspects of language could be\n",
      "captured by unification of attribute-value structures, and a similar\n",
      "approach was elaborated by (Grosz & Stickel, 1983) within the PATR-II\n",
      "formalism. Early work in Lexical-Functional grammar (LFG;\n",
      "(Bresnan, 1982)) introduced the notion of an f-structure that\n",
      "was primarily intended to represent the grammatical relations and\n",
      "predicate-argument structure associated with a constituent structure\n",
      "parse.  (Shieber, 1986) provides an excellent introduction to this\n",
      "phase of research into feature based grammars.\n",
      "One conceptual difficulty with algebraic approaches to feature\n",
      "structures arose when researchers attempted to model negation. An\n",
      "alternative perspective, pioneered by (Kasper & Rounds, 1986) and\n",
      "(Johnson, 1988), argues that grammars involve descriptions of\n",
      "feature structures rather than the structures themselves. These\n",
      "descriptions are combined using logical operations such as\n",
      "conjunction, and negation is just the usual logical operation over\n",
      "feature descriptions. This description-oriented perspective was\n",
      "integral to LFG from the outset (cf. (Huang & Chen, 1989), and was also adopted by later\n",
      "versions of Head-Driven Phrase Structure Grammar (HPSG;\n",
      "(Sag & Wasow, 1999)). A comprehensive bibliography of HPSG literature can be\n",
      "found at http://www.cl.uni-bremen.de/HPSG-Bib/.\n",
      "Feature structures, as presented in this chapter, are unable to\n",
      "capture important constraints on linguistic information. For example,\n",
      "there is no way of saying that the only permissible values for\n",
      "NUM are sg and pl, while a specification such\n",
      "as [NUM=masc] is anomalous. Similarly, we cannot say\n",
      "that the complex value of AGR must contain\n",
      "specifications for the features PER, NUM and\n",
      "gnd, but cannot contain a specification such as\n",
      "[SUBCAT=trans].  Typed feature structures were developed to\n",
      "remedy this deficiency. To begin with, we stipulate that feature\n",
      "values are always typed. In the case of atomic values, the values just\n",
      "are types. For example, we would say that the value of NUM is\n",
      "the type num. Moreover, num is the most general type of value for\n",
      "NUM. Since types are organized hierarchically, we can be more\n",
      "informative by specifying the value of NUM is a subtype\n",
      "of num, namely either sg or pl.\n",
      "In the case of complex values, we say that feature structures are\n",
      "themselves typed. So for example the value of AGR will be a\n",
      "feature structure of type AGR. We also stipulate that all and only\n",
      "PER, NUM and GND are appropriate features for\n",
      "a structure of type AGR.  A good early review of work on typed\n",
      "feature structures is (Emele & Zajac, 1990). A more comprehensive examination of\n",
      "the formal foundations can be found in (Carpenter, 1992), while\n",
      "(Copestake, 2002) focuses on implementing an HPSG-oriented approach\n",
      "to typed feature structures.\n",
      "There is a copious literature on the analysis of German within\n",
      "feature based grammar frameworks. (Nerbonne, Netter, & Pollard, 1994) is a good\n",
      "starting point for the HPSG literature on this topic, while\n",
      "(M{\\\"u}ller, 2002) gives a very extensive and detailed analysis of\n",
      "German syntax in HPSG.\n",
      "Chapter 15 of (Jurafsky & Martin, 2008) discusses feature structures,\n",
      "the unification algorithm, and the integration of unification into\n",
      "parsing algorithms.\n",
      "\n",
      "\n",
      "6   Exercises\n",
      "\n",
      "☼ What constraints are required to correctly parse word sequences like I am\n",
      "happy and she is happy but not *you is happy or\n",
      "*they am happy? Implement two solutions for the present tense\n",
      "paradigm of the verb be in English, first taking Grammar\n",
      "(6) as your starting point, and then taking Grammar (18)\n",
      "as the starting point.\n",
      "\n",
      "☼ Develop a variant of grammar in 1.1 that uses a\n",
      "feature count to make the distinctions shown below:\n",
      "\n",
      "  (54)\n",
      "  a.The boy sings.\n",
      "\n",
      "\n",
      "  b.*Boy sings.\n",
      "\n",
      "\n",
      "\n",
      "  (55)\n",
      "  a.The boys sing.\n",
      "\n",
      "\n",
      "  b.Boys sing.\n",
      "\n",
      "\n",
      "\n",
      "  (56)\n",
      "  a.The boys sing.\n",
      "\n",
      "\n",
      "  b.Boys sing.\n",
      "\n",
      "\n",
      "\n",
      "  (57)\n",
      "  a.The water is precious.\n",
      "\n",
      "\n",
      "  b.Water is precious.\n",
      "\n",
      "\n",
      "\n",
      "☼ Write a function subsumes() which holds of two feature\n",
      "structures fs1 and fs2 just in case fs1 subsumes fs2.\n",
      "\n",
      "☼ Modify the grammar illustrated in (28) to\n",
      "incorporate a bar feature for dealing with phrasal projections.\n",
      "\n",
      "☼ Modify the German grammar in 3.2 to incorporate the\n",
      "treatment of subcategorization presented in 3.\n",
      "\n",
      "◑ Develop a feature based grammar that will correctly describe the following\n",
      "Spanish noun phrases:\n",
      "\n",
      "System Message: ERROR/3 (ch09.rst2, line 2028)\n",
      "Error in \"gloss\" directive: may contain a single table only.\n",
      "\n",
      "\n",
      "System Message: ERROR/3 (ch09.rst2, line 2033)\n",
      "Error in \"gloss\" directive: may contain a single table only.\n",
      "\n",
      "\n",
      "System Message: ERROR/3 (ch09.rst2, line 2038)\n",
      "Error in \"gloss\" directive: may contain a single table only.\n",
      "\n",
      "\n",
      "System Message: ERROR/3 (ch09.rst2, line 2043)\n",
      "Error in \"gloss\" directive: may contain a single table only.\n",
      "\n",
      "\n",
      "◑ Develop your own version of the EarleyChartParser which only\n",
      "prints a trace if the input sequence fails to parse.\n",
      "\n",
      "◑ Consider the feature structures shown in 6.1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "fs1 = nltk.FeatStruct(\"[A = ?x, B= [C = ?x]]\")\n",
      "fs2 = nltk.FeatStruct(\"[B = [D = d]]\")\n",
      "fs3 = nltk.FeatStruct(\"[B = [C = d]]\")\n",
      "fs4 = nltk.FeatStruct(\"[A = (1)[B = b], C->(1)]\")\n",
      "fs5 = nltk.FeatStruct(\"[A = (1)[D = ?x], C = [E -> (1), F = ?x] ]\")\n",
      "fs6 = nltk.FeatStruct(\"[A = [D = d]]\")\n",
      "fs7 = nltk.FeatStruct(\"[A = [D = d], C = [F = [D = d]]]\")\n",
      "fs8 = nltk.FeatStruct(\"[A = (1)[D = ?x, G = ?x], C = [B = ?x, E -> (1)] ]\")\n",
      "fs9 = nltk.FeatStruct(\"[A = [B = b], C = [E = [G = e]]]\")\n",
      "fs10 = nltk.FeatStruct(\"[A = (1)[B = b], C -> (1)]\")\n",
      "\n",
      "\n",
      "Example 6.1 (code_featstructures.py): Figure 6.1: Exploring Feature Structures\n",
      "\n",
      "Work out on paper what the result is of the following\n",
      "unifications. (Hint: you might find it useful to draw the graph structures.)\n",
      "\n",
      "fs1 and fs2\n",
      "fs1 and fs3\n",
      "fs4 and fs5\n",
      "fs5 and fs6\n",
      "fs5 and fs7\n",
      "fs8 and fs9\n",
      "fs8 and fs10\n",
      "\n",
      "Check your answers using Python.\n",
      "\n",
      "◑ List two feature structures that subsume [A=?x, B=?x].\n",
      "\n",
      "◑ Ignoring structure sharing, give an informal algorithm for unifying\n",
      "two feature structures.\n",
      "\n",
      "◑ Extend the German grammar in 3.2 so that it can\n",
      "handle so-called verb-second structures like the following:\n",
      "\n",
      "  (58)Heute sieht der Hund die Katze.\n",
      "\n",
      "\n",
      "◑ Seemingly synonymous verbs have slightly different\n",
      "syntactic properties (Levin, 1993).  Consider the patterns\n",
      "of grammaticality for the verbs loaded, filled, and dumped\n",
      "below.  Can you write grammar productions to handle such data?\n",
      "\n",
      "  (59)\n",
      "  a.The farmer loaded the cart with sand\n",
      "\n",
      "\n",
      "  b.The farmer loaded sand into the cart\n",
      "\n",
      "\n",
      "  c.The farmer filled the cart with sand\n",
      "\n",
      "\n",
      "  d.*The farmer filled sand into the cart\n",
      "\n",
      "\n",
      "  e.*The farmer dumped the cart with sand\n",
      "\n",
      "\n",
      "  f.The farmer dumped sand into the cart\n",
      "\n",
      "\n",
      "\n",
      "★ Morphological paradigms are rarely completely regular, in\n",
      "the sense of every cell in the matrix having a different\n",
      "realization. For example, the present tense conjugation of the\n",
      "lexeme walk only has two distinct forms: walks for the\n",
      "3rd person singular, and walk for all other combinations of\n",
      "person and number. A successful analysis should not require\n",
      "redundantly specifying that 5 out of the 6 possible morphological\n",
      "combinations have the same realization.  Propose and implement a\n",
      "method for dealing with this.\n",
      "\n",
      "★ So-called head features are shared between the parent\n",
      "node and head child. For example, TENSE is a head feature\n",
      "that is shared between a VP and its head V\n",
      "child. See (Gazdar, Klein, & and, 1985) for more details. Most of the\n",
      "features we have looked at are head features — exceptions are\n",
      "SUBCAT and SLASH. Since the sharing of head\n",
      "features is predictable, it should not need to be stated explicitly\n",
      "in the grammar productions. Develop an approach that automatically\n",
      "accounts for this regular behavior of head features.\n",
      "\n",
      "★ Extend NLTK's treatment of feature structures to allow unification into\n",
      "list-valued features, and use this to implement an HPSG-style analysis of\n",
      "subcategorization, whereby the SUBCAT of a head category is the\n",
      "concatenation its complements' categories with the SUBCAT value of its\n",
      "immediate parent.\n",
      "\n",
      "★ Extend NLTK's treatment of feature structures to allow productions with\n",
      "underspecified categories, such as S[-INV] --> ?x S/?x.\n",
      "\n",
      "★ Extend NLTK's treatment of feature structures to allow typed feature\n",
      "structures.\n",
      "\n",
      "★ Pick some grammatical constructions described in (Huddleston & Pullum, 2002),\n",
      "and develop a feature based grammar to account for them.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "About this document...\n",
      "UPDATED FOR NLTK 3.0.\n",
      "This is a chapter from Natural Language Processing with Python,\n",
      "by Steven Bird, Ewan Klein and Edward Loper,\n",
      "Copyright © 2019 the authors.\n",
      "It is distributed with the Natural Language Toolkit [http://nltk.org/],\n",
      "Version 3.0, under the terms of the\n",
      "Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\n",
      "[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\n",
      "This document was built on\n",
      "Wed  4 Sep 2019 11:40:48 ACST\n",
      "vectors[9] = [0, 3, 5, 5, 3, 57, 1, 5, 0, 0, 0, 23, 56, 9, 0, 0, 6, 0, 0, 0, 5, 0, 0, 0, 1, 0, 3, 24, 8, 10, 0, 2, 2, 3, 2, 9, 0, 0, 19, 0, 4, 8, 0, 10, 0, 0, 8, 0, 3, 1, 0, 1, 0, 0, 0, 0, 4, 2, 0, 0, 17, 0, 0, 0, 43, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 27, 0, 0, 0, 0, 3, 10, 0, 0, 0, 1, 1, 6, 0, 0, 0, 0, 0, 1, 74, 0, 1, 0, 19, 1, 2, 1, 9, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 1, 4, 4, 1, 1, 0, 11, 1, 321, 43, 28, 1, 2, 0, 8, 0, 0, 0, 4, 4, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 10, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 3, 4, 2, 0, 0, 1, 7, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 4, 18, 0, 2, 0, 6, 0, 3, 0, 1, 0, 1, 0, 139, 2, 9, 0, 0, 0, 1, 496, 0, 0, 0, 6, 0, 0, 2, 11, 6, 15, 6, 0, 0, 0, 0, 0, 0, 17, 4, 0, 1, 175, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 2, 2, 8, 112, 1, 0, 0, 0, 0, 0, 0, 0, 92, 1, 3, 1, 0, 0, 0, 0, 0, 13, 0, 21, 1, 3, 0, 0, 0, 1, 7, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 2, 0, 4, 0, 1, 1, 6, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 51, 0, 0, 0, 1, 2, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 95, 4, 3, 0, 0, 0, 2, 0, 2, 0, 2, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 13, 0, 0, 5, 0, 10, 0, 1, 0, 14, 0, 1, 0, 1, 8, 1, 1, 3, 11, 2, 0, 1, 100, 3, 0, 6, 182, 0, 4, 0, 2, 1, 0, 1, 0, 0, 2, 3, 0, 0, 0, 8, 2, 5, 2, 5, 20, 13, 7, 0, 5, 0, 4, 1, 2, 0, 0, 0, 2, 0, 0, 2, 0, 7, 0, 0, 1, 0, 22, 3, 0, 4, 0, 2, 0, 3, 0, 0, 2, 0, 0, 0, 0, 2, 6, 0, 3, 0, 0, 1, 0, 1, 16, 0, 5, 0, 3, 0, 0, 0, 0, 0, 7, 2, 1, 0, 0, 6, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0, 1, 7, 1, 1, 2, 0, 0, 4, 0, 0, 12, 2, 2, 0, 26, 0, 0, 66, 0, 0, 20, 7, 0, 0, 10, 0, 7, 3, 0, 0, 5, 0, 1, 0, 0, 0, 3, 11, 3, 11, 1, 0, 1, 4, 11, 0, 4, 6, 12, 0, 0, 0, 15, 1, 3, 6, 2, 1, 0, 14, 2, 0, 0, 1, 0, 1, 1, 0, 0, 1, 2, 3, 0, 0, 0, 7, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 6, 0, 0, 3, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 4, 2, 0, 5, 2, 5, 6, 1, 9, 2, 3, 0, 0, 5, 0, 2, 3, 1, 0, 0, 2, 1, 1, 1, 3, 0, 1, 0, 0, 0, 5, 0, 0, 6, 31, 0, 0, 2, 0, 0, 16, 0, 0, 0, 5, 0, 45, 0, 6, 0, 0, 0, 3, 0, 0, 0, 0, 0, 5, 0, 0, 9, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 2, 11, 5, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 9, 19, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 11, 2, 0, 0, 0, 0, 0, 9, 2, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 119, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 0, 0, 0, 0, 0, 5, 0, 0, 3, 2, 0, 18, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 7, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 38, 0, 8, 6, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 2, 2, 15, 0, 1, 1, 0, 0, 0, 0, 2, 1, 0, 0, 25, 1, 0, 0, 1, 0, 0, 1, 4, 0, 0, 0, 3, 2, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 4, 1, 0, 0, 6, 3, 0, 1, 0, 0, 1, 0, 14, 0, 0, 2, 0, 0, 2, 6, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 6, 0, 0, 0, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 1, 0, 0, 25, 0, 6, 0, 0, 1, 0, 0, 3, 0, 17, 0, 0, 1, 0, 79, 0, 0, 3, 2, 1, 0, 0, 81, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 46, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 2, 1, 1, 2, 1, 416, 1, 1, 2, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 0, 0, 0, 0, 0, 15, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 2, 0, 0, 21, 0, 0, 0, 0, 1, 15, 0, 0, 3, 0, 5, 3, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 3, 0, 0, 0, 1, 0, 6, 0, 0, 1, 0, 3, 0, 0, 0, 0, 7, 2, 0, 7, 39, 0, 0, 1, 0, 3, 1, 1, 1, 0, 1, 14, 0, 0, 2, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 20, 2, 0, 0, 0, 0, 1, 0, 2, 0, 0, 12, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 7, 0, 0, 2, 0, 1, 1, 3, 0, 20, 3, 0, 0, 0, 10, 1, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 6, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 107, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 2, 2, 0, 1, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 2, 0, 2, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 1, 1, 0, 0, 2, 0, 1, 0, 0, 0, 7, 0, 0, 2, 0, 16, 0, 0, 3, 16, 0, 2, 0, 0, 0, 0, 10, 0, 67, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, 1, 0, 0, 0, 6, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0, 0, 2, 5, 0, 0, 0, 0, 0, 0, 0, 0, 2, 12, 4, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 75, 0, 438, 14, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 3, 1, 0, 1, 55, 1, 25, 0, 7, 1, 0, 0, 0, 0, 2, 0, 2, 6, 3, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 312, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 23, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 16, 1, 2, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 1, 0, 0, 0, 4, 0, 0, 2, 4, 25, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 13, 0, 3, 0, 0, 64, 2, 0, 52, 0, 0, 43, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 8, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 1, 0, 0, 0, 0, 1, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 10, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 9, 8, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 105, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 43, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 7, 0, 0, 0, 0, 0, 0, 12, 0, 1, 0, 0, 1, 0, 0, 14, 0, 0, 0, 0, 1, 0, 0, 0, 0, 5, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 1, 1, 1, 11, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 37, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 3, 2, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 50, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 12, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 18, 0, 2, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 68, 0, 0, 0, 0, 0, 16, 0, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 24, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 198, 0, 0, 14, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 12, 0, 0, 1, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 4, 13, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 2, 6, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 56, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 3, 67, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 1, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 4, 0, 0, 0, 0, 1, 2, 32, 0, 1, 0, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 1, 15, 1, 1, 8, 8, 6, 1, 1, 5, 4, 1, 4, 2, 5, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 4, 1, 82, 10, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 1, 1, 1, 1, 1, 1, 2, 113, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 3, 7, 3, 8, 8, 1, 6, 29, 2, 2, 1, 1, 8, 20, 1, 21, 2, 3, 2, 3, 1, 8, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 32, 1, 1, 1, 1, 1, 2, 109, 2, 30, 2, 1, 1, 1, 1, 1, 1, 2, 27, 39, 19, 20, 15, 1, 3, 1, 1, 1, 2, 18, 18, 10, 6, 3, 6, 3, 3, 1, 1, 1, 7, 2, 2, 3, 20, 6, 3, 2, 1, 1, 1, 1, 1, 10, 1, 4, 1, 2, 8, 1, 1, 3, 1, 1, 2, 4, 1, 3, 3, 1, 3, 45, 1, 1, 14, 1, 2, 7, 1, 2, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 4, 3, 1, 4, 1, 3, 2, 1, 1, 6, 1, 1, 22, 20, 9, 1, 3, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 4, 1, 14, 1, 1, 21, 1, 1, 3, 1, 1, 1, 1, 1, 17, 2, 1, 7, 3, 7, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 3, 6, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 3, 5, 2, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 4, 2, 2, 2, 4, 2, 2, 3, 2, 2, 1, 1, 1, 6, 6, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "docs[10] = 10. Analyzing the Meaning of Sentences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10. Analyzing the Meaning of Sentences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We have seen how useful it is to harness the power of a computer to\n",
      "process text on a large scale.  However, now that we have the\n",
      "machinery of parsers and feature based grammars, can we do anything\n",
      "similarly useful by analyzing the meaning of sentences?\n",
      "The goal of this chapter is to answer the following questions:\n",
      "\n",
      "How can we represent natural language meaning so that a computer\n",
      "can process these representations?\n",
      "How can we associate meaning representations with an\n",
      "unlimited set of sentences?\n",
      "How can we use programs that connect the meaning representations of sentences to\n",
      "stores of knowledge?\n",
      "\n",
      "Along the way we will learn some formal techniques in the field of logical semantics,\n",
      "and see how these can be used for interrogating databases that store facts\n",
      "about the world.\n",
      "\n",
      "1   Natural Language Understanding\n",
      "\n",
      "1.1   Querying a Database\n",
      "Suppose we have a program that lets us type in a natural language question and gives\n",
      "us back the right answer:\n",
      "\n",
      "  (1)\n",
      "  a.Which country is Athens in?\n",
      "\n",
      "  b.Greece.\n",
      "\n",
      "How hard is it to write such a program? And can we just use the same techniques that\n",
      "we've encountered so far in this book, or does it involve something new?\n",
      "In this section, we will show that solving the task in a restricted domain is pretty\n",
      "straightforward. But we will also see that to address the problem in a more\n",
      "general way, we have to open up a whole new box of ideas and techniques, involving the\n",
      "representation of meaning.\n",
      "So let's start off by assuming that we have data about cities and\n",
      "countries in a structured form. To be concrete, we will use a database\n",
      "table whose first few rows are shown in 1.1.\n",
      "\n",
      "Note\n",
      "The data illustrated in 1.1 is drawn from the Chat-80 system\n",
      "(Warren & Pereira, 1982).  Population figures are given in thousands,\n",
      "but note that the data used in these examples dates back at least\n",
      "to the 1980s, and was already somewhat out of date at the point\n",
      "when (Warren & Pereira, 1982) was published.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "City\n",
      "Country\n",
      "Population\n",
      "\n",
      "\n",
      "\n",
      "athens\n",
      "greece\n",
      "1368\n",
      "\n",
      "bangkok\n",
      "thailand\n",
      "1178\n",
      "\n",
      "barcelona\n",
      "spain\n",
      "1280\n",
      "\n",
      "berlin\n",
      "east_germany\n",
      "3481\n",
      "\n",
      "birmingham\n",
      "united_kingdom\n",
      "1112\n",
      "\n",
      "\n",
      "Table 1.1: city_table: A table of cities, countries and populations\n",
      "\n",
      "\n",
      "The obvious way to retrieve answers from this tabular data involves\n",
      "writing queries in a database query language such as SQL.\n",
      "\n",
      "Note\n",
      "SQL (Structured Query Language) is a language designed for\n",
      "retrieving and managing data in relational databases.\n",
      "If you want to find out more about SQL,\n",
      "http://www.w3schools.com/sql/ is a convenient online\n",
      "reference.\n",
      "\n",
      "For example, executing the query (2) will pull out the value 'greece':\n",
      "\n",
      "  (2)SELECT Country FROM city_table WHERE City = 'athens'\n",
      "This specifies a result set consisting of all values for the column\n",
      "Country in data rows where the value of the City column is\n",
      "'athens'.\n",
      "How can we get the same effect using English as our input to the query\n",
      "system? The feature-based grammar formalism described in\n",
      "9. makes it easy to translate from\n",
      "English to SQL. The grammar sql0.fcfg illustrates how to assemble\n",
      "a meaning representation for a sentence in tandem with parsing the\n",
      "sentence. Each phrase structure rule is supplemented with a recipe for\n",
      "constructing a value for the feature sem. You can see that these\n",
      "recipes are extremely simple; in each case, we use the string\n",
      "concatenation operation + to splice\n",
      "the values for the child constituents to make a value for the\n",
      "parent constituent.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nltk.data.show_cfg('grammars/book_grammars/sql0.fcfg')\n",
      "% start S\n",
      "S[SEM=(?np + WHERE + ?vp)] -> NP[SEM=?np] VP[SEM=?vp]\n",
      "VP[SEM=(?v + ?pp)] -> IV[SEM=?v] PP[SEM=?pp]\n",
      "VP[SEM=(?v + ?ap)] -> IV[SEM=?v] AP[SEM=?ap]\n",
      "NP[SEM=(?det + ?n)] -> Det[SEM=?det] N[SEM=?n]\n",
      "PP[SEM=(?p + ?np)] -> P[SEM=?p] NP[SEM=?np]\n",
      "AP[SEM=?pp] -> A[SEM=?a] PP[SEM=?pp]\n",
      "NP[SEM='Country=\"greece\"'] -> 'Greece'\n",
      "NP[SEM='Country=\"china\"'] -> 'China'\n",
      "Det[SEM='SELECT'] -> 'Which' | 'What'\n",
      "N[SEM='City FROM city_table'] -> 'cities'\n",
      "IV[SEM=''] -> 'are'\n",
      "A[SEM=''] -> 'located'\n",
      "P[SEM=''] -> 'in'\n",
      "\n",
      "\n",
      "\n",
      "This allows us to parse a query into SQL.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk import load_parser\n",
      ">>> cp = load_parser('grammars/book_grammars/sql0.fcfg')\n",
      ">>> query = 'What cities are located in China'\n",
      ">>> trees = list(cp.parse(query.split()))\n",
      ">>> answer = trees[0].label()['SEM']\n",
      ">>> answer = [s for s in answer if s]\n",
      ">>> q = ' '.join(answer)\n",
      ">>> print(q)\n",
      "SELECT City FROM city_table WHERE Country=\"china\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Run the parser with maximum tracing on, i.e.,\n",
      "cp = load_parser('grammars/book_grammars/sql0.fcfg', trace=3), and examine\n",
      "how the values of sem are built up as complete edges are added\n",
      "to the chart.\n",
      "\n",
      "Finally, we execute the query over the database city.db and\n",
      "retrieve some results.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.sem import chat80\n",
      ">>> rows = chat80.sql_query('corpora/city_database/city.db', q)\n",
      ">>> for r in rows: print(r[0], end=\" \") \n",
      "canton chungking dairen harbin kowloon mukden peking shanghai sian tientsin\n",
      "\n",
      "\n",
      "\n",
      "Since each row r is a one-element tuple, we print out the member of\n",
      "the tuple rather than tuple itself .\n",
      "To summarize, we have defined a task where the computer returns useful data\n",
      "in response to a natural language query, and we implemented this\n",
      "by translating a small subset of English into SQL. We can say that\n",
      "our NLTK code already \"understands\" SQL, given that Python is\n",
      "able to execute SQL queries against a database, and by extension it also \"understands\"\n",
      "queries such as What cities are located in China. This parallels\n",
      "being able to translate from Dutch into English as an example of\n",
      "natural language understanding.\n",
      "Suppose that you are a native speaker of English, and have started to\n",
      "learn Dutch. Your teacher asks if you understand what (3) means:\n",
      "\n",
      "  (3)Margrietje houdt van Brunoke.\n",
      "If you know the meanings of the individual words in (3), and know\n",
      "how these meanings are combined to make up the meaning of the whole\n",
      "sentence, you might say that (3) means the same as Margrietje loves\n",
      "Brunoke\n",
      "An observer — let's call her Olga — might well take\n",
      "this as evidence that you do grasp the meaning of (3). But this\n",
      "would depend on Olga herself understanding English. If she doesn't,\n",
      "then your translation from Dutch to English is not going to convince\n",
      "her of your ability to understand Dutch. We will return to this issue shortly.\n",
      "The grammar sql0.fcfg, together with the NLTK Earley parser, is\n",
      "instrumental in carrying out the translation from English to SQL. How adequate is this\n",
      "grammar? You saw that the SQL translation for the whole sentence was\n",
      "built up from the translations of the components. However, there does\n",
      "not seem to be a lot of justification for these component meaning\n",
      "representations. For example, if we look at the analysis of the\n",
      "noun phrase Which cities, the determiner and noun correspond\n",
      "respectively to the SQL fragments SELECT and City FROM\n",
      "city_table. But neither of\n",
      "these have a well-defined meaning in isolation from the other.\n",
      "There is another criticism we can level at the grammar: we have\n",
      "\"hard-wired\" an embarrassing amount of detail about the database into\n",
      "it.  We need to know the name of the relevant table (e.g.,\n",
      "city_table) and the names of the fields. But our database could\n",
      "have contained exactly the same rows of data yet used a different\n",
      "table name and different field names, in which case the SQL queries\n",
      "would not be executable. Equally, we could have stored our data in a\n",
      "different format, such as XML, in which case retrieving the same\n",
      "results would require us to translate our English queries into an XML\n",
      "query language rather than SQL. These considerations suggest that we\n",
      "should be translating English into something that is more abstract and\n",
      "generic than SQL.\n",
      "In order to sharpen the point, let's consider another English query\n",
      "and its translation:\n",
      "\n",
      "  (4)\n",
      "  a.What cities are in China and have populations above 1,000,000?\n",
      "\n",
      "  b.SELECT City FROM city_table WHERE Country = 'china' AND\n",
      "Population > 1000\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Extend the grammar sql0.fcfg so that it will translate\n",
      "(4a) into (4b), and check the values returned by\n",
      "the query.\n",
      "You will probably find it easiest to first extend the grammar to\n",
      "handle queries like What cities have populations above\n",
      "1,000,000 before tackling conjunction. After you have had a go\n",
      "at this task, you can compare your solution to grammars/book_grammars/sql1.fcfg in\n",
      "the NLTK data distribution.\n",
      "\n",
      "Observe that the and conjunction in (4a) is translated\n",
      "into an AND in the SQL counterpart, (4b). The latter tells us\n",
      "to select results from rows where two conditions are true\n",
      "together: the value of the Country column is 'china' and the\n",
      "value of the Population column is greater than 1000.\n",
      "This interpretation for and involves a new idea: it talks about\n",
      "what is true in some particular situation, and tells us that\n",
      "Cond1 AND Cond2 is true in situation s just in case that\n",
      "condition Cond1 is true in s and condition Cond2 is true in\n",
      "s. Although this doesn't account for the full range of\n",
      "meanings of and in English, it has the nice property that it is\n",
      "independent of any query language. In fact, we have given it the\n",
      "standard interpretation from classical logic. In the following\n",
      "sections, we will explore an approach in which sentences of natural\n",
      "language are translated into logic instead of an executable query\n",
      "language such as SQL. One advantage of logical formalisms is that they\n",
      "are more abstract\n",
      "and therefore more generic. If we wanted to, once we had our\n",
      "translation into logic, we could then translate it into various other\n",
      "special-purpose languages. In fact, most serious attempts to query\n",
      "databases via natural language have used this methodology.\n",
      "\n",
      "\n",
      "1.2   Natural Language, Semantics and Logic\n",
      "We started out trying to capture the meaning of (1a) by translating it into a\n",
      "query in another language, SQL, which the computer could interpret and execute. But\n",
      "this still begged the question whether the translation was correct. Stepping back\n",
      "from database query, we noted that the meaning of and seems to depend on being\n",
      "able to specify when statements are true or not in a particular situation. Instead of\n",
      "translating a sentence S from one language to another, we try to say what S is\n",
      "about by relating it to a situation in the world. Let's pursue this\n",
      "further. Imagine there is a situation s where there are two entities, Margrietje\n",
      "and her favourite doll, Brunoke. In addition, there is a relation holding between the\n",
      "two entities, which we will call the love relation. If you understand the meaning\n",
      "of (3), then you know that it is true in situation s. In part, you know this\n",
      "because you know that Margrietje refers to Margrietje, Brunoke refers to\n",
      "Brunoke, and houdt van refers to the love relation.\n",
      "We have introduced two\n",
      "fundamental notions in semantics. The\n",
      "first is that declarative sentences are true or false in certain situations.\n",
      "The second is that definite noun phrases and proper nouns refer to things\n",
      "in the world. So (3)\n",
      "is true in a situation where Margrietje loves the doll Brunoke,\n",
      "here illustrated in 1.1.\n",
      "\n",
      "\n",
      "Figure 1.1: Depiction of a situation in which Margrietje loves Brunoke.\n",
      "\n",
      "Once we have adopted the notion of truth in a situation, we have a\n",
      "powerful tool for reasoning.  In particular, we can look at\n",
      "sets of sentences, and ask whether they could be true together in some\n",
      "situation. For example, the sentences in (5) can be both true,\n",
      "while those in (6) and  (7) cannot be. In other words, the sentences in\n",
      "(5) are consistent, while those in (6) and (7) are\n",
      "inconsistent.\n",
      "\n",
      "  (5)\n",
      "  a.Sylvania is to the north of Freedonia.\n",
      "\n",
      "  b.Freedonia is a republic.\n",
      "\n",
      "\n",
      "  (6)\n",
      "  a.The capital of Freedonia has a population of 9,000.\n",
      "\n",
      "  b.No city in Freedonia has a population of 9,000.\n",
      "\n",
      "\n",
      "  (7)\n",
      "  a.Sylvania is to the north of Freedonia.\n",
      "\n",
      "  b.Freedonia is to the north of Sylvania.\n",
      "\n",
      "We have chosen sentences about fictional countries (featured in the\n",
      "Marx Brothers' 1933 movie Duck Soup) to emphasize that your ability\n",
      "to reason about these examples does not depend on what is true\n",
      "or false in the actual world. If you know the meaning of the word no, and\n",
      "also know that the capital of a country is a city in that country, then you\n",
      "should be able to conclude that the two sentences in (6) are\n",
      "inconsistent, regardless of where Freedonia is or what the population\n",
      "of its capital is. That is, there's no possible situation in which\n",
      "both sentences could be true. Similarly, if you know that the relation\n",
      "expressed by to the north of is asymmetric, then you should be\n",
      "able to conclude that the two sentences in (7) are inconsistent.\n",
      "Broadly speaking, logic-based approaches to natural language semantics\n",
      "focus on those aspects of natural language which guide our\n",
      "judgments of consistency and inconsistency. The syntax of a logical\n",
      "language is designed to make these features formally explicit. As a\n",
      "result, determining properties like consistency can often be reduced\n",
      "to symbolic manipulation, that is, to a task that can be carried out\n",
      "by a computer. In order to pursue this approach, we first want to\n",
      "develop a technique for representing a possible situation. We do this\n",
      "in terms of something that logicians call a model.\n",
      "\n",
      "A model for a set W of sentences is a formal\n",
      "representation of a situation in which all the sentences in W\n",
      "are true. The usual way of representing models involves set theory. The domain\n",
      "D of discourse (all the entities we currently care about) is a set of\n",
      "individuals, while relations are treated as\n",
      "sets built up from D. Let's look at a concrete example. Our domain\n",
      "D will consist of three\n",
      "children, Stefan, Klaus and Evi, represented respectively as s,\n",
      "k and e. We write this as D = {s, k, e}. The expression boy denotes the\n",
      "set consisting of Stefan and Klaus, the expression girl denotes the\n",
      "set consisting of Evi, and the expression is running denotes the\n",
      "set consisting of Stefan and Evi. 1.2 is a graphical\n",
      "rendering of the model.\n",
      "\n",
      "\n",
      "Figure 1.2: Diagram of a model containing a domain D and subsets of D\n",
      "corresponding to the predicates boy, girl and is\n",
      "running.\n",
      "\n",
      "Later in this chapter we will use models to help evaluate the truth or falsity of\n",
      "English sentences, and in this way to illustrate some methods for representing\n",
      "meaning. However, before going into more detail, let's put the discussion into a\n",
      "broader perspective, and link back to a topic that we briefly raised in\n",
      "5.  Can a computer understand the meaning\n",
      "of a sentence? And how could we tell if it did?  This is similar to asking \"Can a\n",
      "computer think?\" Alan Turing famously proposed to answer this by examining the\n",
      "ability of a computer to hold sensible conversations with a human\n",
      "(Turing, 1950). Suppose you are having a chat session with a person and a computer,\n",
      "but you are not told at the outset which is which. If you cannot identify which of\n",
      "your partners is the computer after chatting with each of them, then the computer has\n",
      "successfully imitated a human. If a computer succeeds in passing itself off as human\n",
      "in this \"imitation game\" (or \"Turing Test\" as it is popularly known), then according\n",
      "to Turing, we should be prepared to say that the computer can think and can be\n",
      "said to be intelligent. So Turing side-stepped the question of somehow examining the\n",
      "internal states of a computer by instead using its behavior as evidence of\n",
      "intelligence. By the same reasoning, we have assumed that in order to say that a\n",
      "computer understands English, it just needs to behave as though it did.  What is\n",
      "important here is not so much the specifics of Turing's imitation game, but rather\n",
      "the proposal to judge a capacity for natural language understanding in terms of\n",
      "observable behavior.\n",
      "\n",
      "\n",
      "\n",
      "2   Propositional Logic\n",
      "A logical language is designed to make reasoning formally explicit. As\n",
      "a result, it can capture aspects of natural language\n",
      "which determine whether a set of sentences is consistent. As part of\n",
      "this approach, we need to develop logical representations of a\n",
      "sentence φ which formally capture the truth-conditions of φ. We'll start\n",
      "off with a simple example:\n",
      "\n",
      "  (8)[Klaus chased Evi] and [Evi ran away].\n",
      "Let's replace the two sub-sentences in (8) by φ and ψ\n",
      "respectively, and put & for the logical operator corresponding to\n",
      "the English word and: φ & ψ. This structure is the\n",
      "logical form of (8).\n",
      "Propositional logic  allows us to represent\n",
      "just those parts of linguistic structure which correspond to certain\n",
      "sentential connectives. We have just looked at and. Other such\n",
      "connectives are not, or and if...,\n",
      "then.... In the formalization of propositional logic, the\n",
      "counterparts of such connectives are sometimes called boolean\n",
      "operators.  The basic expressions of propositional logic are\n",
      "propositional symbols, often written as P,\n",
      "Q, R, etc.  There are varying conventions for\n",
      "representing boolean operators. Since we will be focusing on ways of\n",
      "exploring logic within NLTK, we will stick to the following ASCII\n",
      "versions of the operators:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nltk.boolean_ops()\n",
      "negation            -\n",
      "conjunction         &\n",
      "disjunction         |\n",
      "implication         ->\n",
      "equivalence         <->\n",
      "\n",
      "\n",
      "\n",
      "From the propositional symbols and the boolean operators we can build\n",
      "an infinite set of well formed formulas (or just formulas, for\n",
      "short) of propositional logic. First, every propositional letter is a\n",
      "formula. Then if φ is a formula, so is -φ. And if  φ and\n",
      "ψ are formulas, then so are\n",
      "(φ & ψ)\n",
      "(φ | ψ)\n",
      "(φ -> ψ)\n",
      "(φ <-> ψ).\n",
      "The 2.1 specifies the truth-conditions for\n",
      "formulas containing these operators. As before we use φ and\n",
      "ψ as variables over sentences, and abbreviate if and only if\n",
      "as iff.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Boolean Operator\n",
      "Truth Conditions\n",
      "\n",
      "\n",
      "\n",
      "negation (it is not the case that ...)\n",
      "-φ is true in s\n",
      "iff\n",
      "φ is false in s\n",
      "\n",
      "conjunction (and)\n",
      "(φ & ψ) is true in s\n",
      "iff\n",
      "φ is true in s and ψ is true in s\n",
      "\n",
      "disjunction (or)\n",
      "(φ | ψ) is true in s\n",
      "iff\n",
      "φ is true in s or ψ is true in s\n",
      "\n",
      "implication (if ..., then ...)\n",
      "(φ -> ψ) is true in s\n",
      "iff\n",
      "φ is false in s or ψ is true in s\n",
      "\n",
      "equivalence (if and only if)\n",
      "(φ <-> ψ) is true in s\n",
      "iff\n",
      "φ and ψ are both true in s or both false in s\n",
      "\n",
      "\n",
      "Table 2.1: Truth conditions for the Boolean Operators in Propositional Logic.\n",
      "\n",
      "\n",
      "These rules are generally straightforward, though the truth conditions for\n",
      "implication departs in many cases from our usual intuitions about the\n",
      "conditional in English. A formula of the form (P -> Q) is only false\n",
      "when P is true and Q is false. If P is false (say P\n",
      "corresponds to The moon is made of green cheese) and Q is\n",
      "true (say Q corresponds to Two plus two equals four) then P\n",
      "-> Q will come out true.\n",
      "NLTKs Expression object can process logical expressions into\n",
      "various subclasses of Expression:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> read_expr = nltk.sem.Expression.fromstring\n",
      ">>> read_expr('-(P & Q)')\n",
      "<NegatedExpression -(P & Q)>\n",
      ">>> read_expr('P & Q')\n",
      "<AndExpression (P & Q)>\n",
      ">>> read_expr('P | (R -> Q)')\n",
      "<OrExpression (P | (R -> Q))>\n",
      ">>> read_expr('P <-> -- P')\n",
      "<IffExpression (P <-> --P)>\n",
      "\n",
      "\n",
      "\n",
      "From a computational perspective, logics give us an important tool for performing\n",
      "inference.  Suppose you state that Freedonia is not to the north of Sylvania, and\n",
      "you give as your reasons that Sylvania is to the north of Freedonia. In this case,\n",
      "you have produced an argument. The sentence Sylvania is to the north of\n",
      "Freedonia is the assumption of the argument while Freedonia is not to the\n",
      "north of Sylvania is the conclusion. The step of moving from one or more assumptions\n",
      "to a conclusion is called inference. Informally, it is common to write arguments in a\n",
      "format where the conclusion is preceded by therefore.\n",
      "\n",
      "  (9)\n",
      "Sylvania is to the north of Freedonia.\n",
      "Therefore, Freedonia is not to the north of Sylvania\n",
      "\n",
      "\n",
      "An argument is valid if there is no possible situation in which\n",
      "its premises are all true and its conclusion is not true.\n",
      "Now, the validity of (9) crucially depends on the meaning of the phrase to\n",
      "the north of, in particular, the fact that it is an asymmetric\n",
      "relation:\n",
      "\n",
      "  (10)if x is to the north of y then y is not to the north of\n",
      "x.\n",
      "Unfortunately, we can't express such rules in propositional logic: the smallest\n",
      "elements we have to play with are atomic propositions, and we cannot \"look inside\"\n",
      "these to talk about relations between individuals x and y.  The best\n",
      "we can do in this case is capture a particular case of the asymmetry. Let's use the\n",
      "propositional symbol SnF to stand for Sylvania is to the north of Freedonia\n",
      "and FnS for Freedonia is to the north of Sylvania. To say that Freedonia\n",
      "is not to the north of Sylvania, we write -FnS That is, we treat not\n",
      "as equivalent to the phrase it is not the case that ..., and translate this as\n",
      "the one-place boolean operator -.  So now we can write the implication in\n",
      "(10) as\n",
      "\n",
      "  (11)SnF -> -FnS\n",
      "How about giving a version of the complete argument? We will replace the first\n",
      "sentence of (9) by two formulas of propositional logic: SnF, and also\n",
      "the implication in (11), which expresses (rather poorly) our background\n",
      "knowledge of the meaning of to the north of.  We'll write [A1, ..., An] / C\n",
      "to represent the argument that conclusion C follows from assumptions [A1, ...,\n",
      "An]. This leads to the following as a representation of argument (9):\n",
      "\n",
      "  (12)[SnF, SnF -> -FnS] / -FnS\n",
      "This is a valid argument: if SnF and SnF -> -FnS are both true in a situation\n",
      "s, then -FnS must also be true in s. By contrast, if FnS were true, this would\n",
      "conflict with our understanding that two objects cannot both be to the north of each\n",
      "other in any possible situation. Equivalently, the list [SnF, SnF -> -FnS, FnS]\n",
      "is inconsistent — these sentences cannot all be true together.\n",
      "Arguments can be tested for \"syntactic validity\" by using a proof system.  We will\n",
      "say a little bit more about this later on in 3.  Logical proofs can be carried out with\n",
      "NLTK's inference module, for example via an interface to the\n",
      "third-party theorem prover Prover9. The inputs to the inference mechanism first have to\n",
      "be converted into logical expressions.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> lp = nltk.sem.Expression.fromstring\n",
      ">>> SnF = read_expr('SnF')\n",
      ">>> NotFnS = read_expr('-FnS')\n",
      ">>> R = read_expr('SnF -> -FnS')\n",
      ">>> prover = nltk.Prover9()\n",
      ">>> prover.prove(NotFnS, [SnF, R])\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "Here's another way of seeing why the conclusion follows.  SnF -> -FnS is semantically\n",
      "equivalent to -SnF | -FnS, where \"|\" is the\n",
      "two-place operator corresponding to or. In general, φ | ψ is true in\n",
      "a situation s if either φ is true in s or φ is true in\n",
      "s. Now, suppose  both SnF and -SnF | -FnS are true in situation s. If SnF is\n",
      "true, then -SnF cannot\n",
      "also be true; a fundamental assumption of classical logic is that a\n",
      "sentence cannot be both true and false in a situation. Consequently,\n",
      "-FnS must be true.\n",
      "Recall that we interpret sentences of a logical language relative to a\n",
      "model, which is a very simplified version of the world. A model for\n",
      "propositional logic needs to assign the values True or False\n",
      "to every possible formula. We do this inductively: first, every\n",
      "propositional symbol is assigned a value, and then we compute the\n",
      "value of complex formulas by consulting the meanings of the boolean\n",
      "operators (i.e, 2.1) and applying them to the values of\n",
      "the formula's components. A Valuation is a mapping from basic\n",
      "expressions of the logic to their values. Here's an example:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> val = nltk.Valuation([('P', True), ('Q', True), ('R', False)])\n",
      "\n",
      "\n",
      "\n",
      "We initialize a Valuation with a list of pairs, each of which\n",
      "consists of a semantic symbol and a semantic value. The resulting\n",
      "object is essentially just a dictionary that maps logical expressions\n",
      "(treated as strings) to appropriate values.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> val['P']\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "As we will see later, our models need to be somewhat more complicated\n",
      "in order to handle the more complex logical forms discussed in the\n",
      "next section; for the time being, just ignore the dom and\n",
      "g parameters in the following declarations.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> dom = set()\n",
      ">>> g = nltk.Assignment(dom)\n",
      "\n",
      "\n",
      "\n",
      "Now let's initialize a model m that uses val:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> m = nltk.Model(dom, val)\n",
      "\n",
      "\n",
      "\n",
      "Every model comes with an evaluate() method, which will determine\n",
      "the semantic value of logical expressions, such as formulas of\n",
      "propositional logic; of course, these values depend on the initial\n",
      "truth values we assigned to propositional symbols such as P,\n",
      "Q and R.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(m.evaluate('(P & Q)', g))\n",
      "True\n",
      ">>> print(m.evaluate('-(P & Q)', g))\n",
      "False\n",
      ">>> print(m.evaluate('(P & R)', g))\n",
      "False\n",
      ">>> print(m.evaluate('(P | R)', g))\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Experiment with evaluating different formulas of propositional logic.\n",
      "Does the model give the values that you expected?\n",
      "\n",
      "Up until now, we have been translating our English sentences into\n",
      "propositional logic. Because we are confined to representing atomic\n",
      "sentences with letters like P and Q, we cannot dig into their\n",
      "internal structure. In effect, we are saying that there is nothing of\n",
      "logical interest to dividing atomic sentences into subjects, objects\n",
      "and predicates. However, this seems wrong: if we want to formalize\n",
      "arguments such as (9), we have to be able to \"look inside\"\n",
      "basic sentences. As a result, we will move beyond Propositional Logic\n",
      "to a something more expressive, namely First-Order Logic. This is what\n",
      "we turn to in the next section.\n",
      "\n",
      "\n",
      "3   First-Order Logic\n",
      "In the remainder of this chapter, we will represent the meaning of natural language\n",
      "expressions by translating them into first-order logic.\n",
      "Not all of natural language semantics can be expressed in first-order logic. But it is a good\n",
      "choice for computational semantics because it is expressive enough to represent a\n",
      "good deal, and on the other hand, there are excellent systems available off the shelf\n",
      "for carrying out automated inference in first order logic.\n",
      "Our next step will be to describe how formulas of first-order logic are constructed, and then how\n",
      "such formulas can be evaluated in a model.\n",
      "\n",
      "3.1   Syntax\n",
      "First-order logic keeps all the boolean operators of Propositional Logic. But it\n",
      "adds some important new mechanisms. To start with, propositions are\n",
      "analyzed into predicates and arguments, which takes us a step closer\n",
      "to the structure of natural languages. The standard construction rules\n",
      "for first-order logic recognize terms such as individual variables and\n",
      "individual constants, and predicates which take differing\n",
      "numbers of arguments. For example, Angus walks might be\n",
      "formalized as walk(angus) and Angus sees Bertie as\n",
      "see(angus, bertie). We will call walk a unary\n",
      "predicate, and see a binary predicate. The\n",
      "symbols used as predicates do not have intrinsic meaning, although it\n",
      "is hard to remember this. Returning to one of our earlier examples,\n",
      "there is no logical difference between (13a) and\n",
      "(13b).\n",
      "\n",
      "  (13)\n",
      "  a.love(margrietje, brunoke)\n",
      "\n",
      "  b.houden_van(margrietje, brunoke)\n",
      "\n",
      "\n",
      "By itself, first-order logic has nothing substantive to say about lexical\n",
      "semantics — the meaning of individual words — although\n",
      "some theories of lexical semantics can be encoded in first-order logic. Whether an\n",
      "atomic predication like see(angus, bertie) is true or false in\n",
      "a situation is not a matter of logic, but depends on the particular\n",
      "valuation that we have chosen for the constants see,\n",
      "angus and bertie. For this reason, such expressions\n",
      "are called non-logical constants. By contrast, logical\n",
      "constants (such as the boolean operators) always receive the same\n",
      "interpretation in every model for first-order logic.\n",
      "We should mention here that one binary predicate has special\n",
      "status, namely equality, as in formulas such as angus =\n",
      "aj. Equality is regarded as a logical constant, since for\n",
      "individual terms t1 and t2, the formula t1 = t2 is\n",
      "true if and only if t1 and t2 refer to one and the\n",
      "same entity.\n",
      "\n",
      "It is often helpful to inspect the syntactic structure of expressions\n",
      "of first-order logic, and the usual way of doing this is to assign types to\n",
      "expressions. Following the tradition of Montague grammar, we\n",
      "will use two basic types: e is the type of entities,\n",
      "while  t is the type of formulas, i.e., expressions which have\n",
      "truth values. Given these two basic types, we can form complex\n",
      "types for function expressions. That is, given any types σ and τ, 〈σ,\n",
      "τ〉 is a complex type corresponding to functions from\n",
      "'σ things' to 'τ things'. For example, 〈e,\n",
      "t〉 is the type of expressions from entities to\n",
      "truth values, namely unary predicates. The logical expression can be\n",
      "processed with type checking.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> read_expr = nltk.sem.Expression.fromstring\n",
      ">>> expr = read_expr('walk(angus)', type_check=True)\n",
      ">>> expr.argument\n",
      "<ConstantExpression angus>\n",
      ">>> expr.argument.type\n",
      "e\n",
      ">>> expr.function\n",
      "<ConstantExpression walk>\n",
      ">>> expr.function.type\n",
      "<e,?>\n",
      "\n",
      "\n",
      "\n",
      "Why do we see <e,?> at the end of this example? Although the\n",
      "type-checker will try to infer as many types as possible, in this case\n",
      "it has not managed to fully specify the type of walk,\n",
      "since its result type is unknown. Although we are intending walk\n",
      "to receive type <e, t>, as far as the type-checker knows, in this\n",
      "context it could be of some other type such as <e, e> or <e, <e, t>. To help\n",
      "the type-checker, we need to specify a signature, implemented as\n",
      "a dictionary that explicitly associates types with non-logical\n",
      "constants:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sig = {'walk': '<e, t>'}\n",
      ">>> expr = read_expr('walk(angus)', signature=sig)\n",
      ">>> expr.function.type\n",
      "e\n",
      "\n",
      "\n",
      "\n",
      "A binary predicate has type 〈e, 〈e, t〉〉. Although this is the type of\n",
      "something which combines first with an argument of type e to make\n",
      "a unary predicate, we represent binary predicates as combining\n",
      "directly with their two arguments. For example, the predicate see in the\n",
      "translation of Angus sees Cyril will\n",
      "combine with its arguments to give the result see(angus, cyril).\n",
      "In first-order logic, arguments of predicates can also be individual variables\n",
      "such as x, y and z. In NLTK, we adopt the convention that\n",
      "variables of type e are all lowercase.\n",
      "Individual variables are similar to\n",
      "personal pronouns like he, she and it, in that we\n",
      "need to know about the context of use in order to figure out their\n",
      "denotation.\n",
      "One way of interpreting the pronoun in (14) is by\n",
      "pointing to a relevant individual in the local context.\n",
      "\n",
      "  (14)He disappeared.\n",
      "Another way is to supply a textual antecedent for the pronoun\n",
      "he, for example by uttering (15a) prior to\n",
      "(14). Here, we say that he is coreferential with\n",
      "the noun phrase Cyril. As a result, (14) is semantically\n",
      "equivalent to (15b).\n",
      "\n",
      "  (15)\n",
      "  a.Cyril is Angus's dog.\n",
      "\n",
      "  b.Cyril disappeared.\n",
      "\n",
      "Consider by contrast the occurrence of he in (16a). In\n",
      "this case, it is bound by the indefinite NP\n",
      "a dog,\n",
      "and this is a different relationship than\n",
      "coreference. If we replace the pronoun he by a dog,\n",
      "the result (16b) is not semantically equivalent to (16a).\n",
      "\n",
      "  (16)\n",
      "  a.Angus had a dog but he disappeared.\n",
      "\n",
      "  b.Angus had a dog but a dog disappeared.\n",
      "\n",
      "Corresponding to (17a), we can construct an open formula\n",
      "(17b) with two occurrences of the variable x. (We\n",
      "ignore tense to simplify exposition.)\n",
      "\n",
      "  (17)\n",
      "  a.He is a dog and he disappeared.\n",
      "\n",
      "  b.dog(x)  ∧ disappear(x)\n",
      "\n",
      "By placing an existential quantifier ∃x ('for\n",
      "some x') in front of (17b), we can bind these\n",
      "variables, as in (18a), which means (18b) or, more\n",
      "idiomatically, (18c).\n",
      "\n",
      "  (18)\n",
      "  a.∃x.(dog(x)  ∧ disappear(x))\n",
      "\n",
      "  b.At least one entity is a dog and disappeared.\n",
      "\n",
      "  c.A dog disappeared.\n",
      "\n",
      "The NLTK rendering of (18a):\n",
      "\n",
      "  (19)exists x.(dog(x) & disappear(x))\n",
      "In addition to the existential quantifier, first-order logic offers us the\n",
      "universal quantifier ∀x ('for all\n",
      "x'), illustrated in (20).\n",
      "\n",
      "  (20)\n",
      "  a.∀x.(dog(x) → disappear(x))\n",
      "\n",
      "  b.Everything has the property that if it is a dog, it disappears.\n",
      "\n",
      "  c.Every dog disappeared.\n",
      "\n",
      "The NLTK syntax for (20a):\n",
      "\n",
      "  (21)all x.(dog(x) -> disappear(x))\n",
      "Although (20a) is the standard first-order logic translation of (20c), the truth\n",
      "conditions aren't necessarily what you expect.\n",
      "The formula says that if some x is a dog, then\n",
      "x disappears — but it doesn't say that there are any dogs. So\n",
      "in a situation where there are no dogs, (20a) will still\n",
      "come out true. (Remember that (P -> Q) is true when P\n",
      "is false.) Now you might argue that every dog disappeared does presuppose the\n",
      "existence of dogs, and that the logic formalization is simply wrong.  But it\n",
      "is possible to find other examples which lack such a presupposition.\n",
      "For instance, we might explain that the value of the Python expression\n",
      "astring.replace('ate', '8') is the result of replacing every\n",
      "occurrence of 'ate' in astring by '8', even though there\n",
      "may in fact be no such occurrences (3.2).\n",
      "We have seen a number of examples where variables are bound by quantifiers. What\n",
      "happens in formulas such as the following?:\n",
      "\n",
      "((exists x. dog(x)) -> bark(x))\n",
      "\n",
      "The scope of the exists x quantifier is dog(x), so the occurrence of x\n",
      "in bark(x) is unbound. Consequently it can become bound by some other quantifier,\n",
      "for example all x in the next formula:\n",
      "\n",
      "all x.((exists x. dog(x)) -> bark(x))\n",
      "\n",
      "In general, an occurrence of a variable x in a formula φ is free in\n",
      "φ if that occurrence doesn't fall within the scope of all x or some x in\n",
      "φ. Conversely, if x is free in formula φ, then it is bound in\n",
      "all x.φ and exists x.φ. If all variable occurrences in a formula are bound, the formula is said\n",
      "to be closed.\n",
      "We mentioned before that the Expression object can process\n",
      "strings, and returns objects of class Expression. Each\n",
      "instance expr of this class comes with a method free() which returns the set\n",
      "of variables that are free in expr.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> read_expr = nltk.sem.Expression.fromstring\n",
      ">>> read_expr('dog(cyril)').free()\n",
      "set()\n",
      ">>> read_expr('dog(x)').free()\n",
      "{Variable('x')}\n",
      ">>> read_expr('own(angus, cyril)').free()\n",
      "set()\n",
      ">>> read_expr('exists x.dog(x)').free()\n",
      "set()\n",
      ">>> read_expr('((some x. walk(x)) -> sing(x))').free()\n",
      "{Variable('x')}\n",
      ">>> read_expr('exists x.own(y, x)').free()\n",
      "{Variable('y')}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.2   First Order Theorem Proving\n",
      "Recall the constraint on to the north of which we proposed earlier as (10):\n",
      "\n",
      "  (22)if x is to the north of y then y is not to the north of\n",
      "x.\n",
      "We observed that propositional logic is not expressive enough to\n",
      "represent generalizations about binary predicates, and as a result we\n",
      "did not properly capture the argument Sylvania is to the north of\n",
      "Freedonia. Therefore, Freedonia is not to the north of Sylvania.\n",
      "You have no doubt realized that first order logic, by contrast, is\n",
      "ideal for formalizing such rules:\n",
      "\n",
      "all x. all y.(north_of(x, y) -> -north_of(y, x))\n",
      "\n",
      "Even better, we can perform automated inference to\n",
      "show the validity of the argument.\n",
      "The general case in theorem proving is to\n",
      "determine whether a formula that we want to prove (a proof goal) can be derived\n",
      "by a finite sequence of inference steps from a list of assumed formulas. We write this\n",
      "as S ⊢ g, where S is a (possibly empty) list of assumptions,\n",
      "and g is a proof goal. We will illustrate this with NLTK's interface to the\n",
      "theorem prover Prover9.  First, we parse the required\n",
      "proof goal  and the two assumptions  . Then we create a Prover9\n",
      "instance , and call its prove() method on the goal, given the list of\n",
      "assumptions .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> NotFnS = read_expr('-north_of(f, s)')  \n",
      ">>> SnF = read_expr('north_of(s, f)')    \n",
      ">>> R = read_expr('all x. all y. (north_of(x, y) -> -north_of(y, x))')  \n",
      ">>> prover = nltk.Prover9()   \n",
      ">>> prover.prove(NotFnS, [SnF, R])  \n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "Happily, the theorem prover agrees with us that the argument is valid.  By contrast,\n",
      "it concludes that it is not possible to infer north_of(f, s) from our\n",
      "assumptions:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> FnS = read_expr('north_of(f, s)')\n",
      ">>> prover.prove(FnS, [SnF, R])\n",
      "False\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.3   Summarizing the Language of First Order Logic\n",
      "We'll take this opportunity to restate our earlier syntactic rules for\n",
      "propositional logic and add the formation rules for quantifiers;\n",
      "together, these give us the syntax of first order logic. In\n",
      "addition, we make explicit the types of the expressions\n",
      "involved. We'll adopt the convention that\n",
      "〈en, t〉\n",
      "is the type of a predicate which combines with n arguments\n",
      "of type e to yield an expression\n",
      "of type t. In this case, we say that n is the\n",
      "arity of the predicate.\n",
      "\n",
      "\n",
      "If P is a predicate of type 〈en,\n",
      "t〉,\n",
      "and α1, ... αn\n",
      "are terms of type e, then\n",
      "P(α1, ... αn) is\n",
      "of type t.\n",
      "If α and β are both of type e, then (α = β) and\n",
      "(α != β) are of type t.\n",
      "If φ is of type t, then so is -φ.\n",
      "If φ and  ψ are of type t, then so are\n",
      "(φ & ψ),\n",
      "(φ | ψ),\n",
      "(φ -> ψ) and\n",
      "(φ <-> ψ).\n",
      "If φ is of type t, and x is a variable of type e, then\n",
      "exists x.φ and  all x.φ are of\n",
      "type t.\n",
      "\n",
      "\n",
      "3.1 summarizes the new logical constants of the logic\n",
      "module, and two of the methods of Expressions.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Example\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "=\n",
      "equality\n",
      "\n",
      "!=\n",
      "inequality\n",
      "\n",
      "exists\n",
      "existential quantifier\n",
      "\n",
      "all\n",
      "universal quantifier\n",
      "\n",
      "e.free()\n",
      "show free variables of e\n",
      "\n",
      "e.simplify()\n",
      "carry out β-reduction on e\n",
      "\n",
      "\n",
      "Table 3.1: Summary of new logical relations and operators required for First\n",
      "Order Logic, together with two useful methods of the\n",
      "Expression class.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.4   Truth in Model\n",
      "We have looked at the syntax of first-order logic, and in\n",
      "4 we will examine the task of\n",
      "translating English into first-order logic. Yet as we argued in\n",
      "1, this only gets us further forward if\n",
      "we can give a meaning to sentences of first-order logic. In other words, we need\n",
      "to give a truth-conditional semantics to first-order logic.\n",
      "From the point of view of computational semantics, there are obvious\n",
      "limits in how far one can push this approach. Although we want to talk\n",
      "about sentences being true or false in situations, we only have the\n",
      "means of representing situations in the computer in a symbolic\n",
      "manner. Despite this limitation, it is still possible to gain a\n",
      "clearer picture of truth-conditional semantics by encoding models in\n",
      "NLTK.\n",
      "Given a first-order logic language L, a model M for L is a\n",
      "pair 〈D, Val〉, where D is an\n",
      "nonempty set called the domain of the model, and Val is\n",
      "a function called the valuation function which assigns values\n",
      "from D to expressions of L as follows:\n",
      "\n",
      "\n",
      "For every individual constant c in L,\n",
      "Val(c) is an element of  D.\n",
      "For every predicate symbol P of arity n ≥ 0,\n",
      "Val(P) is a function from  Dn to\n",
      "{True, False}. (If the arity of  P\n",
      "is 0, then Val(P) is simply a truth value, the\n",
      "P is regarded as a propositional symbol.)\n",
      "\n",
      "\n",
      "According to (ii), if P is of arity 2, then Val(P)\n",
      "will be a function f from pairs of elements of  D to\n",
      "{True, False}. In the models we shall build in\n",
      "NLTK, we'll adopt a more convenient alternative, in which\n",
      "Val(P) is a set S of pairs, defined as follows:\n",
      "\n",
      "  (23)S = {s | f(s) = True}\n",
      "Such an f is called the characteristic function of S\n",
      "(as discussed in the further readings).\n",
      "Relations are represented semantically in NLTK in the standard\n",
      "set-theoretic way: as sets of tuples. For example, let's suppose we\n",
      "have a domain of discourse consisting of the individuals Bertie, Olive and Cyril,\n",
      "where Bertie is a boy, Olive is a girl and Cyril is a dog. For mnemonic\n",
      "reasons, we use b, o and c as the corresponding labels\n",
      "in the model. We can declare the domain as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> dom = {'b', 'o', 'c'}\n",
      "\n",
      "\n",
      "\n",
      "We will use the utility function Valuation.fromstring() to convert a\n",
      "list of strings of the form symbol => value\n",
      "into a Valuation object.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> v = \"\"\"\n",
      "... bertie => b\n",
      "... olive => o\n",
      "... cyril => c\n",
      "... boy => {b}\n",
      "... girl => {o}\n",
      "... dog => {c}\n",
      "... walk => {o, c}\n",
      "... see => {(b, o), (c, b), (o, c)}\n",
      "... \"\"\"\n",
      ">>> val = nltk.Valuation.fromstring(v)\n",
      ">>> print(val)\n",
      "{'bertie': 'b',\n",
      " 'boy': {('b',)},\n",
      " 'cyril': 'c',\n",
      " 'dog': {('c',)},\n",
      " 'girl': {('o',)},\n",
      " 'olive': 'o',\n",
      " 'see': {('o', 'c'), ('c', 'b'), ('b', 'o')},\n",
      " 'walk': {('c',), ('o',)}}\n",
      "\n",
      "\n",
      "\n",
      "So according to this valuation, the value of see is a set of\n",
      "tuples such that Bertie sees Olive, Cyril sees Bertie, and\n",
      "Olive sees Cyril.\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Draw a picture of the domain of m and the sets corresponding to\n",
      "each of the unary predicates, by analogy with the diagram shown in\n",
      "1.2.\n",
      "\n",
      "You may have noticed that our unary predicates (i.e, boy, girl,\n",
      "dog) also come out as sets of singleton tuples, rather\n",
      "than just sets of individuals. This is a convenience which allows us\n",
      "to have a uniform treatment of relations of any arity. A predication\n",
      "of the form P(τ1,  ... τn), where\n",
      "P is of arity n, comes out true just in case the\n",
      "tuple of values corresponding to (τ1,  ... τn) belongs to the set of tuples in the value of  P.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> ('o', 'c') in val['see']\n",
      "True\n",
      ">>> ('b',) in val['boy']\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3.5   Individual Variables and Assignments\n",
      "In our models, the counterpart of a context of use is a variable\n",
      "assignment. This is a mapping from individual variables to\n",
      "entities in the domain.  Assignments are created using the\n",
      "Assignment constructor, which also takes the model's domain of\n",
      "discourse as a parameter. We are not required to actually enter any\n",
      "bindings, but if we do, they are in a (variable,\n",
      "value) format similar to what we saw earlier for valuations.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> g = nltk.Assignment(dom, [('x', 'o'), ('y', 'c')])\n",
      ">>> g\n",
      "{'y': 'c', 'x': 'o'}\n",
      "\n",
      "\n",
      "\n",
      "In addition, there is a print() format for assignments which\n",
      "uses a notation closer to that often found in logic textbooks:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(g)\n",
      "g[c/y][o/x]\n",
      "\n",
      "\n",
      "\n",
      "Let's now look at how we can evaluate an atomic formula of\n",
      "first-order logic. First, we create a model, then we call the evaluate() method\n",
      "to compute the truth value.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> m = nltk.Model(dom, val)\n",
      ">>> m.evaluate('see(olive, y)', g)\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "What's happening here? We are evaluating a formula which is similar to\n",
      "our earlier examplle, see(olive, cyril).\n",
      "However, when the interpretation function encounters the variable y,\n",
      "rather than checking for a value in val, it asks the variable\n",
      "assignment g to come up with a value:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> g['y']\n",
      "'c'\n",
      "\n",
      "\n",
      "\n",
      "Since we already know that individuals o and c stand in the see\n",
      "relation, the value True is what we expected. In this case, we can\n",
      "say that assignment g satisfies the formula see(olive, y).\n",
      "By contrast, the following formula evaluates to False relative to\n",
      "g — check that you see why this is.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> m.evaluate('see(y, x)', g)\n",
      "False\n",
      "\n",
      "\n",
      "\n",
      "In our approach (though not in standard first-order logic), variable assignments\n",
      "are partial. For example, g says nothing about any variables\n",
      "apart from x and y. The method purge() clears all\n",
      "bindings from an assignment.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> g.purge()\n",
      ">>> g\n",
      "{}\n",
      "\n",
      "\n",
      "\n",
      "If we now try to evaluate a formula such as see(olive, y) relative to\n",
      "g, it is like trying to interpret a sentence containing a him when\n",
      "we don't know what him refers to. In this case, the evaluation function\n",
      "fails to deliver a truth value.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> m.evaluate('see(olive, y)', g)\n",
      "'Undefined'\n",
      "\n",
      "\n",
      "\n",
      "Since our models already contain rules for interpreting boolean\n",
      "operators, arbitrarily complex formulas can be composed and evaluated.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> m.evaluate('see(bertie, olive) & boy(bertie) & -walk(bertie)', g)\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "The general process of determining truth or falsity of a formula in a\n",
      "model is called model checking.\n",
      "\n",
      "\n",
      "3.6   Quantification\n",
      "One of the crucial insights of modern\n",
      "logic is that the notion of variable satisfaction can be used to\n",
      "provide an interpretation to quantified formulas. Let's\n",
      "use (24) as an example.\n",
      "\n",
      "  (24)exists x.(girl(x) & walk(x))\n",
      "When is it true? Let's think about all the individuals in our domain,\n",
      "i.e., in dom. We want to check whether any of these individuals\n",
      "have the property of being a girl and walking. In other words, we want\n",
      "to know if there is some u in dom such that g[u/x]\n",
      "satisfies the open formula (25).\n",
      "\n",
      "  (25)girl(x) & walk(x)\n",
      "Consider the following:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> m.evaluate('exists x.(girl(x) & walk(x))', g)\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "evaluate() returns True here because there is some u in\n",
      "dom such that (25) is satisfied by an assignment which binds\n",
      "x to u. In fact, o is such a u:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> m.evaluate('girl(x) & walk(x)', g.add('x', 'o'))\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "One useful tool offered by NLTK is the satisfiers() method. This\n",
      "returns a set of all the individuals that satisfy an open formula. The\n",
      "method parameters are a parsed formula, a variable, and an\n",
      "assignment. Here are a few examples:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fmla1 = read_expr('girl(x) | boy(x)')\n",
      ">>> m.satisfiers(fmla1, 'x', g)\n",
      "{'b', 'o'}\n",
      ">>> fmla2 = read_expr('girl(x) -> walk(x)')\n",
      ">>> m.satisfiers(fmla2, 'x', g)\n",
      "{'c', 'b', 'o'}\n",
      ">>> fmla3 = read_expr('walk(x) -> girl(x)')\n",
      ">>> m.satisfiers(fmla3, 'x', g)\n",
      "{'b', 'o'}\n",
      "\n",
      "\n",
      "\n",
      "It's useful to think about why fmla2 and fmla3 receive the\n",
      "values they do. The truth conditions for -> mean that\n",
      "fmla2 is equivalent to -girl(x) | walk(x), which  is satisfied\n",
      "by something which either isn't a girl\n",
      "or walks. Since neither b (Bertie) nor c (Cyril)\n",
      "are girls, according to model m, they both satisfy\n",
      "the whole formula. And of course o satisfies the formula because o\n",
      "satisfies both disjuncts. Now, since every member of the domain of\n",
      "discourse satisfies fmla2, the corresponding universally\n",
      "quantified formula is also true.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> m.evaluate('all x.(girl(x) -> walk(x))', g)\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "In other words, a universally quantified formula ∀x.φ is true with respect to g just in case for\n",
      "every u, φ is true with respect to g[u/x].\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Try to figure out, first with pencil and paper, and then using\n",
      "m.evaluate(), what the truth values are for all x.(girl(x) &\n",
      "walk(x)) and exists x.(boy(x) -> walk(x)). Make sure you\n",
      "understand why they receive these values.\n",
      "\n",
      "\n",
      "\n",
      "3.7   Quantifier Scope Ambiguity\n",
      "What happens when we want to give a formal representation of a\n",
      "sentence with two quantifiers, such as the following?\n",
      "\n",
      "  (26)Everybody admires someone.\n",
      "There are (at least) two ways of expressing (26) in first-order logic:\n",
      "\n",
      "  (27)\n",
      "  a.all x.(person(x) -> exists y.(person(y) & admire(x,y)))\n",
      "\n",
      "  b.exists y.(person(y) & all x.(person(x) -> admire(x,y)))\n",
      "\n",
      "Can we use both of these? The answer is Yes, but they have different\n",
      "meanings. (27b) is logically stronger than (27a): it claims that\n",
      "there is a unique person, say Bruce, who is admired by everyone.\n",
      "(27a), on the other hand, just requires that for every person\n",
      "u, we can find some person u' whom u\n",
      "admires; but this could be a different person  u' in each case. We\n",
      "distinguish between (27a) and (27b) in terms of the scope\n",
      "of the quantifiers. In the first, ∀ has wider scope than\n",
      "∃, while in (27b), the scope ordering is reversed. So now we\n",
      "have two ways of representing the meaning of (26), and they are\n",
      "both quite legitimate. In other words, we are claiming that (26) is\n",
      "ambiguous with respect to quantifier scope, and the formulas in\n",
      "(27) give us a way to make the two readings explicit.\n",
      "However, we are not just interested in associating two\n",
      "distinct representations with (26). We also want to show in detail\n",
      "how the two representations lead to different conditions for truth in\n",
      "a model.\n",
      "In order to examine the ambiguity more closely, let's fix our\n",
      "valuation as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> v2 = \"\"\"\n",
      "... bruce => b\n",
      "... elspeth => e\n",
      "... julia => j\n",
      "... matthew => m\n",
      "... person => {b, e, j, m}\n",
      "... admire => {(j, b), (b, b), (m, e), (e, m)}\n",
      "... \"\"\"\n",
      ">>> val2 = nltk.Valuation.fromstring(v2)\n",
      "\n",
      "\n",
      "\n",
      "The admire relation can be visualized using the\n",
      "mapping diagram shown in (28).\n",
      "\n",
      "  (28)\n",
      "In (28), an arrow between two individuals x and\n",
      "y indicates that x admires\n",
      "y. So j and b both admire b (Bruce is very vain), while e admires\n",
      "m and m admires e. In this model, formula (27a) above\n",
      "is true but (27b) is false. One way of exploring these results is by\n",
      "using the satisfiers() method of Model objects.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> dom2 = val2.domain\n",
      ">>> m2 = nltk.Model(dom2, val2)\n",
      ">>> g2 = nltk.Assignment(dom2)\n",
      ">>> fmla4 = read_expr('(person(x) -> exists y.(person(y) & admire(x, y)))')\n",
      ">>> m2.satisfiers(fmla4, 'x', g2)\n",
      "{'e', 'b', 'm', 'j'}\n",
      "\n",
      "\n",
      "\n",
      "This shows that fmla4 holds of every individual in the domain.\n",
      "By contrast, consider the formula fmla5 below; this has no\n",
      "satisfiers for the variable y.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fmla5 = read_expr('(person(y) & all x.(person(x) -> admire(x, y)))')\n",
      ">>> m2.satisfiers(fmla5, 'y', g2)\n",
      "set()\n",
      "\n",
      "\n",
      "\n",
      "That is, there is no person that is admired by everybody. Taking a\n",
      "different open formula, fmla6, we can verify that there is a\n",
      "person, namely Bruce, who is admired by both Julia and Bruce.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> fmla6 = read_expr('(person(y) & all x.((x = bruce | x = julia) -> admire(x, y)))')\n",
      ">>> m2.satisfiers(fmla6, 'y', g2)\n",
      "{'b'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Devise a new model based on m2 such that (27a)\n",
      "comes out false in your model; similarly, devise a new model such that (27b)\n",
      "comes out true.\n",
      "\n",
      "\n",
      "\n",
      "3.8   Model Building\n",
      "We have been assuming that we already had a model, and wanted to check\n",
      "the truth of a sentence in the model.  By contrast, model building\n",
      "tries to create a new model, given some set of sentences. If it\n",
      "succeeds, then we know that the set is consistent, since we have an\n",
      "existence proof of the model.\n",
      "We invoke the Mace4 model builder by creating an instance of\n",
      "Mace() and calling its build_model()  method, in an\n",
      "analogous way to calling the Prover9 theorem prover. One option is to\n",
      "treat our candidate set of sentences as assumptions, while leaving the\n",
      "goal unspecified.  The following interaction shows how both\n",
      "[a, c1] and [a, c2] are consistent lists, since Mace succeeds\n",
      "in building a model for each of them, while [c1, c2] is\n",
      "inconsistent.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> a3 = read_expr('exists x.(man(x) & walks(x))')\n",
      ">>> c1 = read_expr('mortal(socrates)')\n",
      ">>> c2 = read_expr('-mortal(socrates)')\n",
      ">>> mb = nltk.Mace(5)\n",
      ">>> print(mb.build_model(None, [a3, c1]))\n",
      "True\n",
      ">>> print(mb.build_model(None, [a3, c2]))\n",
      "True\n",
      ">>> print(mb.build_model(None, [c1, c2]))\n",
      "False\n",
      "\n",
      "\n",
      "\n",
      "We can also use the model builder as an adjunct to the theorem prover.\n",
      "Let's suppose we are trying to prove S ⊢ g, i.e. that g is\n",
      "logically derivable from assumptions S = [s1, s2, ..., sn].  We\n",
      "can feed this same input to Mace4, and the model builder will try to find a\n",
      "counterexample, that is, to show that g does not follow from\n",
      "S. So, given this input, Mace4 will try to find a model for the\n",
      "set S together with the negation of g, namely the list S' =\n",
      "[s1, s2, ..., sn, -g]. If g fails to follow from S, then\n",
      "Mace4 may well return with a counterexample faster than Prover9\n",
      "concludes that it cannot find the required proof.  Conversely, if\n",
      "g is provable from S, Mace4 may take a long time\n",
      "unsuccessfully trying to find a countermodel, and will eventually\n",
      "give up.\n",
      "Let's consider a concrete scenario. Our assumptions are the list [There is a\n",
      "woman that every man loves, Adam is a man, Eve is a\n",
      "woman]. Our conclusion is Adam loves Eve. Can Mace4 find a\n",
      "model in which the premises are true but the conclusion is false? In\n",
      "the following code, we use MaceCommand() which will let us inspect\n",
      "the model that has been built.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> a4 = read_expr('exists y. (woman(y) & all x. (man(x) -> love(x,y)))')\n",
      ">>> a5 = read_expr('man(adam)')\n",
      ">>> a6 = read_expr('woman(eve)')\n",
      ">>> g = read_expr('love(adam,eve)')\n",
      ">>> mc = nltk.MaceCommand(g, assumptions=[a4, a5, a6])\n",
      ">>> mc.build_model()\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "So the answer is Yes: Mace4 found a countermodel in which there is some woman other than\n",
      "Eve that Adam loves. But let's have a closer look at Mace4's model,\n",
      "converted to the format we use for valuations.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(mc.valuation)\n",
      "{'C1': 'b',\n",
      " 'adam': 'a',\n",
      " 'eve': 'a',\n",
      " 'love': {('a', 'b')},\n",
      " 'man': {('a',)},\n",
      " 'woman': {('a',), ('b',)}}\n",
      "\n",
      "\n",
      "\n",
      "The general form of this valuation should be familiar to you: it contains\n",
      "some individual constants and predicates, each with an appropriate\n",
      "kind of value. What might be puzzling is the\n",
      "C1. This is a \"skolem constant\" that the model builder introduces as a\n",
      "representative of the existential quantifier. That is, when the model\n",
      "builder encountered the exists y part of a4 above, it knew\n",
      "that there is some individual b in the domain which satisfies the open\n",
      "formula in the body of a4. However, it doesn't know\n",
      "whether b is also the denotation of an individual constant\n",
      "anywhere else in its input, so it makes up a new name for b on the fly,\n",
      "namely  C1. Now, since our premises said nothing about\n",
      "the individual constants adam and eve, the model builder has decided there is no\n",
      "reason to treat them as denoting different entities, and they both get mapped\n",
      "to a. Moreover, we didn't specify that man and woman\n",
      "denote disjoint sets, so the model builder lets their denotations overlap. This\n",
      "illustrates quite dramatically the implicit knowledge that we bring to\n",
      "bear in interpreting our scenario, but which the model builder knows\n",
      "nothing about.\n",
      "So let's add a new assumption which makes the sets of men and women disjoint. The model builder\n",
      "still produces a countermodel, but this time it is more in accord with our intuitions\n",
      "about the situation:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> a7 = read_expr('all x. (man(x) -> -woman(x))')\n",
      ">>> g = read_expr('love(adam,eve)')\n",
      ">>> mc = nltk.MaceCommand(g, assumptions=[a4, a5, a6, a7])\n",
      ">>> mc.build_model()\n",
      "True\n",
      ">>> print(mc.valuation)\n",
      "{'C1': 'c',\n",
      " 'adam': 'a',\n",
      " 'eve': 'b',\n",
      " 'love': {('a', 'c')},\n",
      " 'man': {('a',)},\n",
      " 'woman': {('c',), ('b',)}}\n",
      "\n",
      "\n",
      "\n",
      "On reflection, we can see that there is nothing in our premises which says that\n",
      "Eve is the only woman in the domain of discourse, so the countermodel in fact\n",
      "is acceptable. If we wanted to rule it out, we would have to add a further\n",
      "assumption such as exists y. all x. (woman(x) -> (x = y)) to ensure that\n",
      "there is only one woman in the model.\n",
      "\n",
      "\n",
      "\n",
      "4   The Semantics of English Sentences\n",
      "\n",
      "4.1   Compositional Semantics in Feature-Based Grammar\n",
      "At the beginning of the chapter we briefly illustrated a method of\n",
      "building semantic representations on the basis of a syntactic parse,\n",
      "using the grammar framework developed in 9.. This\n",
      "time, rather than constructing an SQL query, we will build a logical\n",
      "form. One of our guiding ideas for designing such grammars is the\n",
      "Principle of Compositionality.  (Also known as Frege's\n",
      "Principle; see (Gleitman & Liberman, 1995) for the formulation given below.)\n",
      "Principle of Compositionality:\n",
      "The meaning of a whole is a function of the meanings of the parts\n",
      "and of the way they are syntactically combined.\n",
      "We will assume that the semantically relevant parts of a complex\n",
      "expression are given by a theory of syntactic analysis. Within this\n",
      "chapter, we will take it for granted that expressions are parsed\n",
      "against a context-free grammar. However, this is not entailed by the\n",
      "Principle of Compositionality.\n",
      "Our goal now is integrate the construction of a semantic representation\n",
      "in a manner that can be smoothly with the process of parsing. (29)\n",
      "illustrates a first approximation to the kind of analyses we would\n",
      "like to build.\n",
      "\n",
      "  (29)\n",
      "In (29), the sem value at the root node shows a semantic\n",
      "representation for the whole sentence, while the sem values at\n",
      "lower nodes show semantic representations for constituents of the\n",
      "sentence. Since the values of sem have to be treated in special\n",
      "manner, they are distinguished from other feature values by being\n",
      "enclosed in angle brackets.\n",
      "So far, so good, but how do we write grammar rules which will give us\n",
      "this kind of result? Our approach will be similar to that adopted for\n",
      "the grammar sql0.fcfg at the start of this chapter, in that we\n",
      "will assign semantic representations to lexical nodes, and then\n",
      "compose the semantic representations for each phrase from those of its\n",
      "child nodes. However, in the present case we will use function\n",
      "application rather than string concatenation as the mode of\n",
      "composition. To be more specific, suppose we have a NP and\n",
      "VP constituents with appropriate values for their sem\n",
      "nodes. Then the sem value of an S is handled by a rule like\n",
      "(30). (Observe that in the case where the value of sem is a\n",
      "variable, we omit the angle brackets.)\n",
      "\n",
      "  (30)S[SEM=<?vp(?np)>] -> NP[SEM=?np] VP[SEM=?vp]\n",
      "(30) tells us that given some sem value ?np for the subject\n",
      "NP and some sem value ?vp for the VP, the sem\n",
      "value of the S parent is constructed by applying ?vp as a\n",
      "function expression to ?np.  From this, we can conclude that ?vp has to\n",
      "denote a function which has the denotation of ?np in its\n",
      "domain. (30) is a nice example of\n",
      "building semantics using the principle of compositionality.\n",
      "To complete the grammar is very straightforward; all we require are the\n",
      "rules shown below.\n",
      "\n",
      "VP[SEM=?v] -> IV[SEM=?v]\n",
      "NP[SEM=<cyril>] -> 'Cyril'\n",
      "IV[SEM=<\\x.bark(x)>] -> 'barks'\n",
      "\n",
      "The VP rule says that the parent's semantics is the same as the\n",
      "head child's semantics. The two lexical rules provide non-logical\n",
      "constants to serve as the semantic values of Cyril and\n",
      "barks respectively. There is an additional piece of notation in\n",
      "the entry for barks which we will explain shortly.\n",
      "Before launching into compositional semantic rules in more detail, we need to add a\n",
      "new tool to our kit, namely the λ calculus. This provides us with an\n",
      "invaluable tool for combining expressions of first-order logic as we assemble a meaning\n",
      "representation for an English sentence.\n",
      "\n",
      "\n",
      "4.2   The λ-Calculus\n",
      "In 3, we pointed out that\n",
      "mathematical set notation was a helpful method of specifying\n",
      "properties P of words that we wanted to select from a\n",
      "document. We illustrated this with (31), which we\n",
      "glossed as \"the set of all w such that w is an element\n",
      "of V (the vocabulary) and w has property P\".\n",
      "\n",
      "  (31){w | w ∈ V & P(w)}\n",
      "It turns out to be extremely useful to add something to first-order logic that\n",
      "will achieve the same effect. We do this with the λ operator\n",
      "(pronounced \"lambda\"). The λ counterpart to\n",
      "(31) is (32). (Since we are not trying\n",
      "to do set theory here, we just treat V as a unary predicate.)\n",
      "\n",
      "  (32)λw. (V(w) ∧ P(w))\n",
      "\n",
      "Note\n",
      "λ expressions were originally designed by Alonzo Church to represent\n",
      "computable functions and to provide a foundation for mathematics\n",
      "and logic. The theory in which λ expressions are studied is\n",
      "known as the λ-calculus. Note that the λ-calculus is\n",
      "not part of first-order logic — both can be used independently of the other.\n",
      "\n",
      "λ is a binding operator, just as the first-order logic quantifiers are. If\n",
      "we have an open formula such as (33a), then we can bind\n",
      "the variable x with the λ operator, as shown in\n",
      "(33b).  The corresponding NLTK representation is given in\n",
      "(33c).\n",
      "\n",
      "  (33)\n",
      "  a.(walk(x) ∧ chew_gum(x))\n",
      "\n",
      "  b.λx.(walk(x) ∧ chew_gum(x))\n",
      "\n",
      "  c.\\x.(walk(x) & chew_gum(x))\n",
      "\n",
      "Remember that \\ is a special character in Python strings.\n",
      "We could escape it (with another \\), or else use \"raw strings\"\n",
      "(3.4):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> read_expr = nltk.sem.Expression.fromstring\n",
      ">>> expr = read_expr(r'\\x.(walk(x) & chew_gum(x))')\n",
      ">>> expr\n",
      "<LambdaExpression \\x.(walk(x) & chew_gum(x))>\n",
      ">>> expr.free()\n",
      "set()\n",
      ">>> print(read_expr(r'\\x.(walk(x) & chew_gum(y))'))\n",
      "\\x.(walk(x) & chew_gum(y))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We have a special name for the result of binding the variables in an\n",
      "expression: λ abstraction. When you first encounter\n",
      "λ-abstracts, it can be hard to get an intuitive sense of their\n",
      "meaning. A couple of English glosses for (33b) are: \"be\n",
      "an x such that x walks and x chews gum\" or\n",
      "\"have the property of walking and chewing gum\".  It has often been suggested\n",
      "that λ-abstracts are good representations for verb phrases (or\n",
      "subjectless clauses), particularly when these occur as arguments in\n",
      "their own right. This is illustrated in (34a) and its\n",
      "translation (34b).\n",
      "\n",
      "  (34)\n",
      "  a.To walk and chew-gum is hard\n",
      "\n",
      "  b.hard(\\x.(walk(x) & chew_gum(x)))\n",
      "\n",
      "So the general picture is this: given an open formula φ with\n",
      "free variable x, abstracting over x yields a property\n",
      "expression λx.φ — the property of being\n",
      "an x such that φ. Here's a more official version of how abstracts are built:\n",
      "\n",
      "\n",
      "  (35)If α is of type τ, and x is a variable of type e, then\n",
      "\\x.α is of type 〈e, τ〉.\n",
      "(34b) illustrated a case where we say something about a property, namely\n",
      "that it is hard. But what we usually do with properties is attribute them to\n",
      "individuals. And in fact if φ is an open formula, then the abstract λx.φ can be used as a unary predicate. In (36),\n",
      "(33b) is predicated of the term gerald.\n",
      "\n",
      "  (36)\\x.(walk(x) & chew_gum(x)) (gerald)\n",
      "Now (36) says that Gerald has the property of walking and chewing gum,\n",
      "which has the same meaning as (37).\n",
      "\n",
      "  (37)(walk(gerald) & chew_gum(gerald))\n",
      "What we have done here is remove the \\x from the beginning of \\x.(walk(x) &\n",
      "chew_gum(x)) and replaced all occurrences of x in (walk(x) &\n",
      "chew_gum(x)) by gerald. We'll use α[β/x] as\n",
      "notation for the operation of replacing all free occurrences of  x in\n",
      "α by the expression β. So:\n",
      "\n",
      "(walk(x) & chew_gum(x))[gerald/x]\n",
      "\n",
      "is the same expression as (37).  The \"reduction\" of (36) to\n",
      "(37) is an extremely useful operation in simplifying semantic\n",
      "representations, and we shall use it a lot in the rest of this chapter. The operation\n",
      "is often called β-reduction. In order for it to be semantically justified, we\n",
      "want it to hold that λx. α(β) has the same semantic\n",
      "values as α[β/x]. This is indeed true, subject to a slight\n",
      "complication that we will come to shortly.  In order to carry of β-reduction of\n",
      "expressions in NLTK, we can call the simplify() method .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> expr = read_expr(r'\\x.(walk(x) & chew_gum(x))(gerald)')\n",
      ">>> print(expr)\n",
      "\\x.(walk(x) & chew_gum(x))(gerald)\n",
      ">>> print(expr.simplify()) \n",
      "(walk(gerald) & chew_gum(gerald))\n",
      "\n",
      "\n",
      "\n",
      "Although we have so far only considered cases where the body of the\n",
      "λ abstract is an open formula, i.e., of type t, this is not a\n",
      "necessary restriction; the body can be any well-formed\n",
      "expression. Here's an example with two λs.\n",
      "\n",
      "  (38)\\x.\\y.(dog(x) & own(y, x))\n",
      "Just as (33b) plays the role of a unary predicate,\n",
      "(38) works like a binary predicate: it can be applied directly to\n",
      "two arguments . Logical expressions may contain nested λs  such as \\x.\\y. to be\n",
      "written in the abbreviated form \\x y. .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(read_expr(r'\\x.\\y.(dog(x) & own(y, x))(cyril)').simplify())\n",
      "\\y.(dog(cyril) & own(y,cyril))\n",
      ">>> print(read_expr(r'\\x y.(dog(x) & own(y, x))(cyril, angus)').simplify()) \n",
      "(dog(cyril) & own(angus,cyril))\n",
      "\n",
      "\n",
      "\n",
      "All our λ abstracts so far have involved the familiar first order variables:\n",
      "x, y and so on — variables of type e. But suppose we want to treat one abstract, say\n",
      "\\x.walk(x) as the\n",
      "argument of another λ abstract? We might try this:\n",
      "\n",
      "\\y.y(angus)(\\x.walk(x))\n",
      "\n",
      "But since the variable y is stipulated to be of type e,\n",
      "\\y.y(angus) only applies to arguments of type  e  while \\x.walk(x) is\n",
      "of type 〈e, t〉! Instead, we need to allow\n",
      "abstraction over variables of higher type. Let's use P and Q as variables of\n",
      "type  〈e, t〉, and then we can have an abstract such as\n",
      "\\P.P(angus). Since P is of\n",
      "type  〈e, t〉, the whole abstract is of type 〈〈e, t〉, t〉. Then \\P.P(angus)(\\x.walk(x)) is legal, and can\n",
      "be simplified via β-reduction to \\x.walk(x)(angus) and then again to walk(angus)\n",
      "When carrying out β-reduction, some care has to be taken with\n",
      "variables. Consider, for example, the λ terms (39a) and\n",
      "(39b), which differ only in the identity of a free variable.\n",
      "\n",
      "  (39)\n",
      "  a.\\y.see(y, x)\n",
      "\n",
      "  b.\\y.see(y, z)\n",
      "\n",
      "Suppose now that we apply the λ-term \\P.exists x.P(x) to each of these terms:\n",
      "\n",
      "  (40)\n",
      "  a.\\P.exists x.P(x)(\\y.see(y, x))\n",
      "\n",
      "  b.\\P.exists x.P(x)(\\y.see(y, z))\n",
      "\n",
      "We pointed out earlier that the results of the application should be semantically\n",
      "equivalent.\n",
      "But if we let the free variable x in (39a) fall inside the scope of the\n",
      "existential quantifier in (40a), then after reduction, the results\n",
      "will be different:\n",
      "\n",
      "  (41)\n",
      "  a.exists x.see(x, x)\n",
      "\n",
      "  b.exists x.see(x, z)\n",
      "\n",
      "(41a) means there is some x that sees him/herself, whereas\n",
      "(41b) means that there is some x that sees an unspecified\n",
      "individual z. What has gone wrong here? Clearly, we want to forbid\n",
      "the kind of variable \"capture\" shown in (41a).\n",
      "In order to deal with this problem, let's step back a moment. Does it matter what\n",
      "particular name we use for the variable bound\n",
      "by the existential quantifier in the function expression of (40a)? The answer is No.\n",
      "In fact, given any variable-binding expression (involving ∀,\n",
      "∃ or λ), the name chosen for the bound\n",
      "variable is completely arbitrary. For example, exists x.P(x) and\n",
      "exists y.P(y) are equivalent; they are called α equivalents,\n",
      "or alphabetic variants. The process of relabeling bound\n",
      "variables is known as α-conversion. When we test for equality of\n",
      "VariableBinderExpressions in the logic module (i.e., using\n",
      "==), we are in fact testing for α-equivalence:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> expr1 = read_expr('exists x.P(x)')\n",
      ">>> print(expr1)\n",
      "exists x.P(x)\n",
      ">>> expr2 = expr1.alpha_convert(nltk.sem.Variable('z'))\n",
      ">>> print(expr2)\n",
      "exists z.P(z)\n",
      ">>> expr1 == expr2\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "When β-reduction is carried out on an application\n",
      "f(a), we check whether there are free variables in a\n",
      "which also occur as bound variables in any subterms of\n",
      "f. Suppose, as in the example discussed above, that\n",
      "x is free in a, and that f contains the\n",
      "subterm exists x.P(x). In this case, we\n",
      "produce an alphabetic variant of exists x.P(x),\n",
      "say, exists z1.P(z1), and then\n",
      "carry on with the reduction. This relabeling is carried out\n",
      "automatically by the β-reduction code in logic, and the\n",
      "results can be seen in the following example.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> expr3 = read_expr('\\P.(exists x.P(x))(\\y.see(y, x))')\n",
      ">>> print(expr3)\n",
      "(\\P.exists x.P(x))(\\y.see(y,x))\n",
      ">>> print(expr3.simplify())\n",
      "exists z1.see(z1,x)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "As you work through examples like these in the following sections, you may\n",
      "find that the logical expressions which are returned have different variable\n",
      "names; for example you might see z14 in place of z1 in the above\n",
      "formula. This change in labeling is innocuous — in fact, it is just an\n",
      "illustration of alphabetic variants.\n",
      "\n",
      "After this excursus, let's return to the task of building logical forms for English\n",
      "sentences.\n",
      "\n",
      "\n",
      "4.3   Quantified NPs\n",
      "At the start of this section, we briefly described how to build a semantic\n",
      "representation for Cyril barks. You would be forgiven for thinking this was all\n",
      "too easy — surely there is a bit more to building compositional semantics. What\n",
      "about quantifiers, for instance? Right, this is a crucial issue. For example, we want\n",
      "(42a) to be given the logical form in (42b). How can this be accomplished?\n",
      "\n",
      "  (42)\n",
      "  a.A dog barks.\n",
      "\n",
      "  b.exists x.(dog(x) & bark(x))\n",
      "\n",
      "Let's make the assumption that our only operation for building\n",
      "complex semantic representations is function\n",
      "application. Then our problem is this: how do we give a semantic\n",
      "representation to the quantified NPs a dog so that\n",
      "it can be combined with bark to give the result\n",
      "in (42b)? As a first step, let's make the subject's sem value\n",
      "act as the function expression rather than the argument. (This is sometimes called\n",
      "type-raising.) Now we are\n",
      "looking for way of instantiating ?np so that [SEM=<?np(\\x.bark(x))>]\n",
      "is equivalent\n",
      "to [SEM=<exists x.(dog(x) & bark(x))>].\n",
      "Doesn't this look a bit reminiscent of carrying out β-reduction\n",
      "in the λ-calculus? In other words, we want a λ term\n",
      "M to replace ?np so that applying M to\n",
      "'bark' yields (42b).  To do this, we replace the occurrence of\n",
      "'bark' in (42b) by a predicate variable 'P', and bind the variable with\n",
      "λ, as shown in (43).\n",
      "\n",
      "  (43)\\P.exists x.(dog(x) & P(x))\n",
      "We have used a different style of variable in\n",
      "(43) — that is 'P' rather than 'x' or 'y' — to signal\n",
      "that we are abstracting over a different kind of object — not an\n",
      "individual, but a function expression of type  〈e, t〉. So the type of\n",
      "(43) as a whole is  〈〈e, t〉, t〉. We\n",
      "will take this to be the type of NPs in general. To illustrate\n",
      "further, a universally quantified NP will look like (44).\n",
      "\n",
      "  (44)\\P.all x.(dog(x) -> P(x))\n",
      "We are pretty much done now, except that we also want to carry out a\n",
      "further abstraction plus application for the process of combining the\n",
      "semantics of the determiner a, namely (43), with the semantics of\n",
      "dog.\n",
      "\n",
      "  (45)\\Q P.exists x.(Q(x) & P(x))\n",
      "Applying (46) as a function expression to dog yields (43), and applying\n",
      "that to bark gives us\n",
      "\\P.exists x.(dog(x) & P(x))(\\x.bark(x)). Finally, carrying out\n",
      "β-reduction\n",
      "yields just what we wanted, namely (42b).\n",
      "\n",
      "\n",
      "4.4   Transitive Verbs\n",
      "Our next challenge is to deal with sentences containing transitive\n",
      "verbs, such as (46).\n",
      "\n",
      "  (46)Angus chases a dog.\n",
      "The output semantics that we want to build is exists x.(dog(x) & chase(angus, x)).\n",
      "Let's look at how we can use λ-abstraction to get this\n",
      "result. A significant constraint on possible solutions is to require\n",
      "that the semantic representation of a dog be independent of\n",
      "whether the NP acts as subject or object of the sentence. In\n",
      "other words, we want to get the formula above as our output while sticking to\n",
      "(43) as the NP semantics. A second constraint is that\n",
      "VPs should have a uniform type of interpretation regardless\n",
      "of whether they consist of just an intransitive verb or a transitive\n",
      "verb plus object. More specifically, we stipulate that VPs\n",
      "are always of type 〈e, t〉. Given these\n",
      "constraints, here's a semantic representation for chases a dog\n",
      "which does the trick.\n",
      "\n",
      "  (47)\\y.exists x.(dog(x) & chase(y, x))\n",
      "Think of (47) as the property of being a y such that\n",
      "for some dog x, y chases x; or more\n",
      "colloquially, being a y who chases a dog. Our task now\n",
      "resolves to designing a semantic representation for\n",
      "chases which can combine with (43) so as to allow\n",
      "(47) to be derived.\n",
      "Let's carry out the inverse of β-reduction on (47),\n",
      "giving rise to (48).\n",
      "\n",
      "  (48)\\P.exists x.(dog(x) & P(x))(\\z.chase(y, z))\n",
      "(48) may be slightly hard to read at first; you need to see that\n",
      "it involves applying the quantified NP representation from\n",
      "(43) to \\z.chase(y,z). (48) is\n",
      "equivalent via β-reduction to exists x.(dog(x) & chase(y, x)).\n",
      "Now let's replace the function expression in (48) by a variable X of the\n",
      "same type as an NP; that is, of type\n",
      "〈〈e, t〉, t〉.\n",
      "\n",
      "  (49)X(\\z.chase(y, z))\n",
      "The representation of a transitive verb will have to apply to\n",
      "an argument of the type of X to yield a function expression of the type of\n",
      "VPs, that is, of type 〈e, t〉. We can ensure\n",
      "this by abstracting over both the X variable in (49) and also\n",
      "the subject variable y. So the full solution is reached by\n",
      "giving chases the semantic representation shown in (50).\n",
      "\n",
      "  (50)\\X y.X(\\x.chase(y, x))\n",
      "If (50) is applied to (43), the result after β-reduction is\n",
      "equivalent to (47), which is what we wanted all along:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> read_expr = nltk.sem.Expression.fromstring\n",
      ">>> tvp = read_expr(r'\\X x.X(\\y.chase(x,y))')\n",
      ">>> np = read_expr(r'(\\P.exists x.(dog(x) & P(x)))')\n",
      ">>> vp = nltk.sem.ApplicationExpression(tvp, np)\n",
      ">>> print(vp)\n",
      "(\\X x.X(\\y.chase(x,y)))(\\P.exists x.(dog(x) & P(x)))\n",
      ">>> print(vp.simplify())\n",
      "\\x.exists z2.(dog(z2) & chase(x,z2))\n",
      "\n",
      "\n",
      "\n",
      "In order to build a semantic representation for a sentence, we also\n",
      "need to combine in the semantics of the subject NP. If the\n",
      "latter is a quantified expression like every girl, everything\n",
      "proceeds in the same way as we showed for a dog barks earlier\n",
      "on; the subject is translated as a function expression which is applied to the\n",
      "semantic representation of the VP.  However, we now seem to have\n",
      "created another problem for ourselves with proper names. So far, these\n",
      "have been treated semantically as individual constants, and these\n",
      "cannot be applied as functions to expressions like\n",
      "(47). Consequently, we need to come up with a different semantic\n",
      "representation for them. What we do\n",
      "in this case is re-interpret proper names so that they too are\n",
      "function expressions, like quantified NPs. Here is the required\n",
      "λ expression for Angus.\n",
      "\n",
      "  (51)\\P.P(angus)\n",
      "(51) denotes the characteristic function corresponding to the set of\n",
      "all properties which are true of Angus. Converting from an individual\n",
      "constant angus to \\P.P(angus) is another example of\n",
      "type-raising, briefly mentioned earlier, and allows us to replace a\n",
      "Boolean-valued application such as \\x.walk(x)(angus) with an\n",
      "equivalent function application \\P.P(angus)(\\x.walk(x)). By\n",
      "β-reduction, both expressions reduce to walk(angus).\n",
      "\n",
      "The grammar simple-sem.fcfg contains a small set of rules for parsing\n",
      "and translating simple examples of the kind that we have been looking\n",
      "at. Here's a slightly more complicated example.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk import load_parser\n",
      ">>> parser = load_parser('grammars/book_grammars/simple-sem.fcfg', trace=0)\n",
      ">>> sentence = 'Angus gives a bone to every dog'\n",
      ">>> tokens = sentence.split()\n",
      ">>> for tree in parser.parse(tokens):\n",
      "...     print(tree.label()['SEM'])\n",
      "all z2.(dog(z2) -> exists z1.(bone(z1) & give(angus,z1,z2)))\n",
      "\n",
      "\n",
      "\n",
      "NLTK provides some utilities to make it easier to derive and inspect\n",
      "semantic interpretations. The function interpret_sents() is\n",
      "intended for interpretation of a list of input sentences. It\n",
      "builds a dictionary d where for each sentence sent in the\n",
      "input, d[sent] is a list of pairs (synrep, semrep) consisting\n",
      "of trees and semantic representations for sent. The value\n",
      "is a list since sent may be syntactically ambiguous; in the\n",
      "following example, however, there is only one parse tree per sentence\n",
      "in the list.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> sents = ['Irene walks', 'Cyril bites an ankle']\n",
      ">>> grammar_file = 'grammars/book_grammars/simple-sem.fcfg'\n",
      ">>> for results in nltk.interpret_sents(sents, grammar_file):\n",
      "...     for (synrep, semrep) in results:\n",
      "...         print(synrep)\n",
      "(S[SEM=<walk(irene)>]\n",
      "  (NP[-LOC, NUM='sg', SEM=<\\P.P(irene)>]\n",
      "    (PropN[-LOC, NUM='sg', SEM=<\\P.P(irene)>] Irene))\n",
      "  (VP[NUM='sg', SEM=<\\x.walk(x)>]\n",
      "    (IV[NUM='sg', SEM=<\\x.walk(x)>, TNS='pres'] walks)))\n",
      "(S[SEM=<exists z3.(ankle(z3) & bite(cyril,z3))>]\n",
      "  (NP[-LOC, NUM='sg', SEM=<\\P.P(cyril)>]\n",
      "    (PropN[-LOC, NUM='sg', SEM=<\\P.P(cyril)>] Cyril))\n",
      "  (VP[NUM='sg', SEM=<\\x.exists z3.(ankle(z3) & bite(x,z3))>]\n",
      "    (TV[NUM='sg', SEM=<\\X x.X(\\y.bite(x,y))>, TNS='pres'] bites)\n",
      "    (NP[NUM='sg', SEM=<\\Q.exists x.(ankle(x) & Q(x))>]\n",
      "      (Det[NUM='sg', SEM=<\\P Q.exists x.(P(x) & Q(x))>] an)\n",
      "      (Nom[NUM='sg', SEM=<\\x.ankle(x)>]\n",
      "        (N[NUM='sg', SEM=<\\x.ankle(x)>] ankle)))))\n",
      "\n",
      "\n",
      "\n",
      "We have seen now how to convert English sentences into logical forms, and\n",
      "earlier we saw how logical forms could be checked as true or false in a\n",
      "model. Putting these two mappings together, we can check the truth\n",
      "value of English sentences in a given model. Let's take model m as\n",
      "defined above. The utility evaluate_sents() resembles\n",
      "interpret_sents() except that we need to pass a model and a\n",
      "variable assignment as parameters. The output is a triple  (synrep,\n",
      "semrep, value) where synrep,\n",
      "semrep are as before, and value is a truth value. For simplicity,\n",
      "the following example only processes a single sentence.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> v = \"\"\"\n",
      "... bertie => b\n",
      "... olive => o\n",
      "... cyril => c\n",
      "... boy => {b}\n",
      "... girl => {o}\n",
      "... dog => {c}\n",
      "... walk => {o, c}\n",
      "... see => {(b, o), (c, b), (o, c)}\n",
      "... \"\"\"\n",
      ">>> val = nltk.Valuation.fromstring(v)\n",
      ">>> g = nltk.Assignment(val.domain)\n",
      ">>> m = nltk.Model(val.domain, val)\n",
      ">>> sent = 'Cyril sees every boy'\n",
      ">>> grammar_file = 'grammars/book_grammars/simple-sem.fcfg'\n",
      ">>> results = nltk.evaluate_sents([sent], grammar_file, m, g)[0]\n",
      ">>> for (syntree, semrep, value) in results:\n",
      "...     print(semrep)\n",
      "...     print(value)\n",
      "all z4.(boy(z4) -> see(cyril,z4))\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4.5   Quantifier Ambiguity Revisited\n",
      "\n",
      "One important limitation of the methods described above is\n",
      "that they do not deal with scope ambiguity. Our translation method is\n",
      "syntax-driven, in the sense that the semantic representation is\n",
      "closely coupled with the syntactic analysis, and the scope of\n",
      "the quantifiers in the semantics therefore reflects the relative scope of\n",
      "the corresponding NP s in the syntactic parse tree.\n",
      "Consequently, a sentence like (26), repeated here, will always\n",
      "be translated as (53a), not (53b).\n",
      "\n",
      "  (52)Every girl chases a dog.\n",
      "\n",
      "  (53)\n",
      "  a.all x.(girl(x) -> exists y.(dog(y) & chase(x,y)))\n",
      "\n",
      "  b.exists y.(dog(y) & all x.(girl(x) -> chase(x,y)))\n",
      "\n",
      "There are numerous approaches to dealing with scope ambiguity, and we\n",
      "will look very briefly at one of the simplest. To start with, let's\n",
      "briefly consider the structure of scoped formulas. 4.1\n",
      "depicts the way in which the two readings of (52) differ.\n",
      "\n",
      "\n",
      "Figure 4.1: Quantifier Scopings\n",
      "\n",
      "Let's consider the left hand structure first. At the top, we have the\n",
      "quantifier corresponding to every girl. The φ can be thought\n",
      "of as a placeholder for whatever is inside the scope of the\n",
      "quantifier. Moving downwards, we see that we can plug in the\n",
      "quantifier corresponding to a dog as an instantiation of\n",
      "φ. This gives a new placeholder ψ, representing the scope of\n",
      "a dog, and into this we can plug the 'core' of the semantics,\n",
      "namely the open sentence corresponding to x chases\n",
      "y. The structure on the right hand side is identical, except we have\n",
      "swapped round the order of the two quantifiers.\n",
      "In the method known as Cooper storage, a semantic\n",
      "representation is no longer an expression of first-order logic, but instead a pair\n",
      "consisting of a \"core\" semantic representation plus a list of binding\n",
      "operators. For the moment, think of a binding operator as being\n",
      "identical to the semantic representation of a quantified NP such\n",
      "as (44) or (45). Following along the lines indicated in\n",
      "4.1, let's assume that we have constructed a\n",
      "Cooper-storage style semantic representation of sentence (52), and\n",
      "let's take our core to be the open formula chase(x,y). Given a\n",
      "list of binding operators corresponding to the two NPs in\n",
      "(52), we pick a binding operator off the list, and combine it with\n",
      "the core.\n",
      "\n",
      "\\P.exists y.(dog(y) & P(y))(\\z2.chase(z1,z2))\n",
      "\n",
      "Then we take the result, and apply the next binding operator from the\n",
      "list to it.\n",
      "\n",
      "\\P.all x.(girl(x) -> P(x))(\\z1.exists x.(dog(x) & chase(z1,x)))\n",
      "\n",
      "Once the list is empty, we have a conventional logical form for the\n",
      "sentence. Combining binding operators with the core in this way is\n",
      "called S-Retrieval. If we are careful to allow every possible\n",
      "order of binding operators (for example, by taking all permutations\n",
      "of the list, cf 4.5),\n",
      "then we will be able to generate every possible scope ordering of quantifiers.\n",
      "The next question to address is how we build up a core+store\n",
      "representation compositionally. As before, each phrasal and lexical\n",
      "rule in the grammar will have a sem feature, but now there will be\n",
      "embedded features core and store. To illustrate the machinery,\n",
      "let's consider a simpler example, namely Cyril smiles. Here's a\n",
      "lexical rule for the verb smiles (taken from the grammar\n",
      "storage.fcfg) which looks pretty innocuous.\n",
      "\n",
      "IV[SEM=[core=<\\x.smile(x)>, store=(/)]] -> 'smiles'\n",
      "\n",
      "The rule for the proper name Cyril is more complex.\n",
      "\n",
      "NP[SEM=[core=<@x>, store=(<bo(\\P.P(cyril),@x)>)]] -> 'Cyril'\n",
      "\n",
      "The bo predicate has two subparts: the standard (type-raised)\n",
      "representation of a proper name, and the expression @x, which is\n",
      "called the address of the binding operator. (We'll explain the\n",
      "need for the address variable shortly.) @x is a\n",
      "metavariable, that is, a variable that ranges over individual\n",
      "variables of the logic and, as you will see, it also provides the\n",
      "value of core.\n",
      "The rule for VP just\n",
      "percolates up the semantics of the  IV, and the\n",
      "interesting work is done by the S rule.\n",
      "\n",
      "VP[SEM=?s] -> IV[SEM=?s]\n",
      "\n",
      "S[SEM=[core=<?vp(?np)>, store=(?b1+?b2)]] ->\n",
      "   NP[SEM=[core=?np, store=?b1]] VP[SEM=[core=?vp, store=?b2]]\n",
      "\n",
      "The core value at the S node is the result of applying the\n",
      "VP's core value, namely \\x.smile(x), to the subject\n",
      "NP's value. The latter will not be @x, but rather an\n",
      "instantiation of @x, say z3. After β-reduction,\n",
      "<?vp(?np)> will be unified with <smile(z3)>. Now, when\n",
      "@x is instantiated as part of the parsing process, it will be\n",
      "instantiated uniformly. In particular, the occurrence of @x in the\n",
      "subject NP's store will also be mapped to z3, yielding\n",
      "the element bo(\\P.P(cyril),z3). These steps can be seen in\n",
      "the following parse tree.\n",
      "\n",
      "(S[SEM=[core=<smile(z3)>, store=(bo(\\P.P(cyril),z3))]]\n",
      "  (NP[SEM=[core=<z3>, store=(bo(\\P.P(cyril),z3))]] Cyril)\n",
      "  (VP[SEM=[core=<\\x.smile(x)>, store=()]]\n",
      "    (IV[SEM=[core=<\\x.smile(x)>, store=()]] smiles)))\n",
      "\n",
      "Let's return to our more complex example, (52), and see what the\n",
      "storage style sem value is, after parsing with grammar\n",
      "storage.fcfg.\n",
      "\n",
      "core  = <chase(z1,z2)>\n",
      "store = (bo(\\P.all x.(girl(x) -> P(x)),z1), bo(\\P.exists x.(dog(x) & P(x)),z2))\n",
      "\n",
      "\n",
      "It should be clearer now why the address variables are an important\n",
      "part of the binding operator. Recall that during S-retrieval, we will\n",
      "be taking binding operators off the store list and applying them\n",
      "successively to the core. Suppose we start with bo(\\P.all x.(girl(x)\n",
      "-> P(x)),z1), which we want to combine with chase(z1,z2). The\n",
      "quantifier part of binding operator is \\P.all x.(girl(x) -> P(x)),\n",
      "and to combine this with  chase(z1,z2), the latter needs to first be\n",
      "turned into a λ-abstract. How do we know which variable to\n",
      "abstract over? This is what the address z1 tells us; i.e. that\n",
      "every girl has the role of chaser rather than chasee.\n",
      "The module nltk.sem.cooper_storage deals with the task of turning\n",
      "storage-style semantic representations into standard logical\n",
      "forms. First, we construct a CooperStore instance, and inspect its\n",
      "store and core.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.sem import cooper_storage as cs\n",
      ">>> sentence = 'every girl chases a dog'\n",
      ">>> trees = cs.parse_with_bindops(sentence, grammar='grammars/book_grammars/storage.fcfg')\n",
      ">>> semrep = trees[0].label()['SEM']\n",
      ">>> cs_semrep = cs.CooperStore(semrep)\n",
      ">>> print(cs_semrep.core)\n",
      "chase(z2,z4)\n",
      ">>> for bo in cs_semrep.store:\n",
      "...     print(bo)\n",
      "bo(\\P.all x.(girl(x) -> P(x)),z2)\n",
      "bo(\\P.exists x.(dog(x) & P(x)),z4)\n",
      "\n",
      "\n",
      "\n",
      "Finally we call s_retrieve() and check the readings.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> cs_semrep.s_retrieve(trace=True)\n",
      "Permutation 1\n",
      "   (\\P.all x.(girl(x) -> P(x)))(\\z2.chase(z2,z4))\n",
      "   (\\P.exists x.(dog(x) & P(x)))(\\z4.all x.(girl(x) -> chase(x,z4)))\n",
      "Permutation 2\n",
      "   (\\P.exists x.(dog(x) & P(x)))(\\z4.chase(z2,z4))\n",
      "   (\\P.all x.(girl(x) -> P(x)))(\\z2.exists x.(dog(x) & chase(z2,x)))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> for reading in cs_semrep.readings:\n",
      "...     print(reading)\n",
      "exists x.(dog(x) & all z3.(girl(z3) -> chase(z3,x)))\n",
      "all x.(girl(x) -> exists z4.(dog(z4) & chase(x,z4)))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5   Discourse Semantics\n",
      "A discourse is a sequence of sentences. Very often, the\n",
      "interpretation of a sentence in a discourse depends what preceded\n",
      "it. A clear example of this comes from anaphoric pronouns, such as\n",
      "he, she and it. Given discourse such as Angus used\n",
      "to have a dog. But he recently disappeared., you will probably\n",
      "interpret he as referring to Angus's dog. However, in Angus\n",
      "used to have a dog. He took him for walks in New Town., you are\n",
      "more likely to interpret he as referring to Angus himself.\n",
      "\n",
      "5.1   Discourse Representation Theory\n",
      "\n",
      "The standard approach to quantification in first-order logic is limited to single\n",
      "sentences. Yet there seem to be examples where the scope of a\n",
      "quantifier can extend over two or more sentences. We saw one above,\n",
      "and here's a second example, together with a translation.\n",
      "\n",
      "  (54)\n",
      "  a.Angus owns a dog. It bit Irene.\n",
      "\n",
      "  b.∃x.(dog(x) ∧ own(Angus, x) ∧ bite(x, Irene))\n",
      "\n",
      "\n",
      "That is, the NP a dog acts like a quantifier which binds\n",
      "the it in the second sentence.  Discourse Representation Theory\n",
      "(DRT) was developed with the specific goal of\n",
      "providing a means for handling this and other semantic phenomena which\n",
      "seem to be characteristic of discourse.\n",
      "A discourse representation structure (DRS) presents the meaning\n",
      "of discourse in terms of a list of discourse referents and a list\n",
      "of conditions.  The discourse referents are the things\n",
      "under discussion in the discourse, and they correspond to the\n",
      "individual variables of first-order logic.  The DRS conditions apply\n",
      "to those discourse referents, and correspond to atomic open\n",
      "formulas of first-order logic.\n",
      "5.1 illustrates how DRS for the first sentence\n",
      "in (54a) is augmented to become a DRS for both sentences.\n",
      "\n",
      "\n",
      "Figure 5.1: Building a DRS; the DRS on the left hand side represents the result of processing\n",
      "the first sentence in the discourse, while the DRS on the right hand side shows the effect of\n",
      "processing the second sentence and integrating its content.\n",
      "\n",
      "When the second sentence of (54a) is processed, it is interpreted in\n",
      "the context of what is already present in the left hand side of 5.1. The pronoun it\n",
      "triggers the addition of a new discourse referent, say u,\n",
      "and we need to find an anaphoric antecedent for it — that\n",
      "is, we want to work out what it refers to. In DRT, the task\n",
      "of finding the antecedent for an anaphoric pronoun involves\n",
      "linking it to a discourse referent already within the current DRS,\n",
      "and y is the obvious choice. (We will say more about\n",
      "anaphora resolution shortly.) This processing step gives rise\n",
      "to a new condition u = y.\n",
      "The remaining content contributed by\n",
      "the second sentence is also merged with the content of the first, and\n",
      "this is shown on the right hand side of 5.1.\n",
      "5.1 illustrates how a DRS can represent more than just\n",
      "a single sentence. In this case, it is a two-sentence discourse, but\n",
      "in principle a single DRS could correspond to the interpretation of\n",
      "a whole text. We can inquire into the truth conditions of the\n",
      "right hand DRS in 5.1. Informally, it is true in some\n",
      "situation s if there are entities a, c and i in\n",
      "s corresponding to the discourse referents in the DRS such\n",
      "that all the conditions are true in s ; that is, a is named\n",
      "Angus, c is a dog, a owns c, i is named\n",
      "Irene and c bit i.\n",
      "In order to process DRSs computationally, we need to convert them\n",
      "into a linear format. Here's an example, where the DRS is a pair consisting of a\n",
      "list of discourse of referents and a list of DRS conditions:\n",
      "\n",
      "([x, y], [angus(x), dog(y), own(x,y)])\n",
      "\n",
      "The easiest way to build a DRS object in NLTK is by parsing a\n",
      "string representation .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> read_dexpr = nltk.sem.DrtExpression.fromstring\n",
      ">>> drs1 = read_dexpr('([x, y], [angus(x), dog(y), own(x, y)])') \n",
      ">>> print(drs1)\n",
      "([x,y],[angus(x), dog(y), own(x,y)])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We can use the draw() method \n",
      "to visualize the result, as shown in 5.2.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> drs1.draw() \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Figure 5.2: DRS Screenshot\n",
      "\n",
      "When we discussed the truth conditions of the DRSs in\n",
      "5.1, we assumed that the topmost discourse referents were\n",
      "interpreted as existential quantifiers, while the conditions were\n",
      "interpreted as though they are conjoined. In fact, every DRS can be\n",
      "translated into a formula of first-order logic, and the fol() method\n",
      "implements this translation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(drs1.fol())\n",
      "exists x y.(angus(x) & dog(y) & own(x,y))\n",
      "\n",
      "\n",
      "\n",
      "In addition to the functionality available for first-order logic\n",
      "expressions, DRT Expressions have a DRS-concatenation operator,\n",
      "represented as the + symbol.  The concatenation of two DRSs\n",
      "is a single DRS containing the merged discourse referents and the\n",
      "conditions from both arguments.  DRS-concatenation automatically\n",
      "α-converts bound variables to avoid name-clashes.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> drs2 = read_dexpr('([x], [walk(x)]) + ([y], [run(y)])')\n",
      ">>> print(drs2)\n",
      "(([x],[walk(x)]) + ([y],[run(y)]))\n",
      ">>> print(drs2.simplify())\n",
      "([x,y],[walk(x), run(y)])\n",
      "\n",
      "\n",
      "\n",
      "While all the conditions seen so far have been atomic, it is possible\n",
      "to embed one DRS within another, and this is how universal\n",
      "quantification is handled. In drs3, there are no top-level\n",
      "discourse referents, and the sole condition is made up of two\n",
      "sub-DRSs, connected by an implication. Again, we can use\n",
      "fol() to get a handle on the truth conditions.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> drs3 = read_dexpr('([], [(([x], [dog(x)]) -> ([y],[ankle(y), bite(x, y)]))])')\n",
      ">>> print(drs3.fol())\n",
      "all x.(dog(x) -> exists y.(ankle(y) & bite(x,y)))\n",
      "\n",
      "\n",
      "\n",
      "We pointed out earlier that DRT is designed to allow anaphoric pronouns to be\n",
      "interpreted by linking to existing discourse referents. DRT sets\n",
      "constraints on which discourse referents are \"accessible\" as possible\n",
      "antecedents, but is not intended to explain how a particular antecedent\n",
      "is chosen from the set of candidates.\n",
      "The  module nltk.sem.drt_resolve_anaphora adopts a similarly\n",
      "conservative strategy: if the DRS contains a condition of the form\n",
      "PRO(x), the method resolve_anaphora() replaces this with a\n",
      "condition of the form x = [...], where [...] is a list of\n",
      "possible antecedents.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> drs4 = read_dexpr('([x, y], [angus(x), dog(y), own(x, y)])')\n",
      ">>> drs5 = read_dexpr('([u, z], [PRO(u), irene(z), bite(u, z)])')\n",
      ">>> drs6 = drs4 + drs5\n",
      ">>> print(drs6.simplify())\n",
      "([u,x,y,z],[angus(x), dog(y), own(x,y), PRO(u), irene(z), bite(u,z)])\n",
      ">>> print(drs6.simplify().resolve_anaphora())\n",
      "([u,x,y,z],[angus(x), dog(y), own(x,y), (u = [x,y,z]), irene(z), bite(u,z)])\n",
      "\n",
      "\n",
      "\n",
      "Since the algorithm for anaphora resolution has been separated into\n",
      "its own module, this facilitates swapping in alternative procedures\n",
      "which try to make more intelligent guesses about the correct\n",
      "antecedent.\n",
      "Our treatment of DRSs  is fully\n",
      "compatible with the existing machinery for handling λ\n",
      "abstraction, and consequently it is straightforward to build compositional semantic\n",
      "representations which are based on DRT rather than first-order logic. This\n",
      "technique is illustrated in the following rule for indefinites\n",
      "(which is part of the grammar drt.fcfg). For ease of comparison,\n",
      "we have added the parallel rule for indefinites from\n",
      "simple-sem.fcfg.\n",
      "\n",
      "Det[num=sg,SEM=<\\P Q.(([x],[]) + P(x) + Q(x))>] -> 'a'\n",
      "Det[num=sg,SEM=<\\P Q. exists x.(P(x) & Q(x))>] -> 'a'\n",
      "\n",
      "To get a better idea of how the DRT rule works, look at this subtree\n",
      "for the NP a dog.\n",
      "\n",
      "(NP[num='sg', SEM=<\\Q.(([x],[dog(x)]) + Q(x))>]\n",
      "  (Det[num='sg', SEM=<\\P Q.((([x],[]) + P(x)) + Q(x))>] a)\n",
      "  (Nom[num='sg', SEM=<\\x.([],[dog(x)])>]\n",
      "    (N[num='sg', SEM=<\\x.([],[dog(x)])>] dog)))))\n",
      "\n",
      "The λ abstract for the indefinite is applied as a function expression to\n",
      "\\x.([],[dog(x)]) which leads to \\Q.(([x],[]) + ([],[dog(x)]) +\n",
      "Q(x)); after simplification, we get \\Q.(([x],[dog(x)]) + Q(x))\n",
      "as the representation for the NP as a whole.\n",
      "In order to parse with grammar drt.fcfg, we specify in the call to\n",
      "load_parser() that SEM values in feature structures are to be\n",
      "parsed using DrtParser.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk import load_parser\n",
      ">>> parser = load_parser('grammars/book_grammars/drt.fcfg', logic_parser=nltk.sem.drt.DrtParser())\n",
      ">>> trees = list(parser.parse('Angus owns a dog'.split()))\n",
      ">>> print(trees[0].label()['SEM'].simplify())\n",
      "([x,z2],[Angus(x), dog(z2), own(x,z2)])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5.2   Discourse Processing\n",
      "When we interpret a sentence, we use a rich context for\n",
      "interpretation, determined in part by the preceding context and in\n",
      "part by our background assumptions. DRT provides a theory of how the\n",
      "meaning of a sentence is integrated into a representation of the prior\n",
      "discourse, but two things have been glaringly absent from the\n",
      "processing approach just discussed. First, there has been no attempt\n",
      "to incorporate any kind of inference; and second, we have only\n",
      "processed individual sentences. These omissions are redressed by the\n",
      "module nltk.inference.discourse.\n",
      "\n",
      "Whereas a discourse is a\n",
      "sequence s1, ... sn of\n",
      "sentences, a discourse thread is a sequence\n",
      "s1-ri, ... sn-rj\n",
      "of readings, one for each sentence in the discourse.  The\n",
      "module processes sentences incrementally, keeping track of all possible\n",
      "threads when there is ambiguity. For simplicity, the following example\n",
      "ignores scope ambiguity.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> dt = nltk.DiscourseTester(['A student dances', 'Every student is a person'])\n",
      ">>> dt.readings()\n",
      "\n",
      "s0 readings:\n",
      "\n",
      "s0-r0: exists x.(student(x) & dance(x))\n",
      "\n",
      "s1 readings:\n",
      "\n",
      "s1-r0: all x.(student(x) -> person(x))\n",
      "\n",
      "\n",
      "\n",
      "When a new sentence is added to the current discourse, setting the\n",
      "parameter consistchk=True causes consistency to be checked\n",
      "by invoking the model checker for each thread, i.e., sequence of\n",
      "admissible readings. In this case, the user has the option\n",
      "of retracting the sentence in question.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> dt.add_sentence('No person dances', consistchk=True)\n",
      "Inconsistent discourse: d0 ['s0-r0', 's1-r0', 's2-r0']:\n",
      "    s0-r0: exists x.(student(x) & dance(x))\n",
      "    s1-r0: all x.(student(x) -> person(x))\n",
      "    s2-r0: -exists x.(person(x) & dance(x))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> dt.retract_sentence('No person dances', verbose=True)\n",
      "Current sentences are\n",
      "s0: A student dances\n",
      "s1: Every student is a person\n",
      "\n",
      "\n",
      "\n",
      "In a similar manner, we use informchk=True to check whether a new sentence φ\n",
      "is informative relative to the current discourse. The theorem prover treats existing\n",
      "sentences in the thread as assumptions and attempts to prove φ; it is informative\n",
      "if no such proof can be found.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> dt.add_sentence('A person dances', informchk=True)\n",
      "Sentence 'A person dances' under reading 'exists x.(person(x) & dance(x))':\n",
      "Not informative relative to thread 'd0'\n",
      "\n",
      "\n",
      "\n",
      "It is also possible to pass in an additional set of assumptions as\n",
      "background knowledge and use these to filter out inconsistent\n",
      "readings; see the Discourse HOWTO at http://nltk.org/howto for more details.\n",
      "The discourse module can accommodate semantic\n",
      "ambiguity and filter out readings that are not admissible.\n",
      "The following example invokes both Glue Semantics as well as DRT. Since the Glue\n",
      "Semantics module is configured to use the wide-coverage Malt dependency parser, the\n",
      "input (Every dog chases a boy.  He runs.) needs to be tagged as well as tokenized.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.tag import RegexpTagger\n",
      ">>> tagger = RegexpTagger(\n",
      "...     [('^(chases|runs)$', 'VB'),\n",
      "...      ('^(a)$', 'ex_quant'),\n",
      "...      ('^(every)$', 'univ_quant'),\n",
      "...      ('^(dog|boy)$', 'NN'),\n",
      "...      ('^(He)$', 'PRP')\n",
      "... ])\n",
      ">>> rc = nltk.DrtGlueReadingCommand(depparser=nltk.MaltParser(tagger=tagger))\n",
      ">>> dt = nltk.DiscourseTester(['Every dog chases a boy', 'He runs'], rc)\n",
      ">>> dt.readings()\n",
      "\n",
      "s0 readings:\n",
      "\n",
      "s0-r0: ([],[(([x],[dog(x)]) -> ([z3],[boy(z3), chases(x,z3)]))])\n",
      "s0-r1: ([z4],[boy(z4), (([x],[dog(x)]) -> ([],[chases(x,z4)]))])\n",
      "\n",
      "s1 readings:\n",
      "\n",
      "s1-r0: ([x],[PRO(x), runs(x)])\n",
      "\n",
      "\n",
      "\n",
      "The first sentence of the discourse has two possible readings, depending on the\n",
      "quantfier scoping. The unique reading of the second sentence represents the pronoun\n",
      "He via the condition PRO(x)`. Now let's look at the discourse threads that\n",
      "result:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> dt.readings(show_thread_readings=True)\n",
      "d0: ['s0-r0', 's1-r0'] : INVALID: AnaphoraResolutionException\n",
      "d1: ['s0-r1', 's1-r0'] : ([z6,z10],[boy(z6), (([x],[dog(x)]) ->\n",
      "([],[chases(x,z6)])), (z10 = z6), runs(z10)])\n",
      "\n",
      "\n",
      "\n",
      "When we examine threads d0 and d1, we see\n",
      "that reading s0-r0, where every dog out-scopes\n",
      "a boy, is deemed inadmissible because the pronoun in the\n",
      "second sentence cannot be resolved.  By contrast, in thread d1 the\n",
      "pronoun (relettered to z24) has been bound via the\n",
      "equation (z24 = z20).\n",
      "Inadmissible readings can be filtered out by passing the parameter\n",
      "filter=True.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> dt.readings(show_thread_readings=True, filter=True)\n",
      "d1: ['s0-r1', 's1-r0'] : ([z12,z15],[boy(z12), (([x],[dog(x)]) ->\n",
      "([],[chases(x,z12)])), (z17 = z12), runs(z15)])\n",
      "\n",
      "\n",
      "\n",
      "Although this little discourse is extremely limited, it should give you a feel for\n",
      "the kind of semantic processing issues that arise when we go beyond single sentences, and also a\n",
      "feel for the techniques that can be deployed to address them.\n",
      "\n",
      "\n",
      "\n",
      "6   Summary\n",
      "\n",
      "First order logic is a suitable language for representing\n",
      "natural language meaning in a computational setting since it is\n",
      "flexible enough to represent many useful aspects of natural meaning, and\n",
      "there are efficient theorem provers for reasoning with first order\n",
      "logic. (Equally, there are a variety of phenomena in natural language semantics\n",
      "which are believed to require more powerful logical mechanisms.)\n",
      "As well as translating natural language sentences into first order\n",
      "logic, we can state the truth conditions of these sentences by\n",
      "examining models of first order formulas.\n",
      "In order to build meaning representations compositionally, we\n",
      "supplement first order logic with the λ calculus.\n",
      "β-reduction in the λ-calculus corresponds semantically\n",
      "to application of a function to an argument. Syntactically, it\n",
      "involves replacing a variable bound by λ in the function expression with\n",
      "the expression that provides the argument in the function\n",
      "application.\n",
      "A key part of constructing a model lies in building a valuation\n",
      "which assigns interpretations to non-logical constants. These are\n",
      "interpreted as either n-ary predicates or as\n",
      "individual constants.\n",
      "An open expression is an expression containing one or more free\n",
      "variables. Open expressions only receive an interpretation when\n",
      "their free variables receive values from a\n",
      "variable assignment.\n",
      "Quantifiers are interpreted by constructing, for a formula φ[x] open in variable x, the set of individuals\n",
      "which make φ[x] true when an assignment g\n",
      "assigns them as the value of x. The quantifier then places\n",
      "constraints on that set.\n",
      "A closed expression is one that has no free variables; that is, the\n",
      "variables are all bound. A closed sentence is true or\n",
      "false with respect to all variable assignments.\n",
      "If two formulas differ only in the label of the variable bound by\n",
      "binding operator (i.e, λ or a quantifier) , they are said to\n",
      "be α equivalents. The result of relabeling a bound variable in\n",
      "a formula is\n",
      "called α-conversion.\n",
      "Given a formula with two nested quantifiers Q1 and Q2, the outermost quantifier Q1 is said to have wide\n",
      "scope (or scope over Q2). English sentences are frequently\n",
      "ambiguous with respect to the scope of the quantifiers they\n",
      "contain.\n",
      "English sentences can be associated with a semantic representation\n",
      "by treating sem as a feature in a feature-based grammar. The\n",
      "sem value of a complex expressions typically involves functional\n",
      "application of the sem values of the component expressions.\n",
      "\n",
      "\n",
      "\n",
      "7   Further Reading\n",
      "Consult http://nltk.org/ for further materials on this chapter and on how to install the\n",
      "Prover9 theorem prover and Mace4 model builder. General information about these two\n",
      "inference tools is given by (McCune, 2008).\n",
      "For more examples of semantic analysis with NLTK, please see the semantics and\n",
      "logic HOWTOs at http://nltk.org/howto.  Note that there are implementations of two other\n",
      "approaches to scope ambiguity, namely Hole semantics as described in\n",
      "(Blackburn & Bos, 2005) and Glue semantics as described in (Dalrymple, 1999).\n",
      "There are many phenomena in natural language semantics which have not been touched on\n",
      "in this chapter, most notably:\n",
      "\n",
      "events, tense and aspect;\n",
      "semantic roles;\n",
      "generalized quantifiers such as most;\n",
      "intensional constructions involving, for example, verbs like may and\n",
      "believe.\n",
      "\n",
      "While (1) and (2) can be dealt with using first-order logic, (3) and (4) require different logics.\n",
      "These issues are covered by many of the references in the readings below.\n",
      "A comprehensive overview of results and techniques in building natural language\n",
      "front-ends to databases can be found in (Androutsopoulos, Ritchie, & Thanisch, 1995).\n",
      "Any introductory book to modern logic will present propositional and first order\n",
      "logic. (Hodges, 1977) is highly recommended as an entertaining and insightful text\n",
      "with many insightful illustrations from natural language.\n",
      "For a wide-ranging, two-volume textbook on logic that also presents contemporary\n",
      "material on the formal semantics of natural language, including Montague Grammar and\n",
      "intensional logic, see (Gamut, 1991) and (Gamut, 1991). (Kamp & Reyle, 1993) provides\n",
      "the definitive account of Discourse Representation Theory, and covers a large and\n",
      "interesting fragment of natural language, including tense, aspect and\n",
      "modality. Another comprehensive study of the semantics of many natural language\n",
      "constructions is (Carpenter, 1997).\n",
      "There are numerous works that introduce logical semantics within the framework of\n",
      "linguistic theory. (Chierchia & McConnell-Ginet, 1990) is relatively agnostic about syntax, while\n",
      "(Heim & Kratzer, 1998) and (Larson & Segal, 1995) are both more explicitly oriented towards\n",
      "integrating truth-conditional semantics into a Chomskyan framework.\n",
      "(Blackburn & Bos, 2005) is the first textbook devoted to computational\n",
      "semantics, and provides an excellent introduction to the area. It\n",
      "expands on many of the topics covered in this chapter, including\n",
      "underspecification of quantifier scope ambiguity, first order\n",
      "inference, and discourse processing.\n",
      "To gain an overview of more advanced contemporary approaches to semantics, including\n",
      "treatments of tense and generalized quantifiers, try consulting (Lappin, 1996) or\n",
      "(Benthem & Meulen, 1997).\n",
      "\n",
      "\n",
      "8   Exercises\n",
      "\n",
      "☼ Translate the following sentences into propositional logic\n",
      "and verify that they can be processed with Expression.fromstring().\n",
      "Provide a key which shows how the propositional variables in your\n",
      "translation correspond to expressions of English.\n",
      "\n",
      "If Angus sings, it is not the case that Bertie sulks.\n",
      "Cyril runs and barks.\n",
      "It will snow if it doesn't rain.\n",
      "It's not the case that Irene will be happy if Olive or Tofu comes.\n",
      "Pat didn't cough or sneeze.\n",
      "If you don't come if I call, I won't come if you call.\n",
      "\n",
      "\n",
      "☼ Translate the following sentences into\n",
      "predicate-argument formula of first order logic.\n",
      "\n",
      "Angus likes Cyril and Irene hates Cyril.\n",
      "Tofu is taller than Bertie.\n",
      "Bruce loves himself and Pat does too.\n",
      "Cyril saw Bertie, but Angus didn't.\n",
      "Cyril is a fourlegged friend.\n",
      "Tofu and Olive are near each other.\n",
      "\n",
      "\n",
      "☼ Translate the following sentences into\n",
      "quantified formulas of first order logic.\n",
      "\n",
      "Angus likes someone and someone likes Julia.\n",
      "Angus loves a dog who loves him.\n",
      "Nobody smiles at Pat.\n",
      "Somebody coughs and sneezes.\n",
      "Nobody coughed or sneezed.\n",
      "Bruce loves somebody other than Bruce.\n",
      "Nobody other than Matthew loves somebody Pat.\n",
      "Cyril likes everyone except for Irene.\n",
      "Exactly one person is asleep.\n",
      "\n",
      "\n",
      "☼ Translate the following verb phrases using λ abstracts.\n",
      "quantified formulas of first order logic.\n",
      "\n",
      "feed Cyril and give a capuccino to Angus\n",
      "be given 'War and Peace' by Pat\n",
      "be loved by everyone\n",
      "be loved or detested by everyone\n",
      "be loved by everyone and detested by no-one\n",
      "\n",
      "\n",
      "☼ Consider the following statements:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> read_expr = nltk.sem.Expression.fromstring\n",
      ">>> e2 = read_expr('pat')\n",
      ">>> e3 = nltk.sem.ApplicationExpression(e1, e2)\n",
      ">>> print(e3.simplify())\n",
      "exists y.love(pat, y)\n",
      "\n",
      "\n",
      "\n",
      "Clearly something is missing here, namely a declaration of the\n",
      "value of e1. In order for ApplicationExpression(e1, e2)\n",
      "to be β-convertible to exists y.love(pat, y), e1\n",
      "must be a λ-abstract which can take pat as an\n",
      "argument. Your task is to construct such an abstract, bind it to\n",
      "e1, and satisfy yourself that the statements above are all\n",
      "satisfied (up to alphabetic variance). In addition, provide an\n",
      "informal English translation of e3.simplify().\n",
      "Now carry on doing this same task for the further cases of\n",
      "e3.simplify() shown below.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(e3.simplify())\n",
      "exists y.(love(pat,y) | love(y,pat))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(e3.simplify())\n",
      "exists y.(love(pat,y) | love(y,pat))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> print(e3.simplify())\n",
      "walk(fido)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "☼ As in the preceding exercise, find a λ abstract e1 that yields\n",
      "results equivalent to those shown below.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> e2 = read_expr('chase')\n",
      ">>> e3 = nltk.sem.ApplicationExpression(e1, e2)\n",
      ">>> print(e3.simplify())\n",
      "\\x.all y.(dog(y) -> chase(x,pat))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> e2 = read_expr('chase')\n",
      ">>> e3 = nltk.sem.ApplicationExpression(e1, e2)\n",
      ">>> print(e3.simplify())\n",
      "\\x.exists y.(dog(y) & chase(pat,x))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> e2 = read_expr('give')\n",
      ">>> e3 = nltk.sem.ApplicationExpression(e1, e2)\n",
      ">>> print(e3.simplify())\n",
      "\\x0 x1.exists y.(present(y) & give(x1,y,x0))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "☼ As in the preceding exercise, find a λ abstract e1 that yields\n",
      "results equivalent to those shown below.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> e2 = read_expr('bark')\n",
      ">>> e3 = nltk.sem.ApplicationExpression(e1, e2)\n",
      ">>> print(e3.simplify())\n",
      "exists y.(dog(x) & bark(x))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> e2 = read_expr('bark')\n",
      ">>> e3 = nltk.sem.ApplicationExpression(e1, e2)\n",
      ">>> print(e3.simplify())\n",
      "bark(fido)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> e2 = read_expr('\\\\P. all x. (dog(x) -> P(x))')\n",
      ">>> e3 = nltk.sem.ApplicationExpression(e1, e2)\n",
      ">>> print(e3.simplify())\n",
      "all x.(dog(x) -> bark(x))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "◑ Develop a method for translating English sentences into\n",
      "formulas with binary generalized quantifiers. In such an\n",
      "approach, given a generalized quantifier Q, a quantified\n",
      "formula is of the form Q(A, B), where both A and B are\n",
      "expressions of type 〈e, t〉. Then, for example,\n",
      "all(A, B) is true iff A denotes a subset of what B denotes.\n",
      "\n",
      "◑ Extend the approach in the preceding exercise so that the\n",
      "truth conditions for quantifiers like most and exactly three can be\n",
      "computed in a model.\n",
      "\n",
      "◑ Modify the sem.evaluate code so that it will\n",
      "give a helpful error message if an expression is not in the domain\n",
      "of a model's valuation function.\n",
      "\n",
      "★ Select three or four contiguous sentences from a book for children. A possible\n",
      "source of examples are the collections of stories in\n",
      "nltk.corpus.gutenberg: bryant-stories.txt,\n",
      "burgess-busterbrown.txt and\n",
      "edgeworth-parents.txt. Develop a grammar which will allow your\n",
      "sentences to be translated into first order logic, and build a\n",
      "model which will allow those translations to be checked for truth\n",
      "or falsity.\n",
      "\n",
      "★ Carry out the preceding exercise, but use DRT as the meaning representation.\n",
      "\n",
      "★ Taking (Warren & Pereira, 1982) as a starting point, develop a technique\n",
      "for converting a natural language query into a form that can be\n",
      "evaluated more efficiently in a model. For example, given a query\n",
      "of the form (P(x) & Q(x)), convert it to (Q(x) & P(x)) if\n",
      "the extension of Q is smaller than the extension of\n",
      "P.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "About this document...\n",
      "UPDATED FOR NLTK 3.0.\n",
      "This is a chapter from Natural Language Processing with Python,\n",
      "by Steven Bird, Ewan Klein and Edward Loper,\n",
      "Copyright © 2019 the authors.\n",
      "It is distributed with the Natural Language Toolkit [http://nltk.org/],\n",
      "Version 3.0, under the terms of the\n",
      "Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\n",
      "[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\n",
      "This document was built on\n",
      "Wed  4 Sep 2019 11:40:48 ACST\n",
      "vectors[10] = [0, 3, 28, 42, 8, 35, 8, 11, 0, 0, 3, 28, 22, 29, 0, 0, 9, 0, 0, 1, 4, 0, 2, 0, 8, 0, 4, 24, 12, 16, 0, 1, 3, 2, 1, 8, 16, 1, 28, 0, 12, 8, 0, 12, 0, 1, 16, 1, 4, 7, 5, 3, 0, 3, 0, 7, 24, 9, 1, 0, 7, 0, 0, 0, 52, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 16, 4, 45, 2, 2, 0, 1, 0, 5, 0, 0, 0, 0, 0, 1, 179, 2, 0, 1, 30, 15, 2, 0, 3, 0, 0, 7, 1, 1, 1, 3, 35, 1, 3, 2, 5, 0, 1, 1, 0, 2, 0, 0, 6, 0, 2, 0, 0, 5, 4, 9, 16, 1, 0, 13, 1, 983, 76, 59, 0, 0, 0, 11, 0, 0, 0, 6, 6, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 10, 3, 22, 0, 5, 0, 0, 0, 0, 5, 0, 0, 0, 10, 22, 1, 1, 18, 2, 0, 0, 4, 6, 0, 1, 0, 0, 0, 1, 0, 0, 0, 8, 5, 14, 0, 2, 0, 19, 2, 8, 3, 0, 2, 0, 0, 104, 0, 11, 1, 0, 2, 1, 237, 0, 0, 0, 5, 0, 2, 0, 19, 9, 54, 27, 0, 0, 0, 0, 0, 3, 13, 9, 0, 3, 11, 0, 0, 0, 0, 0, 10, 0, 2, 0, 2, 7, 0, 0, 1, 2, 2, 8, 41, 1, 0, 8, 0, 20, 0, 0, 1, 71, 2, 4, 1, 0, 0, 0, 5, 2, 14, 2, 24, 0, 0, 0, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 1, 2, 0, 13, 2, 7, 1, 0, 0, 0, 1, 1, 0, 0, 0, 2, 39, 1, 3, 1, 0, 0, 1, 2, 4, 4, 0, 0, 0, 0, 0, 2, 1, 0, 11, 8, 6, 19, 0, 8, 1, 3, 0, 1, 2, 45, 0, 8, 0, 0, 0, 2, 2, 70, 1, 0, 1, 0, 0, 11, 0, 4, 1, 11, 0, 7, 0, 26, 0, 0, 68, 0, 0, 1, 11, 1, 3, 0, 30, 0, 2, 17, 414, 0, 2, 0, 0, 5, 0, 1, 1, 0, 0, 2, 0, 0, 0, 20, 6, 7, 3, 14, 10, 105, 7, 3, 54, 41, 7, 5, 0, 1, 1, 0, 3, 2, 0, 6, 0, 0, 0, 0, 0, 0, 31, 7, 0, 1, 0, 1, 0, 5, 0, 0, 78, 0, 0, 2, 0, 0, 78, 0, 6, 1, 0, 3, 0, 4, 26, 0, 7, 2, 1, 0, 0, 0, 0, 0, 3, 5, 0, 0, 0, 3, 0, 0, 1, 0, 6, 1, 0, 6, 1, 0, 0, 5, 3, 2, 2, 1, 0, 23, 0, 0, 1, 12, 0, 3, 51, 0, 0, 217, 0, 0, 45, 3, 0, 0, 68, 0, 5, 0, 0, 0, 0, 2, 4, 23, 0, 0, 10, 12, 1, 74, 10, 2, 4, 1, 5, 1, 8, 5, 49, 0, 4, 0, 60, 3, 1, 33, 2, 0, 6, 18, 0, 6, 9, 2, 0, 0, 2, 0, 1, 9, 2, 0, 0, 0, 0, 7, 39, 0, 4, 2, 6, 1, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 4, 4, 1, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 13, 5, 3, 9, 1, 14, 0, 6, 3, 0, 10, 4, 7, 0, 0, 31, 10, 36, 1, 6, 3, 0, 1, 1, 1, 3, 9, 0, 5, 0, 0, 0, 0, 0, 0, 4, 2, 0, 0, 0, 0, 0, 12, 0, 3, 0, 0, 5, 19, 0, 67, 0, 13, 0, 0, 0, 0, 0, 0, 0, 17, 0, 3, 8, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 4, 0, 0, 3, 6, 11, 0, 0, 0, 1, 4, 1, 4, 0, 0, 1, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 6, 1, 0, 0, 0, 0, 0, 0, 8, 6, 1, 0, 38, 0, 8, 2, 10, 12, 0, 88, 11, 8, 1, 2, 0, 0, 0, 0, 19, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 1, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 1, 13, 115, 1, 7, 3, 1, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 5, 2, 0, 0, 2, 1, 3, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 3, 2, 4, 0, 0, 0, 2, 0, 0, 0, 5, 1, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 42, 0, 7, 16, 0, 1, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 4, 6, 10, 0, 4, 1, 0, 0, 8, 0, 2, 2, 0, 0, 7, 0, 0, 1, 0, 0, 0, 5, 13, 0, 0, 0, 19, 1, 8, 0, 0, 5, 8, 0, 1, 0, 1, 0, 0, 4, 19, 1, 2, 0, 11, 3, 0, 1, 0, 0, 0, 94, 0, 0, 2, 0, 2, 2, 8, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 6, 0, 2, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 1, 0, 0, 45, 0, 8, 0, 0, 2, 0, 1, 9, 0, 6, 1, 0, 0, 3, 35, 1, 0, 1, 4, 1, 0, 0, 1, 3, 0, 0, 2, 1, 13, 0, 0, 2, 0, 0, 3, 17, 0, 0, 8, 6, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 9, 0, 0, 0, 0, 0, 2, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 25, 6, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 25, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 22, 101, 3, 12, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 3, 1, 1, 2, 1, 100, 1, 1, 2, 1, 1, 1, 4, 2, 1, 1, 1, 1, 6, 1, 1, 2, 6, 1, 9, 0, 2, 1, 4, 0, 9, 0, 0, 2, 0, 0, 1, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 6, 2, 0, 0, 5, 2, 2, 0, 0, 0, 13, 1, 0, 11, 0, 0, 0, 0, 8, 8, 0, 0, 1, 0, 1, 0, 4, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 2, 0, 0, 0, 31, 1, 0, 35, 1, 2, 2, 0, 6, 0, 0, 0, 0, 38, 0, 0, 20, 0, 0, 0, 0, 0, 12, 8, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 1, 1, 1, 4, 0, 0, 15, 3, 2, 1, 2, 2, 0, 13, 0, 0, 0, 0, 0, 8, 0, 0, 1, 4, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 5, 0, 26, 0, 0, 0, 0, 1, 3, 2, 0, 0, 7, 3, 0, 0, 0, 0, 7, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 9, 0, 48, 0, 0, 4, 0, 0, 12, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 4, 0, 0, 0, 0, 0, 0, 3, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 9, 0, 1, 0, 0, 146, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 52, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 1, 5, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 12, 3, 0, 0, 0, 0, 0, 0, 0, 2, 7, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 1, 6, 2, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 12, 0, 3, 0, 0, 1, 5, 1, 5, 1, 0, 0, 11, 0, 0, 3, 0, 2, 0, 0, 1, 3, 0, 2, 0, 0, 0, 0, 69, 0, 71, 0, 0, 0, 0, 0, 4, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 10, 0, 21, 3, 1, 1, 11, 0, 0, 1, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 1, 0, 4, 18, 0, 0, 1, 8, 0, 0, 2, 1, 3, 7, 2, 0, 1, 1, 0, 1, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 10, 30, 0, 151, 9, 0, 0, 1, 0, 1, 0, 0, 18, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 0, 0, 0, 0, 0, 2, 14, 5, 8, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 4, 0, 0, 0, 1, 0, 0, 35, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 14, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 8, 0, 0, 0, 0, 2, 0, 6, 0, 6, 0, 2, 0, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 11, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 1, 2, 2, 0, 1, 1, 0, 0, 0, 3, 0, 11, 0, 1, 1, 5, 4, 1, 3, 1, 3, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 14, 10, 2, 1, 1, 14, 2, 16, 20, 1, 4, 60, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 28, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 8, 3, 0, 0, 0, 1, 0, 0, 0, 3, 0, 1, 0, 1, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 8, 27, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 2, 0, 0, 0, 0, 22, 0, 0, 0, 0, 0, 0, 9, 5, 4, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 1, 0, 0, 2, 3, 4, 10, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 10, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 11, 2, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 1, 2, 5, 5, 1, 0, 0, 0, 1, 0, 10, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 3, 0, 0, 0, 15, 6, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 19, 2, 0, 2, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 3, 3, 1, 6, 0, 0, 2, 0, 0, 3, 0, 1, 0, 1, 0, 0, 0, 0, 2, 12, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 8, 6, 0, 0, 0, 0, 5, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 3, 2, 1, 0, 4, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 5, 0, 521, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 10, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 2, 0, 1, 0, 4, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 3, 1, 0, 1, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 37, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 1, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 6, 1, 0, 0, 0, 0, 15, 11, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 18, 0, 0, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 15, 0, 1, 0, 0, 0, 0, 0, 2, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 2, 0, 0, 0, 0, 4, 1, 0, 0, 6, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 4, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 116, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 20, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 63, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 7, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 6, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 1, 0, 2, 0, 0, 6, 0, 5, 1, 2, 1, 0, 0, 0, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 5, 0, 4, 0, 0, 1, 6, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 3, 0, 3, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 83, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 26, 0, 0, 0, 1, 0, 4, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 0, 7, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 0, 1, 0, 0, 1, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 24, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 4, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 12, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 30, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 3, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 6, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 13, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 0, 0, 29, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 3, 0, 1, 0, 0, 45, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 3, 0, 0, 77, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 4, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 23, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 2, 23, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 9, 0, 19, 0, 0, 0, 2, 2, 10, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 17, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 6, 1, 0, 0, 0, 0, 1, 0, 3, 1, 1, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 1, 4, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 2, 1, 7, 2, 1, 1, 2, 1, 6, 2, 3, 4, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 9, 2, 9, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 31, 14, 18, 1, 1, 1, 1, 1, 2, 1, 3, 5, 6, 3, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 26, 54, 23, 1, 2, 1, 3, 6, 7, 1, 1, 1, 1, 56, 2, 1, 1, 5, 1, 1, 2, 1, 4, 3, 1, 22, 18, 1, 2, 8, 1, 4, 3, 1, 1, 19, 1, 10, 1, 3, 1, 58, 1, 18, 8, 1, 1, 1, 2, 1, 3, 5, 15, 3, 15, 1, 2, 2, 2, 1, 2, 2, 1, 1, 48, 4, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 7, 7, 42, 2, 2, 1, 1, 1, 2, 3, 1, 3, 1, 1, 1, 2, 24, 1, 7, 3, 1, 1, 2, 1, 1, 8, 2, 7, 1, 1, 1, 6, 7, 23, 1, 2, 1, 1, 1, 15, 3, 11, 5, 3, 3, 3, 1, 2, 2, 1, 1, 6, 2, 1, 3, 1, 13, 2, 9, 2, 5, 3, 1, 3, 1, 2, 5, 8, 5, 1, 5, 9, 3, 1, 1, 1, 1, 4, 2, 3, 1, 1, 3, 5, 4, 3, 3, 3, 9, 10, 3, 6, 2, 3, 2, 4, 1, 14, 4, 4, 2, 1, 1, 4, 10, 3, 5, 3, 3, 6, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 3, 10, 22, 1, 1, 35, 1, 1, 1, 1, 1, 1, 4, 1, 16, 4, 1, 1, 1, 4, 2, 1, 8, 1, 11, 9, 1, 4, 2, 5, 1, 1, 3, 2, 1, 2, 2, 1, 1, 1, 3, 1, 1, 4, 3, 1, 1, 1, 17, 3, 6, 1, 1, 1, 1, 1, 5, 2, 1, 9, 1, 1, 1, 1, 2, 1, 9, 5, 22, 3, 5, 8, 14, 11, 9, 4, 6, 20, 1, 2, 1, 1, 16, 1, 1, 1, 2, 1, 1, 2, 2, 23, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 5, 2, 4, 2, 3, 14, 22, 2, 2, 1, 1, 5, 6, 1, 4, 4, 4, 19, 1, 1, 3, 8, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 3, 1, 2, 1, 3, 1, 7, 2, 1, 1, 2, 1, 1, 1, 1, 8, 1, 2, 10, 12, 15, 2, 1, 2, 4, 1, 2, 1, 1, 1, 1, 1, 4, 1, 1, 2, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 1, 4, 3, 2, 2, 1, 2, 1, 4, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 2, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3, 2, 19, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "docs[11] = 11. Managing Linguistic Data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "11. Managing Linguistic Data\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Structured collections of annotated linguistic data are essential in most areas of NLP,\n",
      "however, we still face many obstacles in using them.\n",
      "The goal of this chapter is to answer the following questions:\n",
      "\n",
      "How do we design a new language resource and ensure that its\n",
      "coverage, balance, and documentation support a wide range of uses?\n",
      "When existing data is in the wrong format for some analysis tool,\n",
      "how can we convert it to a suitable format?\n",
      "What is a good way to document the existence of a resource we\n",
      "have created so that others can easily find it?\n",
      "\n",
      "Along the way, we will study the design of existing corpora, the\n",
      "typical workflow for creating a corpus, and the lifecycle of corpus.\n",
      "As in other chapters, there will be many examples drawn from\n",
      "practical experience managing linguistic data, including\n",
      "data that has been collected in the course of linguistic fieldwork,\n",
      "laboratory work, and web crawling.\n",
      "\n",
      "1   Corpus Structure: a Case Study\n",
      "The TIMIT corpus of read speech was the first annotated speech database to be\n",
      "widely distributed, and it has an especially clear organization.\n",
      "TIMIT was developed by a consortium including Texas Instruments and MIT, from which\n",
      "it derives its name.\n",
      "It was designed to provide data for the acquisition of acoustic-phonetic knowledge and to\n",
      "support the development and evaluation of automatic speech recognition systems.\n",
      "\n",
      "1.1   The Structure of TIMIT\n",
      "Like the Brown Corpus, which displays a balanced selection of text genres and sources,\n",
      "TIMIT includes a balanced selection of dialects, speakers, and materials.  For each of\n",
      "eight dialect regions, 50 male and female speakers having a range of ages and educational\n",
      "backgrounds each read ten carefully chosen sentences.  Two sentences, read by all\n",
      "speakers, were designed to bring out dialect variation:\n",
      "\n",
      "  (1)\n",
      "  a.she had your dark suit in greasy wash water all year\n",
      "\n",
      "  b.don't ask me to carry an oily rag like that\n",
      "\n",
      "The remaining sentences were chosen to be phonetically rich, involving all phones (sounds) and\n",
      "a comprehensive range of diphones (phone bigrams).  Additionally, the design strikes a balance\n",
      "between multiple speakers saying the same sentence in order to permit comparison across\n",
      "speakers, and having a large range of sentences covered by the corpus to get maximal\n",
      "coverage of diphones.  Five of the sentences read by each speaker are also read by six other\n",
      "speakers (for comparability).  The remaining three sentences read by each speaker were unique\n",
      "to that speaker (for coverage).\n",
      "NLTK includes a sample from the TIMIT corpus.  You can access its documentation in the usual\n",
      "way, using help(nltk.corpus.timit).  Print nltk.corpus.timit.fileids() to see a list of the\n",
      "160 recorded utterances in the corpus sample.\n",
      "Each file name has internal structure as shown in 1.1.\n",
      "\n",
      "\n",
      "Figure 1.1: Structure of a TIMIT Identifier: Each recording is labeled using a string made\n",
      "up of the speaker's dialect region, gender, speaker identifier, sentence type,\n",
      "and sentence identifier.\n",
      "\n",
      "Each item has a phonetic transcription which can be accessed using the phones()\n",
      "method.  We can access the corresponding word tokens in the customary way.  Both access\n",
      "methods permit an optional argument offset=True which includes the start and end offsets\n",
      "of the corresponding span in the audio file.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> phonetic = nltk.corpus.timit.phones('dr1-fvmh0/sa1')\n",
      ">>> phonetic\n",
      "['h#', 'sh', 'iy', 'hv', 'ae', 'dcl', 'y', 'ix', 'dcl', 'd', 'aa', 'kcl',\n",
      "'s', 'ux', 'tcl', 'en', 'gcl', 'g', 'r', 'iy', 's', 'iy', 'w', 'aa',\n",
      "'sh', 'epi', 'w', 'aa', 'dx', 'ax', 'q', 'ao', 'l', 'y', 'ih', 'ax', 'h#']\n",
      ">>> nltk.corpus.timit.word_times('dr1-fvmh0/sa1')\n",
      "[('she', 7812, 10610), ('had', 10610, 14496), ('your', 14496, 15791),\n",
      "('dark', 15791, 20720), ('suit', 20720, 25647), ('in', 25647, 26906),\n",
      "('greasy', 26906, 32668), ('wash', 32668, 37890), ('water', 38531, 42417),\n",
      "('all', 43091, 46052), ('year', 46052, 50522)]\n",
      "\n",
      "\n",
      "\n",
      "In addition to this text data, TIMIT includes a lexicon that provides the canonical\n",
      "pronunciation of every word, which can be compared with a particular utterance:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> timitdict = nltk.corpus.timit.transcription_dict()\n",
      ">>> timitdict['greasy'] + timitdict['wash'] + timitdict['water']\n",
      "['g', 'r', 'iy1', 's', 'iy', 'w', 'ao1', 'sh', 'w', 'ao1', 't', 'axr']\n",
      ">>> phonetic[17:30]\n",
      "['g', 'r', 'iy', 's', 'iy', 'w', 'aa', 'sh', 'epi', 'w', 'aa', 'dx', 'ax']\n",
      "\n",
      "\n",
      "\n",
      "This gives us a sense of what a speech processing system\n",
      "would have to do in producing or recognizing speech in this particular dialect\n",
      "(New England).  Finally, TIMIT includes demographic data about the speakers,\n",
      "permitting fine-grained study of vocal, social, and gender characteristics.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> nltk.corpus.timit.spkrinfo('dr1-fvmh0')\n",
      "SpeakerInfo(id='VMH0', sex='F', dr='1', use='TRN', recdate='03/11/86',\n",
      "birthdate='01/08/60', ht='5\\'05\"', race='WHT', edu='BS',\n",
      "comments='BEST NEW ENGLAND ACCENT SO FAR')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.2   Notable Design Features\n",
      "TIMIT illustrates several key features of corpus design.\n",
      "First, the corpus contains two layers of annotation, at the phonetic and orthographic\n",
      "levels.  In general, a text or speech corpus may be annotated at many different linguistic levels,\n",
      "including morphological, syntactic, and discourse levels.  Moreover, even at a given\n",
      "level there may be different labeling schemes or even disagreement amongst annotators,\n",
      "such that we want to represent multiple versions.\n",
      "A second property of TIMIT is its balance across multiple dimensions of variation,\n",
      "for coverage of dialect regions and diphones.  The inclusion of speaker\n",
      "demographics brings in many more independent variables, that may help to\n",
      "account for variation in the data, and which facilitate later uses of the\n",
      "corpus for purposes that were not envisaged when the corpus was created,\n",
      "such as sociolinguistics.\n",
      "A third property is that there is a sharp division between the original\n",
      "linguistic event captured as an audio recording, and the annotations of that event.\n",
      "The same holds true of text corpora, in the sense that the original text usually\n",
      "has an external source, and is considered to be an immutable artifact.  Any transformations\n",
      "of that artifact which involve human judgment — even something as\n",
      "simple as tokenization — are subject to later revision, thus it is important to\n",
      "retain the source material in a form that is as close to the original as possible.\n",
      "\n",
      "\n",
      "Figure 1.2: Structure of the Published TIMIT Corpus: The CD-ROM contains doc, train, and test directories\n",
      "at the top level; the train and test directories both have 8 sub-directories, one per\n",
      "dialect region; each of these contains further subdirectories, one per speaker;\n",
      "the contents of the directory for female speaker aks0 are listed, showing\n",
      "10 wav files accompanied by a text transcription, a word-aligned transcription,\n",
      "and a phonetic transcription.\n",
      "\n",
      "A fourth feature of TIMIT is the hierarchical structure of the corpus.\n",
      "With 4 files per sentence, and 10 sentences for each of 500 speakers, there are 20,000 files.  These are\n",
      "organized into a tree structure, shown schematically in 1.2.\n",
      "At the top level there is a split between training and testing sets, which gives\n",
      "away its intended use for developing and evaluating statistical models.\n",
      "Finally, notice that even though TIMIT is a speech corpus, its transcriptions and associated\n",
      "data are just text, and can be processed using programs just like any other text corpus.\n",
      "Therefore, many of the computational methods described in this book are applicable.\n",
      "Moreover, notice that all of the data types included in the TIMIT corpus fall into\n",
      "the two basic categories of lexicon and text, which we will discuss below.\n",
      "Even the speaker demographics data is just another instance of the lexicon data type.\n",
      "This last observation is less surprising when we consider that text and record structures are the\n",
      "primary domains for the two subfields of computer science that focus on data management,\n",
      "namely text retrieval and databases.  A notable feature of linguistic data management is\n",
      "that usually brings both data types together, and that it can draw on results and techniques\n",
      "from both fields.\n",
      "\n",
      "\n",
      "1.3   Fundamental Data Types\n",
      "\n",
      "\n",
      "Figure 1.3: Basic Linguistic Data Types — Lexicons and Texts: amid their diversity,\n",
      "lexicons have a record structure, while annotated texts have a temporal organization.\n",
      "\n",
      "Despite its complexity, the TIMIT corpus only contains two fundamental data types,\n",
      "namely lexicons and texts.\n",
      "As we saw in 2., most lexical resources can be represented using\n",
      "a record structure, i.e. a key plus one or more fields, as\n",
      "shown in 1.3.  A lexical resource could be a conventional\n",
      "dictionary or comparative wordlist, as illustrated.  It could also\n",
      "be a phrasal lexicon, where the key field is a phrase rather than a single word.\n",
      "A thesaurus also consists of record-structured data, where we look up entries\n",
      "via non-key fields that correspond to topics.\n",
      "We can also construct special tabulations (known as paradigms)\n",
      "to illustrate contrasts and systematic variation, as shown in\n",
      "1.3 for three verbs.  TIMIT's speaker table is also a kind\n",
      "of lexicon.\n",
      "At the most abstract level, a text is a representation of a real or fictional speech event,\n",
      "and the time-course of that event carries over into the text itself.  A text could be a small\n",
      "unit, such as a word or sentence, or a complete narrative or dialogue.  It may come with\n",
      "annotations such as part-of-speech tags, morphological analysis, discourse structure, and so forth.\n",
      "As we saw in the IOB tagging technique (7.), it is possible to represent higher-level\n",
      "constituents using tags on individual words.  Thus the abstraction of text shown in\n",
      "1.3 is sufficient.\n",
      "Despite the complexities and idiosyncrasies of individual corpora, at base they are\n",
      "collections of texts together with record-structured data.  The contents of a corpus\n",
      "are often biased towards one or other of these types.\n",
      "For example, the Brown Corpus contains 500 text files, but we still use a table to relate\n",
      "the files to 15 different genres.  At the other end of the spectrum, WordNet\n",
      "contains 117,659 synset records, yet it incorporates many example sentences (mini-texts)\n",
      "to illustrate word usages.  TIMIT is an interesting mid-point on this spectrum, containing substantial\n",
      "free-standing material of both the text and lexicon types.\n",
      "\n",
      "\n",
      "\n",
      "2   The Life-Cycle of a Corpus\n",
      "Corpora are not born fully-formed, but involve careful preparation\n",
      "and input from many people over an extended period.  Raw data needs\n",
      "to be collected, cleaned up, documented, and stored in a systematic\n",
      "structure.  Various layers of annotation might be applied, some requiring\n",
      "specialized knowledge of the morphology or syntax of the language.\n",
      "Success at this stage depends on creating an efficient workflow\n",
      "involving appropriate tools and format converters.\n",
      "Quality control procedures can be put in place to find inconsistencies\n",
      "in the annotations, and to ensure the highest\n",
      "possible level of inter-annotator agreement.  Because of the\n",
      "scale and complexity of the task, large corpora may take years to\n",
      "prepare, and involve tens or hundreds of person-years of effort.\n",
      "In this section we briefly review the various stages in the\n",
      "life-cycle of a corpus.\n",
      "\n",
      "2.1   Three Corpus Creation Scenarios\n",
      "In one type of corpus, the design unfolds over\n",
      "in the course of the creator's explorations. This is the pattern\n",
      "typical of traditional \"field linguistics,\" in which material from\n",
      "elicitation sessions is analyzed as it is gathered, with tomorrow's elicitation often based\n",
      "on questions that arise in analyzing today's. The resulting corpus\n",
      "is then used during subsequent years of research, and may serve\n",
      "as an archival resource indefinitely.  Computerization is an\n",
      "obvious boon to work of this type, as exemplified by the popular\n",
      "program Shoebox, now over two decades old and re-released as Toolbox\n",
      "(see 4).\n",
      "Other software tools, even simple word processors and spreadsheets, are routinely\n",
      "used to acquire the data.  In the next section we will look at how to extract\n",
      "data from these sources.\n",
      "Another corpus creation scenario is typical of experimental research\n",
      "where a body of carefully-designed material is collected from a range of human subjects,\n",
      "then analyzed to evaluate a hypothesis or develop a technology.\n",
      "It has become common for such databases to be shared and re-used\n",
      "within a laboratory or company, and often to be published more\n",
      "widely. Corpora of this type are the basis of the\n",
      "\"common task\" method of research management, which over the\n",
      "past two decades has become the norm in government-funded research\n",
      "programs in language technology.\n",
      "We have already encountered many such corpora in the earlier chapters;\n",
      "we will see how to write Python programs to implement the kinds of\n",
      "curation tasks that are necessary before such corpora are published.\n",
      "Finally, there are efforts to gather a \"reference corpus\" for a\n",
      "particular language, such as the American National Corpus (ANC)\n",
      "and the British National Corpus (BNC). Here the goal\n",
      "has been to produce a comprehensive record of the\n",
      "many forms, styles and uses of a language.\n",
      "Apart from the sheer challenge of scale, there is a heavy reliance\n",
      "on automatic annotation tools together with post-editing to\n",
      "fix any errors.  However, we can write programs to locate\n",
      "and repair the errors, and also to analyze the corpus for balance.\n",
      "\n",
      "\n",
      "2.2   Quality Control\n",
      "Good tools for automatic and manual preparation of data\n",
      "are essential.  However the creation of a high-quality corpus depends just\n",
      "as much on such mundane things as documentation, training, and workflow.\n",
      "Annotation guidelines define the task and document the markup\n",
      "conventions.  They may be regularly updated to cover difficult\n",
      "cases, along with new rules that are devised to achieve more\n",
      "consistent annotations.  Annotators need to be trained in the\n",
      "procedures, including methods for resolving cases not covered\n",
      "in the guidelines.  A workflow needs to be established, possibly\n",
      "with supporting software, to keep track of which files have been initialized,\n",
      "annotated, validated, manually checked, and so on.  There may be multiple layers of\n",
      "annotation, provided by different specialists.  Cases of uncertainty\n",
      "or disagreement may require adjudication.\n",
      "Large annotation tasks require multiple annotators, which\n",
      "raises the problem of achieving consistency.\n",
      "How consistently can a group of annotators perform?\n",
      "We can easily measure consistency by having a portion of\n",
      "the source material independently annotated by two people.\n",
      "This may reveal shortcomings in the guidelines or\n",
      "differing abilities with the annotation task.\n",
      "In cases where quality is paramount, the entire corpus can\n",
      "be annotated twice, and any inconsistencies adjudicated\n",
      "by an expert.\n",
      "It is considered best practice to report the inter-annotator\n",
      "agreement that was achieved for a corpus (e.g. by double-annotating\n",
      "10% of the corpus).  This score serves as a helpful upper bound on\n",
      "the expected performance of any automatic system that is trained\n",
      "on this corpus.\n",
      "\n",
      "Caution!\n",
      "Care should be exercised when interpreting an inter-annotator\n",
      "agreement score, since annotation tasks vary greatly in their\n",
      "difficulty.  For example, 90% agreement would be a terrible\n",
      "score for part-of-speech tagging, but an exceptional score\n",
      "for semantic role labeling.\n",
      "\n",
      "The Kappa coefficient K measures agreement between\n",
      "two people making category judgments, correcting for expected\n",
      "chance agreement.  For example, suppose an item is to be annotated,\n",
      "and four coding options are equally likely.\n",
      "Then two people coding randomly would be expected to agree 25% of the time.\n",
      "Thus, an agreement of 25% will be assigned K = 0, and better\n",
      "levels of agreement will be scaled accordingly.\n",
      "For an agreement of 50%, we would get\n",
      "K = 0.333, as 50 is a third of the way from 25 to 100.\n",
      "Many other agreement measures exist; see help(nltk.metrics.agreement) for details.\n",
      "\n",
      "\n",
      "Figure 2.1: Three Segmentations of a Sequence: The small rectangles represent characters,\n",
      "words, sentences, in short, any sequence which might be divided into\n",
      "linguistic units; S1 and S2 are in close\n",
      "agreement, but both differ significantly from S3.\n",
      "\n",
      "We can also measure the agreement between two independent segmentations\n",
      "of language input, e.g. for tokenization, sentence segmentation,\n",
      "named-entity detection.  In 2.1 we see three\n",
      "possible segmentations of a sequence of items\n",
      "which might have been produced by annotators (or programs).\n",
      "Although none of them agree exactly, S1 and S2\n",
      "are in close agreement, and we would like a suitable measure.\n",
      "Windowdiff is a simple algorithm for evaluating the agreement of\n",
      "two segmentations by running a sliding window over the\n",
      "data and awarding partial credit for near misses.\n",
      "If we preprocess our tokens into a sequence of zeros and ones, to\n",
      "record when a token is followed by a boundary, we can represent the\n",
      "segmentations as strings, and apply the windowdiff scorer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> s1 = \"00000010000000001000000\"\n",
      ">>> s2 = \"00000001000000010000000\"\n",
      ">>> s3 = \"00010000000000000001000\"\n",
      ">>> nltk.windowdiff(s1, s1, 3)\n",
      "0.0\n",
      ">>> nltk.windowdiff(s1, s2, 3)\n",
      "0.190...\n",
      ">>> nltk.windowdiff(s2, s3, 3)\n",
      "0.571...\n",
      "\n",
      "\n",
      "\n",
      "In the above example, the window had a size of 3.\n",
      "The windowdiff computation slides this window across a pair of strings.\n",
      "At each position it totals up the number of boundaries found inside this\n",
      "window, for both strings, then computes the difference.  These differences\n",
      "are then summed.\n",
      "We can increase or shrink the window size to control the sensitivity of the\n",
      "measure.\n",
      "\n",
      "\n",
      "2.3   Curation vs Evolution\n",
      "As large corpora are published, researchers are increasingly\n",
      "likely to base their investigations on balanced, focused\n",
      "subsets that were derived from corpora produced for entirely\n",
      "different reasons.  For instance, the Switchboard database,\n",
      "originally collected for speaker identification research,\n",
      "has since been used as the basis for published studies in speech recognition,\n",
      "word pronunciation, disfluency, syntax, intonation and discourse structure.\n",
      "The motivations for recycling linguistic corpora include the\n",
      "desire to save time and effort, the desire to work on material\n",
      "available to others for replication, and sometimes a desire to study\n",
      "more naturalistic forms of linguistic behavior than would be possible\n",
      "otherwise. The process of choosing a subset for such a study may\n",
      "count as a non-trivial contribution in itself.\n",
      "In addition to selecting an appropriate subset of a corpus, this\n",
      "new work could involve reformatting a text file\n",
      "(e.g. converting to XML), renaming files, retokenizing the text,\n",
      "selecting a subset of the data to enrich, and so forth.\n",
      "Multiple research groups might do this work independently, as illustrated\n",
      "in 2.2.  At a later date, should someone want to combine sources\n",
      "of information from different versions, the task will probably be extremely onerous.\n",
      "\n",
      "\n",
      "Figure 2.2: Evolution of a Corpus over Time: After a corpus is published, research\n",
      "groups will use it independently, selecting and enriching different pieces;\n",
      "later research that seeks to integrate separate annotations confronts\n",
      "the difficult challenge of aligning the annotations.\n",
      "\n",
      "The task of using derived corpora is made even more difficult by the lack of\n",
      "any record about how the derived version was created, and which version is the most\n",
      "up-to-date.\n",
      "An alternative to this chaotic situation is for a corpus to be centrally curated,\n",
      "and for committees of experts to revise and extend it at periodic\n",
      "intervals, considering submissions from third-parties, and publishing new releases\n",
      "from time to time.  Print dictionaries and national corpora may be centrally curated in\n",
      "this way.  However, for most corpora this model is simply impractical.\n",
      "A middle course is for the original corpus publication to have a scheme for identifying\n",
      "any sub-part.  Each sentence, tree, or lexical entry, could have a globally unique\n",
      "identifier, and each token, node or field (respectively) could have a relative offset.\n",
      "Annotations, including segmentations, could reference the source using\n",
      "this identifier scheme (a method which is known as standoff annotation).\n",
      "This way, new annotations could be distributed independently of the source, and\n",
      "multiple independent annotations of the same source could be\n",
      "compared and updated without touching the source.\n",
      "If the corpus publication is provided in multiple versions, the version\n",
      "number or date could be part of the identification scheme.\n",
      "A table of correspondences between identifiers across editions of the corpus\n",
      "would permit any standoff annotations to be updated easily.\n",
      "\n",
      "Caution!\n",
      "Sometimes an updated corpus contains revisions of base material that\n",
      "has been externally annotated.  Tokens might be split or merged, and constituents\n",
      "may have been rearranged.  There may not be a one-to-one correspondence between\n",
      "old and new identifiers.  It is better to cause standoff annotations to break\n",
      "on such components of the new version than to silently allow their identifiers\n",
      "to refer to incorrect locations.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3   Acquiring Data\n",
      "\n",
      "3.1   Obtaining Data from the Web\n",
      "The Web is a rich source of data for language analysis purposes.  We have already\n",
      "discussed methods for accessing individual files, RSS feeds, and search engine\n",
      "results (see 3.1).  However, in some cases we want to\n",
      "obtain large quantities of web text.\n",
      "The simplest approach is to obtain a published corpus of web text.  The ACL\n",
      "Special Interest Group on Web as Corpus (SIGWAC) maintains a list of resources\n",
      "at http://www.sigwac.org.uk/.\n",
      "The advantage of using a well-defined web corpus is that they are documented,\n",
      "stable, and permit reproducible experimentation.\n",
      "If the desired content is localized to a particular website, there are many\n",
      "utilities for capturing all the accessible contents of a site, such as\n",
      "GNU Wget http://www.gnu.org/software/wget/.\n",
      "For maximal flexibility and control, a web crawler can be used,\n",
      "such as Heritrix http://crawler.archive.org/.\n",
      "Crawlers permit fine-grained control over where to look, which links to\n",
      "follow, and how to organize the results (Croft, Metzler, & Strohman, 2009).\n",
      "For example, if we want to compile a\n",
      "bilingual text collection having corresponding pairs of documents in each language,\n",
      "the crawler needs to detect the structure of the site in order to extract the\n",
      "correspondence between the documents, and it needs to organize the downloaded\n",
      "pages in such a way that the correspondence is captured.  It might be tempting\n",
      "to write your own web-crawler, but there are dozens of pitfalls to do with\n",
      "detecting MIME types, converting relative to absolute URLs, avoiding getting\n",
      "trapped in cyclic link structures, dealing with network latencies, avoiding\n",
      "overloading the site or being banned from accessing the site, and so on.\n",
      "\n",
      "\n",
      "3.2   Obtaining Data from Word Processor Files\n",
      "Word processing software is often used in the manual preparation\n",
      "of texts and lexicons in projects that have limited computational\n",
      "infrastructure.  Such projects often provide templates for data\n",
      "entry, though the word processing software does not ensure that\n",
      "the data is correctly structured.  For example, each text may\n",
      "be required to have a title and date.  Similarly, each lexical\n",
      "entry may have certain obligatory fields.\n",
      "As the data grows in size and complexity, a larger\n",
      "proportion of time may be spent maintaining its consistency.\n",
      "How can we extract the content of such files so\n",
      "that we can manipulate it in external programs?\n",
      "Moreover, how can we validate the content of these files to\n",
      "help authors create well-structured data, so that the\n",
      "quality of the data can be maximized in the context of\n",
      "the original authoring process?\n",
      "Consider a dictionary in\n",
      "which each entry has a part-of-speech field, drawn from a set of 20\n",
      "possibilities, displayed after the pronunciation field, and rendered\n",
      "in 11-point bold.  No conventional word processor has search or macro\n",
      "functions capable of verifying that all part-of-speech fields have\n",
      "been correctly entered and displayed.  This task requires exhaustive\n",
      "manual checking.  If the word processor permits the document to be\n",
      "saved in a non-proprietary format, such as text, HTML, or XML, we can\n",
      "sometimes write programs to do this checking automatically.\n",
      "Consider the following fragment of a lexical entry:\n",
      "\"sleep [sli:p] v.i. condition of body and mind...\".\n",
      "We can enter this in MSWord, then \"Save as Web Page\",\n",
      "then inspect the resulting HTML file:\n",
      "\n",
      "<p class=MsoNormal>sleep\n",
      "  <span style='mso-spacerun:yes'> </span>\n",
      "  [<span class=SpellE>sli:p</span>]\n",
      "  <span style='mso-spacerun:yes'> </span>\n",
      "  <b><span style='font-size:11.0pt'>v.i.</span></b>\n",
      "  <span style='mso-spacerun:yes'> </span>\n",
      "  <i>a condition of body and mind ...<o:p></o:p></i>\n",
      "</p>\n",
      "\n",
      "Observe that the entry is represented as an HTML paragraph, using the\n",
      "<p> element, and that the part of speech appears inside a <span\n",
      "style='font-size:11.0pt'> element.  The following program defines\n",
      "the set of legal parts-of-speech, legal_pos.  Then it extracts all\n",
      "11-point content from the dict.htm file and stores it in the set\n",
      "used_pos.  Observe that the search pattern contains a\n",
      "parenthesized sub-expression; only the material that matches this\n",
      "sub-expression is returned by re.findall.  Finally, the program\n",
      "constructs the set of illegal parts-of-speech as used_pos -\n",
      "legal_pos:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> legal_pos = set(['n', 'v.t.', 'v.i.', 'adj', 'det'])\n",
      ">>> pattern = re.compile(r\"'font-size:11.0pt'>([a-z.]+)<\")\n",
      ">>> document = open(\"dict.htm\", encoding=\"windows-1252\").read()\n",
      ">>> used_pos = set(re.findall(pattern, document))\n",
      ">>> illegal_pos = used_pos.difference(legal_pos)\n",
      ">>> print(list(illegal_pos))\n",
      "['v.i', 'intrans']\n",
      "\n",
      "\n",
      "\n",
      "This simple program represents the tip of the iceberg.  We can develop\n",
      "sophisticated tools to check the consistency of word processor files,\n",
      "and report errors so that the maintainer of the dictionary can correct\n",
      "the original file using the original word processor.\n",
      "Once we know the data is correctly formatted, we\n",
      "can write other programs to convert the data into a different format.\n",
      "The program in 3.1 strips out the HTML markup using the BeautifulSoup library,\n",
      "extracts the words and their pronunciations, and generates output\n",
      "in \"comma-separated value\" (CSV) format.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "def lexical_data(html_file, encoding=\"utf-8\"):\n",
      "    SEP = '_ENTRY'\n",
      "    html = open(html_file, encoding=encoding).read()\n",
      "    html = re.sub(r'<p', SEP + '<p', html)\n",
      "    text = BeautifulSoup(html, 'html.parser').get_text()\n",
      "    text = ' '.join(text.split())\n",
      "    for entry in text.split(SEP):\n",
      "        if entry.count(' ') > 2:\n",
      "            yield entry.split(' ', 3)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> import csv\n",
      ">>> writer = csv.writer(open(\"dict1.csv\", \"w\", encoding=\"utf-8\"))\n",
      ">>> writer.writerows(lexical_data(\"dict.htm\", encoding=\"windows-1252\"))\n",
      "\n",
      "\n",
      "Example 3.1 (code_html2csv.py): Figure 3.1: Converting HTML Created by Microsoft Word into Comma-Separated Values\n",
      "\n",
      "\n",
      "with gzip.open(fn+\".gz\",\"wb\") as f_out:\n",
      "f_out.write(bytes(s, 'UTF-8'))\n",
      "\n",
      "\n",
      "\n",
      "3.3   Obtaining Data from Spreadsheets and Databases\n",
      "Spreadsheets are often used for acquiring wordlists or paradigms.\n",
      "For example, a comparative wordlist may be created using a spreadsheet,\n",
      "with a row for each cognate set, and a column for each language\n",
      "(cf. nltk.corpus.swadesh, and www.rosettaproject.org).\n",
      "Most spreadsheet software can export their data in CSV\n",
      "\"comma-separated value\" format.  As we see below, it is easy for\n",
      "Python programs to access these using the csv module.\n",
      "Sometimes lexicons are stored in a full-fledged relational database.\n",
      "When properly normalized, these databases can ensure the validity\n",
      "of the data.  For example, we can\n",
      "require that all parts-of-speech come from a specified vocabulary by\n",
      "declaring that the part-of-speech field is an enumerated type\n",
      "or a foreign key that references a separate part-of-speech table.\n",
      "However, the relational model requires the structure of the data\n",
      "(the schema) be declared in advance, and this runs counter to\n",
      "the dominant approach to structuring linguistic data, which is\n",
      "highly exploratory.  Fields which were assumed to be obligatory\n",
      "and unique often turn out to be optional and repeatable.  A relational\n",
      "database can accommodate this when it is fully known in advance, however\n",
      "if it is not, or if just about every property turns out to be optional\n",
      "or repeatable, the relational approach is unworkable.\n",
      "Nevertheless, when our goal is simply to extract the contents from a database,\n",
      "it is enough to dump out the tables (or SQL query results)\n",
      "in CSV format and load them into our program.  Our program might perform\n",
      "a linguistically motivated query which cannot be expressed in SQL, e.g.\n",
      "select all words that appear\n",
      "in example sentences for which no dictionary entry is provided.\n",
      "For this task, we would need to extract enough information from a record\n",
      "for it to be uniquely identified, along with the headwords and example\n",
      "sentences.  Let's suppose this information was now available in a CSV file\n",
      "dict.csv:\n",
      "\n",
      "\"sleep\",\"sli:p\",\"v.i\",\"a condition of body and mind ...\"\n",
      "\"walk\",\"wo:k\",\"v.intr\",\"progress by lifting and setting down each foot ...\"\n",
      "\"wake\",\"weik\",\"intrans\",\"cease to sleep\"\n",
      "\n",
      "Now we can express this query as shown below:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> import csv\n",
      ">>> lexicon = csv.reader(open('dict.csv'))\n",
      ">>> pairs = [(lexeme, defn) for (lexeme, _, _, defn) in lexicon]\n",
      ">>> lexemes, defns = zip(*pairs)\n",
      ">>> defn_words = set(w for defn in defns for w in defn.split())\n",
      ">>> sorted(defn_words.difference(lexemes))\n",
      "['...', 'a', 'and', 'body', 'by', 'cease', 'condition', 'down', 'each',\n",
      "'foot', 'lifting', 'mind', 'of', 'progress', 'setting', 'to']\n",
      "\n",
      "\n",
      "\n",
      "This information would then guide the ongoing work to enrich the lexicon,\n",
      "work that updates the content of the relational database.\n",
      "\n",
      "\n",
      "3.4   Converting Data Formats\n",
      "Annotated linguistic data rarely arrives in the most convenient format,\n",
      "and it is often necessary to perform various kinds of format conversion.\n",
      "Converting between character encodings has already been discussed\n",
      "(see 3.3).  Here we focus on the structure of the data.\n",
      "In the simplest case, the input and output formats are isomorphic.\n",
      "For instance, we might be converting lexical data from Toolbox\n",
      "format to XML, and it is straightforward to transliterate the\n",
      "entries one at a time (4).  The structure\n",
      "of the data is reflected in the structure of the required\n",
      "program: a for loop whose body takes care\n",
      "of a single entry.\n",
      "In another common case, the output is a digested form of the\n",
      "input, such as an inverted file index.  Here it is necessary\n",
      "to build an index structure in memory (see 4.8),\n",
      "then write it to a file in the desired format.\n",
      "The following example constructs an index that maps\n",
      "the words of a dictionary definition to the corresponding\n",
      "lexeme  for each lexical entry ,\n",
      "having tokenized the definition text ,\n",
      "and discarded short words .  Once the index has\n",
      "been constructed we open a file and then iterate over\n",
      "the index entries, to write out the lines in the required format .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> idx = nltk.Index((defn_word, lexeme) \n",
      "...                  for (lexeme, defn) in pairs \n",
      "...                  for defn_word in nltk.word_tokenize(defn) \n",
      "...                  if len(defn_word) > 3) \n",
      ">>> with open(\"dict.idx\", \"w\") as idx_file:\n",
      "...     for word in sorted(idx):\n",
      "...         idx_words = ', '.join(idx[word])\n",
      "...         idx_line = \"{}: {}\".format(word, idx_words) \n",
      "...         print(idx_line, file=idx_file)\n",
      "\n",
      "\n",
      "\n",
      "The resulting file dict.idx contains the following lines.  (With a larger\n",
      "dictionary we would expect to find multiple lexemes listed for each index entry.)\n",
      "\n",
      "body: sleep\n",
      "cease: wake\n",
      "condition: sleep\n",
      "down: walk\n",
      "each: walk\n",
      "foot: walk\n",
      "lifting: walk\n",
      "mind: sleep\n",
      "progress: walk\n",
      "setting: walk\n",
      "sleep: wake\n",
      "\n",
      "In some cases, the input and output data both consist of two or more dimensions.\n",
      "For instance, the input might be a set of files, each containing a single\n",
      "column of word frequency data.  The required output might be a two-dimensional\n",
      "table in which the original columns appear as rows.  In such cases we populate\n",
      "an internal data structure by filling up one column at a time, then read off\n",
      "the data one row at a time as we write data to the output file.\n",
      "In the most vexing cases, the source and target formats have slightly different\n",
      "coverage of the domain, and information is unavoidably lost when translating between them.\n",
      "For example, we could combine multiple Toolbox files to create a single CSV file\n",
      "containing a comparative wordlist, loosing all but the \\lx field of the input files.\n",
      "If the CSV file was later modified, it would be a labor-intensive process to inject\n",
      "the changes into the original Toolbox files.  A partial solution to this \"round-tripping\"\n",
      "problem is to associate explicit identifiers each linguistic object, and to propagate\n",
      "the identifiers with the objects.\n",
      "\n",
      "\n",
      "3.5   Deciding Which Layers of Annotation to Include\n",
      "Published corpora vary greatly in the richness of the information they contain.\n",
      "At a minimum, a corpus will typically\n",
      "contain at least a sequence of sound or orthographic symbols.  At the\n",
      "other end of the spectrum, a corpus could contain a large amount of\n",
      "information about the syntactic structure, morphology, prosody, and\n",
      "semantic content of every sentence, plus annotation of discourse relations\n",
      "or dialogue acts.  These extra layers of annotation\n",
      "may be just what someone needs for performing a particular data analysis task.\n",
      "For example, it may be much easier to find a given\n",
      "linguistic pattern if we can search for specific syntactic structures;\n",
      "and it may be easier to categorize a linguistic pattern if every word\n",
      "has been tagged with its sense.  Here are some commonly provided\n",
      "annotation layers:\n",
      "\n",
      "Word Tokenization: The orthographic form of text does not unambiguously\n",
      "identify its tokens.  A tokenized and normalized version,\n",
      "in addition to the conventional orthographic version,\n",
      "may be a very convenient resource.\n",
      "Sentence Segmentation: As we saw in 3, sentence\n",
      "segmentation can be more difficult than it seems.  Some corpora\n",
      "therefore use explicit annotations to mark sentence segmentation.\n",
      "Paragraph Segmentation: Paragraphs and other structural elements\n",
      "(headings, chapters, etc.) may be explicitly annotated.\n",
      "Part of Speech: The syntactic category of each word in a document.\n",
      "Syntactic Structure: A tree structure showing the constituent\n",
      "structure of a sentence.\n",
      "Shallow Semantics: Named entity and coreference annotations, semantic role labels.\n",
      "Dialogue and Discourse: dialogue act tags, rhetorical structure\n",
      "\n",
      "Unfortunately, there is not much consistency between existing corpora\n",
      "in how they represent their annotations.  However, two general classes\n",
      "of annotation representation should be distinguished.  Inline\n",
      "annotation modifies the original document by inserting special\n",
      "symbols or control sequences that carry the annotated information.\n",
      "For example, when part-of-speech tagging a document, the string\n",
      "\"fly\" might be replaced with the string \"fly/NN\", to indicate\n",
      "that the word fly is a noun in this context.  In contrast, standoff\n",
      "annotation does not modify the original document, but instead\n",
      "creates a new file that adds annotation information using pointers\n",
      "that reference the original document.  For example, this new document might\n",
      "contain the string \"<token id=8 pos='NN'/>\", to indicate\n",
      "that token 8 is a noun.  (We would want to be sure that the tokenization\n",
      "itself was not subject to change, since it would cause such references\n",
      "to break silently.)\n",
      "\n",
      "\n",
      "3.6   Standards and Tools\n",
      "For a corpus to be widely useful, it needs to be available in a widely\n",
      "supported format.  However, the cutting edge of NLP research depends\n",
      "on new kinds of annotations, which by definition are not widely supported.\n",
      "In general, adequate tools for creation, publication and use of\n",
      "linguistic data are not widely available.  Most projects must\n",
      "develop their own set of tools for internal use, which is no help\n",
      "to others who lack the necessary resources.\n",
      "Furthermore, we do not have adequate, generally-accepted standards for\n",
      "expressing the structure and content of corpora. Without\n",
      "such standards, general-purpose tools are impossible — though at the\n",
      "same time, without available tools, adequate standards are unlikely to\n",
      "be developed, used and accepted.\n",
      "One response to this situation has been to forge ahead with developing\n",
      "a generic format which is sufficiently expressive to capture a wide variety of\n",
      "annotation types (see 8 for examples).\n",
      "The challenge for NLP is to write programs that cope with the generality\n",
      "of such formats.\n",
      "For example, if the programming task involves tree data, and the\n",
      "file format permits arbitrary directed graphs, then input data must\n",
      "be validated to check for tree properties such as rootedness, connectedness,\n",
      "and acyclicity.\n",
      "If the input files contain other layers of annotation, the program\n",
      "would need to know how to ignore them when the data was loaded,\n",
      "but not invalidate or obliterate those layers when the tree data\n",
      "was saved back to the file.\n",
      "Another response has been to write one-off scripts\n",
      "to manipulate corpus formats; such scripts litter the filespaces of many\n",
      "NLP researchers.\n",
      "NLTK's corpus readers are a more systematic\n",
      "approach, founded on the premise that the work of parsing a corpus format\n",
      "should only be done once (per programming language).\n",
      "\n",
      "\n",
      "Figure 3.2: A Common Format vs A Common Interface\n",
      "\n",
      "Instead of focussing on a common format, we believe it is more promising to\n",
      "develop a common interface (cf. nltk.corpus).  Consider the\n",
      "case of treebanks, an important corpus type for work in NLP.\n",
      "There are many ways to store a phrase structure tree in a file.\n",
      "We can use nested parentheses, or nested XML elements,\n",
      "or a dependency notation with a (child-id, parent-id) pair on each line,\n",
      "or an XML version of the dependency notation, etc.\n",
      "However, in each case the logical structure is almost the same.\n",
      "It is much easier to devise a common interface that allows\n",
      "application programmers to write code to access tree data\n",
      "using methods such as children(), leaves(), depth(), and\n",
      "so forth.\n",
      "Note that this approach follows accepted practice within\n",
      "computer science, viz. abstract data types, object oriented design,\n",
      "and the three layer architecture (3.2).\n",
      "The last of these — from the world of relational databases —\n",
      "allows end-user applications to use a common model (the \"relational model\")\n",
      "and a common language (SQL), to abstract away from the idiosyncrasies\n",
      "of file storage, and allowing innovations in filesystem technologies\n",
      "to occur without disturbing end-user applications.\n",
      "In the same way, a common corpus interface\n",
      "insulates application programs from data formats.\n",
      "In this context, when creating a new corpus for dissemination, it is\n",
      "expedient to use an existing widely-used format wherever possible.\n",
      "When this is not possible, the corpus could be accompanied with software\n",
      "— such as an nltk.corpus module — that supports\n",
      "existing interface methods.\n",
      "\n",
      "\n",
      "3.7   Special Considerations when Working with Endangered Languages\n",
      "The importance of language to science and the arts is matched in\n",
      "significance by the cultural treasure embodied in language.\n",
      "Each of the world's ~7,000 human languages is rich in unique respects,\n",
      "in its oral histories and creation legends, down to its grammatical\n",
      "constructions and its very words and their nuances of meaning.\n",
      "Threatened remnant cultures have words to distinguish plant subspecies\n",
      "according to therapeutic uses that are unknown to science.  Languages\n",
      "evolve over time as they come into contact with each other, and each\n",
      "one provides a unique window onto human pre-history.\n",
      "In many parts of the world, small linguistic variations\n",
      "from one town to the next add up to a completely different language in\n",
      "the space of a half-hour drive.  For its breathtaking complexity and\n",
      "diversity, human language is as a colorful tapestry stretching\n",
      "through time and space.\n",
      "However, most of the world's languages face extinction.\n",
      "In response to this, many linguists are hard at work documenting the languages, constructing\n",
      "rich records of this important facet of the world's linguistic heritage.\n",
      "What can the field of NLP offer to help with this effort?  Developing\n",
      "taggers, parsers, named-entity recognizers, etc,\n",
      "is not an early priority, and there is usually insufficient data for\n",
      "developing such tools in any case.  Instead, the most frequently voiced need is\n",
      "to have better tools for collecting and curating data, with a focus\n",
      "on texts and lexicons.\n",
      "On the face of things, it should be a straightforward matter to start collecting texts\n",
      "in an endangered language.  Even if we ignore vexed issues such as who\n",
      "owns the texts, and sensitivities surrounding cultural knowledge contained in the texts,\n",
      "there is the obvious practical issue of transcription.\n",
      "Most languages lack a standard orthography.  When a language\n",
      "has no literary tradition, the conventions of spelling and punctuation\n",
      "are not well-established.  Therefore it is common practice\n",
      "to create a lexicon in tandem with a text collection, continually updating\n",
      "the lexicon as new words appear in the texts.  This work could be done\n",
      "using a text processor (for the texts) and a spreadsheet (for the lexicon).\n",
      "Better still, SIL's free linguistic software Toolbox and Fieldworks\n",
      "provide sophisticated support for integrated creation of texts and lexicons.\n",
      "When speakers of the language in question are trained to enter texts themselves,\n",
      "a common obstacle is an overriding concern for correct spelling.  Having\n",
      "a lexicon greatly helps this process, but we need to have lookup methods\n",
      "that do not assume someone can determine the citation form of an arbitrary\n",
      "word.  The problem may be acute for languages having a complex morphology that\n",
      "includes prefixes.  In such cases it helps to tag lexical items with\n",
      "semantic domains, and to permit lookup by semantic domain or by gloss.\n",
      "Permitting lookup by pronunciation similarity is also a big help.\n",
      "Here's a simple demonstration of how to do this.\n",
      "The first step is to identify confusible letter sequences,\n",
      "and map complex versions to simpler versions.  We might also notice\n",
      "that the relative order of letters within a cluster of consonants\n",
      "is a source of spelling errors, and so we normalize the\n",
      "order of consonants.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> mappings = [('ph', 'f'), ('ght', 't'), ('^kn', 'n'), ('qu', 'kw'),\n",
      "...             ('[aeiou]+', 'a'), (r'(.)\\1', r'\\1')]\n",
      ">>> def signature(word):\n",
      "...     for patt, repl in mappings:\n",
      "...         word = re.sub(patt, repl, word)\n",
      "...     pieces = re.findall('[^aeiou]+', word)\n",
      "...     return ''.join(char for piece in pieces for char in sorted(piece))[:8]\n",
      ">>> signature('illefent')\n",
      "'lfnt'\n",
      ">>> signature('ebsekwieous')\n",
      "'bskws'\n",
      ">>> signature('nuculerr')\n",
      "'nclr'\n",
      "\n",
      "\n",
      "\n",
      "Next, we create a mapping from signatures to words, for all the words\n",
      "in our lexicon.  We can use this to get candidate corrections for a\n",
      "given input word (but we must first compute that word's signature).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> signatures = nltk.Index((signature(w), w) for w in nltk.corpus.words.words())\n",
      ">>> signatures[signature('nuculerr')]\n",
      "['anicular', 'inocular', 'nucellar', 'nuclear', 'unicolor', 'uniocular', 'unocular']\n",
      "\n",
      "\n",
      "\n",
      "Finally, we should rank the results in terms of similarity with the original word.\n",
      "This is done by the function rank().  The only remaining function provides a\n",
      "simple interface to the user:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> def rank(word, wordlist):\n",
      "...     ranked = sorted((nltk.edit_distance(word, w), w) for w in wordlist)\n",
      "...     return [word for (_, word) in ranked]\n",
      ">>> def fuzzy_spell(word):\n",
      "...     sig = signature(word)\n",
      "...     if sig in signatures:\n",
      "...         return rank(word, signatures[sig])\n",
      "...     else:\n",
      "...         return []\n",
      ">>> fuzzy_spell('illefent')\n",
      "['olefiant', 'elephant', 'oliphant', 'elephanta']\n",
      ">>> fuzzy_spell('ebsekwieous')\n",
      "['obsequious']\n",
      ">>> fuzzy_spell('nucular')\n",
      "['anicular', 'inocular', 'nucellar', 'nuclear', 'unocular', 'uniocular', 'unicolor']\n",
      "\n",
      "\n",
      "\n",
      "This is just one illustration where a simple program can facilitate access to lexical\n",
      "data in a context where the writing system of a language may not be standardized, or\n",
      "where users of the language may not have a good command of spellings.  Other simple\n",
      "applications of NLP in this area include: building indexes to facilitate access to\n",
      "data, gleaning wordlists from texts,\n",
      "locating examples of word usage in constructing a lexicon, detecting prevalent or\n",
      "exceptional patterns in poorly understood data, and performing specialized validation\n",
      "on data created using various linguistic software tools.  We will return to the last\n",
      "of these in 5.\n",
      "\n",
      "\n",
      "\n",
      "4   Working with XML\n",
      "The Extensible Markup Language (XML) provides a framework for designing\n",
      "domain-specific markup languages.  It is sometimes used for representing\n",
      "annotated text and for lexical resources.  Unlike HTML with its predefined\n",
      "tags, XML permits us to make up our own tags.  Unlike a database, XML\n",
      "permits us to create data without first specifying its structure, and it\n",
      "permits us to have optional and repeatable elements.  In this section\n",
      "we briefly review some features of XML that are relevant for representing\n",
      "linguistic data, and show how to access data stored in XML files using\n",
      "Python programs.\n",
      "\n",
      "4.1   Using XML for Linguistic Structures\n",
      "Thanks to its flexibility and extensibility, XML is a natural\n",
      "choice for representing linguistic structures.  Here's an example\n",
      "of a simple lexical entry.\n",
      "\n",
      "  (2)\n",
      "<entry>\n",
      "  <headword>whale</headword>\n",
      "  <pos>noun</pos>\n",
      "  <gloss>any of the larger cetacean mammals having a streamlined\n",
      "    body and breathing through a blowhole on the head</gloss>\n",
      "</entry>\n",
      "\n",
      "\n",
      "It consists of a series of XML tags enclosed in angle brackets.\n",
      "Each opening tag, like <gloss> is matched with a closing tag, like </gloss>;\n",
      "together they constitute an XML element.\n",
      "The above example has been laid out nicely using whitespace, but it could\n",
      "equally have been put on a single, long line.  Our approach to processing\n",
      "XML will usually not be sensitive to whitespace.  In order for XML to be\n",
      "well formed, all opening tags must have corresponding closing tags, at the\n",
      "same level of nesting (i.e. the XML document must be a well-formed tree).\n",
      "XML permits us to repeat elements, e.g. to add another gloss field as we see below.\n",
      "We will use different whitespace to underscore the point that layout\n",
      "does not matter.\n",
      "\n",
      "  (3)\n",
      "<entry><headword>whale</headword><pos>noun</pos><gloss>any of the\n",
      "larger cetacean mammals having a streamlined body and breathing\n",
      "through a blowhole on the head</gloss><gloss>a very large person;\n",
      "impressive in size or qualities</gloss></entry>\n",
      "\n",
      "\n",
      "A further step might be to link our lexicon to some external resource, such as WordNet,\n",
      "using external identifiers.  In (4) we group the gloss and a synset identifier\n",
      "inside a new element which we have called \"sense\".\n",
      "\n",
      "  (4)\n",
      "<entry>\n",
      "  <headword>whale</headword>\n",
      "  <pos>noun</pos>\n",
      "  <sense>\n",
      "    <gloss>any of the larger cetacean mammals having a streamlined\n",
      "      body and breathing through a blowhole on the head</gloss>\n",
      "    <synset>whale.n.02</synset>\n",
      "  </sense>\n",
      "  <sense>\n",
      "    <gloss>a very large person; impressive in size or qualities</gloss>\n",
      "    <synset>giant.n.04</synset>\n",
      "  </sense>\n",
      "</entry>\n",
      "\n",
      "\n",
      "Alternatively, we could have represented the synset identifier using an XML attribute,\n",
      "without the need for any nested structure, as in (5).\n",
      "\n",
      "  (5)\n",
      "<entry>\n",
      "  <headword>whale</headword>\n",
      "  <pos>noun</pos>\n",
      "  <gloss synset=\"whale.n.02\">any of the larger cetacean mammals having\n",
      "      a streamlined body and breathing through a blowhole on the head</gloss>\n",
      "  <gloss synset=\"giant.n.04\">a very large person; impressive in size or\n",
      "      qualities</gloss>\n",
      "</entry>\n",
      "\n",
      "\n",
      "This illustrates some of the flexibility of XML.  If it seems somewhat arbitrary\n",
      "that's because it is!  Following the rules of XML we can invent new attribute\n",
      "names, and nest them as deeply as we like.  We can repeat elements, leave them\n",
      "out, and put them in a different order each time.  We can have fields whose\n",
      "presence depends on the value of some other field, e.g. if the part of speech\n",
      "is \"verb\", then the entry can have a past_tense element to hold the\n",
      "past tense of the verb, but if the part of speech is \"noun\" no past_tense\n",
      "element is permitted.  To impose some order over all\n",
      "this freedom, we can constrain the structure of an XML file using a \"schema,\"\n",
      "which is a declaration akin to a context free grammar.  Tools exist for\n",
      "testing the validity of an XML file with respect to a schema.\n",
      "\n",
      "\n",
      "4.2   The Role of XML\n",
      "We can use XML to represent many kinds of linguistic information.\n",
      "However, the flexibility comes at a price.  Each time we introduce a complication, such as by permitting\n",
      "an element to be optional or repeated, we make more work for any program\n",
      "that accesses the data.  We also make it more difficult to check the validity of the\n",
      "data, or to interrogate the data using one of the XML query languages.\n",
      "Thus, using XML to represent linguistic structures does not magically solve the data\n",
      "modeling problem.  We still have to work out how to structure the data,\n",
      "then define that structure with a schema, and then\n",
      "write programs to read and write the format and convert it to other formats.\n",
      "Similarly, we still need to follow some standard principles concerning\n",
      "data normalization.  It is wise to avoid making duplicate copies of the\n",
      "same information, so that we don't end up with inconsistent data when\n",
      "only one copy is changed.  For example, a cross-reference that was\n",
      "represented as <xref>headword</xref> would duplicate the storage\n",
      "of the headword of some other lexical entry, and the link would break\n",
      "if the copy of the string at the other location was modified.\n",
      "Existential dependencies between information types need to be\n",
      "modeled, so that we can't create elements without a home.\n",
      "For example, if sense definitions cannot exist independently\n",
      "of a lexical entry, the sense element can be nested inside the entry\n",
      "element.  Many-to-many relations need to be abstracted out of\n",
      "hierarchical structures.  For example, if a word can have many corresponding\n",
      "senses, and a sense can have several corresponding words, then\n",
      "both words and senses must be enumerated separately, as must the\n",
      "list of (word, sense) pairings.  This complex structure might even be split\n",
      "across three separate XML files.\n",
      "As we can see, although XML provides us with a convenient format\n",
      "accompanied by an extensive collection of tools, it offers no panacea.\n",
      "\n",
      "\n",
      "4.3   The ElementTree Interface\n",
      "Python's ElementTree module provides a convenient way to access\n",
      "data stored in XML files.  ElementTree is part of Python's\n",
      "standard library (since Python 2.5), and is also provided as\n",
      "part of NLTK in case you are using Python 2.4.\n",
      "We will illustrate the use of ElementTree using a collection\n",
      "of Shakespeare plays that have been formatted using XML.\n",
      "Let's load the XML file and inspect the raw data, first\n",
      "at the top of the file , where we see some\n",
      "XML headers and the name of a schema called play.dtd,\n",
      "followed by the root element PLAY.\n",
      "We pick it up again at the start of Act 1 .\n",
      "(Some blank lines have been omitted from the output.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> merchant_file = nltk.data.find('corpora/shakespeare/merchant.xml')\n",
      ">>> raw = open(merchant_file).read()\n",
      ">>> print(raw[:163]) \n",
      "<?xml version=\"1.0\"?>\n",
      "<?xml-stylesheet type=\"text/css\" href=\"shakes.css\"?>\n",
      "<!-- <!DOCTYPE PLAY SYSTEM \"play.dtd\"> -->\n",
      "<PLAY>\n",
      "<TITLE>The Merchant of Venice</TITLE>\n",
      ">>> print(raw[1789:2006]) \n",
      "<TITLE>ACT I</TITLE>\n",
      "<SCENE><TITLE>SCENE I.  Venice. A street.</TITLE>\n",
      "<STAGEDIR>Enter ANTONIO, SALARINO, and SALANIO</STAGEDIR>\n",
      "<SPEECH>\n",
      "<SPEAKER>ANTONIO</SPEAKER>\n",
      "<LINE>In sooth, I know not why I am so sad:</LINE>\n",
      "\n",
      "\n",
      "\n",
      "We have just accessed the XML data as a string.  As we can see,\n",
      "the string at the start of Act 1 contains XML tags for title, scene, stage directions, and so forth.\n",
      "The next step is to process the file contents as structured XML data,\n",
      "using ElementTree.  We are processing a file (a multi-line string)\n",
      "and building a tree, so its not surprising that the method name is parse .\n",
      "The variable merchant contains an XML element PLAY .\n",
      "This element has internal structure; we can use an index\n",
      "to get its first child, a TITLE element .\n",
      "We can also see the text content of this element, the title of the play .\n",
      "To get a list of all the child elements, we use the\n",
      "getchildren() method .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from xml.etree.ElementTree import ElementTree\n",
      ">>> merchant = ElementTree().parse(merchant_file) \n",
      ">>> merchant\n",
      "<Element 'PLAY' at 0x10ac43d18> # [_element-play]\n",
      ">>> merchant[0]\n",
      "<Element 'TITLE' at 0x10ac43c28> # [_element-title]\n",
      ">>> merchant[0].text\n",
      "'The Merchant of Venice' # [_element-text]\n",
      ">>> merchant.getchildren() \n",
      "[<Element 'TITLE' at 0x10ac43c28>, <Element 'PERSONAE' at 0x10ac43bd8>,\n",
      "<Element 'SCNDESCR' at 0x10b067f98>, <Element 'PLAYSUBT' at 0x10af37048>,\n",
      "<Element 'ACT' at 0x10af37098>, <Element 'ACT' at 0x10b936368>,\n",
      "<Element 'ACT' at 0x10b934b88>, <Element 'ACT' at 0x10cfd8188>,\n",
      "<Element 'ACT' at 0x10cfadb38>]\n",
      "\n",
      "\n",
      "\n",
      "The play consists of a title, the personae, a scene description, a subtitle, and five acts.\n",
      "Each act has a title and some scenes, and each scene consists of speeches which are made\n",
      "up of lines, a structure with four levels of nesting.  Let's dig down into Act IV:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> merchant[-2][0].text\n",
      "'ACT IV'\n",
      ">>> merchant[-2][1]\n",
      "<Element 'SCENE' at 0x10cfd8228>\n",
      ">>> merchant[-2][1][0].text\n",
      "'SCENE I.  Venice. A court of justice.'\n",
      ">>> merchant[-2][1][54]\n",
      "<Element 'SPEECH' at 0x10cfb02c8>\n",
      ">>> merchant[-2][1][54][0]\n",
      "<Element 'SPEAKER' at 0x10cfb0318>\n",
      ">>> merchant[-2][1][54][0].text\n",
      "'PORTIA'\n",
      ">>> merchant[-2][1][54][1]\n",
      "<Element 'LINE' at 0x10cfb0368>\n",
      ">>> merchant[-2][1][54][1].text\n",
      "\"The quality of mercy is not strain'd,\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Your Turn:\n",
      "Repeat some of the above methods, for one of the other Shakespeare plays\n",
      "included in the corpus, such as Romeo and Juliet or Macbeth;\n",
      "for a list, see nltk.corpus.shakespeare.fileids().\n",
      "\n",
      "Although we can access the entire tree this way, it is more convenient to search for\n",
      "sub-elements with particular names.  Recall that the elements at the top level have\n",
      "several types.  We can iterate over just the types we are interested in (such as\n",
      "the acts), using merchant.findall('ACT').  Here's an example of doing such\n",
      "tag-specific searches at every level of nesting:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> for i, act in enumerate(merchant.findall('ACT')):\n",
      "...     for j, scene in enumerate(act.findall('SCENE')):\n",
      "...         for k, speech in enumerate(scene.findall('SPEECH')):\n",
      "...             for line in speech.findall('LINE'):\n",
      "...                 if 'music' in str(line.text):\n",
      "...                     print(\"Act %d Scene %d Speech %d: %s\" % (i+1, j+1, k+1, line.text))\n",
      "Act 3 Scene 2 Speech 9: Let music sound while he doth make his choice;\n",
      "Act 3 Scene 2 Speech 9: Fading in music: that the comparison\n",
      "Act 3 Scene 2 Speech 9: And what is music then? Then music is\n",
      "Act 5 Scene 1 Speech 23: And bring your music forth into the air.\n",
      "Act 5 Scene 1 Speech 23: Here will we sit and let the sounds of music\n",
      "Act 5 Scene 1 Speech 23: And draw her home with music.\n",
      "Act 5 Scene 1 Speech 24: I am never merry when I hear sweet music.\n",
      "Act 5 Scene 1 Speech 25: Or any air of music touch their ears,\n",
      "Act 5 Scene 1 Speech 25: By the sweet power of music: therefore the poet\n",
      "Act 5 Scene 1 Speech 25: But music for the time doth change his nature.\n",
      "Act 5 Scene 1 Speech 25: The man that hath no music in himself,\n",
      "Act 5 Scene 1 Speech 25: Let no such man be trusted. Mark the music.\n",
      "Act 5 Scene 1 Speech 29: It is your music, madam, of the house.\n",
      "Act 5 Scene 1 Speech 32: No better a musician than the wren.\n",
      "\n",
      "\n",
      "\n",
      "Instead of navigating each step of the way down the hierarchy, we can search for\n",
      "particular embedded elements.  For example, let's examine the sequence of speakers.\n",
      "We can use a frequency distribution to see who has the most to say:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from collections import Counter\n",
      ">>> speaker_seq = [s.text for s in merchant.findall('ACT/SCENE/SPEECH/SPEAKER')]\n",
      ">>> speaker_freq = Counter(speaker_seq)\n",
      ">>> top5 = speaker_freq.most_common(5)\n",
      ">>> top5\n",
      "[('PORTIA', 117), ('SHYLOCK', 79), ('BASSANIO', 73),\n",
      "('GRATIANO', 48), ('LORENZO', 47)]\n",
      "\n",
      "\n",
      "\n",
      "We can also look for patterns in who follows who in the dialogues.\n",
      "Since there's 23 speakers, we need to reduce the \"vocabulary\"\n",
      "to a manageable size first, using the method described in\n",
      "3.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from collections import defaultdict\n",
      ">>> abbreviate = defaultdict(lambda: 'OTH')\n",
      ">>> for speaker, _ in top5:\n",
      "...     abbreviate[speaker] = speaker[:4]\n",
      "...\n",
      ">>> speaker_seq2 = [abbreviate[speaker] for speaker in speaker_seq]\n",
      ">>> cfd = nltk.ConditionalFreqDist(nltk.bigrams(speaker_seq2))\n",
      ">>> cfd.tabulate()\n",
      "     ANTO BASS GRAT  OTH PORT SHYL\n",
      "ANTO    0   11    4   11    9   12\n",
      "BASS   10    0   11   10   26   16\n",
      "GRAT    6    8    0   19    9    5\n",
      " OTH    8   16   18  153   52   25\n",
      "PORT    7   23   13   53    0   21\n",
      "SHYL   15   15    2   26   21    0\n",
      "\n",
      "\n",
      "\n",
      "Ignoring the entries for exchanges between people\n",
      "other than the top 5 (labeled OTH), the largest value suggests\n",
      "that Portia and Bassanio have the most frequent interactions.\n",
      "\n",
      "\n",
      "4.4   Using ElementTree for Accessing Toolbox Data\n",
      "In 4 we saw a simple interface\n",
      "for accessing Toolbox data, a popular and well-established format\n",
      "used by linguists for managing data.\n",
      "In this section we discuss a variety of\n",
      "techniques for manipulating Toolbox data in ways that are not supported\n",
      "by the Toolbox software.  The methods we discuss could be\n",
      "applied to other record-structured data, regardless of the actual file format.\n",
      "We can use the toolbox.xml() method to access a Toolbox\n",
      "file and load it into an elementtree object.  This file\n",
      "contains a lexicon for the Rotokas language of Papua New Guinea.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import toolbox\n",
      ">>> lexicon = toolbox.xml('rotokas.dic')\n",
      "\n",
      "\n",
      "\n",
      "There are two ways to access the contents of the lexicon object, by\n",
      "indexes and by paths.  Indexes use the familiar syntax, thus\n",
      "lexicon[3] returns entry number 3 (which is actually the fourth\n",
      "entry counting from zero); lexicon[3][0] returns its first field:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> lexicon[3][0]\n",
      "<Element 'lx' at 0x10b2f6958>\n",
      ">>> lexicon[3][0].tag\n",
      "'lx'\n",
      ">>> lexicon[3][0].text\n",
      "'kaa'\n",
      "\n",
      "\n",
      "\n",
      "The second way to access the contents of the lexicon object uses\n",
      "paths.  The lexicon is a series of record objects, each containing\n",
      "a series of field objects, such as lx and ps.  We can\n",
      "conveniently address all of the lexemes using the path record/lx.\n",
      "Here we use the findall() function to search for any matches to\n",
      "the path record/lx, and we access the text content of the element,\n",
      "normalizing it to lowercase.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> [lexeme.text.lower() for lexeme in lexicon.findall('record/lx')]\n",
      "['kaa', 'kaa', 'kaa', 'kaakaaro', 'kaakaaviko', 'kaakaavo', 'kaakaoko',\n",
      "'kaakasi', 'kaakau', 'kaakauko', 'kaakito', 'kaakuupato', ..., 'kuvuto']\n",
      "\n",
      "\n",
      "\n",
      "Let's view the Toolbox data in XML format.  The write() method of\n",
      "ElementTree expects a file object.  We usually create one of these\n",
      "using Python's built-in open() function.  In order to see the output\n",
      "displayed on the screen, we can use a special pre-defined file object\n",
      "called stdout  (standard output), defined in Python's sys module.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> import sys\n",
      ">>> from nltk.util import elementtree_indent\n",
      ">>> from xml.etree.ElementTree import ElementTree\n",
      ">>> elementtree_indent(lexicon)\n",
      ">>> tree = ElementTree(lexicon[3])\n",
      ">>> tree.write(sys.stdout, encoding='unicode') \n",
      "<record>\n",
      "  <lx>kaa</lx>\n",
      "  <ps>N</ps>\n",
      "  <pt>MASC</pt>\n",
      "  <cl>isi</cl>\n",
      "  <ge>cooking banana</ge>\n",
      "  <tkp>banana bilong kukim</tkp>\n",
      "  <pt>itoo</pt>\n",
      "  <sf>FLORA</sf>\n",
      "  <dt>12/Aug/2005</dt>\n",
      "  <ex>Taeavi iria kaa isi kovopaueva kaparapasia.</ex>\n",
      "  <xp>Taeavi i bin planim gaden banana bilong kukim tasol long paia.</xp>\n",
      "  <xe>Taeavi planted banana in order to cook it.</xe>\n",
      "</record>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4.5   Formatting Entries\n",
      "We can use the same idea we saw above to generate HTML tables instead of plain text.\n",
      "This would be useful for publishing a Toolbox lexicon on the web.\n",
      "It produces HTML elements <table>, <tr> (table row), and\n",
      "<td> (table data).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> html = \"<table>\\n\"\n",
      ">>> for entry in lexicon[70:80]:\n",
      "...     lx = entry.findtext('lx')\n",
      "...     ps = entry.findtext('ps')\n",
      "...     ge = entry.findtext('ge')\n",
      "...     html += \"  <tr><td>%s</td><td>%s</td><td>%s</td></tr>\\n\" % (lx, ps, ge)\n",
      ">>> html += \"</table>\"\n",
      ">>> print(html)\n",
      "<table>\n",
      "  <tr><td>kakae</td><td>???</td><td>small</td></tr>\n",
      "  <tr><td>kakae</td><td>CLASS</td><td>child</td></tr>\n",
      "  <tr><td>kakaevira</td><td>ADV</td><td>small-like</td></tr>\n",
      "  <tr><td>kakapikoa</td><td>???</td><td>small</td></tr>\n",
      "  <tr><td>kakapikoto</td><td>N</td><td>newborn baby</td></tr>\n",
      "  <tr><td>kakapu</td><td>V</td><td>place in sling for purpose of carrying</td></tr>\n",
      "  <tr><td>kakapua</td><td>N</td><td>sling for lifting</td></tr>\n",
      "  <tr><td>kakara</td><td>N</td><td>arm band</td></tr>\n",
      "  <tr><td>Kakarapaia</td><td>N</td><td>village name</td></tr>\n",
      "  <tr><td>kakarau</td><td>N</td><td>frog</td></tr>\n",
      "</table>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5   Working with Toolbox Data\n",
      "Given the popularity of Toolbox amongst linguists, we will discuss some further\n",
      "methods for working with Toolbox data.  Many of the methods discussed in previous\n",
      "chapters, such as counting, building frequency distributions, tabulating co-occurrences,\n",
      "can be applied to the content of Toolbox entries.  For example, we can trivially\n",
      "compute the average number of fields for each entry:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from nltk.corpus import toolbox\n",
      ">>> lexicon = toolbox.xml('rotokas.dic')\n",
      ">>> sum(len(entry) for entry in lexicon) / len(lexicon)\n",
      "13.635...\n",
      "\n",
      "\n",
      "\n",
      "In this section we will discuss two tasks that arise in the context of documentary\n",
      "linguistics, neither of which is supported by the Toolbox software.\n",
      "\n",
      "5.1   Adding a Field to Each Entry\n",
      "It is often convenient to add new fields that are derived automatically from\n",
      "existing ones.  Such fields often facilitate search and analysis.\n",
      "For instance, in 5.1 we define a function cv() which\n",
      "maps a string of consonants and vowels to the corresponding CV sequence,\n",
      "e.g. kakapua would map to CVCVCVV.\n",
      "This mapping has four steps.  First, the string is converted to lowercase,\n",
      "then we replace any non-alphabetic characters [^a-z] with an underscore.\n",
      "Next, we replace all vowels with V.  Finally, anything that is not\n",
      "a V or an underscore must be a consonant, so we replace it with a C.\n",
      "Now, we can scan the lexicon and add a new cv field after every lx field.\n",
      "5.1 shows what this does to a particular entry; note\n",
      "the last line of output, which shows the new cv field.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "from xml.etree.ElementTree import SubElement\n",
      "\n",
      "def cv(s):\n",
      "    s = s.lower()\n",
      "    s = re.sub(r'[^a-z]',     r'_', s)\n",
      "    s = re.sub(r'[aeiou]',    r'V', s)\n",
      "    s = re.sub(r'[^V_]',      r'C', s)\n",
      "    return (s)\n",
      "\n",
      "def add_cv_field(entry):\n",
      "    for field in entry:\n",
      "        if field.tag == 'lx':\n",
      "            cv_field = SubElement(entry, 'cv')\n",
      "            cv_field.text = cv(field.text)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> lexicon = toolbox.xml('rotokas.dic')\n",
      ">>> add_cv_field(lexicon[53])\n",
      ">>> print(nltk.toolbox.to_sfm_string(lexicon[53]))\n",
      "\\lx kaeviro\n",
      "\\ps V\n",
      "\\pt A\n",
      "\\ge lift off\n",
      "\\ge take off\n",
      "\\tkp go antap\n",
      "\\sc MOTION\n",
      "\\vx 1\n",
      "\\nt used to describe action of plane\n",
      "\\dt 03/Jun/2005\n",
      "\\ex Pita kaeviroroe kepa kekesia oa vuripierevo kiuvu.\n",
      "\\xp Pita i go antap na lukim haus win i bagarapim.\n",
      "\\xe Peter went to look at the house that the wind destroyed.\n",
      "\\cv CVVCVCV\n",
      "\n",
      "\n",
      "Example 5.1 (code_add_cv_field.py): Figure 5.1: Adding a new cv field to a lexical entry\n",
      "\n",
      "\n",
      "Note\n",
      "If a Toolbox file is being continually updated, the program in\n",
      "code-add-cv-field will need to be run more than once.  It would\n",
      "be possible to modify add_cv_field() to modify the contents\n",
      "of an existing entry.  However, it is a safer practice to use such\n",
      "programs to create enriched files for the purpose of data analysis,\n",
      "without replacing the manually curated source files.\n",
      "\n",
      "\n",
      "\n",
      "5.2   Validating a Toolbox Lexicon\n",
      "Many lexicons in Toolbox format do not conform to any particular schema.\n",
      "Some entries may include extra fields, or may order existing fields\n",
      "in a new way.\n",
      "Manually inspecting thousands of lexical entries is not practicable.\n",
      "However, we can easily identify frequent field sequences,\n",
      "with the help of a Counter:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from collections import Counter\n",
      ">>> field_sequences = Counter(':'.join(field.tag for field in entry) for entry in lexicon)\n",
      ">>> field_sequences.most_common()\n",
      "[('lx:ps:pt:ge:tkp:dt:ex:xp:xe', 41), ('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe', 37),\n",
      "('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 27), ('lx:ps:pt:ge:tkp:nt:dt:ex:xp:xe', 20), ...]\n",
      "\n",
      "\n",
      "\n",
      "After inspecting these field sequences we could devise a context\n",
      "free grammar for lexical entries.  The grammar in 5.2\n",
      "uses the CFG format we saw in 8..  Such a grammar models the implicit\n",
      "nested structure of Toolbox entries, and builds a tree structure in which the\n",
      "leaves of the tree are individual field names.  Finally, we iterate\n",
      "over the entries and report their conformance with the grammar, as\n",
      "shown in 5.2.\n",
      "Those that are accepted by the grammar are prefixed with a '+' ,\n",
      "and those that are rejected are prefixed with a '-' .\n",
      "During the process of developing such a grammar\n",
      "it helps to filter out some of the tags .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "grammar = nltk.CFG.fromstring('''\n",
      "  S -> Head PS Glosses Comment Date Sem_Field Examples\n",
      "  Head -> Lexeme Root\n",
      "  Lexeme -> \"lx\"\n",
      "  Root -> \"rt\" |\n",
      "  PS -> \"ps\"\n",
      "  Glosses -> Gloss Glosses |\n",
      "  Gloss -> \"ge\" | \"tkp\" | \"eng\"\n",
      "  Date -> \"dt\"\n",
      "  Sem_Field -> \"sf\"\n",
      "  Examples -> Example Ex_Pidgin Ex_English Examples |\n",
      "  Example -> \"ex\"\n",
      "  Ex_Pidgin -> \"xp\"\n",
      "  Ex_English -> \"xe\"\n",
      "  Comment -> \"cmt\" | \"nt\" |\n",
      "  ''')\n",
      "\n",
      "def validate_lexicon(grammar, lexicon, ignored_tags):\n",
      "    rd_parser = nltk.RecursiveDescentParser(grammar)\n",
      "    for entry in lexicon:\n",
      "        marker_list = [field.tag for field in entry if field.tag not in ignored_tags]\n",
      "        if list(rd_parser.parse(marker_list)):\n",
      "            print(\"+\", ':'.join(marker_list)) \n",
      "        else:\n",
      "            print(\"-\", ':'.join(marker_list)) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> lexicon = toolbox.xml('rotokas.dic')[10:20]\n",
      ">>> ignored_tags = ['arg', 'dcsv', 'pt', 'vx'] \n",
      ">>> validate_lexicon(grammar, lexicon, ignored_tags)\n",
      "- lx:ps:ge:tkp:sf:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe\n",
      "- lx:rt:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe\n",
      "- lx:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe\n",
      "- lx:ps:ge:tkp:nt:sf:dt\n",
      "- lx:ps:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe\n",
      "- lx:ps:ge:ge:ge:tkp:cmt:dt:ex:xp:xe\n",
      "- lx:rt:ps:ge:ge:tkp:dt\n",
      "- lx:rt:ps:ge:eng:eng:eng:ge:tkp:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe\n",
      "- lx:rt:ps:ge:tkp:dt:ex:xp:xe\n",
      "- lx:ps:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe\n",
      "\n",
      "\n",
      "Example 5.2 (code_toolbox_validation.py): Figure 5.2: Validating Toolbox Entries Using a Context Free Grammar\n",
      "\n",
      "Another approach would be to use a chunk parser (7.),\n",
      "since these are much more effective at identifying partial\n",
      "structures, and can report the partial structures that have\n",
      "been identified.  In 5.3 we set up\n",
      "a chunk grammar for the entries of a lexicon, then parse each entry.\n",
      "A sample of the output from this program is shown in 5.4.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "grammar = r\"\"\"\n",
      "      lexfunc: {<lf>(<lv><ln|le>*)*}\n",
      "      example: {<rf|xv><xn|xe>*}\n",
      "      sense:   {<sn><ps><pn|gv|dv|gn|gp|dn|rn|ge|de|re>*<example>*<lexfunc>*}\n",
      "      record:   {<lx><hm><sense>+<dt>}\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ">>> from xml.etree.ElementTree import ElementTree\n",
      ">>> from nltk.toolbox import ToolboxData\n",
      ">>> db = ToolboxData()\n",
      ">>> db.open(nltk.data.find('corpora/toolbox/iu_mien_samp.db'))\n",
      ">>> lexicon = db.parse(grammar, encoding='utf8')\n",
      ">>> tree = ElementTree(lexicon)\n",
      ">>> with open(\"iu_mien_samp.xml\", \"wb\") as output:\n",
      "...     tree.write(output)\n",
      "\n",
      "\n",
      "Example 5.3 (code_chunk_toolbox.py): Figure 5.3: Chunking a Toolbox Lexicon: A chunk grammar describing the structure of\n",
      "entries for a lexicon for Iu Mien, a language of China.\n",
      "\n",
      "\n",
      "\n",
      "Figure 5.4: XML Representation of a Lexical Entry, Resulting from Chunk Parsing a Toolbox Record\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6   Describing Language Resources using OLAC Metadata\n",
      "Members of the NLP community have a common need for\n",
      "discovering language resources with high precision and recall.\n",
      "The solution which has been developed by the Digital Libraries\n",
      "community involves metadata aggregation.\n",
      "\n",
      "6.1   What is Metadata?\n",
      "The simplest definition of metadata is \"structured data about data.\"\n",
      "Metadata is descriptive information about an object or resource whether\n",
      "it be physical or electronic. While the term metadata itself is relatively new,\n",
      "the underlying concepts behind metadata have been in use for as long as\n",
      "collections of information have been organized.\n",
      "Library catalogs represent a well-established type of metadata;\n",
      "they have served as collection management and resource discovery\n",
      "tools for decades. Metadata can be generated either\n",
      "\"by hand\" or generated automatically using software.\n",
      "The Dublin Core Metadata Initiative began in 1995 to develop\n",
      "conventions for resource discovery on the web.\n",
      "The Dublin Core metadata elements represent a broad,\n",
      "interdisciplinary consensus about the core set of elements\n",
      "that are likely to be widely useful to support resource discovery.\n",
      "The Dublin Core consists of 15 metadata elements, where each\n",
      "element is optional and repeatable: Title, Creator, Subject,\n",
      "Description, Publisher, Contributor, Date, Type, Format,\n",
      "Identifier, Source, Language, Relation, Coverage, Rights.\n",
      "This metadata set can be used to describe resources that\n",
      "exist in digital or traditional formats.\n",
      "The Open Archives initiative (OAI) provides a common framework\n",
      "across digital repositories of scholarly materials regardless of their type,\n",
      "including documents, data, software, recordings, physical artifacts,\n",
      "digital surrogates, and so forth.\n",
      "Each repository consists of a network accessible server offering\n",
      "public access to archived items.  Each item has a unique identifier,\n",
      "and is associated with a Dublin Core metadata record (and possibly additional\n",
      "records in other formats).  The OAI defines a protocol for metadata search\n",
      "services to \"harvest\" the contents of repositories.\n",
      "\n",
      "\n",
      "6.2   OLAC: Open Language Archives Community\n",
      "The Open Language Archives Community (OLAC) is an international\n",
      "partnership of institutions and individuals who are creating a\n",
      "worldwide virtual library of language resources by:\n",
      "(i) developing consensus on best current practice for\n",
      "the digital archiving of language resources, and\n",
      "(ii) developing a network of interoperating repositories\n",
      "and services for housing and accessing such resources.\n",
      "OLAC's home on the web is at http://www.language-archives.org/.\n",
      "OLAC Metadata is a standard for describing language resources.\n",
      "Uniform description across repositories is ensured by limiting\n",
      "the values of certain metadata elements to the use of terms\n",
      "from controlled vocabularies. OLAC metadata can be used to\n",
      "describe data and tools, in both physical and digital formats.\n",
      "OLAC metadata extends the Dublin Core Metadata Set,\n",
      "a widely accepted standard for describing resources of all types.\n",
      "To this core set, OLAC adds descriptors to cover fundamental\n",
      "properties of language resources, such as subject language and\n",
      "linguistic type.  Here's an example of a complete OLAC record:\n",
      "\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<olac:olac xmlns:olac=\"http://www.language-archives.org/OLAC/1.1/\"\n",
      "           xmlns=\"http://purl.org/dc/elements/1.1/\"\n",
      "           xmlns:dcterms=\"http://purl.org/dc/terms/\"\n",
      "           xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n",
      "           xsi:schemaLocation=\"http://www.language-archives.org/OLAC/1.1/\n",
      "                http://www.language-archives.org/OLAC/1.1/olac.xsd\">\n",
      "  <title>A grammar of Kayardild. With comparative notes on Tangkic.</title>\n",
      "  <creator>Evans, Nicholas D.</creator>\n",
      "  <subject>Kayardild grammar</subject>\n",
      "  <subject xsi:type=\"olac:language\" olac:code=\"gyd\">Kayardild</subject>\n",
      "  <language xsi:type=\"olac:language\" olac:code=\"en\">English</language>\n",
      "  <description>Kayardild Grammar (ISBN 3110127954)</description>\n",
      "  <publisher>Berlin - Mouton de Gruyter</publisher>\n",
      "  <contributor xsi:type=\"olac:role\" olac:code=\"author\">Nicholas Evans</contributor>\n",
      "  <format>hardcover, 837 pages</format>\n",
      "  <relation>related to ISBN 0646119966</relation>\n",
      "  <coverage>Australia</coverage>\n",
      "  <type xsi:type=\"olac:linguistic-type\" olac:code=\"language_description\"/>\n",
      "  <type xsi:type=\"dcterms:DCMIType\">Text</type>\n",
      "</olac:olac>\n",
      "\n",
      "Participating language archives publish their catalogs in an XML\n",
      "format, and these records are regularly \"harvested\" by OLAC services\n",
      "using the OAI protocol.  In addition to this software infrastructure,\n",
      "OLAC has documented a series of best practices for describing\n",
      "language resources, through a process that involved extended\n",
      "consultation with the language resources community (e.g. see\n",
      "http://www.language-archives.org/REC/bpr.html).\n",
      "OLAC repositories can be searched using a query engine on the OLAC website.\n",
      "Searching for \"German lexicon\" finds the following resources, amongst others:\n",
      "\n",
      "CALLHOME German Lexicon\n",
      "http://www.language-archives.org/item/oai:www.ldc.upenn.edu:LDC97L18\n",
      "MULTILEX multilingual lexicon\n",
      "http://www.language-archives.org/item/oai:elra.icp.inpg.fr:M0001\n",
      "Slelex Siemens Phonetic lexicon\n",
      "http://www.language-archives.org/item/oai:elra.icp.inpg.fr:S0048\n",
      "\n",
      "Searching for \"Korean\" finds a newswire corpus, a treebank, a lexicon,\n",
      "a child-language corpus, interlinear glossed texts.  It also finds software\n",
      "including a syntactic analyzer and a morphological analyzer.\n",
      "Observe that the above URLs include a substring of the form:\n",
      "oai:www.ldc.upenn.edu:LDC97L18.  This is an OAI identifier,\n",
      "using a URI scheme registered with ICANN\n",
      "(the Internet Corporation for Assigned Names and Numbers).\n",
      "These identifiers have the format oai:archive:local_id,\n",
      "where oai is the name of the URI scheme,\n",
      "archive is an archive identifier such as www.ldc.upenn.edu,\n",
      "and local_id is the resource identifier assigned by the archive, e.g. LDC97L18.\n",
      "Given an OAI identifier for an OLAC resource, it is possible to retrieve\n",
      "the complete XML record for the resource using a URL of the following form:\n",
      "\n",
      "http://www.language-archives.org/static-records/oai:archive:local_id\n",
      "\n",
      "\n",
      "\n",
      "6.3   Disseminating Language Resources\n",
      "The Linguistic Data Consortium hosts the NLTK Data Repository,\n",
      "an open-access archive where community members can upload corpora and\n",
      "saved models.  These resources can be easily accessed using NLTK's\n",
      "downloader tool.\n",
      "\n",
      "\n",
      "\n",
      "7   Summary\n",
      "\n",
      "Fundamental data types, present in most corpora, are annotated texts\n",
      "and lexicons.  Texts have a temporal structure, while lexicons have a\n",
      "record structure.\n",
      "The lifecycle of a corpus includes data collection, annotation, quality\n",
      "control, and publication.  The lifecycle continues after publication as\n",
      "the corpus is modified and enriched during the course of research.\n",
      "Corpus development involves a balance between capturing a representative\n",
      "sample of language usage, and capturing enough material from any one\n",
      "source or genre to be useful; multiplying out the dimensions of\n",
      "variability is usually not feasible because of resource limitations.\n",
      "XML provides a useful format for the storage and interchange of linguistic\n",
      "data, but provides no shortcuts for solving pervasive data modeling problems.\n",
      "Toolbox format is widely used in language documentation projects; we can\n",
      "write programs to support the curation of Toolbox files, and to convert\n",
      "them to XML.\n",
      "The Open Language Archives Community (OLAC) provides an infrastructure for\n",
      "documenting and discovering language resources.\n",
      "\n",
      "\n",
      "\n",
      "8   Further Reading\n",
      "Extra materials for this chapter are posted at http://nltk.org/, including links to freely\n",
      "available resources on the web.\n",
      "The primary sources of linguistic corpora are the Linguistic Data Consortium and\n",
      "the European Language Resources Agency, both with extensive online catalogs.\n",
      "More details concerning the major corpora mentioned in the chapter are available:\n",
      "American National Corpus (Reppen, Ide, & Suderman, 2005),\n",
      "British National Corpus ({BNC}, 1999),\n",
      "Thesaurus Linguae Graecae ({TLG}, 1999),\n",
      "Child Language Data Exchange System (CHILDES) (MacWhinney, 1995),\n",
      "TIMIT (S., Lamel, & William, 1986).\n",
      "Two special interest groups of the Association for Computational Linguistics\n",
      "that organize regular workshops with published proceedings are\n",
      "SIGWAC, which promotes the use of the web as a corpus and has\n",
      "sponsored the CLEANEVAL task for removing HTML markup,\n",
      "and SIGANN, which is encouraging efforts towards interoperability\n",
      "of linguistic annotations.\n",
      "Full details of the Toolbox data format are provided with the distribution (Buseman, Buseman, & Early, 1996),\n",
      "and with the latest distribution, freely available from http://www.sil.org/computing/toolbox/.\n",
      "For guidelines on the process of constructing a Toolbox lexicon see\n",
      "http://www.sil.org/computing/ddp/.\n",
      "More examples of our efforts with the Toolbox\n",
      "are documented in (Tamanji, Hirotani, & Hall, 1999), (Robinson, Aumann, & Bird, 2007).\n",
      "Dozens of other tools for linguistic data management are available, some\n",
      "surveyed by (Bird & Simons, 2003).\n",
      "See also the proceedings of the \"LaTeCH\" workshops on\n",
      "language technology for cultural heritage data.\n",
      "There are many excellent resources for XML (e.g. http://zvon.org/)\n",
      "and for writing Python programs to work with XML.  Many editors have XML modes.\n",
      "XML formats for lexical information include\n",
      "OLIF http://www.olif.net/\n",
      "and LIFT http://code.google.com/p/lift-standard/.\n",
      "For a survey of linguistic annotation software, see the\n",
      "Linguistic Annotation Page at http://www.ldc.upenn.edu/annotation/.\n",
      "The initial proposal for standoff annotation was (Thompson & McKelvie, 1997).\n",
      "An abstract data model for linguistic annotations, called\n",
      "\"annotation graphs\", was proposed in (Bird & Liberman, 2001).\n",
      "A general-purpose ontology for linguistic description (GOLD) is\n",
      "documented at http://www.linguistics-ontology.org/.\n",
      "For guidance on planning and constructing a corpus, see (Meyer, 2002), (Farghaly, 2003)\n",
      "More details of methods for scoring inter-annotator agreement are available\n",
      "in (Artstein & Poesio, 2008), (Pevzner & Hearst, 2002).\n",
      "Rotokas data was provided by Stuart Robinson, and Iu Mien data was provided by Greg Aumann.\n",
      "For more information about the Open Language Archives Community, visit\n",
      "http://www.language-archives.org/, or see (Simons & Bird, 2003).\n",
      "\n",
      "\n",
      "9   Exercises\n",
      "\n",
      "◑ In 5.1 the new field appeared at the bottom of the\n",
      "entry.  Modify this program so that it inserts the new subelement right after\n",
      "the lx field.  (Hint: create the new cv field using Element('cv'),\n",
      "assign a text value to it, then use the insert() method of the parent element.)\n",
      "\n",
      "◑ Write a function that deletes a specified field from a lexical entry.\n",
      "(We could use this to sanitize our lexical data before giving it to others,\n",
      "e.g. by removing fields containing irrelevant or uncertain content.)\n",
      "\n",
      "◑ Write a program that scans an HTML dictionary file to find entries\n",
      "having an illegal part-of-speech field, and reports the headword for\n",
      "each entry.\n",
      "\n",
      "◑ Write a program to find any parts of speech (ps field) that\n",
      "occurred less than ten times.  Perhaps these are typing mistakes?\n",
      "\n",
      "◑ We saw a method for discovering cases of whole-word reduplication.\n",
      "Write a function to find words that may contain partial\n",
      "reduplication.  Use the re.search() method, and the following\n",
      "regular expression: (..+)\\1\n",
      "\n",
      "◑ We saw a method for adding a cv field.  There is an interesting\n",
      "issue with keeping this up-to-date when someone modifies the content\n",
      "of the lx field on which it is based.  Write a version of this\n",
      "program to add a cv field, replacing any existing cv field.\n",
      "\n",
      "◑ Write a function to add a new field syl which gives a count of\n",
      "the number of syllables in the word.\n",
      "\n",
      "◑ Write a function which displays the complete entry for a lexeme.\n",
      "When the lexeme is incorrectly spelled it should display the entry\n",
      "for the most similarly spelled lexeme.\n",
      "\n",
      "◑ Write a function that takes a lexicon and finds which pairs of\n",
      "consecutive fields are most frequent (e.g. ps is often followed by pt).\n",
      "(This might help us to discover some of the structure of a lexical entry.)\n",
      "\n",
      "◑ Create a spreadsheet using office software, containing one lexical entry per\n",
      "row, consisting of a headword, a part of speech, and a gloss.  Save the\n",
      "spreadsheet in CSV format.  Write Python code to read the CSV file and print it in\n",
      "Toolbox format, using lx for the headword, ps for the part of speech,\n",
      "and gl for the gloss.\n",
      "\n",
      "◑ Index the words of Shakespeare's plays, with the help of nltk.Index.\n",
      "The resulting data structure should permit lookup on individual words such as music,\n",
      "returning a list of references to acts, scenes and speeches, of the form\n",
      "[(3, 2, 9), (5, 1, 23), ...], where (3, 2, 9) indicates\n",
      "Act 3 Scene 2 Speech 9.\n",
      "\n",
      "◑ Construct a conditional frequency distribution which records the word length\n",
      "for each speech in The Merchant of Venice, conditioned on the name of the character,\n",
      "e.g. cfd['PORTIA'][12] would give us the number of speeches by Portia\n",
      "consisting of 12 words.\n",
      "\n",
      "★ Obtain a comparative wordlist in CSV format, and write a program\n",
      "that prints those cognates having an edit-distance of at least three\n",
      "from each other.\n",
      "\n",
      "★ Build an index of those lexemes which appear in example sentences.\n",
      "Suppose the lexeme for a given entry is w.\n",
      "Then add a single cross-reference field xrf to this entry, referencing\n",
      "the headwords of other entries having example sentences containing\n",
      "w.  Do this for all entries and save the result as a toolbox-format file.\n",
      "\n",
      "◑ Write a recursive function to produce an XML representation for a\n",
      "tree, with non-terminals represented as XML elements, and leaves represented\n",
      "as text content, e.g.:\n",
      "\n",
      "<S>\n",
      "  <NP type=\"SBJ\">\n",
      "    <NP>\n",
      "      <NNP>Pierre</NNP>\n",
      "      <NNP>Vinken</NNP>\n",
      "    </NP>\n",
      "    <COMMA>,</COMMA>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "About this document...\n",
      "UPDATED FOR NLTK 3.0.\n",
      "This is a chapter from Natural Language Processing with Python,\n",
      "by Steven Bird, Ewan Klein and Edward Loper,\n",
      "Copyright © 2019 the authors.\n",
      "It is distributed with the Natural Language Toolkit [http://nltk.org/],\n",
      "Version 3.0, under the terms of the\n",
      "Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\n",
      "[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\n",
      "This document was built on\n",
      "Wed  4 Sep 2019 11:40:48 ACST\n",
      "vectors[11] = [0, 1, 3, 71, 6, 90, 0, 15, 0, 0, 5, 20, 8, 1, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 1, 0, 2, 2, 4, 8, 8, 2, 2, 14, 2, 5, 2, 0, 23, 0, 19, 10, 2, 64, 4, 0, 12, 2, 6, 3, 0, 4, 2, 2, 0, 0, 4, 5, 3, 4, 2, 0, 1, 0, 43, 4, 0, 6, 0, 71, 0, 2, 15, 11, 2, 17, 16, 0, 0, 0, 0, 2, 9, 1, 0, 0, 12, 0, 6, 0, 1, 0, 0, 0, 0, 168, 8, 2, 4, 27, 4, 13, 0, 4, 1, 0, 10, 1, 2, 0, 55, 6, 6, 0, 5, 3, 4, 0, 0, 0, 104, 0, 2, 7, 1, 2, 0, 0, 1, 12, 4, 17, 17, 5, 4, 1, 188, 39, 37, 7, 2, 17, 115, 4, 2, 0, 26, 26, 24, 9, 5, 8, 8, 0, 0, 0, 0, 0, 0, 0, 3, 17, 0, 3, 0, 0, 1, 3, 1, 0, 0, 0, 0, 24, 0, 2, 27, 5, 5, 4, 3, 3, 0, 1, 0, 2, 0, 2, 0, 2, 4, 23, 3, 11, 0, 1, 0, 5, 0, 29, 34, 1, 2, 1, 1, 303, 2, 8, 2, 2, 0, 3, 209, 0, 0, 1, 5, 0, 2, 3, 18, 5, 24, 8, 36, 1, 0, 1, 0, 3, 47, 5, 0, 1, 5, 2, 0, 1, 0, 0, 5, 15, 13, 1, 0, 3, 0, 0, 1, 2, 12, 4, 73, 1, 0, 0, 0, 5, 0, 1, 0, 7, 2, 1, 0, 0, 0, 2, 3, 1, 7, 0, 15, 0, 0, 0, 0, 2, 1, 39, 0, 0, 0, 5, 2, 0, 0, 7, 0, 0, 0, 6, 0, 5, 0, 5, 8, 15, 8, 0, 0, 0, 0, 0, 1, 1, 0, 10, 62, 0, 22, 0, 0, 1, 1, 0, 1, 8, 0, 2, 0, 0, 2, 5, 3, 0, 51, 11, 51, 1, 1, 3, 0, 1, 1, 1, 0, 16, 0, 3, 1, 0, 1, 9, 0, 8, 2, 0, 0, 2, 3, 3, 0, 6, 3, 15, 0, 14, 0, 1, 2, 2, 10, 0, 1, 0, 3, 0, 11, 0, 54, 3, 8, 21, 74, 0, 1, 0, 7, 0, 0, 1, 0, 4, 0, 4, 3, 0, 0, 39, 6, 5, 8, 5, 5, 25, 6, 4, 4, 1, 14, 6, 0, 0, 0, 0, 12, 0, 0, 2, 0, 2, 0, 0, 0, 0, 30, 11, 0, 0, 0, 6, 0, 4, 0, 0, 3, 1, 0, 0, 0, 8, 10, 0, 3, 0, 1, 6, 0, 1, 9, 6, 0, 1, 4, 0, 0, 0, 4, 0, 13, 0, 1, 1, 7, 2, 5, 0, 3, 0, 0, 5, 0, 5, 1, 3, 1, 16, 1, 0, 1, 0, 1, 5, 0, 2, 15, 8, 58, 0, 13, 0, 0, 107, 6, 1, 35, 7, 0, 0, 49, 1, 2, 0, 3, 1, 5, 1, 4, 0, 0, 0, 2, 11, 1, 2, 3, 2, 3, 2, 11, 4, 13, 0, 22, 3, 1, 1, 16, 3, 3, 1, 1, 1, 6, 22, 2, 0, 1, 2, 0, 0, 5, 0, 0, 6, 9, 1, 0, 1, 0, 3, 1, 0, 3, 0, 0, 0, 0, 1, 13, 0, 5, 0, 0, 3, 0, 0, 0, 8, 1, 0, 0, 0, 0, 10, 0, 0, 5, 0, 0, 10, 0, 12, 1, 0, 1, 2, 3, 3, 2, 4, 3, 2, 23, 53, 2, 0, 1, 3, 5, 4, 2, 0, 6, 4, 12, 1, 0, 1, 3, 6, 0, 3, 0, 8, 0, 5, 4, 1, 1, 0, 1, 0, 4, 7, 1, 2, 0, 0, 0, 27, 20, 1, 0, 4, 4, 44, 1, 37, 0, 1, 0, 2, 0, 0, 0, 0, 0, 8, 0, 1, 3, 0, 0, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 1, 0, 0, 5, 6, 7, 1, 1, 0, 0, 12, 2, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 0, 20, 2, 5, 0, 0, 6, 0, 3, 2, 1, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 18, 1, 6, 2, 0, 0, 0, 6, 3, 0, 2, 3, 0, 1, 2, 4, 0, 5, 1, 66, 0, 0, 0, 0, 0, 0, 0, 1, 0, 19, 14, 0, 0, 0, 0, 0, 0, 1, 0, 0, 18, 0, 0, 0, 0, 0, 6, 1, 4, 2, 3, 0, 0, 0, 3, 0, 0, 1, 0, 2, 1, 1, 1, 2, 16, 1, 0, 0, 0, 0, 0, 2, 0, 0, 2, 42, 3, 3, 3, 2, 5, 0, 0, 2, 0, 4, 1, 0, 0, 2, 1, 6, 2, 14, 5, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 14, 2, 0, 21, 4, 3, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 0, 9, 12, 17, 3, 0, 0, 0, 0, 3, 2, 3, 0, 1, 2, 1, 1, 6, 0, 0, 0, 1, 0, 2, 2, 9, 1, 5, 1, 0, 6, 10, 1, 0, 0, 1, 0, 0, 3, 0, 3, 1, 2, 1, 0, 3, 0, 0, 10, 0, 40, 1, 1, 1, 0, 6, 2, 8, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 7, 0, 0, 0, 2, 3, 1, 6, 0, 3, 0, 0, 18, 0, 2, 1, 0, 0, 0, 3, 9, 5, 22, 5, 0, 0, 1, 19, 4, 1, 1, 0, 0, 0, 0, 5, 3, 0, 1, 0, 1, 5, 0, 3, 1, 3, 0, 0, 0, 0, 0, 7, 51, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 3, 29, 1, 0, 0, 0, 1, 6, 3, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 18, 3, 3, 2, 0, 1, 5, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 4, 2, 0, 8, 0, 0, 0, 5, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 7, 0, 0, 0, 0, 0, 0, 0, 1, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 15, 12, 10, 12, 4, 6, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 37, 0, 0, 1, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 11, 0, 0, 1, 17, 1, 1, 2, 3, 29, 1, 1, 16, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 4, 1, 2, 1, 1, 0, 0, 1, 0, 7, 2, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 13, 0, 0, 2, 0, 1, 0, 0, 3, 13, 0, 0, 4, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 10, 1, 0, 1, 3, 1, 0, 0, 1, 0, 6, 0, 3, 1, 0, 15, 0, 0, 0, 0, 8, 0, 0, 6, 0, 0, 0, 0, 0, 6, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 2, 0, 0, 0, 0, 0, 2, 4, 2, 0, 1, 100, 9, 0, 1, 0, 2, 3, 8, 0, 0, 0, 1, 0, 1, 2, 3, 6, 1, 0, 0, 0, 2, 2, 0, 3, 10, 1, 0, 3, 0, 0, 1, 9, 1, 0, 0, 0, 13, 2, 4, 1, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 17, 0, 0, 172, 2, 0, 1, 0, 0, 4, 2, 0, 0, 0, 2, 0, 1, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 29, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 39, 4, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 8, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 0, 0, 0, 0, 1, 0, 0, 1, 3, 5, 0, 0, 0, 0, 0, 0, 0, 7, 0, 3, 14, 1, 2, 1, 0, 3, 0, 0, 0, 1, 5, 2, 8, 4, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 2, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 2, 0, 0, 1, 3, 0, 0, 3, 0, 0, 0, 1, 0, 2, 3, 0, 0, 0, 8, 0, 0, 0, 6, 0, 1, 2, 0, 1, 4, 7, 0, 147, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 3, 0, 9, 3, 2, 1, 0, 0, 6, 10, 0, 2, 1, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 3, 0, 7, 0, 0, 1, 0, 0, 0, 1, 5, 0, 0, 1, 0, 3, 3, 1, 0, 1, 6, 0, 3, 1, 3, 0, 10, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 9, 17, 2, 71, 14, 0, 0, 0, 0, 3, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 1, 0, 13, 0, 2, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 24, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 3, 0, 3, 0, 1, 0, 3, 0, 0, 2, 0, 2, 0, 0, 0, 0, 8, 2, 3, 0, 0, 0, 0, 0, 10, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 2, 0, 2, 1, 0, 0, 0, 2, 20, 0, 0, 0, 2, 0, 2, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 6, 1, 1, 0, 2, 0, 1, 2, 4, 1, 0, 1, 0, 1, 1, 0, 8, 1, 0, 0, 3, 3, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 5, 2, 0, 0, 1, 0, 2, 2, 1, 0, 5, 18, 1, 2, 0, 14, 1, 0, 19, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 4, 0, 0, 0, 0, 4, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 6, 4, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 3, 2, 0, 4, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 6, 2, 0, 3, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 0, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 6, 0, 0, 0, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 4, 0, 0, 1, 0, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 13, 1, 0, 0, 2, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 1, 2, 1, 8, 0, 0, 2, 2, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 0, 4, 1, 1, 0, 6, 1, 7, 2, 0, 0, 0, 0, 0, 0, 1, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 4, 4, 1, 0, 1, 2, 2, 0, 0, 0, 0, 0, 4, 0, 1, 3, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 1, 22, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 22, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 11, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 1, 23, 9, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 5, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 1, 1, 0, 0, 2, 0, 1, 0, 3, 0, 0, 8, 0, 0, 0, 0, 64, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 3, 1, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 2, 42, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 74, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 2, 1, 1, 4, 0, 0, 0, 0, 0, 0, 0, 24, 2, 0, 31, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 7, 0, 2, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 7, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 3, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 28, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 9, 0, 0, 0, 0, 2, 0, 0, 3, 0, 15, 0, 0, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 2, 1, 0, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 8, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 4, 6, 6, 0, 0, 4, 28, 19, 0, 0, 1, 2, 1, 0, 19, 3, 27, 0, 0, 0, 0, 0, 0, 27, 0, 2, 0, 1, 0, 28, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 1, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2, 1, 4, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 9, 1, 2, 1, 0, 0, 0, 32, 20, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 1, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 3, 0, 0, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 3, 1, 1, 3, 2, 1, 1, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 5, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 4, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 11, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 18, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 45, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 1, 0, 0, 0, 9, 1, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 3, 1, 1, 4, 3, 3, 3, 1, 2, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 2, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 3, 4, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 4, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 1, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 88, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 3, 0, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 4, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 1, 6, 1, 3, 3, 2, 1, 6, 1, 1, 2, 5, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 4, 1, 2, 1, 2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 6, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 5, 5, 1, 2, 3, 3, 1, 2, 2, 2, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 2, 1, 3, 7, 1, 4, 4, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 10, 2, 1, 1, 1, 6, 1, 1, 1, 1, 2, 6, 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 4, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 2, 1, 1, 2, 1, 2, 1, 1, 1, 18, 3, 1, 3, 1, 2, 1, 1, 1, 5, 2, 2, 1, 1, 1, 1, 2, 1, 3, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 3, 1, 2, 1, 1, 1, 4, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 5, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 23, 67, 2, 3, 3, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 3, 2, 1, 1, 3, 2, 1, 7, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 7, 1, 1, 1, 2, 4, 2, 2, 4, 1, 2, 4, 4, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 3, 5, 2, 1, 2, 12, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 3, 2, 2, 2, 7, 1, 1, 1, 4, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "docs[12] = Afterword: The Language Challenge\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Afterword: The Language Challenge\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Natural language throws up some interesting computational challenges.\n",
      "We've explored many of these in the preceding chapters, including\n",
      "tokenization, tagging, classification, information extraction,\n",
      "and building syntactic and semantic representations.\n",
      "You should now be equipped to work with large datasets, to create\n",
      "robust models of linguistic phenomena, and to extend them into\n",
      "components for practical language technologies.  We hope that\n",
      "the Natural Language Toolkit (NLTK) has served to open up the exciting\n",
      "endeavor of practical natural language processing to a broader\n",
      "audience than before.\n",
      "In spite of all that has come before, language presents us with\n",
      "far more than a temporary challenge for computation.  Consider the following\n",
      "sentences which attest to the riches of language:\n",
      "\n",
      "  (1)\n",
      "  a.Overhead the day drives level and grey, hiding the sun by a flight of grey spears.  (William Faulkner, As I Lay Dying, 1935)\n",
      "\n",
      "  b.When using the toaster please ensure that the exhaust fan is turned on. (sign in dormitory kitchen)\n",
      "\n",
      "  c.Amiodarone weakly inhibited CYP2C9, CYP2D6, and CYP3A4-mediated activities with Ki values of 45.1-271.6 μM (Medline, PMID: 10718780)\n",
      "\n",
      "  d.Iraqi Head Seeks Arms (spoof news headline)\n",
      "\n",
      "  e.The earnest prayer of a righteous man has great power and wonderful results. (James 5:16b)\n",
      "\n",
      "  f.Twas brillig, and the slithy toves did gyre and gimble in the wabe (Lewis Carroll, Jabberwocky, 1872)\n",
      "\n",
      "  g.There are two ways to do this, AFAIK :smile:  (internet discussion archive)\n",
      "\n",
      "Other evidence for the riches of language is the vast array of disciplines\n",
      "whose work centers on language.  Some obvious disciplines include\n",
      "translation, literary criticism, philosophy, anthropology and psychology.\n",
      "Many less obvious disciplines investigate language use, including\n",
      "law, hermeneutics, forensics, telephony, pedagogy, archaeology, cryptanalysis and speech\n",
      "pathology.  Each applies distinct methodologies to gather\n",
      "observations, develop theories and test hypotheses.  All serve to\n",
      "deepen our understanding of language and of the intellect that is\n",
      "manifested in language.\n",
      "In view of the complexity of language and the broad range of interest\n",
      "in studying it from different angles, it's clear that we have barely\n",
      "scratched the surface here.  Additionally, within NLP itself,\n",
      "there are many important methods and applications that we haven't\n",
      "mentioned.\n",
      "In our closing remarks we will take a broader view of NLP,\n",
      "including its foundations and the further directions you might\n",
      "want to explore.  Some of the topics are not well-supported by NLTK,\n",
      "and you might like to rectify that problem by contributing new software\n",
      "and data to the toolkit.\n",
      "\n",
      "Language Processing vs Symbol Processing\n",
      "The very notion that natural language could be treated in a\n",
      "computational manner grew out of a research program, dating back to\n",
      "the early 1900s, to reconstruct mathematical reasoning using logic,\n",
      "most clearly manifested in work by Frege, Russell, Wittgenstein,\n",
      "Tarski, Lambek and Carnap.  This work led to the notion of language as\n",
      "a formal system amenable to automatic processing.  Three later\n",
      "developments laid the foundation for natural language processing.  The\n",
      "first was formal language theory.  This defined a language as a set\n",
      "of strings accepted by a class of automata, such as context-free\n",
      "languages and pushdown automata, and provided the underpinnings for\n",
      "computational syntax.\n",
      "The second development was symbolic logic. This provided a\n",
      "formal method for capturing selected aspects of natural language that\n",
      "are relevant for expressing logical proofs. A formal calculus in\n",
      "symbolic logic provides the syntax of a language, together with rules\n",
      "of inference and, possibly, rules of interpretation in a set-theoretic\n",
      "model; examples are propositional logic and First Order Logic.  Given\n",
      "such a calculus, with a well-defined syntax and semantics, it becomes\n",
      "possible to associate meanings with expressions of natural language by\n",
      "translating them into expressions of the formal calculus. For example,\n",
      "if we translate John saw Mary into a formula saw(j,m), we\n",
      "(implicitly or explicitly) intepret the English verb saw as a\n",
      "binary relation, and John and Mary as denoting\n",
      "individuals.  More general statements like All birds fly require\n",
      "quantifiers, in this case ∀, meaning for all: ∀x (bird(x) → fly(x)).  This use of logic provided\n",
      "the technical machinery to perform inferences that are an important\n",
      "part of language understanding.\n",
      "A closely related development was the principle of\n",
      "compositionality, namely that the meaning of a complex expression\n",
      "is composed from the meaning of its parts and their mode of\n",
      "combination (10.).\n",
      "This principle provided a useful correspondence between\n",
      "syntax and semantics, namely that the meaning of a complex expression\n",
      "could be computed recursively.  Consider the sentence It is not true\n",
      "that p, where p is a proposition.  We can\n",
      "represent the meaning of this sentence as not(p).  Similarly, we\n",
      "can represent the meaning of John saw Mary as saw(j, m).  Now we\n",
      "can compute the interpretation of It is not true that John saw Mary\n",
      "recursively, using the above information, to get\n",
      "not(saw(j,m)).\n",
      "The approaches just outlined share the premise that computing with\n",
      "natural language crucially relies on rules for manipulating symbolic\n",
      "representations. For a certain period in the development of NLP,\n",
      "particularly during the 1980s, this premise provided a common starting\n",
      "point for both linguists and practitioners of NLP, leading to a family\n",
      "of grammar formalisms known as unification-based (or feature-based)\n",
      "grammar (cf. 9.), and to NLP applications implemented in the Prolog\n",
      "programming language. Although grammar-based NLP is still a\n",
      "significant area of research, it has become somewhat eclipsed in the\n",
      "last 15–20 years due to a variety of factors. One\n",
      "significant influence came from automatic speech recognition. Although\n",
      "early work in speech processing adopted a model that emulated the\n",
      "kind of rule-based phonological  processing typified\n",
      "by the Sound Pattern of English (Chomsky & Halle, 1968),\n",
      "this turned out to be hopelessly inadequate in dealing\n",
      "with the hard problem of recognizing actual speech in anything like\n",
      "real time. By contrast, systems which involved learning patterns from\n",
      "large bodies of speech data were significantly more accurate,\n",
      "efficient and robust. In addition, the speech community found that\n",
      "progress in building better systems was hugely assisted by the\n",
      "construction of shared resources for quantitatively measuring\n",
      "performance against common test data. Eventually, much of the NLP\n",
      "community embraced a data intensive orientation to language\n",
      "processing, coupled with a growing use of machine-learning techniques\n",
      "and evaluation-led methodology.\n",
      "\n",
      "\n",
      "Contemporary Philosophical Divides\n",
      "The contrasting approaches to NLP described in the preceding section\n",
      "relate back to early metaphysical debates about rationalism\n",
      "versus empiricism and realism versus idealism that\n",
      "occurred in the Enlightenment period of Western philosophy.  These\n",
      "debates took place against a backdrop of orthodox thinking in which\n",
      "the source of all knowledge was believed to be divine revelation.\n",
      "During this period of the seventeenth and eighteenth centuries,\n",
      "philosophers argued that human reason or sensory experience has\n",
      "priority over revelation.  Descartes and Leibniz, amongst others, took\n",
      "the rationalist position, asserting that all truth has its origins in\n",
      "human thought, and in the existence of \"innate ideas\" implanted in our\n",
      "minds from birth.  For example, they argued that the principles of\n",
      "Euclidean geometry were developed using human reason, and were not the\n",
      "result of supernatural revelation or sensory experience.  In contrast,\n",
      "Locke and others took the empiricist view, that our primary source of\n",
      "knowledge is the experience of our faculties, and that human reason\n",
      "plays a secondary role in reflecting on that experience.  Often-cited\n",
      "evidence for this position was Galileo's discovery — based on\n",
      "careful observation of the motion of the planets — that the\n",
      "solar system is heliocentric and not geocentric.  In the context of\n",
      "linguistics, this debate leads to the following question: to what\n",
      "extent does human linguistic experience, versus our innate \"language\n",
      "faculty\", provide the basis for our knowledge of language?  In NLP\n",
      "this issue surfaces in debates about the priority of corpus data\n",
      "versus linguistic introspection in the construction of computational models.\n",
      "A further concern, enshrined in the debate between realism and\n",
      "idealism, was the metaphysical status of the constructs of a theory.\n",
      "Kant argued for a distinction between phenomena, the manifestations we\n",
      "can experience, and \"things in themselves\" which can never be\n",
      "known directly.  A linguistic realist would take a theoretical\n",
      "construct like noun phrase to be a real world entity that exists\n",
      "independently of human perception and reason, and which actually\n",
      "causes the observed linguistic phenomena.  A linguistic idealist, on\n",
      "the other hand, would argue that noun phrases, along with more\n",
      "abstract constructs like semantic representations, are intrinsically\n",
      "unobservable, and simply play the role of useful fictions.  The way\n",
      "linguists write about theories often betrays a realist position, while\n",
      "NLP practitioners occupy neutral territory or else lean towards the\n",
      "idealist position.  Thus, in NLP, it is often enough if a theoretical\n",
      "abstraction leads to a useful result; it does not matter whether this\n",
      "result sheds any light on human linguistic processing.\n",
      "These issues are still alive today, and show up in the distinctions\n",
      "between symbolic vs statistical methods, deep vs shallow processing,\n",
      "binary vs gradient classifications, and scientific vs engineering\n",
      "goals.  However, such contrasts are now highly nuanced, and the debate\n",
      "is no longer as polarized as it once was.  In fact, most of the\n",
      "discussions — and most of the advances even — involve a\n",
      "\"balancing act.\"  For example, one intermediate position is to assume\n",
      "that humans are innately endowed with analogical and memory-based\n",
      "learning methods (weak rationalism), and to use these methods to identify\n",
      "meaningful patterns in their sensory language experience (empiricism).\n",
      "We have seen many examples of this methodology throughout this book.\n",
      "Statistical methods inform symbolic models any time corpus statistics\n",
      "guide the selection of productions in a context free grammar,\n",
      "i.e. \"grammar engineering.\"\n",
      "Symbolic methods inform statistical models any time a corpus that\n",
      "was created using rule-based methods is used as a source of features\n",
      "for training a statistical language model,\n",
      "i.e. \"grammatical inference.\"  The circle is closed.\n",
      "\n",
      "\n",
      "NLTK Roadmap\n",
      "The Natural Language Toolkit is work-in-progress, and is being\n",
      "continually expanded as people contribute code.  Some areas of\n",
      "NLP and linguistics are not (yet) well supported in NLTK,\n",
      "and contributions in these areas are especially welcome.\n",
      "Check http://nltk.org/ for news about developments after the publication\n",
      "date of the book.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Phonology and Morphology:\n",
      " Computational approaches to the study of sound patterns and word structures\n",
      "typically use a finite state toolkit.  Phenomena such as suppletion and\n",
      "non-concatenative morphology are difficult to address using the string\n",
      "processing methods we have been studying.  The technical challenge is\n",
      "not only to link NLTK to a high-performance finite state toolkit, but\n",
      "to avoid duplication of lexical data and to link the morphosyntactic\n",
      "features needed by morph analyzers and syntactic parsers.\n",
      "\n",
      "High-Performance Components:\n",
      " Some NLP tasks are too computationally intensive for pure Python\n",
      "implementations to be feasible.  However, in some cases the expense\n",
      "only arises when training models, not when using them to label inputs.\n",
      "NLTK's package system provides a convenient way to distribute\n",
      "trained models, even models trained using corpora that cannot be\n",
      "freely distributed.  Alternatives are to develop Python interfaces\n",
      "to high-performance machine learning tools, or to expand the\n",
      "reach of Python by using parallel programming techniques like MapReduce.\n",
      "\n",
      "Lexical Semantics:\n",
      " This is a vibrant area of current research, encompassing\n",
      "inheritance models of the lexicon, ontologies, multiword expressions,\n",
      "etc, mostly outside the scope of NLTK as it stands.\n",
      "A conservative goal would be to access lexical information\n",
      "from rich external stores in support of tasks in\n",
      "word sense disambiguation, parsing, and semantic interpretation.\n",
      "\n",
      "Natural Language Generation:\n",
      " Producing coherent text from underlying representations of meaning\n",
      "is an important part of NLP; a unification based approach to\n",
      "NLG has been developed in NLTK, and there is scope for more\n",
      "contributions in this area.\n",
      "\n",
      "Linguistic Fieldwork:\n",
      " A major challenge faced by linguists is to document thousands of\n",
      "endangered languages, work which generates heterogeneous and\n",
      "rapidly evolving data in large quantities.  More fieldwork\n",
      "data formats, including interlinear text formats and lexicon\n",
      "interchange formats, could be supported in NLTK,\n",
      "helping linguists to curate and analyze this data,\n",
      "while liberating them to spend as much time as possible\n",
      "on data elicitation.\n",
      "\n",
      "Other Languages:\n",
      " Improved support for NLP in languages other than English could\n",
      "involve work in two areas: obtaining permission to distribute\n",
      "more corpora with NLTK's data collection;\n",
      "writing language-specific HOWTOs for posting at http://nltk.org/howto,\n",
      "illustrating the use of NLTK and discussing language-specific\n",
      "problems for NLP including character encodings, word segmentation,\n",
      "and morphology.\n",
      "NLP researchers with expertise in a particular language\n",
      "could arrange to translate this book and host a copy on the\n",
      "NLTK website; this would go beyond translating the discussions\n",
      "to providing equivalent worked examples using data in the\n",
      "target language, a non-trivial undertaking.\n",
      "\n",
      "NLTK-Contrib:Many of NLTK's core components were contributed by members of the\n",
      "NLP community, and were initially housed in NLTK's \"Contrib\"\n",
      "package, nltk_contrib.  The only requirement for software to\n",
      "be added to this package is that it must be written in Python,\n",
      "relevant to NLP, and given the same open source license as the\n",
      "rest of NLTK.  Imperfect software is welcome, and will probably\n",
      "be improved over time by other members of the NLP community.\n",
      "\n",
      "Teaching Materials:\n",
      " Since the earliest days of NLTK development, teaching materials have\n",
      "accompanied the software, materials that have gradually expanded to\n",
      "fill this book, plus a substantial quantity of online materials as well.\n",
      "We hope that instructors who supplement these materials with presentation slides,\n",
      "problem sets, solution sets, and more detailed treatments of the\n",
      "topics we have covered, will make them available, and will notify\n",
      "the authors so we can link them from http://nltk.org/.\n",
      "Of particular value are materials that help NLP become a mainstream course in\n",
      "the undergraduate programs of computer science and linguistics departments,\n",
      "or that make NLP accessible at the secondary level where there is\n",
      "significant scope for including computational content in the language, literature,\n",
      "computer science, and information technology curricula.\n",
      "\n",
      "Only a Toolkit:As stated in the preface, NLTK is a toolkit, not a system.\n",
      "Many problems will be tackled with a combination of NLTK,\n",
      "Python, other Python libraries, and interfaces to external\n",
      "NLP tools and formats.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Envoi...\n",
      "Linguists are sometimes asked how many languages they speak,\n",
      "and have to explain that this field actually concerns the\n",
      "study of abstract structures that are shared by languages,\n",
      "a study which is more profound and elusive than learning to speak\n",
      "as many languages as possible.\n",
      "Similarly, computer scientists are sometimes asked how many\n",
      "programming languages they know, and have to explain that computer science\n",
      "actually concerns the study of data structures and algorithms that can be\n",
      "implemented in any programming language,\n",
      "a study which is more profound and elusive than striving for\n",
      "fluency in as many programming languages as possible.\n",
      "This book has covered many topics in the field of Natural Language Processing.\n",
      "Most of the examples have used Python and English.\n",
      "However, it would be unfortunate if readers concluded that\n",
      "NLP is about how to write Python programs to manipulate English text,\n",
      "or more broadly, about how to write programs (in any programming language)\n",
      "to manipulate text (in any natural language).\n",
      "Our selection of Python and English was expedient, nothing more.\n",
      "Even our focus on programming itself was only a means to an end:\n",
      "as a way to understand data structures and algorithms\n",
      "for representing and manipulating collections of linguistically annotated text,\n",
      "as a way to build new language technologies to better serve\n",
      "the needs of the information society,\n",
      "and ultimately as a pathway into deeper understanding of the vast riches\n",
      "of human language.\n",
      "But for the present: happy hacking!\n",
      "\n",
      "\n",
      "About this document...\n",
      "UPDATED FOR NLTK 3.0.\n",
      "This is a chapter from Natural Language Processing with Python,\n",
      "by Steven Bird, Ewan Klein and Edward Loper,\n",
      "Copyright © 2019 the authors.\n",
      "It is distributed with the Natural Language Toolkit [http://nltk.org/],\n",
      "Version 3.0, under the terms of the\n",
      "Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\n",
      "[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\n",
      "This document was built on\n",
      "Wed  4 Sep 2019 11:40:48 ACST\n",
      "vectors[12] = [1, 5, 14, 55, 13, 10, 1, 2, 0, 0, 9, 5, 6, 6, 0, 0, 3, 0, 7, 1, 0, 0, 0, 1, 1, 0, 0, 5, 2, 4, 24, 0, 0, 1, 0, 1, 4, 0, 2, 0, 5, 0, 0, 3, 0, 0, 1, 1, 0, 0, 3, 0, 0, 0, 1, 0, 0, 3, 0, 3, 8, 0, 0, 0, 7, 0, 0, 2, 0, 5, 0, 1, 0, 0, 0, 1, 5, 0, 0, 2, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 29, 2, 0, 0, 8, 1, 2, 0, 2, 0, 1, 2, 1, 1, 0, 2, 1, 5, 0, 1, 6, 3, 1, 0, 0, 5, 0, 2, 0, 0, 0, 1, 0, 0, 10, 1, 2, 4, 1, 0, 8, 27, 24, 4, 0, 0, 4, 14, 0, 1, 0, 5, 5, 5, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 1, 3, 1, 0, 0, 0, 4, 0, 0, 0, 4, 9, 0, 1, 2, 1, 1, 0, 3, 1, 0, 0, 0, 6, 0, 6, 0, 1, 3, 11, 2, 0, 0, 0, 0, 1, 0, 3, 4, 1, 0, 0, 7, 23, 3, 2, 0, 1, 0, 0, 8, 0, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 10, 1, 0, 2, 3, 0, 0, 0, 0, 0, 2, 0, 1, 0, 3, 0, 0, 0, 0, 1, 6, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 1, 1, 0, 0, 0, 1, 0, 4, 0, 0, 0, 2, 1, 0, 0, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 9, 0, 0, 0, 0, 1, 0, 1, 2, 0, 1, 6, 0, 1, 0, 1, 0, 2, 2, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 4, 0, 4, 0, 4, 2, 0, 0, 0, 0, 0, 4, 0, 2, 0, 2, 2, 3, 0, 11, 0, 0, 0, 0, 1, 2, 2, 6, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 4, 0, 0, 1, 14, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 0, 0, 1, 0, 0, 3, 2, 3, 4, 8, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 1, 0, 6, 3, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 6, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 1, 0, 0, 0, 1, 1, 2, 0, 0, 1, 2, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2, 3, 1, 0, 1, 0, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 6, 3, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 3, 6, 1, 0, 0, 2, 5, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 6, 3, 12, 0, 1, 0, 1, 0, 0, 1, 2, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 1, 0, 5, 2, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 5, 1, 0, 0, 0, 0, 0, 1, 5, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 4, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 3, 5, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 2, 2, 1, 1, 3, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 6, 2, 2, 2, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1]\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# コードブックを素性とする文書ベクトルを作る (直接ベクトル生成)\n",
    "def make_vectors_eng(docs, codebook):\n",
    "    '''コードブックを素性とする文書ベクトルを作る（直接ベクトル生成）\n",
    "\n",
    "    :param docs(list): 1文書1文字列で保存。複数文書をリストとして並べたもの。\n",
    "    :param codebook(list): ユニークな単語一覧。\n",
    "    :return (list): コードブックを元に、出現回数を特徴量とするベクトルを返す。\n",
    "    '''\n",
    "    vectors = []\n",
    "    wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    for doc in docs:\n",
    "        this_vector = []\n",
    "        fdist = nltk.FreqDist()\n",
    "        for sent in sent_tokenize(doc):\n",
    "            for word in wordpunct_tokenize(sent):\n",
    "                this_word = wnl.lemmatize(word.lower())\n",
    "                fdist[this_word] += 1\n",
    "        for word in codebook:\n",
    "            this_vector.append(fdist[word])\n",
    "        vectors.append(this_vector)\n",
    "    return vectors\n",
    "\n",
    "vectors = make_vectors_eng(docs3, codebook)\n",
    "for index in range(len(docs3)):\n",
    "    print('docs[{}] = {}'.format(index,docs3[index]))\n",
    "    print('vectors[{}] = {}'.format(index,vectors[index]))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(vectors):\n",
    "    vectors = np.array(vectors)\n",
    "    distances = []\n",
    "    for i in range(len(vectors)):\n",
    "        temp = []\n",
    "        for j in range(len(vectors)):\n",
    "            temp.append(np.linalg.norm(vectors[i] - vectors[j]))\n",
    "        distances.append(temp)\n",
    "    return distances\n",
    "\n",
    "def cosine_similarity(vectors):\n",
    "    vectors = np.array(vectors)\n",
    "    distances = []\n",
    "    for i in range(len(vectors)):\n",
    "        temp = []\n",
    "        for j in range(len(vectors)):\n",
    "            temp.append(round(1-distance.cosine(vectors[i], vectors[j]),3))\n",
    "        distances.append(temp)\n",
    "    return distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from collections import Counter\n",
    "#ch01=docs2[1]\n",
    "#ch03=docs2[3]\n",
    "\n",
    "#words01 = ch01.split()\n",
    "#ch01counter = Counter(ch01)\n",
    "\n",
    "#words03 = ch03.split()\n",
    "#ch03counter = Counter(ch03)\n",
    "\n",
    "#count=0;\n",
    "\n",
    "#ch01とch03の出現頻度の高い上位10単語を表示する。\n",
    "#for word in ch01counter.most_common(10):\n",
    "#    print(word,end=\" \") \n",
    "#    topword=0;\n",
    "#    for word3 in ch03counter.most_common():\n",
    "#        if count==topword:\n",
    "#            print(word3)\n",
    "#        topword+=1;\n",
    "#    count+=1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# normal BoW\n",
      "[1.0, 0.505, 0.336, 0.397, 0.439, 0.281, 0.237, 0.231, 0.201, 0.133, 0.238, 0.417, 0.693]\n",
      "[0.505, 1.0, 0.622, 0.718, 0.713, 0.479, 0.357, 0.279, 0.27, 0.139, 0.263, 0.394, 0.351]\n",
      "[0.336, 0.622, 1.0, 0.611, 0.57, 0.517, 0.338, 0.308, 0.26, 0.13, 0.213, 0.467, 0.273]\n",
      "[0.397, 0.718, 0.611, 1.0, 0.655, 0.459, 0.302, 0.33, 0.268, 0.154, 0.265, 0.447, 0.284]\n",
      "[0.439, 0.713, 0.57, 0.655, 1.0, 0.458, 0.347, 0.291, 0.271, 0.185, 0.314, 0.419, 0.28]\n",
      "[0.281, 0.479, 0.517, 0.459, 0.458, 1.0, 0.38, 0.528, 0.267, 0.172, 0.226, 0.376, 0.253]\n",
      "[0.237, 0.357, 0.338, 0.302, 0.347, 0.38, 1.0, 0.308, 0.218, 0.242, 0.237, 0.299, 0.242]\n",
      "[0.231, 0.279, 0.308, 0.33, 0.291, 0.528, 0.308, 1.0, 0.508, 0.277, 0.284, 0.325, 0.217]\n",
      "[0.201, 0.27, 0.26, 0.268, 0.271, 0.267, 0.218, 0.508, 1.0, 0.401, 0.326, 0.279, 0.221]\n",
      "[0.133, 0.139, 0.13, 0.154, 0.185, 0.172, 0.242, 0.277, 0.401, 1.0, 0.268, 0.159, 0.14]\n",
      "[0.238, 0.263, 0.213, 0.265, 0.314, 0.226, 0.237, 0.284, 0.326, 0.268, 1.0, 0.227, 0.284]\n",
      "[0.417, 0.394, 0.467, 0.447, 0.419, 0.376, 0.299, 0.325, 0.279, 0.159, 0.227, 1.0, 0.405]\n",
      "[0.693, 0.351, 0.273, 0.284, 0.28, 0.253, 0.242, 0.217, 0.221, 0.14, 0.284, 0.405, 1.0]\n",
      "# BoW + tfidf\n",
      "[1.0, 0.369, 0.223, 0.29, 0.335, 0.186, 0.163, 0.134, 0.125, 0.069, 0.135, 0.255, 0.503]\n",
      "[0.369, 1.0, 0.434, 0.537, 0.549, 0.305, 0.22, 0.151, 0.161, 0.07, 0.146, 0.227, 0.232]\n",
      "[0.223, 0.434, 1.0, 0.418, 0.409, 0.316, 0.2, 0.157, 0.143, 0.064, 0.107, 0.282, 0.163]\n",
      "[0.29, 0.537, 0.418, 1.0, 0.506, 0.294, 0.185, 0.186, 0.162, 0.078, 0.146, 0.277, 0.183]\n",
      "[0.335, 0.549, 0.409, 0.506, 1.0, 0.309, 0.219, 0.169, 0.182, 0.113, 0.18, 0.268, 0.192]\n",
      "[0.186, 0.305, 0.316, 0.294, 0.309, 1.0, 0.242, 0.415, 0.16, 0.092, 0.112, 0.207, 0.149]\n",
      "[0.163, 0.22, 0.2, 0.185, 0.219, 0.242, 1.0, 0.197, 0.123, 0.113, 0.108, 0.149, 0.152]\n",
      "[0.134, 0.151, 0.157, 0.186, 0.169, 0.415, 0.197, 1.0, 0.365, 0.163, 0.151, 0.165, 0.112]\n",
      "[0.125, 0.161, 0.143, 0.162, 0.182, 0.16, 0.123, 0.365, 1.0, 0.304, 0.197, 0.152, 0.14]\n",
      "[0.069, 0.07, 0.064, 0.078, 0.113, 0.092, 0.113, 0.163, 0.304, 1.0, 0.164, 0.074, 0.071]\n",
      "[0.135, 0.146, 0.107, 0.146, 0.18, 0.112, 0.108, 0.151, 0.197, 0.164, 1.0, 0.108, 0.176]\n",
      "[0.255, 0.227, 0.282, 0.277, 0.268, 0.207, 0.149, 0.165, 0.152, 0.074, 0.108, 1.0, 0.224]\n",
      "[0.503, 0.232, 0.163, 0.183, 0.192, 0.149, 0.152, 0.112, 0.14, 0.071, 0.176, 0.224, 1.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn.feature_extraction.text as fe_text\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def bow(docs):\n",
    "    '''Bag-of-Wordsによるベクトルを生成。\n",
    "\n",
    "    :param docs(list): 1文書1文字列で保存。複数文書をリストとして並べたもの。\n",
    "    :return: 文書ベクトル。\n",
    "    '''\n",
    "    vectorizer = fe_text.CountVectorizer(stop_words='english')\n",
    "    vectors = vectorizer.fit_transform(docs)\n",
    "    return vectors.toarray(), vectorizer\n",
    "\n",
    "vectors, vectorizer = bow(docs3)\n",
    "print('# normal BoW')\n",
    "#print(vectorizer.get_feature_names())\n",
    "#print(vectors)\n",
    "similarities = cosine_similarity(vectors)\n",
    "\n",
    "for index in range(len(similarities)):\n",
    "    print(similarities[index])\n",
    "  \n",
    "fig, ax =plt.subplots(figsize=(9, 9))\n",
    "#sns.heatmap((similarities), ax=ax,annot=True,square=True)\n",
    "#ax.set_ylim(len(similarities), 0)\n",
    "#plt.show()\n",
    "\n",
    "def bow_tfidf(docs):\n",
    "    '''Bag-of-WordsにTF-IDFで重み調整したベクトルを生成。\n",
    "\n",
    "    :param docs(list): 1文書1文字列で保存。複数文書をリストとして並べたもの。\n",
    "    :return: 重み調整したベクトル。\n",
    "    '''\n",
    "    vectorizer = fe_text.TfidfVectorizer(norm=None, stop_words='english')\n",
    "    vectors = vectorizer.fit_transform(docs)\n",
    "    return vectors.toarray(), vectorizer\n",
    "\n",
    "vectors, vectorizer = bow_tfidf(docs3)\n",
    "print('# BoW + tfidf')\n",
    "similarities = cosine_similarity(vectors)\n",
    "\n",
    "for index in range(len(similarities)):\n",
    "    print(similarities[index])\n",
    "    \n",
    "plt.figure()\n",
    "\n",
    "\n",
    "sns.heatmap((similarities), ax=ax,annot=True,square=True)\n",
    "ax.set_ylim(len(similarities), 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ch01           ch04         ch03\n",
      "('the', 694) ('the', 817) ('the', 786)\n",
      "('of', 402) ('a', 518) ('a', 495)\n",
      "('a', 324) ('of', 460) ('of', 434)\n",
      "('to', 315) ('to', 409) ('to', 397)\n",
      "('and', 263) ('in', 304) ('and', 323)\n",
      "('in', 234) ('and', 296) ('>>>', 310)\n",
      "('is', 186) ('>>>', 261) ('in', 284)\n",
      "('>>>', 184) ('is', 250) ('is', 207)\n",
      "('for', 134) ('=', 193) ('we', 193)\n",
      "('that', 133) ('for', 180) ('for', 190)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 694) ('the', 817)\n",
      "('of', 402) ('a', 518)\n",
      "('a', 324) ('of', 460)\n",
      "('to', 315) ('to', 409)\n",
      "('and', 263) ('in', 304)\n",
      "('in', 234) ('and', 296)\n",
      "('is', 186) ('>>>', 261)\n",
      "('>>>', 184) ('is', 250)\n",
      "('for', 134) ('=', 193)\n",
      "('that', 133) ('for', 180)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ...  1703  1721 30672]\n",
      "{'preface\\n\\n\\n\\n\\npreface\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nthis': 0, 'is': 1, 'a': 2, 'book': 3, 'about': 4, 'natural': 5, 'language': 6, 'processing': 7, '.': 8, 'by': 9, 'natural\\nlanguage': 10, 'we': 11, 'mean': 12, 'that': 13, 'used': 14, 'for': 15, 'everyday\\ncommunication': 16, 'humans;': 17, 'languages': 18, 'like': 19, 'english,': 20, 'hindi': 21, 'or\\nportuguese': 22, 'in': 23, 'contrast': 24, 'to': 25, 'artificial': 26, 'such': 27, 'as\\nprogramming': 28, 'and': 29, 'mathematical': 30, 'notations,': 31, 'have\\nevolved': 32, 'as': 33, 'they': 34, 'pass': 35, 'from': 36, 'generation': 37, 'generation,': 38, 'are': 39, 'hard': 40, 'to\\npin': 41, 'down': 42, 'with': 43, 'explicit': 44, 'rules': 45, 'will': 46, 'take': 47, '—': 48, 'or': 49, 'nlp\\nfor': 50, 'short': 51, 'wide': 52, 'sense': 53, 'cover': 54, 'any': 55, 'kind': 56, 'of': 57, 'computer': 58, 'manipulation\\nof': 59, 'at': 60, 'one': 61, 'extreme,': 62, 'it': 63, 'could': 64, 'be': 65, 'simple': 66, 'counting\\nword': 67, 'frequencies': 68, 'compare': 69, 'different': 70, 'writing': 71, 'styles': 72, '.\\nat': 73, 'the': 74, 'other': 75, 'nlp': 76, 'involves': 77, 'understanding': 78, 'complete': 79, 'human\\nutterances,': 80, 'least': 81, 'extent': 82, 'being': 83, 'able': 84, 'give': 85, 'useful\\nresponses': 86, 'them': 87, '.\\ntechnologies': 88, 'based': 89, 'on': 90, 'nlp\\nare': 91, 'becoming': 92, 'increasingly': 93, 'widespread': 94, 'example,': 95, 'phones': 96, 'handheld': 97, 'computers\\nsupport': 98, 'predictive': 99, 'text': 100, 'handwriting': 101, 'recognition;': 102, '': 103, 'web\\nsearch': 104, 'engines': 105, 'access': 106, 'information': 107, 'locked': 108, 'up': 109, 'unstructured\\ntext;': 110, 'machine': 111, 'translation': 112, 'allows': 113, 'us': 114, 'retrieve': 115, 'texts': 116, 'written': 117, 'in\\nchinese': 118, 'read': 119, 'spanish;': 120, 'analysis': 121, 'enables': 122, 'to\\ndetect': 123, 'sentiment': 124, 'tweets': 125, 'blogs': 126, 'providing': 127, 'more': 128, 'human-machine': 129, 'interfaces,': 130, 'more\\nsophisticated': 131, 'stored': 132, 'information,': 133, 'has\\ncome': 134, 'play': 135, 'central': 136, 'role': 137, 'multilingual': 138, 'society': 139, '.\\nthis': 140, 'provides': 141, 'highly': 142, 'accessible': 143, 'introduction': 144, 'field': 145, '.\\nit': 146, 'can': 147, 'individual': 148, 'study': 149, 'textbook': 150, 'course\\non': 151, 'computational': 152, 'linguistics,\\nor': 153, 'supplement': 154, 'courses': 155, 'intelligence,\\ntext': 156, 'mining,': 157, 'corpus': 158, 'linguistics': 159, '.\\nthe': 160, 'intensely': 161, 'practical,': 162, 'containing\\nhundreds': 163, 'fully-worked': 164, 'examples': 165, 'graded': 166, 'exercises': 167, 'python': 168, 'programming': 169, 'together': 170, 'an': 171, 'open': 172, 'source\\nlibrary': 173, 'called': 174, 'toolkit': 175, '(nltk)': 176, '.\\nnltk': 177, 'includes': 178, 'extensive': 179, 'software,': 180, 'data,': 181, 'documentation,': 182, 'all': 183, 'freely': 184, 'downloadable': 185, 'http://nltk': 186, '.org/': 187, '.\\ndistributions': 188, 'provided': 189, 'windows,': 190, 'macintosh': 191, 'unix': 192, 'platforms': 193, '.\\nwe': 194, 'strongly': 195, 'encourage': 196, 'you': 197, 'download': 198, 'nltk,': 199, 'try': 200, 'out': 201, 'the\\nexamples': 202, 'along': 203, 'way': 204, '.\\n\\naudience\\nnlp': 205, 'important': 206, 'scientific,': 207, 'economic,': 208, 'social,': 209, 'cultural': 210, 'reasons': 211, '.\\nnlp': 212, 'experiencing': 213, 'rapid': 214, 'growth': 215, 'its': 216, 'theories': 217, 'methods': 218, 'deployed': 219, 'in\\na': 220, 'variety': 221, 'new': 222, 'technologies': 223, 'this': 224, 'reason': 225, 'is\\nimportant': 226, 'range': 227, 'people': 228, 'have': 229, 'working': 230, 'knowledge': 231, '.\\nwithin': 232, 'industry,': 233, 'in\\nhuman-computer': 234, 'interaction,': 235, 'business': 236, 'analysis,\\nand': 237, 'web': 238, 'software': 239, 'development': 240, 'academia,': 241, 'areas': 242, 'from\\nhumanities': 243, 'computing': 244, 'linguistics\\nthrough': 245, 'science': 246, 'intelligence': 247, '.\\n(to': 248, 'many': 249, 'known': 250, 'name': 251, 'of\\ncomputational': 252, '.)\\nthis': 253, 'intended': 254, 'diverse': 255, 'who': 256, 'want': 257, 'to\\nlearn': 258, 'how': 259, 'write': 260, 'programs': 261, 'analyze': 262, 'language,\\nregardless': 263, 'previous': 264, 'experience:\\n\\n\\n\\n\\nnew': 265, 'programming?:\\n\\xa0the': 266, 'early': 267, 'chapters': 268, 'suitable\\nfor': 269, 'readers': 270, 'no': 271, 'prior': 272, 'programming,': 273, 'so': 274, 'long': 275, \"you\\naren't\": 276, 'afraid': 277, 'tackle': 278, 'concepts': 279, 'develop': 280, 'skills': 281, 'full': 282, 'copy': 283, 'and\\ntry': 284, 'yourself,': 285, 'hundreds': 286, '.\\nif': 287, 'need': 288, 'general': 289, 'python,': 290, 'see': 291, 'the\\nlist': 292, 'resources': 293, 'http://docs': 294, '.python': 295, '.\\n\\nnew': 296, 'python?:experienced': 297, 'programmers': 298, 'quickly': 299, 'learn': 300, 'enough\\npython': 301, 'using': 302, 'get': 303, 'immersed': 304, '.\\nall': 305, 'relevant': 306, 'features': 307, 'carefully': 308, 'explained': 309, 'exemplified,\\nand': 310, 'come': 311, 'appreciate': 312, \"python's\": 313, 'suitability': 314, 'this\\napplication': 315, 'area': 316, 'index': 317, 'help': 318, 'locate': 319, 'relevant\\ndiscussions': 320, '.\\n\\nalready': 321, 'dreaming': 322, 'python?:\\n\\xa0skim': 323, 'examples\\nand': 324, 'dig': 325, 'into': 326, 'interesting': 327, 'analysis\\nmaterial': 328, 'starts': 329, '1': 330, \".\\nyou'll\": 331, 'soon': 332, 'applying': 333, 'your': 334, 'fascinating': 335, 'domain': 336, '.\\n\\n\\n\\n\\n\\nemphasis\\nthis': 337, 'practical': 338, 'by\\nexample,': 339, 'real': 340, 'programs,': 341, 'grasp': 342, 'value': 343, 'to\\ntest': 344, 'idea': 345, 'through': 346, 'implementation': 347, 'if': 348, \"haven't\": 349, 'learnt': 350, 'already,\\nthis': 351, 'teach': 352, 'unlike': 353, 'programming\\nbooks,': 354, 'provide': 355, 'illustrations': 356, 'the\\napproach': 357, 'taken': 358, 'also': 359, 'principled,': 360, 'the\\ntheoretical': 361, 'underpinnings': 362, \"don't\": 363, 'shy': 364, 'away': 365, 'careful': 366, 'linguistic\\nand': 367, 'tried': 368, 'pragmatic': 369, 'in\\nstriking': 370, 'balance': 371, 'between': 372, 'theory': 373, 'application,': 374, 'identifying': 375, 'the\\nconnections': 376, 'tensions': 377, 'finally,': 378, 'recognize': 379, \"you\\nwon't\": 380, 'unless': 381, 'pleasurable,': 382, 'have\\ntried': 383, 'include': 384, 'applications': 385, 'interesting\\nand': 386, 'entertaining,': 387, 'sometimes': 388, 'whimsical': 389, '.\\nnote': 390, 'not': 391, 'reference': 392, 'work': 393, 'coverage': 394, 'python\\nand': 395, 'selective,': 396, 'presented': 397, 'tutorial': 398, 'style': 399, 'for\\nreference': 400, 'material,': 401, 'please': 402, 'consult': 403, 'substantial': 404, 'quantity': 405, 'of\\nsearchable': 406, 'available': 407, 'http://python': 408, 'advanced': 409, 'content': 410, 'ranges': 411, 'introductory': 412, 'intermediate,': 413, 'is\\ndirected': 414, 'analyze\\ntext': 415, '.\\nto': 416, 'algorithms': 417, 'implemented': 418, 'nltk,\\nyou': 419, 'examine': 420, 'code': 421, 'linked': 422, '.org/,\\nand': 423, 'materials': 424, 'cited': 425, '.\\n\\n\\nwhat': 426, 'learn\\nby': 427, 'digging': 428, 'material': 429, 'here,': 430, 'learn:\\n\\nhow': 431, 'manipulate': 432, 'analyze\\nlanguage': 433, 'these': 434, 'programs\\nhow': 435, 'key': 436, 'describe': 437, 'and\\nanalyse': 438, 'language\\nhow': 439, 'data': 440, 'structures': 441, 'nlp\\nhow': 442, 'standard': 443, 'formats,': 444, 'can\\nbe': 445, 'evaluate': 446, 'performance': 447, 'techniques\\n\\ndepending': 448, 'background,': 449, 'motivation': 450, 'interested': 451, 'nlp,\\nyou': 452, 'gain': 453, 'kinds': 454, 'book,': 455, 'set': 456, 'out\\nin': 457, 'iii': 458, '.1': 459, '.\\n\\n\\n\\n\\n\\n\\n\\ngoals\\nbackground': 460, 'arts': 461, 'humanities\\nbackground': 462, 'engineering\\n\\n\\n\\nlanguage': 463, 'analysis\\nmanipulating': 464, 'large': 465, 'corpora,\\nexploring': 466, 'linguistic': 467, 'models,\\nand': 468, 'testing': 469, 'empirical': 470, 'claims': 471, '.\\nusing': 472, 'techniques': 473, 'modeling,\\ndata': 474, 'discovery\\nto': 475, '.\\n\\nlanguage': 476, 'technology\\nbuilding': 477, 'robust': 478, 'systems': 479, 'to\\nperform': 480, 'tasks\\nwith': 481, 'technological': 482, 'and\\ndata': 483, 'robust\\nlanguage': 484, '.\\n\\n\\ntable': 485, '.1:': 486, 'gained': 487, 'reading': 488, 'depending': 489, \"readers'\": 490, 'goals': 491, 'background\\n\\n\\n\\n\\norganization\\nthe': 492, 'organized': 493, 'order': 494, 'conceptual': 495, 'difficulty,\\nstarting': 496, 'processing\\nthat': 497, 'shows': 498, 'explore': 499, 'bodies': 500, 'using\\ntiny': 501, '(chapters': 502, '1-3)': 503, 'followed': 504, 'by\\na': 505, 'chapter': 506, 'structured': 507, '(chapter': 508, '4)': 509, 'consolidates': 510, 'the\\nprogramming': 511, 'topics': 512, 'scattered': 513, 'across': 514, 'preceding': 515, '.\\nafter': 516, 'this,': 517, 'pace': 518, 'picks': 519, 'up,': 520, 'move\\non': 521, 'series': 522, 'covering': 523, 'fundamental': 524, 'in\\nlanguage': 525, 'processing:\\ntagging,': 526, 'classification,': 527, 'extraction': 528, '5-7)': 529, 'next': 530, 'three': 531, 'look': 532, 'ways': 533, 'parse': 534, 'sentence,': 535, 'recognize\\nits': 536, 'syntactic': 537, 'structure,': 538, 'construct': 539, 'representations': 540, 'meaning': 541, '8-10)': 542, 'final': 543, 'devoted': 544, 'managed': 545, 'effectively': 546, '11)': 547, 'concludes': 548, 'afterword,': 549, 'briefly': 550, 'discussing': 551, 'past': 552, 'future': 553, 'each': 554, 'chapter,': 555, 'switch': 556, 'presentation': 557, '.\\nin': 558, 'style,': 559, 'driver': 560, 'language,\\nexplore': 561, 'concepts,': 562, 'use': 563, 'support\\nthe': 564, 'discussion': 565, 'often': 566, 'employ': 567, 'constructs': 568, 'have\\nnot': 569, 'been': 570, 'introduced': 571, 'systematically,': 572, 'their': 573, 'purpose\\nbefore': 574, 'delving': 575, 'details': 576, 'why': 577, 'just': 578, 'learning': 579, 'idiomatic': 580, 'expressions': 581, 'foreign': 582, \"language:\\nyou're\": 583, 'buy': 584, 'nice': 585, 'pastry': 586, 'without': 587, 'first': 588, 'having': 589, 'learnt\\nthe': 590, 'intricacies': 591, 'question': 592, 'formation': 593, 'presentation,': 594, \".\\nwe'll\": 595, 'algorithms,': 596, 'examples\\nwill': 597, 'supporting': 598, '.\\neach': 599, 'ends': 600, 'exercises,\\nwhich': 601, 'useful': 602, 'consolidating': 603, 'according': 604, 'following': 605, 'scheme:\\n☼': 606, 'easy': 607, 'involve': 608, 'minor': 609, 'modifications\\nto': 610, 'supplied': 611, 'samples': 612, 'activities;\\n◑': 613, 'intermediate': 614, 'aspect\\nof': 615, 'depth,': 616, 'requiring': 617, 'design;\\n★': 618, 'difficult,': 619, 'open-ended': 620, 'tasks': 621, 'challenge': 622, 'your\\nunderstanding': 623, 'force': 624, 'think': 625, 'independently\\n(readers': 626, 'should': 627, 'skip': 628, 'these)': 629, 'has': 630, 'further': 631, 'section': 632, 'online': 633, 'extras\\nsection': 634, '.org/,': 635, 'pointers': 636, 'and\\nonline': 637, 'versions': 638, 'also\\navailable': 639, 'there': 640, '.\\n\\n\\nwhy': 641, 'python?\\npython': 642, 'yet': 643, 'powerful': 644, 'excellent\\nfunctionality': 645, 'be\\ndownloaded': 646, 'free': 647, '.\\ninstallers': 648, '.\\nhere': 649, 'five-line': 650, 'program': 651, 'processes': 652, 'file': 653, '.txt\\nand': 654, 'prints': 655, 'words': 656, 'ending': 657, 'ing:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 658, 'line': 659, 'open(file': 660, '.txt):\\n': 661, 'word': 662, '.split():\\n': 663, \".endswith('ing'):\\n\": 664, 'print(word)\\n\\n\\n\\nthis': 665, 'illustrates': 666, 'some': 667, 'main': 668, 'first,\\nwhitespace': 669, 'nest': 670, 'lines': 671, 'code,': 672, 'thus': 673, 'starting\\nwith': 674, 'falls': 675, 'inside': 676, 'scope': 677, 'starting': 678, 'with\\nfor;': 679, 'ensures': 680, 'ing': 681, 'test': 682, 'performed': 683, 'each\\nword': 684, 'second,': 685, 'object-oriented;': 686, 'variable': 687, 'entity\\nthat': 688, 'certain': 689, 'defined': 690, 'attributes': 691, 'the\\nvalue': 692, 'than': 693, 'sequence': 694, 'characters': 695, 'string': 696, 'object': 697, 'method': 698, '(or': 699, 'operation)': 700, 'called\\nsplit()': 701, 'break': 702, 'apply\\na': 703, 'object,': 704, 'name,': 705, 'period,\\nfollowed': 706, 'i': 707, '.e': 708, '.split()': 709, 'third,': 710, 'methods\\nhave': 711, 'arguments': 712, 'expressed': 713, 'parentheses': 714, 'instance,': 715, 'the\\nexample,': 716, \".endswith('ing')\": 717, 'had': 718, 'argument': 719, \"'ing'\": 720, 'to\\nindicate': 721, 'wanted': 722, 'something': 723, 'else': 724, '.\\nfinally': 725, 'most': 726, 'importantly': 727, '—\\npython': 728, 'readable,': 729, 'much': 730, 'fairly': 731, 'guess\\nwhat': 732, 'does': 733, 'even': 734, 'never': 735, 'program\\nbefore': 736, 'chose': 737, 'because': 738, 'shallow': 739, 'curve,\\nits': 740, 'syntax': 741, 'semantics': 742, 'transparent,\\nand': 743, 'good': 744, 'string-handling': 745, 'functionality': 746, 'interpreted\\nlanguage,': 747, 'facilitates': 748, 'interactive': 749, 'exploration': 750, 'an\\nobject-oriented': 751, 'language,': 752, 'permits': 753, 'be\\nencapsulated': 754, 're-used': 755, 'easily': 756, 'dynamic': 757, 'python\\npermits': 758, 'added': 759, 'objects': 760, 'fly,': 761, 'permits\\nvariables': 762, 'typed': 763, 'dynamically,': 764, 'facilitating': 765, '.\\npython': 766, 'comes': 767, 'extensive\\nstandard': 768, 'library,': 769, 'including': 770, 'components': 771, 'graphical': 772, 'programming,\\nnumerical': 773, 'processing,': 774, 'connectivity': 775, 'heavily': 776, 'scientific': 777, 'research,': 778, 'education\\naround': 779, 'world': 780, 'praised': 781, 'facilitates\\nproductivity,': 782, 'quality,': 783, 'maintainability': 784, 'collection': 785, 'of\\npython': 786, 'success': 787, 'stories': 788, 'posted': 789, '.org/about/success/': 790, 'defines': 791, 'infrastructure': 792, 'build': 793, 'nlp\\nprograms': 794, 'provides\\nbasic': 795, 'classes': 796, 'representing': 797, 'processing;\\nstandard': 798, 'interfaces': 799, 'performing': 800, 'as\\npart-of-speech': 801, 'tagging,': 802, 'parsing,': 803, 'classification;\\nand': 804, 'implementations': 805, 'task': 806, 'which': 807, 'combined': 808, 'solve': 809, 'complex': 810, 'problems': 811, 'documentation': 812, 'addition': 813, 'this\\nbook,': 814, 'website': 815, 'api': 816, 'documentation\\nthat': 817, 'covers': 818, 'every': 819, 'module,': 820, 'class': 821, 'function': 822, 'toolkit,\\nspecifying': 823, 'parameters': 824, 'giving': 825, 'usage': 826, '.\\n\\n\\npython': 827, '3': 828, 'nltk': 829, '3\\nthis': 830, 'version': 831, 'updated': 832, 'support': 833, 'nltk\\n3': 834, 'significant': 835, 'changes:\\n\\nthe': 836, 'print': 837, 'statement': 838, 'now': 839, 'parentheses;\\nmany': 840, 'functions': 841, 'return': 842, 'iterators': 843, 'instead': 844, 'lists': 845, '(to': 846, 'save': 847, 'memory': 848, 'usage);\\ninteger': 849, 'division': 850, 'returns': 851, 'floating': 852, 'point': 853, 'number\\nall': 854, 'unicode\\nstrings': 855, 'formatted': 856, 'format': 857, 'method\\n\\nfor': 858, 'detailed': 859, 'list': 860, 'changes,': 861, 'see\\nhttps://docs': 862, '.org/dev/whatsnew/3': 863, '.0': 864, '.html': 865, '.\\nthere': 866, 'utility': 867, '2to3': 868, '.py': 869, 'convert': 870, '2\\ncode': 871, '3;': 872, '.org/2/library/2to3': 873, 'pervasive': 874, 'changes:\\n\\nmany': 875, 'types': 876, 'initialised': 877, 'strings': 878, 'fromstring()': 879, 'method\\nmany': 880, 'lists\\ncontextfreegrammar': 881, 'cfg': 882, 'weightedgrammar': 883, 'pcfg\\nbatch_tokenize()': 884, 'tokenize_sents();': 885, 'corresponding': 886, 'changes': 887, 'batch': 888, 'taggers,': 889, 'parsers,': 890, 'classifiers\\nsome': 891, 'removed': 892, 'favour': 893, 'external': 894, 'packages,': 895, 'maintained': 896, 'adequately\\n\\nfor': 897, 'see\\nhttps://github': 898, '.com/nltk/nltk/wiki/porting-your-code-to-nltk-3': 899, '.\\n\\n\\nsoftware': 900, 'requirements\\nto': 901, 'install': 902, 'several': 903, 'packages': 904, '.\\ncurrent': 905, 'instructions': 906, '.\\n\\n\\n\\n\\npython:the': 907, 'assumes': 908, '.2': 909, 'later': 910, '.\\n(note': 911, 'works': 912, '2': 913, '.6': 914, '.7': 915, '.)\\n\\nnltk:the': 916, 'subsequent': 917, 'releases': 918, 'nltk\\nwill': 919, 'backward-compatible': 920, '.\\n\\nnltk-data:this': 921, 'contains': 922, 'corpora': 923, 'analyzed': 924, 'processed': 925, '.\\n\\nnumpy:(recommended)\\nthis': 926, 'library': 927, 'multidimensional': 928, 'arrays': 929, 'and\\nlinear': 930, 'algebra,': 931, 'required': 932, 'probability,': 933, 'clustering,': 934, 'classification\\ntasks': 935, '.\\n\\nmatplotlib:(recommended)\\nthis': 936, '2d': 937, 'plotting': 938, 'visualization,\\nand': 939, \"book's\": 940, 'produce': 941, 'graphs': 942, 'bar': 943, 'charts': 944, '.\\n\\nstanford': 945, 'tools:\\n\\xa0(recommended)\\nnltk': 946, 'stanford': 947, 'tools': 948, 'scale\\nlanguage': 949, '(see': 950, 'http://nlp': 951, '.stanford': 952, '.edu/software/)': 953, '.\\n\\nnetworkx:(optional)\\nthis': 954, 'storing': 955, 'manipulating': 956, 'network': 957, 'consisting': 958, 'of\\nnodes': 959, 'edges': 960, 'visualizing': 961, 'semantic': 962, 'networks,': 963, 'graphviz': 964, '.\\n\\nprover9:(optional)\\nthis': 965, 'automated': 966, 'theorem': 967, 'prover': 968, 'first-order': 969, 'equational': 970, 'logic,': 971, 'used\\nto': 972, 'inference': 973, '.\\n\\n\\n\\n\\n\\nnatural': 974, '(nltk)\\nnltk': 975, 'was': 976, 'originally': 977, 'created': 978, '2001': 979, 'part': 980, 'linguistics\\ncourse': 981, 'department': 982, 'the\\nuniversity': 983, 'pennsylvania': 984, 'since': 985, 'then': 986, 'developed\\nand': 987, 'expanded': 988, 'dozens': 989, 'contributors': 990, 'been\\nadopted': 991, 'universities,': 992, 'serves': 993, 'basis\\nof': 994, 'research': 995, 'projects': 996, 'viii': 997, 'most\\nimportant': 998, 'modules': 999, '.\\n\\n\\n\\n\\n\\n\\n\\nlanguage': 1000, 'task\\nnltk': 1001, 'modules\\nfunctionality\\n\\n\\n\\naccessing': 1002, 'corpora\\ncorpus\\nstandardized': 1003, 'lexicons\\n\\nstring': 1004, 'processing\\ntokenize,': 1005, 'stem\\ntokenizers,': 1006, 'sentence': 1007, 'tokenizers,': 1008, 'stemmers\\n\\ncollocation': 1009, 'discovery\\ncollocations\\nt-test,': 1010, 'chi-squared,': 1011, 'point-wise': 1012, 'mutual': 1013, 'information\\n\\npart-of-speech': 1014, 'tagging\\ntag\\nn-gram,': 1015, 'backoff,': 1016, 'brill,': 1017, 'hmm,': 1018, 'tnt\\n\\nmachine': 1019, 'learning\\nclassify,': 1020, 'cluster,': 1021, 'tbl\\ndecision': 1022, 'tree,': 1023, 'maximum': 1024, 'entropy,': 1025, 'naive': 1026, 'bayes,': 1027, 'em,': 1028, 'k-means\\n\\nchunking\\nchunk\\nregular': 1029, 'expression,': 1030, 'n-gram,': 1031, 'named-entity\\n\\nparsing\\nparse,': 1032, 'ccg\\nchart,': 1033, 'feature-based,': 1034, 'unification,': 1035, 'probabilistic,': 1036, 'dependency\\n\\nsemantic': 1037, 'interpretation\\nsem,': 1038, 'inference\\nlambda': 1039, 'calculus,': 1040, 'model': 1041, 'checking\\n\\nevaluation': 1042, 'metrics\\nmetrics\\nprecision,': 1043, 'recall,': 1044, 'agreement': 1045, 'coefficients\\n\\nprobability': 1046, 'estimation\\nprobability\\nfrequency': 1047, 'distributions,': 1048, 'smoothed': 1049, 'probability': 1050, 'distributions\\n\\napplications\\napp,': 1051, 'chat\\ngraphical': 1052, 'concordancer,': 1053, 'wordnet': 1054, 'browser,': 1055, 'chatbots\\n\\nlinguistic': 1056, 'fieldwork\\ntoolbox\\nmanipulate': 1057, 'sil': 1058, 'toolbox': 1059, 'format\\n\\n\\ntable': 1060, 'functionality\\n\\n\\nnltk': 1061, 'designed': 1062, 'four': 1063, 'primary': 1064, 'mind:\\n\\n\\n\\n\\nsimplicity:to': 1065, 'intuitive': 1066, 'framework': 1067, 'with\\nsubstantial': 1068, 'building': 1069, 'blocks,': 1070, 'users': 1071, 'practical\\nknowledge': 1072, 'getting': 1073, 'bogged': 1074, 'tedious\\nhouse-keeping': 1075, 'usually': 1076, 'associated': 1077, 'annotated\\nlanguage': 1078, 'data\\n\\nconsistency:to': 1079, 'uniform': 1080, 'consistent\\ninterfaces': 1081, 'structures,': 1082, 'easily-guessable': 1083, 'names\\n\\nextensibility:to': 1084, 'structure': 1085, 'software\\nmodules': 1086, 'accommodated,': 1087, 'alternative\\nimplementations': 1088, 'competing': 1089, 'approaches': 1090, 'same': 1091, 'task\\n\\nmodularity:to': 1092, 'used\\nindependently': 1093, 'needing': 1094, 'understand': 1095, 'rest': 1096, 'of\\nthe': 1097, 'toolkit\\n\\n\\n\\ncontrasting': 1098, 'non-requirements': 1099, '—\\npotentially': 1100, 'qualities': 1101, 'deliberately': 1102, 'avoided': 1103, 'first,\\nwhile': 1104, 'functions,': 1105, 'not\\nencyclopedic;': 1106, 'toolkit,': 1107, 'system,': 1108, 'will\\ncontinue': 1109, 'evolve': 1110, '.\\nsecond,': 1111, 'while': 1112, 'efficient': 1113, 'enough': 1114, 'support\\nmeaningful': 1115, 'tasks,': 1116, 'optimized': 1117, 'runtime': 1118, 'performance;\\nsuch': 1119, 'optimizations': 1120, 'algorithms,\\nor': 1121, 'lower-level': 1122, 'c': 1123, 'c++': 1124, 'would': 1125, 'make': 1126, 'less': 1127, 'readable': 1128, 'difficult': 1129, '.\\nthird,': 1130, 'avoid': 1131, 'clever': 1132, 'tricks,\\nsince': 1133, 'believe': 1134, 'clear': 1135, 'preferable\\nto': 1136, 'ingenious': 1137, 'indecipherable': 1138, 'ones': 1139, '.\\n\\n\\nfor': 1140, 'instructors\\nnatural': 1141, 'taught': 1142, 'within': 1143, 'the\\nconfines': 1144, 'single-semester': 1145, 'course': 1146, 'undergraduate': 1147, 'level\\nor': 1148, 'postgraduate': 1149, 'level': 1150, 'instructors': 1151, 'found': 1152, 'is\\ndifficult': 1153, 'both': 1154, 'theoretical': 1155, 'sides': 1156, 'the\\nsubject': 1157, 'span': 1158, 'time': 1159, 'focus': 1160, 'to\\nthe': 1161, 'exclusion': 1162, 'exercises,': 1163, 'deprive': 1164, 'students': 1165, 'the\\nchallenge': 1166, 'excitement': 1167, 'automatically': 1168, 'process\\nlanguage': 1169, 'simply': 1170, 'for\\nlinguists,': 1171, 'do': 1172, 'manage': 1173, 'developed': 1174, 'address': 1175, 'problem,\\nmaking': 1176, 'feasible': 1177, 'amount': 1178, 'and\\npractice': 1179, 'course,': 1180, 'no\\nprior': 1181, 'experience': 1182, '.\\na': 1183, 'fraction': 1184, 'syllabus': 1185, 'deals': 1186, 'with\\nalgorithms': 1187, 'own': 1188, 'rather\\ndry,': 1189, 'but': 1190, 'brings': 1191, 'life': 1192, 'of\\ninteractive': 1193, 'user': 1194, 'possible\\nto': 1195, 'view': 1196, 'step-by-step': 1197, 'include\\na': 1198, 'demonstration': 1199, 'performs': 1200, 'without\\nrequiring': 1201, 'special': 1202, 'input': 1203, '.\\nan': 1204, 'effective': 1205, 'deliver': 1206, 'interactive\\npresentation': 1207, 'entering': 1208, 'session,\\nobserving': 1209, 'what': 1210, 'do,': 1211, 'modifying': 1212, 'empirical\\nor': 1213, 'issue': 1214, 'used\\nas': 1215, 'basis': 1216, 'student': 1217, 'assignments': 1218, 'simplest': 1219, 'involve\\nmodifying': 1220, 'fragment': 1221, 'specified': 1222, 'to\\nanswer': 1223, 'concrete': 1224, 'end': 1225, 'spectrum,': 1226, 'nltk\\nprovides': 1227, 'flexible': 1228, 'graduate-level': 1229, 'projects,\\nwith': 1230, 'basic': 1231, 'structures\\nand': 1232, 'widely': 1233, 'datasets': 1234, '(corpora),\\nand': 1235, 'extensible': 1236, 'architecture': 1237, 'additional': 1238, 'for\\nteaching': 1239, '.\\n\\nwe': 1240, 'unique': 1241, 'comprehensive\\nframework': 1242, 'context': 1243, 'learning\\nto': 1244, 'sets': 1245, 'these\\nmaterials': 1246, 'apart': 1247, 'tight': 1248, 'coupling': 1249, 'chapters\\nand': 1250, 'those': 1251, 'with\\nno': 1252, 'to\\nnlp': 1253, 'after': 1254, 'completing': 1255, 'materials,': 1256, 'ready': 1257, 'to\\nattempt': 1258, 'textbooks,': 1259, 'speech': 1260, 'and\\nlanguage': 1261, 'jurafsky': 1262, 'martin': 1263, '(prentice': 1264, 'hall,': 1265, '2008)': 1266, 'presents': 1267, 'unusual': 1268, 'order,': 1269, 'beginning\\nwith': 1270, 'non-trivial': 1271, 'type': 1272, 'introducing\\nnon-trivial': 1273, 'control': 1274, 'comprehensions': 1275, 'conditionals': 1276, '.\\nthese': 1277, 'idioms': 1278, 'permit': 1279, 'start': 1280, '.\\nonce': 1281, 'place,': 1282, 'systematic': 1283, 'presentation\\nof': 1284, 'strings,': 1285, 'loops,': 1286, 'files,': 1287, 'forth': 1288, 'way,': 1289, 'ground': 1290, 'conventional': 1291, 'approaches,\\nwithout': 1292, 'expecting': 1293, 'programming\\nlanguage': 1294, 'sake': 1295, '.\\ntwo': 1296, 'possible': 1297, 'plans': 1298, 'illustrated': 1299, 'ix': 1300, 'first\\none': 1301, 'presumes': 1302, 'arts/humanities': 1303, 'audience,': 1304, 'whereas': 1305, 'second': 1306, 'presumes\\na': 1307, 'science/engineering': 1308, 'audience': 1309, 'first\\nfive': 1310, 'chapters,': 1311, 'devote': 1312, 'remaining': 1313, 'single': 1314, 'area,\\nsuch': 1315, 'classification': 1316, '6-7),': 1317, '8-9),\\nsemantics': 1318, '10),': 1319, 'management': 1320, '.\\n\\n\\n\\n\\n\\n\\n\\nchapter\\narts': 1321, 'humanities\\nscience': 1322, 'engineering\\n\\n\\n\\n1': 1323, 'python\\n2-4\\n2\\n\\n2': 1324, 'accessing': 1325, 'lexical': 1326, 'resources\\n2-4\\n2\\n\\n3': 1327, 'raw': 1328, 'text\\n2-4\\n2\\n\\n4': 1329, 'programs\\n2-4\\n1-2\\n\\n5': 1330, 'categorizing': 1331, 'tagging': 1332, 'words\\n2-4\\n2-4\\n\\n6': 1333, 'classify': 1334, 'text\\n0-2\\n2-4\\n\\n7': 1335, 'extracting': 1336, 'text\\n2\\n2-4\\n\\n8': 1337, 'analyzing': 1338, 'structure\\n2-4\\n2-4\\n\\n9': 1339, 'feature': 1340, 'grammars\\n2-4\\n1-4\\n\\n10': 1341, 'sentences\\n1-2\\n1-4\\n\\n11': 1342, 'managing': 1343, 'data\\n1-2\\n1-4\\n\\ntotal\\n18-36\\n18-36\\n\\n\\ntable': 1344, 'suggested': 1345, 'plans;': 1346, 'approximate': 1347, 'number': 1348, 'lectures': 1349, 'per': 1350, 'chapter\\n\\n\\n\\n\\nconventions': 1351, 'book\\nthe': 1352, 'typographical': 1353, 'conventions': 1354, 'book:\\nbold': 1355, '--': 1356, 'indicates': 1357, 'terms': 1358, '.\\nitalic': 1359, 'paragraphs': 1360, 'refer': 1361, 'examples,\\nthe': 1362, 'names': 1363, 'texts,': 1364, 'urls;': 1365, 'filenames': 1366, 'extensions': 1367, '.\\nconstant': 1368, 'width': 1369, 'listings,\\nas': 1370, 'well': 1371, 'elements\\nsuch': 1372, 'names,': 1373, 'statements,': 1374, 'keywords;\\nalso': 1375, 'bold': 1376, 'commands': 1377, 'should\\nbe': 1378, 'literally': 1379, 'italic': 1380, 'be\\nreplaced': 1381, 'user-supplied': 1382, 'values': 1383, 'values\\ndetermined': 1384, 'context;': 1385, 'metavariables': 1386, '.\\n\\nnote\\nthis': 1387, 'icon': 1388, 'signifies': 1389, 'tip,': 1390, 'suggestion,': 1391, 'note': 1392, '.\\n\\n\\ncaution!\\nthis': 1393, 'warning': 1394, 'caution': 1395, '.\\n\\n\\n\\nusing': 1396, 'examples\\nthis': 1397, 'here': 1398, 'job': 1399, 'done': 1400, 'general,': 1401, 'may': 1402, 'in\\nthis': 1403, 'contact': 1404, 'for\\npermission': 1405, 'youõre': 1406, 'reproducing': 1407, 'portion': 1408, 'example,\\nwriting': 1409, 'uses': 1410, 'chunks': 1411, 'require\\npermission': 1412, 'selling': 1413, 'distributing': 1414, 'cd-rom': 1415, 'oõreilly': 1416, 'books': 1417, 'does\\nrequire': 1418, 'permission': 1419, 'answering': 1420, 'citing': 1421, 'quoting': 1422, 'example\\ncode': 1423, 'require': 1424, 'incorporating': 1425, 'example': 1426, 'code\\nfrom': 1427, 'productõs': 1428, 'appreciate,': 1429, 'require,': 1430, 'attribution': 1431, 'title,\\nauthor,': 1432, 'publisher,': 1433, 'isbn': 1434, 'example:': 1435, 'ònatural': 1436, 'python,\\nby': 1437, 'steven': 1438, 'bird,': 1439, 'ewan': 1440, 'klein,': 1441, 'edward': 1442, 'loper': 1443, \"o'reilly\": 1444, 'media,': 1445, '978-0-596-51649-9': 1446, '.ó\\nif': 1447, 'feel': 1448, 'outside': 1449, 'fair': 1450, 'given': 1451, 'above,\\nfeel': 1452, 'permissions@oreilly': 1453, '.com': 1454, '.\\n\\n\\nacknowledgments\\nthe': 1455, 'authors': 1456, 'indebted': 1457, 'feedback': 1458, 'on\\nearlier': 1459, 'drafts': 1460, 'book:\\ndoug': 1461, 'arnold,\\nmichaela': 1462, 'atterer,\\ngreg': 1463, 'aumann,\\nkenneth': 1464, 'beesley,\\nsteven': 1465, 'bethard,\\nondrej': 1466, 'bojar,\\nchris': 1467, 'cieri,\\nrobin': 1468, 'cooper,\\ngrev': 1469, 'corbett,\\njames': 1470, 'curran,\\ndan': 1471, 'garrette,\\njean': 1472, 'mark': 1473, 'gawron,\\ndoug': 1474, 'hellmann,\\nnitin': 1475, 'indurkhya,\\nmark': 1476, 'liberman,\\npeter': 1477, 'ljunglöf,\\nstefan': 1478, 'müller,\\nrobin': 1479, 'munn,\\njoel': 1480, 'nothman,\\nadam': 1481, 'przepiorkowski,\\nbrandon': 1482, 'rhodes,\\nstuart': 1483, 'robinson,\\njussi': 1484, 'salmela,\\nkyle': 1485, 'schlansker,\\nrob': 1486, 'speer,\\nand\\nrichard': 1487, 'sproat': 1488, 'thankful': 1489, 'colleagues': 1490, 'comments': 1491, 'on\\nthe': 1492, 'evolved': 1493, 'chapters,\\nincluding': 1494, 'participants': 1495, 'summer': 1496, 'schools\\nin': 1497, 'brazil,': 1498, 'india,': 1499, 'usa': 1500, 'exist': 1501, 'members': 1502, 'nltk-dev\\ndeveloper': 1503, 'community,': 1504, 'named': 1505, 'website,\\nwho': 1506, 'expertise': 1507, 'extending': 1508, 'grateful': 1509, 'u': 1510, '.s': 1511, 'national': 1512, 'foundation,': 1513, 'consortium,\\nan': 1514, 'clarence': 1515, 'dyason': 1516, 'fellowship,\\nand': 1517, 'universities': 1518, 'pennsylvania,': 1519, 'edinburgh,': 1520, 'melbourne': 1521, 'our': 1522, 'work\\non': 1523, 'thank': 1524, 'julie': 1525, 'steele,': 1526, 'abby': 1527, 'fox,': 1528, 'loranah': 1529, 'dimant,': 1530, 'team,': 1531, 'for\\norganizing': 1532, 'comprehensive': 1533, 'reviews': 1534, 'nlp\\nand': 1535, 'communities,': 1536, 'cheerfully': 1537, 'customizing': 1538, \"o'reilly's\": 1539, 'production': 1540, 'tools,\\nand': 1541, 'meticulous': 1542, 'copy-editing': 1543, 'preparing': 1544, 'revised': 1545, 'edition': 1546, '3,': 1547, 'to\\nmichael': 1548, 'korobov': 1549, 'leading': 1550, 'effort': 1551, 'port': 1552, 'to\\nantoine': 1553, 'trux': 1554, 'his': 1555, '.\\nfinally,': 1556, 'owe': 1557, 'huge': 1558, 'debt': 1559, 'gratitude': 1560, 'mimo': 1561, 'jee\\nfor': 1562, 'love,': 1563, 'patience,': 1564, 'over': 1565, 'years': 1566, 'worked': 1567, 'hope': 1568, 'children': 1569, 'andrew,': 1570, 'alison,': 1571, 'kirsten,': 1572, 'leonie,': 1573, 'maaike': 1574, '—\\ncatch': 1575, 'enthusiasm': 1576, 'computation': 1577, 'pages': 1578, '.\\n\\n\\n\\nabout': 1579, 'authors\\nsteven': 1580, 'bird': 1581, 'associate': 1582, 'professor': 1583, 'the\\ndepartment': 1584, 'engineering\\nat': 1585, 'university': 1586, 'melbourne,': 1587, 'senior': 1588, 'the\\nlinguistic': 1589, 'consortium': 1590, '.\\nhe': 1591, 'completed': 1592, 'phd': 1593, 'phonology': 1594, 'of\\nedinburgh': 1595, '1990,': 1596, 'supervised': 1597, 'klein': 1598, 'moved': 1599, 'cameroon': 1600, 'conduct': 1601, 'fieldwork': 1602, 'the\\ngrassfields': 1603, 'bantu': 1604, 'under': 1605, 'auspices': 1606, 'institute\\nof': 1607, 'recently,': 1608, 'he': 1609, 'spent\\nseveral': 1610, 'director': 1611, 'consortium\\nwhere': 1612, 'led': 1613, 'r&d': 1614, 'team': 1615, 'create': 1616, 'models': 1617, 'large\\ndatabases': 1618, 'annotated': 1619, 'university,\\nhe': 1620, 'established': 1621, 'technology': 1622, 'group': 1623, 'has\\ntaught': 1624, 'levels': 1625, 'curriculum': 1626, '2009,': 1627, 'president': 1628, 'association': 1629, '.\\newan': 1630, 'school': 1631, 'of\\ninformatics': 1632, 'edinburgh': 1633, 'on\\nformal': 1634, 'cambridge': 1635, '1978': 1636, 'some\\nyears': 1637, 'sussex': 1638, 'newcastle': 1639, 'upon': 1640, 'tyne,\\newan': 1641, 'took': 1642, 'teaching': 1643, 'position': 1644, 'involved': 1645, 'the\\nestablishment': 1646, \"edinburgh's\": 1647, '1993,': 1648, 'has\\nbeen': 1649, 'closely': 1650, 'ever': 1651, '2000–2002,\\nhe': 1652, 'leave': 1653, 'act': 1654, 'manager': 1655, 'the\\nedinburgh-based': 1656, 'edify': 1657, 'corporation,\\nsanta': 1658, 'clara,': 1659, 'responsible': 1660, 'spoken': 1661, 'dialogue': 1662, 'ewan\\nis': 1663, 'european': 1664, 'for\\ncomputational': 1665, 'founding': 1666, 'member': 1667, 'coordinator': 1668, 'excellence': 1669, 'human': 1670, 'technologies\\n(elsnet)': 1671, '.\\nedward': 1672, 'recently': 1673, 'phd\\non': 1674, 'processing\\nat': 1675, \"steven's\": 1676, 'graduate': 1677, 'on\\ncomputational': 1678, 'fall': 1679, '2000,': 1680, 'and\\nwent': 1681, 'ta': 1682, 'share': 1683, 'of\\nnltk': 1684, 'has\\nhelped': 1685, 'two': 1686, 'documenting': 1687, 'and\\ntesting': 1688, 'epydoc': 1689, 'doctest': 1690, '.\\n\\n\\nroyalties\\nroyalties': 1691, 'sale': 1692, '.\\n\\n\\nfigure': 1693, 'xiv': 1694, 'loper,': 1695, 'stanford,': 1696, 'july': 1697, '2007\\n\\n\\n\\n\\nabout': 1698, 'document': 1699, '.\\nupdated': 1700, 'loper,\\ncopyright': 1701, '©': 1702, '2019': 1703, 'distributed': 1704, '[http://nltk': 1705, '.org/],\\nversion': 1706, '.0,': 1707, 'the\\ncreative': 1708, 'commons': 1709, 'attribution-noncommercial-no': 1710, 'derivative': 1711, 'united': 1712, 'states': 1713, 'license\\n[http://creativecommons': 1714, '.org/licenses/by-nc-nd/3': 1715, '.0/us/]': 1716, 'built': 1717, 'on\\nwed': 1718, '4': 1719, 'sep': 1720, '11:40:48': 1721, 'acst1': 1722, 'python\\n\\n\\n\\n\\n\\n1': 1723, 'python\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nit': 1724, 'hands': 1725, 'millions': 1726, '.\\nwhat': 1727, 'it,': 1728, 'assuming': 1729, 'programs?\\nin': 1730, \"we'll\": 1731, 'questions:\\n\\nwhat': 1732, 'achieve': 1733, 'combining': 1734, 'quantities': 1735, 'text?\\nhow': 1736, 'extract': 1737, 'phrases': 1738, 'sum': 1739, 'text?\\nwhat': 1740, 'work?\\nwhat': 1741, 'challenges': 1742, 'processing?\\n\\nthis': 1743, 'divided': 1744, 'sections': 1745, 'quite\\ndifferent': 1746, 'will\\ntake': 1747, 'linguistically': 1748, 'motivated': 1749, 'necessarily\\nexplaining': 1750, 'closer': 1751, 'we\\nwill': 1752, 'systematically': 1753, 'review': 1754, 'flag': 1755, 'titles,\\nbut': 1756, 'mix': 1757, 'up-front': 1758, 'gives': 1759, 'an\\nauthentic': 1760, 'taste': 1761, 'later,': 1762, 'of\\nelementary': 1763, 'familiarity': 1764, 'areas,': 1765, 'to\\n5;\\nwe': 1766, 'repeat': 1767, 'points': 1768, 'miss': 1769, 'anything\\nyou': 1770, 'completely': 1771, 'you,': 1772, 'raise\\nmore': 1773, 'questions': 1774, 'answers,': 1775, 'addressed': 1776, 'in\\nthe': 1777, '.\\n\\n1\\xa0\\xa0\\xa0computing': 1778, 'language:': 1779, \"words\\nwe're\": 1780, 'very': 1781, 'familiar': 1782, 'text,': 1783, 'day': 1784, 'treat': 1785, 'write,\\nprograms': 1786, '.\\nbut': 1787, 'before': 1788, 'started': 1789, 'interpreter': 1790, '.\\n\\n1': 1791, '.1\\xa0\\xa0\\xa0getting': 1792, 'python\\none': 1793, 'friendly': 1794, 'things': 1795, 'you\\nto': 1796, 'directly': 1797, '—\\nthe': 1798, 'running': 1799, '.\\nyou': 1800, 'interface\\ncalled': 1801, 'environment': 1802, '(idle)': 1803, '.\\non': 1804, 'mac': 1805, 'find': 1806, 'applications→macpython,\\nand': 1807, 'windows': 1808, 'programs→python': 1809, '.\\nunder': 1810, 'run': 1811, 'shell': 1812, 'typing': 1813, 'idle\\n(if': 1814, 'installed,': 1815, 'python)': 1816, 'blurb': 1817, 'version;\\nsimply': 1818, 'check': 1819, 'later\\n(here': 1820, '.4': 1821, '.2):\\n\\n\\n\\n\\n\\xa0\\n\\npython': 1822, '(default,': 1823, 'oct': 1824, '15': 1825, '2014,': 1826, '22:01:37)\\n[gcc': 1827, 'compatible': 1828, 'apple': 1829, 'llvm': 1830, '5': 1831, '(clang-503': 1832, '.40)]': 1833, 'darwin\\ntype': 1834, 'help,': 1835, 'copyright,': 1836, 'credits': 1837, 'license': 1838, '.\\n>>>\\n\\n\\n\\n\\nnote\\nif': 1839, 'unable': 1840, 'interpreter,': 1841, 'probably': 1842, \"don't\\nhave\": 1843, 'installed': 1844, 'correctly': 1845, 'visit': 1846, 'for\\ndetailed': 1847, 'and\\n2': 1848, 'older': 1849, 'versions,': 1850, 'that\\nthe': 1851, '/': 1852, 'operator': 1853, 'rounds\\nfractional': 1854, 'results': 1855, 'downwards': 1856, '(so': 1857, '1/3': 1858, '0)': 1859, 'expected': 1860, 'behavior': 1861, 'division\\nyou': 1862, 'type:': 1863, '__future__': 1864, 'import': 1865, 'division\\n\\nthe': 1866, '>>>': 1867, 'prompt': 1868, 'waiting\\nfor': 1869, 'when': 1870, 'copying': 1871, 'type\\nthe': 1872, 'yourself': 1873, 'now,': 1874, \"let's\": 1875, 'begin': 1876, 'calculator:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 1877, '+': 1878, '*': 1879, '-': 1880, '3\\n8\\n>>>\\n\\n\\n\\nonce': 1881, 'finished': 1882, 'calculating': 1883, 'answer': 1884, 'displaying': 1885, 'the\\nprompt': 1886, 'reappears': 1887, 'means': 1888, 'waiting': 1889, 'another': 1890, 'instruction': 1891, '.\\n\\nnote\\nyour': 1892, 'turn:\\nenter': 1893, 'few': 1894, 'asterisk': 1895, '(*)\\nfor': 1896, 'multiplication': 1897, 'slash': 1898, '(/)': 1899, 'division,': 1900, 'for\\nbracketing': 1901, '.\\n\\n\\nthe': 1902, 'demonstrate': 1903, 'interactively': 1904, 'the\\npython': 1905, 'experimenting': 1906, 'various': 1907, 'language\\nto': 1908, '.\\nnow': 1909, 'nonsensical': 1910, 'expression': 1911, 'handles': 1912, 'it:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 1913, '+\\n': 1914, '<stdin>,': 1915, '1\\n': 1916, '^\\nsyntaxerror:': 1917, 'invalid': 1918, 'syntax\\n>>>\\n\\n\\n\\nthis': 1919, 'produced': 1920, 'error': 1921, \"doesn't\": 1922, 'sense\\nto': 1923, 'plus': 1924, 'sign': 1925, 'interpreter\\nindicates': 1926, 'where': 1927, 'problem': 1928, 'occurred': 1929, '(line': 1930, '<stdin>,\\nwhich': 1931, 'stands': 1932, 'input)': 1933, \"we're\": 1934, 'working\\nwith': 1935, '.\\n\\n\\n1': 1936, '.2\\xa0\\xa0\\xa0getting': 1937, 'nltk\\nbefore': 1938, 'going': 1939, '.\\nfollow': 1940, 'platform': 1941, \"you've\": 1942, 'as\\nbefore,': 1943, 'by\\ntyping': 1944, 'prompt,': 1945, 'selecting\\nthe': 1946, 'shown': 1947, '.\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 1948, 'nltk\\n>>>': 1949, '.download()\\n\\n\\n\\n\\n\\nfigure': 1950, 'downloading': 1951, 'collection:': 1952, 'browse': 1953, 'packages\\nusing': 1954, '.download()': 1955, 'collections': 1956, 'tab': 1957, 'downloader\\nshows': 1958, 'grouped': 1959, 'sets,': 1960, 'select': 1961, 'labeled\\nbook': 1962, 'obtain': 1963, 'all\\ndata': 1964, 'consists\\nof': 1965, '30': 1966, 'compressed': 1967, 'files': 1968, '100mb': 1969, 'disk': 1970, 'space': 1971, '(i': 1972, '.,': 1973, 'downloader)': 1974, 'is\\nnearly': 1975, 'ten': 1976, 'times': 1977, 'size': 1978, '(at': 1979, 'writing)': 1980, 'continues': 1981, 'expand': 1982, '.\\n\\nonce': 1983, 'downloaded': 1984, 'machine,': 1985, 'load': 1986, 'it\\nusing': 1987, 'step': 1988, 'command': 1989, 'tells': 1990, 'to\\nexplore:': 1991, '.book': 1992, 'says': 1993, \"nltk's\": 1994, 'load\\nall': 1995, 'items': 1996, 'module': 1997, 'need\\nas': 1998, 'printing': 1999, 'welcome': 2000, 'message,': 2001, 'loads\\nthe': 2002, '(this': 2003, 'seconds)': 2004, \"here's\": 2005, 'the\\ncommand': 2006, 'again,': 2007, 'output': 2008, 'that\\nyou': 2009, 'care': 2010, 'spelling': 2011, 'punctuation': 2012, 'right,': 2013, 'and\\nremember': 2014, '*\\n***': 2015, '***\\nloading': 2016, 'text1,': 2017, 'text9': 2018, 'sent1,': 2019, 'sent9\\ntype': 2020, '.\\ntype:': 2021, \"'texts()'\": 2022, \"'sents()'\": 2023, '.\\ntext1:': 2024, 'moby': 2025, 'dick': 2026, 'herman': 2027, 'melville': 2028, '1851\\ntext2:': 2029, 'sensibility': 2030, 'jane': 2031, 'austen': 2032, '1811\\ntext3:': 2033, 'genesis\\ntext4:': 2034, 'inaugural': 2035, 'corpus\\ntext5:': 2036, 'chat': 2037, 'corpus\\ntext6:': 2038, 'monty': 2039, 'holy': 2040, 'grail\\ntext7:': 2041, 'wall': 2042, 'street': 2043, 'journal\\ntext8:': 2044, 'personals': 2045, 'corpus\\ntext9:': 2046, 'man': 2047, 'thursday': 2048, 'g': 2049, 'k': 2050, 'chesterton': 2051, '1908\\n>>>\\n\\n\\n\\nany': 2052, 'have\\nto': 2053, 'enter': 2054, 'prompt:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2055, 'text1\\n<text:': 2056, '1851>\\n>>>': 2057, 'text2\\n<text:': 2058, '1811>\\n>>>\\n\\n\\n\\nnow': 2059, \"with,\\nwe're\": 2060, '.3\\xa0\\xa0\\xa0searching': 2061, 'text\\nthere': 2062, 'from\\nsimply': 2063, 'concordance': 2064, 'occurrence': 2065, 'word,': 2066, 'together\\nwith': 2067, 'monstrous': 2068, 'moby\\ndick': 2069, 'text1': 2070, 'period,': 2071, 'term\\nconcordance,': 2072, 'placing': 2073, 'parentheses:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2074, '.concordance(monstrous)\\ndisplaying': 2075, '11': 2076, 'matches:\\nong': 2077, 'former': 2078, ',': 2079, 'came': 2080, 'towards': 2081, ',\\non': 2082, 'psalms': 2083, 'touching': 2084, 'bulk': 2085, 'whale': 2086, 'ork': 2087, 'r\\nll': 2088, 'heathenish': 2089, 'array': 2090, 'clubs': 2091, 'spears': 2092, 'were': 2093, 'thick\\nd': 2094, 'gazed': 2095, 'wondered': 2096, 'cannibal': 2097, 'savage': 2098, 'hav\\nthat': 2099, 'survived': 2100, 'flood': 2101, ';': 2102, 'mountainous': 2103, '!': 2104, 'himmal\\nthey': 2105, 'might': 2106, 'scout': 2107, 'fable': 2108, 'still': 2109, 'worse': 2110, 'de\\nth': 2111, 'radney': 2112, \".'\": 2113, '55': 2114, 'pictures': 2115, 'whales': 2116, 'shall': 2117, 'ere': 2118, 'l\\ning': 2119, 'scenes': 2120, 'connexion': 2121, 'am': 2122, 'strongly\\nere': 2123, 'fo\\nght': 2124, 'rummaged': 2125, 'cabinet': 2126, 'telling': 2127, 'but\\nof': 2128, 'bones': 2129, 'oftentimes': 2130, 'cast': 2131, 'dead': 2132, 'u\\n>>>\\n\\n\\n\\nthe': 2133, 'particular': 2134, 'takes': 2135, 'a\\nfew': 2136, 'extra': 2137, 'seconds': 2138, 'searches': 2139, 'fast': 2140, 'turn:\\ntry': 2141, 'searching': 2142, 'words;': 2143, 're-typing,': 2144, 'to\\nuse': 2145, 'up-arrow,': 2146, 'ctrl-up-arrow': 2147, 'alt-p': 2148, 'modify': 2149, 'searched': 2150, 'included': 2151, '.\\nfor': 2152, 'search': 2153, 'word\\naffection,': 2154, 'text2': 2155, '.concordance(affection)': 2156, 'genesis\\nto': 2157, 'lived,': 2158, 'using\\ntext3': 2159, '.concordance(lived)': 2160, 'text4,': 2161, 'the\\ninaugural': 2162, 'corpus,': 2163, 'english': 2164, 'going\\nback': 2165, '1789,': 2166, 'nation,': 2167, 'terror,': 2168, 'god\\nto': 2169, 'differently': 2170, \".\\nwe've\": 2171, 'text5,': 2172, 'nps': 2173, 'corpus:': 2174, 'for\\nunconventional': 2175, 'im,': 2176, 'ur,': 2177, 'lol': 2178, 'uncensored!)\\n\\nonce': 2179, 'spent': 2180, 'little': 2181, 'examining': 2182, 'new\\nsense': 2183, 'richness': 2184, 'diversity': 2185, 'chapter\\nyou': 2186, 'broader': 2187, 'in\\nlanguages': 2188, 'saw': 2189, 'that\\nmonstrous': 2190, 'contexts': 2191, '___': 2192, 'pictures\\nand': 2193, 'appear': 2194, 'similar': 2195, 'range\\nof': 2196, 'contexts?': 2197, 'out\\nby': 2198, 'appending': 2199, 'term': 2200, 'in\\nquestion,': 2201, 'inserting': 2202, '.similar(monstrous)\\nmean': 2203, 'maddens': 2204, 'doleful': 2205, 'gamesome': 2206, 'subtly': 2207, 'uncommon': 2208, 'untoward\\nexasperate': 2209, 'loving': 2210, 'passing': 2211, 'mouldy': 2212, 'christian': 2213, 'true': 2214, 'mystifying\\nimperial': 2215, 'modifies': 2216, 'contemptible\\n>>>': 2217, '.similar(monstrous)\\nvery': 2218, 'heartily': 2219, 'exceedingly': 2220, 'remarkably': 2221, 'vast': 2222, 'great': 2223, 'amazingly\\nextremely': 2224, 'sweet\\n>>>\\n\\n\\n\\nobserve': 2225, '.\\nausten': 2226, 'quite': 2227, 'melville;': 2228, 'her,': 2229, 'has\\npositive': 2230, 'connotations,': 2231, 'intensifier': 2232, 'word\\nvery': 2233, 'common_contexts': 2234, 'the\\ncontexts': 2235, 'shared': 2236, 'words,': 2237, 'monstrous\\nand': 2238, 'enclose': 2239, 'square': 2240, 'brackets': 2241, 'as\\nwell': 2242, 'parentheses,': 2243, 'separate': 2244, 'comma:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2245, '.common_contexts([monstrous,': 2246, 'very])\\na_pretty': 2247, 'is_pretty': 2248, 'am_glad': 2249, 'be_glad': 2250, 'a_lucky\\n>>>\\n\\n\\n\\n\\nnote\\nyour': 2251, 'turn:\\npick': 2252, 'pair': 2253, 'using\\nthe': 2254, 'similar()': 2255, 'common_contexts()': 2256, '.\\n\\nit': 2257, 'thing': 2258, 'detect': 2259, 'occurs': 2260, 'text,\\nand': 2261, 'display': 2262, 'however,': 2263, 'determine\\nthe': 2264, 'location': 2265, 'text:': 2266, 'beginning': 2267, 'appears': 2268, 'positional': 2269, 'displayed': 2270, 'dispersion': 2271, 'plot': 2272, 'stripe': 2273, 'represents': 2274, 'instance\\nof': 2275, 'row': 2276, 'entire': 2277, 'we\\nsee': 2278, 'striking': 2279, 'patterns': 2280, 'last': 2281, '220': 2282, 'years\\n(in': 2283, 'constructed': 2284, 'joining\\nthe': 2285, 'end-to-end)': 2286, 'below': 2287, '(e': 2288, '.g': 2289, 'liberty,': 2290, 'constitution),\\nand': 2291, 'predict': 2292, 'the\\ndispersion': 2293, 'it?': 2294, 'before,': 2295, 'take\\ncare': 2296, 'quotes,': 2297, 'commas,': 2298, 'exactly': 2299, 'right': 2300, 'text4': 2301, '.dispersion_plot([citizens,': 2302, 'democracy,': 2303, 'freedom,': 2304, 'duties,': 2305, 'america])\\n>>>\\n\\n\\n\\n\\n\\nfigure': 2306, '.2:': 2307, 'presidential': 2308, 'addresses:\\nthis': 2309, 'investigate': 2310, '.\\n\\n\\nnote\\nimportant:\\nyou': 2311, 'numpy': 2312, 'matplotlib': 2313, 'installed\\nin': 2314, 'plots': 2315, '.\\nplease': 2316, 'installation': 2317, '.\\n\\n\\nnote\\nyou': 2318, 'frequency': 2319, 'using\\nhttps://books': 2320, '.google': 2321, '.com/ngrams\\n\\nnow,': 2322, 'fun,': 2323, 'generating': 2324, 'random': 2325, 'various\\nstyles': 2326, 'seen': 2327, 'text\\nfollowed': 2328, 'generate': 2329, '(we': 2330, 'the\\nparentheses,': 2331, \"there's\": 2332, 'nothing': 2333, 'goes': 2334, '.)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2335, 'text3': 2336, '.generate()\\nin': 2337, 'brother': 2338, 'hairy': 2339, 'whose': 2340, 'top': 2341, 'reach\\nunto': 2342, 'heaven': 2343, 'ye': 2344, 'sow': 2345, 'land': 2346, 'egypt': 2347, 'bread': 2348, 'in\\nall': 2349, 'month': 2350, 'earth': 2351, 'thy\\nwages': 2352, '?': 2353, 'made': 2354, 'father': 2355, 'isaac': 2356, 'old': 2357, 'kissed\\nhim': 2358, ':': 2359, 'laban': 2360, 'cattle': 2361, 'midst': 2362, 'esau': 2363, 'thy\\nfirst': 2364, 'born': 2365, 'phichol': 2366, 'chief': 2367, 'butler': 2368, 'unto': 2369, 'son': 2370, 'she\\n>>>\\n\\n\\n\\n\\nnote\\nthe': 2371, 'generate()': 2372, 'be\\nreinstated': 2373, '.\\n\\n\\n\\n\\n\\n1': 2374, '.4\\xa0\\xa0\\xa0counting': 2375, 'vocabulary\\nthe': 2376, 'obvious': 2377, 'fact': 2378, 'emerges': 2379, 'that\\nthey': 2380, 'differ': 2381, 'vocabulary': 2382, 'the\\ncomputer': 2383, 'count': 2384, '.\\nas': 2385, 'jump': 2386, 'experiment': 2387, 'with\\nthe': 2388, 'though': 2389, 'studied': 2390, 'systematically\\nyet': 2391, 'examples,': 2392, 'trying': 2393, 'the\\nexercises': 2394, \".\\nlet's\": 2395, 'finding': 2396, 'length': 2397, 'finish,\\nin': 2398, 'symbols': 2399, 'the\\nterm': 2400, 'len': 2401, 'something,': 2402, 'apply': 2403, 'the\\nbook': 2404, 'genesis:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2405, 'len(text3)\\n44764\\n>>>\\n\\n\\n\\nso': 2406, 'genesis': 2407, '44,764': 2408, 'symbols,': 2409, 'tokens': 2410, 'token': 2411, 'technical': 2412, 'characters\\n—': 2413, 'hairy,': 2414, 'his,': 2415, ':)': 2416, 'a\\ngroup': 2417, 'say,': 2418, 'phrase\\nto': 2419, 'be,': 2420, 'counting': 2421, 'occurrences': 2422, 'these\\nsequences': 2423, 'thus,': 2424, 'phrase': 2425, 'to,\\ntwo': 2426, 'are\\nonly': 2427, 'distinct': 2428, '.\\nhow': 2429, 'contain?\\nto': 2430, 'pose': 2431, 'slightly\\ndifferently': 2432, 'tokens\\nthat': 2433, 'uses,': 2434, 'set,': 2435, 'duplicates': 2436, 'collapsed\\ntogether': 2437, 'the\\ncommand:': 2438, 'set(text3)': 2439, 'screens': 2440, 'will\\nfly': 2441, 'following:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2442, 'sorted(set(text3))': 2443, \"\\n['!',\": 2444, \"',\": 2445, \"'(',\": 2446, \"')',\": 2447, \"',',\": 2448, \"',)',\": 2449, \"'\": 2450, \".',\": 2451, \".)',\": 2452, \"':',\": 2453, \"';',\": 2454, \"';)',\": 2455, \"'?',\": 2456, \"'?)',\\n'a',\": 2457, \"'abel',\": 2458, \"'abelmizraim',\": 2459, \"'abidah',\": 2460, \"'abide',\": 2461, \"'abimael',\": 2462, \"'abimelech',\\n'abr',\": 2463, \"'abrah',\": 2464, \"'abraham',\": 2465, \"'abram',\": 2466, \"'accad',\": 2467, \"'achbor',\": 2468, \"'adah',\": 2469, '.]\\n>>>': 2470, 'len(set(text3))': 2471, '\\n2789\\n>>>\\n\\n\\n\\nby': 2472, 'wrapping': 2473, 'sorted()': 2474, 'around': 2475, 'set(text3)\\n,': 2476, 'sorted': 2477, 'items,': 2478, 'continuing': 2479, 'all\\ncapitalized': 2480, 'precede': 2481, 'lowercase': 2482, 'discover': 2483, 'indirectly,': 2484, 'asking\\nfor': 2485, 'again': 2486, 'to\\nobtain': 2487, 'although': 2488, 'tokens,': 2489, 'book\\nhas': 2490, 'only': 2491, '2,789': 2492, 'form': 2493, 'independently': 2494, 'its\\nspecific': 2495, 'is,': 2496, 'the\\nword': 2497, 'considered': 2498, 'item': 2499, 'items\\nwill': 2500, 'generally': 2501, 'call': 2502, 'these\\nunique': 2503, '.\\nnow,': 2504, 'calculate': 2505, 'measure': 2506, 'lexical\\nrichness': 2507, 'of\\ndistinct': 2508, '6%': 2509, 'total': 2510, 'equivalently\\nthat': 2511, '16': 2512, 'average\\n(remember': 2513, \"you're\": 2514, '2,': 2515, 'division)': 2516, 'len(text3)\\n0': 2517, '.06230453042623537\\n>>>\\n\\n\\n\\nnext,': 2518, 'occurs\\nin': 2519, 'compute': 2520, 'percentage': 2521, 'specific': 2522, 'word:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2523, '.count(smote)\\n5\\n>>>': 2524, '100': 2525, \".count('a')\": 2526, 'len(text4)\\n1': 2527, '.4643016433938312\\n>>>\\n\\n\\n\\n\\nnote\\nyour': 2528, 'turn:\\nhow': 2529, 'text5?\\nhow': 2530, 'words\\nin': 2531, 'text?\\n\\nyou': 2532, 'calculations': 2533, 'texts,\\nbut': 2534, 'tedious': 2535, 'keep': 2536, 'retyping': 2537, 'formula': 2538, 'instead,\\nyou': 2539, 'task,': 2540, 'like\\nlexical_diversity': 2541, 'percentage,': 2542, 'block': 2543, 'short\\nname': 2544, 'and\\nyou': 2545, 're-use': 2546, 'a\\ntask': 2547, 'function,': 2548, 'and\\nwe': 2549, 'define': 2550, 'keyword': 2551, 'def': 2552, 'the\\nnext': 2553, 'functions,\\nlexical_diversity()': 2554, 'percentage():\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2555, 'lexical_diversity(text):': 2556, '\\n': 2557, 'len(set(text))': 2558, 'len(text)': 2559, '.\\n>>>': 2560, 'percentage(count,': 2561, 'total):': 2562, 'total\\n': 2563, '.\\n\\n\\n\\n\\ncaution!\\nthe': 2564, 'from\\n>>>': 2565, 'encountering': 2566, 'colon': 2567, 'the\\nend': 2568, 'indicates\\nthat': 2569, 'expects': 2570, 'indented': 2571, 'indentation,': 2572, 'four\\nspaces': 2573, 'hitting': 2574, 'finish': 2575, 'just\\nenter': 2576, 'blank': 2577, '.\\n\\nin': 2578, 'definition': 2579, 'lexical_diversity()': 2580, 'we\\nspecify': 2581, 'parameter': 2582, 'is\\na': 2583, 'placeholder': 2584, 'actual': 2585, 'to\\ncompute,': 2586, 'reoccurs': 2587, 'the\\nfunction': 2588, 'similarly,': 2589, 'percentage()': 2590, 'to\\ntake': 2591, 'parameters,': 2592, 'knows': 2593, 'percentage()\\nare': 2594, 'blocks\\nof': 2595, 'go': 2596, 'ahead': 2597, 'functions:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2598, 'lexical_diversity(text3)\\n0': 2599, '.06230453042623537\\n>>>': 2600, 'lexical_diversity(text5)\\n0': 2601, '.13477005109975562\\n>>>': 2602, 'percentage(4,': 2603, '5)\\n80': 2604, '.0\\n>>>': 2605, 'percentage(text4': 2606, \".count('a'),\": 2607, 'len(text4))\\n1': 2608, '.4643016433938312\\n>>>\\n\\n\\n\\nto': 2609, 'recap,': 2610, 'followed\\nby': 2611, 'parenthesis,': 2612, 'close\\nparenthesis': 2613, 'show': 2614, 'often;': 2615, 'separate\\nthe': 2616, 'data\\nthat': 2617, 'place': 2618, 'a\\nfunction': 2619, 'already': 2620, 'encountered': 2621, 'such\\nas': 2622, 'len(),': 2623, 'set(),': 2624, 'convention,': 2625, 'will\\nalways': 2626, 'add': 2627, 'empty': 2628, 'in\\nlen(),': 2629, 'talking': 2630, 'rather': 2631, '.\\nfunctions': 2632, 'concept': 2633, 'only\\nmention': 2634, 'outset': 2635, 'newcomers': 2636, 'the\\npower': 2637, 'creativity': 2638, 'worry': 2639, 'bit\\nconfusing': 2640, '.\\nlater': 2641, 'tabulating': 2642, 'table': 2643, 'but\\nwith': 2644, 'repetitive': 2645, '.\\n\\n\\n\\n\\n\\n\\n\\n\\ngenre\\ntokens\\ntypes\\nlexical': 2646, 'diversity\\n\\n\\n\\nskill': 2647, 'hobbies\\n82345\\n11935\\n0': 2648, '.145\\n\\nhumor\\n21695\\n5017\\n0': 2649, '.231\\n\\nfiction:': 2650, 'science\\n14470\\n3233\\n0': 2651, '.223\\n\\npress:': 2652, 'reportage\\n100554\\n14394\\n0': 2653, '.143\\n\\nfiction:': 2654, 'romance\\n70022\\n8452\\n0': 2655, '.121\\n\\nreligion\\n39399\\n6373\\n0': 2656, '.162\\n\\n\\ntable': 2657, 'genres': 2658, 'brown': 2659, 'corpus\\n\\n\\n\\n\\n\\n2\\xa0\\xa0\\xa0a': 2660, 'python:': 2661, \"words\\n\\nyou've\": 2662, 'elements': 2663, 'moments': 2664, '.\\n\\n2': 2665, '.1\\xa0\\xa0\\xa0lists\\n\\nwhat': 2666, 'text?': 2667, 'level,': 2668, 'page': 2669, 'up\\nof': 2670, 'sections,': 2671, 'paragraphs,\\nand': 2672, 'purposes,': 2673, 'nothing\\nmore': 2674, 'represent\\ntext': 2675, 'case': 2676, 'opening': 2677, 'dick:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2678, 'sent1': 2679, '=': 2680, \"['call',\": 2681, \"'me',\": 2682, \"'ishmael',\": 2683, \".']\\n>>>\\n\\n\\n\\nafter\": 2684, \"we've\": 2685, 'equals': 2686, 'sign,': 2687, 'quoted': 2688, 'separated': 2689, 'with\\ncommas,': 2690, 'surrounded': 2691, 'bracketed': 2692, 'material\\nis': 2693, 'store': 2694, 'inspect': 2695, 'ask': 2696, \"\\n['call',\": 2697, \".']\\n>>>\": 2698, 'len(sent1)': 2699, '\\n4\\n>>>': 2700, 'lexical_diversity(sent1)': 2701, '\\n1': 2702, '.0\\n>>>\\n\\n\\n\\nsome': 2703, 'you,\\none': 2704, 'texts,\\nsent2': 2705, '…': 2706, 'sent9': 2707, 'them\\nhere;': 2708, 'interpreter\\n(if': 2709, 'sent2': 2710, 'defined,': 2711, 'you\\nneed': 2712, '*)': 2713, \"sent2\\n['the',\": 2714, \"'family',\": 2715, \"'of',\": 2716, \"'dashwood',\": 2717, \"'had',\": 2718, \"'long',\\n'been',\": 2719, \"'settled',\": 2720, \"'in',\": 2721, \"'sussex',\": 2722, \"sent3\\n['in',\": 2723, \"'the',\": 2724, \"'beginning',\": 2725, \"'god',\": 2726, \"'created',\": 2727, \"'the',\\n'heaven',\": 2728, \"'and',\": 2729, \"'earth',\": 2730, \".']\\n>>>\\n\\n\\n\\n\\nnote\\nyour\": 2731, 'turn:\\nmake': 2732, 'sentences': 2733, 'own,': 2734, 'equals\\nsign,': 2735, 'this:\\nex1': 2736, \"['monty',\": 2737, \"'python',\": 2738, \"'holy',\": 2739, \"'grail']\": 2740, '.\\nrepeat': 2741, 'operations': 2742, 'earlier': 2743, 'in\\n1,\\ne': 2744, 'sorted(ex1),': 2745, 'len(set(ex1)),': 2746, 'ex1': 2747, \".count('the')\": 2748, '.\\n\\na': 2749, 'pleasant': 2750, 'surprise': 2751, '.\\nadding': 2752, 'creates': 2753, 'list\\nwith': 2754, 'everything': 2755, 'list,': 2756, 'list:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2757, \"'python']\": 2758, \"['and',\": 2759, \"\\n['monty',\": 2760, \"'grail']\\n>>>\\n\\n\\n\\n\\nnote\\nthis\": 2761, 'operation': 2762, 'concatenation;\\nit': 2763, 'combines': 2764, 'concatenate\\nsentences': 2765, 'either;': 2766, 'short\\nnames': 2767, 'pre-defined': 2768, 'sent4': 2769, \"sent1\\n['fellow',\": 2770, \"'-',\": 2771, \"'citizens',\": 2772, \"'senate',\": 2773, \"'the',\\n'house',\": 2774, \"'representatives',\": 2775, \"'call',\": 2776, \".']\\n>>>\\n\\n\\n\\nwhat\": 2777, 'list?': 2778, '.\\nwhen': 2779, 'append()': 2780, 'itself': 2781, 'result\\nof': 2782, '.append(some)\\n>>>': 2783, \"sent1\\n['call',\": 2784, \"'some']\\n>>>\\n\\n\\n\\n\\n\\n2\": 2785, '.2\\xa0\\xa0\\xa0indexing': 2786, 'lists\\n\\n\\nas': 2787, 'seen,': 2788, 'represented\\nusing': 2789, 'combination': 2790, 'quotes': 2791, 'ordinary\\npage': 2792, 'text1\\nwith': 2793, 'len(text1),': 2794, 'a\\nparticular': 2795, \"'heaven'\": 2796, \".count('heaven')\": 2797, '.\\nwith': 2798, 'pick': 2799, '1st,': 2800, '173rd,': 2801, '14,278th\\nword': 2802, 'printed': 2803, 'analogously,': 2804, 'identify': 2805, 'a\\npython': 2806, 'that\\nrepresents': 2807, \"item's\": 2808, 'instruct': 2809, 'python\\nto': 2810, '173': 2811, 'text\\nby': 2812, 'brackets:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2813, \"text4[173]\\n'awaken'\\n>>>\\n\\n\\n\\nwe\": 2814, 'converse;': 2815, 'first\\noccurs:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2816, \".index('awaken')\\n173\\n>>>\\n\\n\\n\\nindexes\": 2817, 'common': 2818, 'text,\\nor,': 2819, 'generally,': 2820, 'sublists': 2821, 'well,': 2822, 'extracting\\nmanageable': 2823, 'pieces': 2824, 'technique\\nknown': 2825, 'slicing': 2826, \"text5[16715:16735]\\n['u86',\": 2827, \"'thats',\": 2828, \"'why',\": 2829, \"'something',\": 2830, \"'like',\": 2831, \"'gamefly',\": 2832, \"'is',\": 2833, \"'so',\": 2834, \"'good',\\n'because',\": 2835, \"'you',\": 2836, \"'can',\": 2837, \"'actually',\": 2838, \"'play',\": 2839, \"'a',\": 2840, \"'full',\": 2841, \"'game',\": 2842, \"'without',\\n'buying',\": 2843, \"'it']\\n>>>\": 2844, \"text6[1600:1625]\\n['we',\": 2845, \"'re',\": 2846, \"'an',\": 2847, \"'anarcho',\": 2848, \"'syndicalist',\": 2849, \"'commune',\": 2850, \"'we',\\n'take',\": 2851, \"'it',\": 2852, \"'turns',\": 2853, \"'to',\": 2854, \"'act',\": 2855, \"'as',\": 2856, \"'sort',\": 2857, \"'executive',\\n'officer',\": 2858, \"'for',\": 2859, \"'week']\\n>>>\\n\\n\\n\\nindexes\": 2860, 'subtleties,': 2861, 'sentence:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2862, 'sent': 2863, \"['word1',\": 2864, \"'word2',\": 2865, \"'word3',\": 2866, \"'word4',\": 2867, \"'word5',\\n\": 2868, \"'word6',\": 2869, \"'word7',\": 2870, \"'word8',\": 2871, \"'word9',\": 2872, \"'word10']\\n>>>\": 2873, \"sent[0]\\n'word1'\\n>>>\": 2874, \"sent[9]\\n'word10'\\n>>>\\n\\n\\n\\nnotice\": 2875, 'indexes': 2876, 'zero:': 2877, 'element': 2878, 'zero,': 2879, 'sent[0],\\nis': 2880, \"'word1',\": 2881, '9': 2882, \"'word10'\": 2883, 'simple:': 2884, 'moment': 2885, 'accesses': 2886, 'from\\nthe': 2887, \"computer's\": 2888, 'memory,': 2889, 'element;\\nwe': 2890, 'tell': 2891, 'forward': 2892, '.\\nthus,': 2893, 'zero': 2894, 'steps': 2895, 'leaves': 2896, 'practice': 2897, 'initially': 2898, 'confusing,\\nbut': 2899, 'typical': 2900, 'modern': 2901, 'hang': 2902, \"if\\nyou've\": 2903, 'mastered': 2904, 'system': 2905, 'centuries': 2906, '19xy': 2907, 'year\\nin': 2908, '20th': 2909, 'century,': 2910, 'live': 2911, 'country': 2912, 'floors': 2913, 'of\\na': 2914, 'numbered': 2915, '1,': 2916, 'walking': 2917, 'n-1': 2918, 'flights': 2919, 'of\\nstairs': 2920, 'n': 2921, '.\\n\\nnow,': 2922, 'accidentally': 2923, 'too': 2924, 'large,': 2925, 'error:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2926, 'sent[10]\\ntraceback': 2927, '(most': 2928, 'recent': 2929, 'last):\\n': 2930, '?\\nindexerror:': 2931, 'range\\n>>>\\n\\n\\n\\nthis': 2932, 'error,': 2933, 'syntactically': 2934, 'correct': 2935, '.\\ninstead,': 2936, 'produces': 2937, 'traceback': 2938, 'message': 2939, 'that\\nshows': 2940, 'error,\\nindexerror,': 2941, 'brief': 2942, 'explanation': 2943, 'slicing,': 2944, 'verify': 2945, 'slice': 2946, '5:8': 2947, 'at\\nindexes': 2948, '5,': 2949, '6,': 2950, '7:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2951, \"sent[5:8]\\n['word6',\": 2952, \"'word8']\\n>>>\": 2953, \"sent[5]\\n'word6'\\n>>>\": 2954, \"sent[6]\\n'word7'\\n>>>\": 2955, \"sent[7]\\n'word8'\\n>>>\\n\\n\\n\\nby\": 2956, 'm:n': 2957, 'm…n-1': 2958, 'shows,\\nwe': 2959, 'omit': 2960, 'begins': 2961, ':\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 2962, 'sent[:3]': 2963, \"\\n['word1',\": 2964, \"'word3']\\n>>>\": 2965, 'text2[141525:]': 2966, \"\\n['among',\": 2967, \"'merits',\": 2968, \"'happiness',\": 2969, \"'elinor',\": 2970, \"'marianne',\\n',',\": 2971, \"'let',\": 2972, \"'not',\": 2973, \"'be',\": 2974, \"'ranked',\": 2975, \"'least',\": 2976, \"'considerable',\": 2977, \"',',\\n'that',\": 2978, \"'though',\": 2979, \"'sisters',\": 2980, \"'living',\": 2981, \"'almost',\": 2982, \"'within',\": 2983, \"'sight',\": 2984, \"'of',\\n'each',\": 2985, \"'other',\": 2986, \"'they',\": 2987, \"'could',\": 2988, \"'live',\": 2989, \"'without',\": 2990, \"'disagreement',\": 2991, \"'between',\\n'themselves',\": 2992, \"'or',\": 2993, \"'producing',\": 2994, \"'coolness',\": 2995, \"'between',\": 2996, \"'their',\": 2997, \"'husbands',\": 2998, \".',\\n'the',\": 2999, \"'end']\\n>>>\\n\\n\\n\\nwe\": 3000, 'assigning': 3001, 'put': 3002, 'sent[0]': 3003, 'left': 3004, 'also\\nreplace': 3005, 'consequence': 3006, 'this\\nlast': 3007, 'change': 3008, 'elements,': 3009, 'value\\ngenerates': 3010, \"'first'\": 3011, '\\n>>>': 3012, 'sent[9]': 3013, \"'last'\\n>>>\": 3014, 'len(sent)\\n10\\n>>>': 3015, 'sent[1:9]': 3016, \"['second',\": 3017, \"'third']\": 3018, \"sent\\n['first',\": 3019, \"'second',\": 3020, \"'third',\": 3021, \"'last']\\n>>>\": 3022, '\\ntraceback': 3023, 'range\\n>>>\\n\\n\\n\\n\\nnote\\nyour': 3024, 'turn:\\ntake': 3025, 'minutes': 3026, 'and\\ngroups': 3027, '(slices)': 3028, 'understanding\\nby': 3029, '.\\n\\n\\n\\n2': 3030, '.3\\xa0\\xa0\\xa0variables\\nfrom': 3031, 'had\\naccess': 3032, 'text2,': 3033, 'saved': 3034, 'lot\\nof': 3035, '250,000-word': 3036, 'name\\nlike': 3037, 'this!': 3038, 'anything': 3039, 'care\\nto': 3040, 'did': 3041, 'ourselves': 3042, 'e': 3043, '.,\\ndefining': 3044, 'follows:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 3045, \".']\\n>>>\\n\\n\\n\\nsuch\": 3046, 'form:': 3047, 'evaluate\\nthe': 3048, 'result': 3049, 'process': 3050, 'is\\ncalled': 3051, 'assignment': 3052, 'output;\\nyou': 3053, 'its\\nown': 3054, 'contents': 3055, 'slightly': 3056, 'misleading,\\nsince': 3057, 'moving': 3058, 'side': 3059, 'left-arrow': 3060, 'like,': 3061, 'my_sent,': 3062, 'xyzzy': 3063, 'must': 3064, 'letter,': 3065, 'numbers': 3066, 'underscores': 3067, 'variables': 3068, 'assignments:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 3069, 'my_sent': 3070, \"['bravely',\": 3071, \"'bold',\": 3072, \"'sir',\": 3073, \"'robin',\": 3074, \"'rode',\\n\": 3075, \"'forth',\": 3076, \"'from',\": 3077, \"'camelot',\": 3078, 'noun_phrase': 3079, 'my_sent[1:4]\\n>>>': 3080, \"noun_phrase\\n['bold',\": 3081, \"'robin']\\n>>>\": 3082, 'sorted(noun_phrase)\\n>>>': 3083, \"words\\n['robin',\": 3084, \"'bold']\\n>>>\\n\\n\\n\\nremember\": 3085, 'capitalized': 3086, '.\\n\\nnote\\nnotice': 3087, 'split': 3088, 'definition\\nof': 3089, 'across\\nmultiple': 3090, 'lines,': 3091, 'happens': 3092, 'indicate': 3093, 'is\\nexpected': 3094, 'matter': 3095, 'indentation': 3096, 'these\\ncontinuation': 3097, 'makes': 3098, 'easier': 3099, 'choose': 3100, 'meaningful': 3101, 'remind': 3102, 'anyone\\nelse': 3103, 'reads': 3104, 'meant': 3105, 'names;': 3106, 'blindly': 3107, 'follows': 3108, 'instructions,\\nand': 3109, 'confusing,': 3110, \"'two'\": 3111, 'restriction': 3112, 'that\\na': 3113, 'cannot': 3114, 'reserved': 3115, 'as\\ndef,': 3116, 'if,': 3117, 'not,\\nand': 3118, \"'camelot'\": 3119, '\\nfile': 3120, \"'camelot'\\n\": 3121, 'syntax\\n>>>\\n\\n\\n\\nwe': 3122, 'hold': 3123, 'computation,': 3124, 'especially\\nwhen': 3125, 'follow': 3126, 'len(set(text1))': 3127, 'written:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 3128, 'vocab': 3129, 'set(text1)\\n>>>': 3130, 'vocab_size': 3131, 'len(vocab)\\n>>>': 3132, 'vocab_size\\n19317\\n>>>\\n\\n\\n\\n\\ncaution!\\ntake': 3133, 'choice': 3134, 'identifiers)': 3135, 'python\\nvariables': 3136, 'first,': 3137, 'optionally\\nfollowed': 3138, 'digits': 3139, '(0': 3140, '9)': 3141, 'letters': 3142, 'abc23': 3143, 'fine,': 3144, 'but\\n23abc': 3145, 'cause': 3146, '.\\nnames': 3147, 'case-sensitive,': 3148, 'myvar': 3149, 'myvar\\nare': 3150, 'contain': 3151, 'whitespace,\\nbut': 3152, 'underscore,': 3153, '.,\\nmy_var': 3154, 'insert': 3155, 'hyphen': 3156, 'an\\nunderscore:': 3157, 'my-var': 3158, 'wrong,': 3159, 'interprets': 3160, 'the\\n-': 3161, 'minus': 3162, '.4\\xa0\\xa0\\xa0strings\\nsome': 3163, 'words,\\nor': 3164, 'assign': 3165, ',\\nindex': 3166, \"'monty'\": 3167, 'name[0]': 3168, \"\\n'm'\\n>>>\": 3169, 'name[:4]': 3170, \"\\n'mont'\\n>>>\\n\\n\\n\\nwe\": 3171, 'perform': 3172, 'strings:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 3173, \"2\\n'montymonty'\\n>>>\": 3174, \"'!'\\n'monty!'\\n>>>\\n\\n\\n\\nwe\": 3175, 'join': 3176, 'string,': 3177, \".join(['monty',\": 3178, \"'python'])\\n'monty\": 3179, \"python'\\n>>>\": 3180, \"'monty\": 3181, \"python'\": 3182, \".split()\\n['monty',\": 3183, \"'python']\\n>>>\\n\\n\\n\\nwe\": 3184, 'back': 3185, 'topic': 3186, 'being,': 3187, 'blocks\\n—': 3188, '—\\nand': 3189, '.\\n\\n\\n\\n3\\xa0\\xa0\\xa0computing': 3190, \"statistics\\n\\nlet's\": 3191, 'bring': 3192, 'computational\\nresources': 3193, 'bear': 3194, 'began': 3195, 'in\\n1,': 3196, 'context,': 3197, 'compile': 3198, 'random\\ntext': 3199, 'distinct,\\nand': 3200, 'automatic': 3201, 'characteristic': 3202, 'expressions\\nof': 3203, 'try\\nnew': 3204, 'interpreter,\\nand': 3205, \"you'll\": 3206, '.\\nbefore': 3207, 'further,': 3208, 'the\\nlast': 3209, 'predicting': 3210, 'use\\nthe': 3211, 'whether': 3212, 'got': 3213, 'sure': 3214, 'how\\nto': 3215, 'section\\nbefore': 3216, 'saying': 3217, \"['after',\": 3218, \"'all',\": 3219, \"'said',\": 3220, \"'done',\\n\": 3221, \"'more',\": 3222, \"'than',\": 3223, \"'done']\\n>>>\": 3224, 'set(saying)\\n>>>': 3225, 'sorted(tokens)\\n>>>': 3226, 'tokens[-2:]\\nwhat': 3227, 'expect': 3228, 'here?\\n>>>\\n\\n\\n\\n\\n3': 3229, '.1\\xa0\\xa0\\xa0frequency': 3230, 'distributions\\nhow': 3231, 'most\\ninformative': 3232, 'genre': 3233, 'imagine': 3234, 'might\\ngo': 3235, '50': 3236, 'frequent': 3237, 'method\\nwould': 3238, 'tally': 3239, 'item,': 3240, 'thousands': 3241, 'rows,': 3242, 'exceedingly\\nlaborious': 3243, 'laborious': 3244, 'appearing': 3245, '(a': 3246, 'distribution)\\n\\nthe': 3247, 'distribution,\\nand': 3248, '.\\n(in': 3249, 'observable': 3250, 'event': 3251, '.)\\nit': 3252, 'distribution\\nbecause': 3253, 'text\\nare': 3254, '.\\nsince': 3255, 'distributions': 3256, 'built-in': 3257, 'freqdist': 3258, 'the\\n50': 3259, 'fdist1': 3260, 'freqdist(text1)': 3261, 'print(fdist1)': 3262, '\\n<freqdist': 3263, '19317': 3264, '260819': 3265, 'outcomes>\\n>>>': 3266, '.most_common(50)': 3267, \"\\n[(',',\": 3268, '18713),': 3269, \"('the',\": 3270, '13721),': 3271, \"('\": 3272, '6862),': 3273, \"('of',\": 3274, '6536),': 3275, \"('and',\": 3276, \"6024),\\n('a',\": 3277, '4569),': 3278, \"('to',\": 3279, '4542),': 3280, \"(';',\": 3281, '4072),': 3282, \"('in',\": 3283, '3916),': 3284, \"('that',\": 3285, \"2982),\\n(',\": 3286, '2684),': 3287, \"('-',\": 3288, '2552),': 3289, \"('his',\": 3290, '2459),': 3291, \"('it',\": 3292, '2209),': 3293, \"('i',\": 3294, \"2124),\\n('s',\": 3295, '1739),': 3296, \"('is',\": 3297, '1695),': 3298, \"('he',\": 3299, '1661),': 3300, \"('with',\": 3301, '1659),': 3302, \"('was',\": 3303, \"1632),\\n('as',\": 3304, '1620),': 3305, \"('',\": 3306, '1478),': 3307, \"('all',\": 3308, '1462),': 3309, \"('for',\": 3310, '1414),': 3311, \"('this',\": 3312, \"1280),\\n('!',\": 3313, '1269),': 3314, \"('at',\": 3315, '1231),': 3316, \"('by',\": 3317, '1137),': 3318, \"('but',\": 3319, '1113),': 3320, \"('not',\": 3321, \"1103),\\n('--',\": 3322, '1070),': 3323, \"('him',\": 3324, '1058),': 3325, \"('from',\": 3326, '1052),': 3327, \"('be',\": 3328, '1030),': 3329, \"('on',\": 3330, \"1005),\\n('so',\": 3331, '918),': 3332, \"('whale',\": 3333, '906),': 3334, \"('one',\": 3335, '889),': 3336, \"('you',\": 3337, '841),': 3338, \"('had',\": 3339, \"767),\\n('have',\": 3340, '760),': 3341, \"('there',\": 3342, '715),': 3343, '705),': 3344, \"('or',\": 3345, '697),': 3346, \"('were',\": 3347, \"680),\\n('now',\": 3348, '646),': 3349, \"('which',\": 3350, '640),': 3351, \"('?',\": 3352, '637),': 3353, \"('me',\": 3354, '627),': 3355, \"('like',\": 3356, '624)]\\n>>>': 3357, \"fdist1['whale']\\n906\\n>>>\\n\\n\\n\\nwhen\": 3358, 'invoke': 3359, 'freqdist,': 3360, 'an\\nargument': 3361, '(outcomes)\\nthat': 3362, 'counted': 3363, '260,819': 3364, 'the\\ncase': 3365, 'most_common(50)': 3366, 'frequently': 3367, 'occurring': 3368, 'distribution': 3369, 'for\\ntext2': 3370, 'uppercase': 3371, 'nameerror:': 3372, \"'freqdist'\": 3373, 'defined,\\nyou': 3374, '*\\n\\n\\ndo': 3375, 'text?\\nonly': 3376, 'whale,': 3377, 'informative!': 3378, '900': 3379, 'text;': 3380, \"they're\": 3381, 'plumbing': 3382, 'proportion': 3383, 'words?\\nwe': 3384, 'cumulative': 3385, 'words,\\nusing': 3386, '.plot(50,': 3387, 'cumulative=true),': 3388, 'graph': 3389, 'account': 3390, 'nearly': 3391, 'half': 3392, 'book!\\n\\n\\nfigure': 3393, 'dick:\\nthese': 3394, '.\\n\\nif': 3395, 'us,': 3396, 'occur': 3397, 'once\\nonly,': 3398, 'so-called': 3399, 'hapaxes?': 3400, '.hapaxes()': 3401, 'lexicographer,': 3402, 'cetological,\\ncontraband,': 3403, 'expostulations,': 3404, '9,000': 3405, 'others': 3406, 'seems': 3407, 'rare': 3408, 'seeing': 3409, 'the\\ncontext': 3410, \"can't\": 3411, 'guess': 3412, 'hapaxes': 3413, 'case!\\nsince': 3414, 'neither': 3415, 'nor': 3416, 'infrequent': 3417, 'try\\nsomething': 3418, '.\\n\\n\\n3': 3419, '.2\\xa0\\xa0\\xa0fine-grained': 3420, 'selection': 3421, 'words\\nnext,': 3422, 'perhaps': 3423, 'be\\nmore': 3424, 'informative': 3425, 'adapt': 3426, 'notation\\nfrom': 3427, 'vocabulary\\nof': 3428, 'call\\nthis': 3429, 'property': 3430, 'p,': 3431, 'p(w)': 3432, 'true\\nif': 3433, 'w': 3434, 'express': 3435, 'interest': 3436, 'mathematical\\nset': 3437, 'notation': 3438, '(1a)': 3439, 'an\\nelement': 3440, 'v': 3441, '(the': 3442, 'vocabulary)': 3443, 'p': 3444, '.\\n\\n': 3445, '(1)\\n': 3446, '.{w': 3447, '|': 3448, '∈': 3449, '&': 3450, 'p(w)}\\n\\n': 3451, 'b': 3452, '.[w': 3453, 'p(w)]\\n\\nthe': 3454, '(1b)': 3455, '.)\\nobserve': 3456, 'notations': 3457, 'and\\nwrite': 3458, 'executable': 3459, 'code:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 3460, 'long_words': 3461, '[w': 3462, 'len(w)': 3463, '>': 3464, '15]\\n>>>': 3465, \"sorted(long_words)\\n['circumnavigation',\": 3466, \"'physiognomically',\": 3467, \"'apprehensiveness',\": 3468, \"'cannibalistically',\\n'characteristically',\": 3469, \"'circumnavigating',\": 3470, \"'circumnavigation',\": 3471, \"'circumnavigations',\\n'comprehensiveness',\": 3472, \"'hermaphroditical',\": 3473, \"'indiscriminately',\": 3474, \"'indispensableness',\\n'irresistibleness',\": 3475, \"'preternaturalness',\": 3476, \"'responsibilities',\\n'simultaneousness',\": 3477, \"'subterraneousness',\": 3478, \"'supernaturalness',\": 3479, \"'superstitiousness',\\n'uncomfortableness',\": 3480, \"'uncompromisedness',\": 3481, \"'undiscriminating',\": 3482, \"'uninterpenetratingly']\\n>>>\\n\\n\\n\\nfor\": 3483, 'v,': 3484, 'whether\\nlen(w)': 3485, 'greater': 3486, '15;': 3487, 'will\\nbe': 3488, 'ignored': 3489, 'discuss': 3490, 'statements': 3491, 'changing': 3492, 'condition': 3493, '.\\ndoes': 3494, 'difference': 3495, 'the\\nvariable': 3496, '[word': 3497, \".]?\\n\\nlet's\": 3498, 'characterize': 3499, '.\\nnotice': 3500, 'reflect': 3501, 'focus\\n—': 3502, 'constitutionally,': 3503, 'transcontinental': 3504, '—\\nwhereas': 3505, 'text5': 3506, 'informal': 3507, 'content:\\nboooooooooooglyyyyyy': 3508, 'yuuuuuuuuuuuummmmmmmmmmmm': 3509, '.\\nhave': 3510, 'succeeded': 3511, 'typify\\na': 3512, 'unique)\\nand': 3513, 'better': 3514, 'occurring\\nlong': 3515, 'promising': 3516, 'eliminates\\nfrequent': 3517, 'the)': 3518, 'words\\n(e': 3519, 'antiphilosophists)': 3520, 'corpus\\nthat': 3521, 'longer': 3522, 'seven': 3523, 'characters,': 3524, 'times:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 3525, 'fdist5': 3526, 'freqdist(text5)\\n>>>': 3527, 'sorted(w': 3528, 'set(text5)': 3529, '7': 3530, 'fdist5[w]': 3531, \"7)\\n['#14-19teens',\": 3532, \"'#talkcity_adults',\": 3533, \"'((((((((((',\": 3534, \"'question',\\n'actually',\": 3535, \"'anything',\": 3536, \"'computer',\": 3537, \"'cute\": 3538, \".-ass',\": 3539, \"'everyone',\": 3540, \"'football',\\n'innocent',\": 3541, \"'listening',\": 3542, \"'remember',\": 3543, \"'seriously',\": 3544, \"'together',\\n'tomorrow',\": 3545, \"'watching']\\n>>>\\n\\n\\n\\nnotice\": 3546, 'conditions:': 3547, 'the\\nwords': 3548, 'letters,': 3549, 'that\\nthese': 3550, 'to\\nautomatically': 3551, 'frequently-occurring': 3552, 'content-bearing\\nwords': 3553, 'modest': 3554, 'milestone:': 3555, 'tiny': 3556, 'piece': 3557, 'code,\\nprocessing': 3558, 'tens': 3559, '.3\\xa0\\xa0\\xa0collocations': 3560, 'bigrams\\na': 3561, 'collocation': 3562, 'together\\nunusually': 3563, 'red': 3564, 'wine': 3565, 'collocation,': 3566, 'the\\nwine': 3567, 'collocations': 3568, 'are\\nresistant': 3569, 'substitution': 3570, 'senses;\\nfor': 3571, 'maroon': 3572, 'sounds': 3573, 'definitely': 3574, 'odd': 3575, 'handle': 3576, 'collocations,': 3577, 'off': 3578, 'text\\na': 3579, 'pairs,': 3580, 'bigrams': 3581, 'easily\\naccomplished': 3582, 'bigrams():\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 3583, \"list(bigrams(['more',\": 3584, \"'done']))\\n[('more',\": 3585, \"'is'),\": 3586, \"'said'),\": 3587, \"('said',\": 3588, \"'than'),\": 3589, \"('than',\": 3590, \"'done')]\\n>>>\\n\\n\\n\\n\\nnote\\nif\": 3591, 'omitted': 3592, 'list()': 3593, 'above,': 3594, \"bigrams(['more',\": 3595, '.]),\\nyou': 3596, '<generator': 3597, '0x10fb8b3a8>': 3598, 'compute\\na': 3599, 'case,': 3600, 'need\\nto': 3601, 'know': 3602, '.\\n\\nhere': 3603, 'than-done': 3604, 'bigram,': 3605, 'write\\nit': 3606, \"'done')\": 3607, 'essentially\\njust': 3608, 'bigrams,': 3609, 'except': 3610, 'pay': 3611, 'attention': 3612, 'the\\ncases': 3613, 'particular,': 3614, 'find\\nbigrams': 3615, 'collocations()': 3616, 'function\\ndoes': 3617, '.collocations()\\nunited': 3618, 'states;': 3619, 'fellow': 3620, 'citizens;': 3621, 'years;': 3622, 'ago;': 3623, 'federal\\ngovernment;': 3624, 'government;': 3625, 'american': 3626, 'people;': 3627, 'vice': 3628, 'president;': 3629, 'old\\nworld;': 3630, 'almighty': 3631, 'god;': 3632, 'magistrate;': 3633, 'justice;\\ngod': 3634, 'bless;': 3635, 'citizen;': 3636, 'indian': 3637, 'tribes;': 3638, 'public': 3639, 'debt;': 3640, 'another;\\nforeign': 3641, 'nations;': 3642, 'political': 3643, 'parties\\n>>>': 3644, 'text8': 3645, '.collocations()\\nwould': 3646, 'like;': 3647, 'medium': 3648, 'build;': 3649, 'social': 3650, 'drinker;': 3651, 'quiet': 3652, 'nights;': 3653, 'non': 3654, 'smoker;\\nlong': 3655, 'term;': 3656, 'age': 3657, 'open;': 3658, 'going;': 3659, 'financially': 3660, 'secure;': 3661, 'fun\\ntimes;': 3662, 'interests;': 3663, 'weekends': 3664, 'away;': 3665, 'poss': 3666, 'rship;': 3667, 'well\\npresented;': 3668, 'married;': 3669, 'mum;': 3670, 'permanent': 3671, 'relationship;': 3672, 'slim\\nbuild\\n>>>\\n\\n\\n\\nthe': 3673, 'emerge': 3674, 'the\\ntexts': 3675, 'would\\nneed': 3676, 'larger': 3677, 'body': 3678, 'things\\ncounting': 3679, 'useful,': 3680, 'can\\nlook': 3681, 'lengths': 3682, 'creating': 3683, 'freqdist\\nout': 3684, 'numbers,': 3685, 'corresponding\\nword': 3686, 'text:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 3687, '[len(w)': 3688, 'text1]': 3689, '\\n[1,': 3690, '4,': 3691, '8,': 3692, '9,': 3693, '11,': 3694, '7,': 3695, 'fdist': 3696, 'freqdist(len(w)': 3697, 'text1)': 3698, 'print(fdist)': 3699, '19': 3700, 'fdist\\nfreqdist({3:': 3701, '50223,': 3702, '1:': 3703, '47933,': 3704, '4:': 3705, '42345,': 3706, '2:': 3707, '38513,': 3708, '5:': 3709, '26597,': 3710, '6:': 3711, '17111,': 3712, '7:': 3713, '14399,\\n': 3714, '8:': 3715, '9966,': 3716, '9:': 3717, '6428,': 3718, '10:': 3719, '3528,': 3720, '.})\\n>>>\\n\\n\\n\\nwe': 3721, 'deriving': 3722, 'text1\\n,\\nand': 3723, 'counts': 3724, 'these\\noccurs': 3725, 'containing\\na': 3726, 'quarter': 3727, 'million': 3728, 'a\\nword': 3729, '20': 3730, 'distinct\\nitems': 3731, 'counted,': 3732, '20,': 3733, '20\\ndifferent': 3734, 'character,\\ntwo': 3735, 'twenty': 3736, 'none': 3737, 'more\\ncharacters': 3738, 'wonder': 3739, 'are\\n(e': 3740, 'five\\nthan': 3741, 'four,': 3742, 'etc)': 3743, '.most_common()\\n[(3,': 3744, '50223),': 3745, '(1,': 3746, '47933),': 3747, '(4,': 3748, '42345),': 3749, '(2,': 3750, '38513),': 3751, '(5,': 3752, '26597),': 3753, '(6,': 3754, '17111),': 3755, '(7,': 3756, '14399),\\n(8,': 3757, '9966),': 3758, '(9,': 3759, '6428),': 3760, '(10,': 3761, '3528),': 3762, '(11,': 3763, '1873),': 3764, '(12,': 3765, '1053),': 3766, '(13,': 3767, '567),': 3768, '(14,': 3769, '177),\\n(15,': 3770, '70),': 3771, '(16,': 3772, '22),': 3773, '(17,': 3774, '12),': 3775, '(18,': 3776, '1),': 3777, '(20,': 3778, '1)]\\n>>>': 3779, '.max()\\n3\\n>>>': 3780, 'fdist[3]\\n50223\\n>>>': 3781, '.freq(3)\\n0': 3782, '.19255882431878046\\n>>>\\n\\n\\n\\nfrom': 3783, 'that\\nwords': 3784, 'roughly': 3785, '50,000': 3786, '20%)': 3787, 'making': 3788, 'pursue': 3789, 'word\\nlength': 3790, 'differences': 3791, 'authors,': 3792, 'genres,': 3793, 'or\\nlanguages': 3794, '.\\n3': 3795, 'summarizes': 3796, '.\\n\\n\\n\\n\\n\\n\\nexample\\ndescription\\n\\n\\n\\nfdist': 3797, 'freqdist(samples)\\ncreate': 3798, 'containing': 3799, 'samples\\n\\nfdist[sample]': 3800, '+=': 3801, '1\\nincrement': 3802, \"sample\\n\\nfdist['monstrous']\\ncount\": 3803, 'sample': 3804, 'occurred\\n\\nfdist': 3805, \".freq('monstrous')\\nfrequency\": 3806, 'sample\\n\\nfdist': 3807, '.n()\\ntotal': 3808, 'samples\\n\\nfdist': 3809, '.most_common(n)\\nthe': 3810, 'frequencies\\n\\nfor': 3811, 'fdist:\\niterate': 3812, '.max()\\nsample': 3813, 'greatest': 3814, 'count\\n\\nfdist': 3815, '.tabulate()\\ntabulate': 3816, 'distribution\\n\\nfdist': 3817, '.plot()\\ngraphical': 3818, '.plot(cumulative=true)\\ncumulative': 3819, 'distribution\\n\\nfdist1': 3820, '|=': 3821, 'fdist2\\nupdate': 3822, 'fdist2\\n\\nfdist1': 3823, '<': 3824, 'fdist2\\ntest': 3825, 'fdist2\\n\\n\\ntable': 3826, 'distributions\\n\\n\\nour': 3827, 'concepts,\\nand': 3828, '.\\n\\n\\n\\n\\n4\\xa0\\xa0\\xa0back': 3829, 'decisions': 3830, 'taking': 3831, 'control\\n\\nso': 3832, 'far,': 3833, 'qualities:\\nthe': 3834, 'ability': 3835, 'and\\nthe': 3836, 'potential': 3837, 'automation': 3838, 'machines': 3839, 'to\\nmake': 3840, 'behalf,': 3841, 'executing': 3842, 'when\\ncertain': 3843, 'conditions': 3844, 'met,': 3845, 'repeatedly': 3846, 'looping': 3847, 'through\\ntext': 3848, 'until': 3849, 'satisfied': 3850, 'feature\\nis': 3851, 'control,': 3852, '.\\n\\n4': 3853, '.1\\xa0\\xa0\\xa0conditionals\\npython': 3854, 'supports': 3855, 'operators,': 3856, '>=,': 3857, 'for\\ntesting': 3858, 'relationship': 3859, 'relational\\noperators': 3860, '.\\n\\n\\n\\n\\n\\n\\noperator\\nrelationship\\n\\n\\n\\n<\\nless': 3861, 'than\\n\\n<=\\nless': 3862, 'equal': 3863, 'to\\n\\n==\\nequal': 3864, '(note': 3865, 'signs,': 3866, 'one)\\n\\n!=\\nnot': 3867, 'to\\n\\n>\\ngreater': 3868, 'than\\n\\n>=\\ngreater': 3869, 'to\\n\\n\\ntable': 3870, 'numerical': 3871, 'comparison': 3872, 'operators\\n\\n\\nwe': 3873, 'news': 3874, 'changed': 3875, 'one\\nline': 3876, 'sent7,': 3877, 'text7\\n(wall': 3878, 'journal)': 3879, 'sent7\\nis': 3880, 'undefined,': 3881, '*\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 3882, \"sent7\\n['pierre',\": 3883, \"'vinken',\": 3884, \"'61',\": 3885, \"'years',\": 3886, \"'old',\": 3887, \"'will',\": 3888, \"'join',\": 3889, \"'the',\\n'board',\": 3890, \"'nonexecutive',\": 3891, \"'director',\": 3892, \"'nov\": 3893, \"'29',\": 3894, 'sent7': 3895, \"4]\\n[',',\": 3896, '<=': 3897, '==': 3898, \"4]\\n['will',\": 3899, '!=': 3900, \"4]\\n['pierre',\": 3901, \"'board',\\n'as',\": 3902, \".']\\n>>>\\n\\n\\n\\nthere\": 3903, 'pattern': 3904, 'examples:\\n[w': 3905, '],': 3906, 'yields': 3907, 'either': 3908, 'false': 3909, 'cases': 3910, 'always': 3911, '.\\nhowever,': 3912, 'properties': 3913, 'listed': 3914, '.\\n\\n\\n\\n\\n\\n\\nfunction\\nmeaning\\n\\n\\n\\ns': 3915, '.startswith(t)\\ntest': 3916, 's': 3917, 't\\n\\ns': 3918, '.endswith(t)\\ntest': 3919, 't\\n\\nt': 3920, 's\\ntest': 3921, 't': 3922, 'substring': 3923, 's\\n\\ns': 3924, '.islower()\\ntest': 3925, 'cased': 3926, 'lowercase\\n\\ns': 3927, '.isupper()\\ntest': 3928, 'uppercase\\n\\ns': 3929, '.isalpha()\\ntest': 3930, 'non-empty': 3931, 'alphabetic\\n\\ns': 3932, '.isalnum()\\ntest': 3933, 'alphanumeric\\n\\ns': 3934, '.isdigit()\\ntest': 3935, 'digits\\n\\ns': 3936, '.istitle()\\ntest': 3937, 'titlecased\\n(i': 3938, 'initial': 3939, 'capitals)\\n\\n\\ntable': 3940, 'operators\\n\\n\\nhere': 3941, 'operators': 3942, 'to\\nselect': 3943, 'texts:\\nwords': 3944, '-ableness;\\nwords': 3945, 'gnt;\\nwords': 3946, 'capital;\\nand': 3947, 'entirely': 3948, 'set(text1)': 3949, \".endswith('ableness'))\\n['comfortableness',\": 3950, \"'honourableness',\": 3951, \"'immutableness',\": 3952, \"'indispensableness',\": 3953, 'sorted(term': 3954, 'set(text4)': 3955, \"'gnt'\": 3956, \"term)\\n['sovereignty',\": 3957, \"'sovereignties',\": 3958, \"'sovereignty']\\n>>>\": 3959, 'sorted(item': 3960, 'set(text6)': 3961, \".istitle())\\n['a',\": 3962, \"'aaaaaaaaah',\": 3963, \"'aaaaaaaah',\": 3964, \"'aaaaaah',\": 3965, \"'aaaah',\": 3966, \"'aaaaugh',\": 3967, \"'aaagh',\": 3968, 'set(sent7)': 3969, \".isdigit())\\n['29',\": 3970, \"'61']\\n>>>\\n\\n\\n\\nwe\": 3971, 'a\\ncondition,': 3972, 'c1': 3973, 'c2,\\nthen': 3974, 'combine': 3975, 'conjunction': 3976, 'disjunction:\\nc1': 3977, 'c2,\\nc1': 3978, 'c2': 3979, 'turn:\\nrun': 3980, 'explain': 3981, '.\\nnext,': 3982, '.\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 3983, 'set(text7)': 3984, \"'-'\": 3985, \"'index'\": 3986, 'w)\\n>>>': 3987, 'sorted(wd': 3988, 'wd': 3989, '.istitle()': 3990, 'len(wd)': 3991, '10)\\n>>>': 3992, '.islower())\\n>>>': 3993, 'sorted(t': 3994, 'set(text2)': 3995, \"'cie'\": 3996, \"'cei'\": 3997, 't)\\n\\n\\n\\n\\n\\n4': 3998, '.2\\xa0\\xa0\\xa0operating': 3999, 'element\\nin': 4000, 'of\\ncounting': 4001, 'used:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 4002, 'text1]\\n[1,': 4003, '.upper()': 4004, \"text1]\\n['[',\": 4005, \"'moby',\": 4006, \"'dick',\": 4007, \"'by',\": 4008, \"'herman',\": 4009, \"'melville',\": 4010, \"'1851',\": 4011, \"']',\": 4012, \"'etymology',\": 4013, '.]\\n>>>\\n\\n\\n\\nthese': 4014, '[f(w)': 4015, '.]': 4016, '.f()': 4017, '.],': 4018, 'where\\nf': 4019, 'operates': 4020, 'length,': 4021, 'to\\nconvert': 4022, 'f(w)': 4023, 'and\\nw': 4024, 'instead,': 4025, 'idiom': 4026, 'the\\nsame': 4027, 'through\\neach': 4028, 'turn': 4029, 'and\\nperforming': 4030, '.\\n\\nnote\\nthe': 4031, 'described': 4032, 'comprehension': 4033, 'example\\nof': 4034, 'idiom,': 4035, 'fixed': 4036, 'habitually': 4037, 'bothering': 4038, 'to\\nanalyze': 4039, 'mastering': 4040, 'a\\nfluent': 4041, 'programmer': 4042, \".\\n\\nlet's\": 4043, 'size,': 4044, 'here:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 4045, 'len(text1)\\n260819\\n>>>': 4046, 'len(set(text1))\\n19317\\n>>>': 4047, 'len(set(word': 4048, '.lower()': 4049, 'text1))\\n17231\\n>>>\\n\\n\\n\\nnow': 4050, 'double-counting': 4051, 'only\\nin': 4052, 'capitalization,': 4053, 'wiped': 4054, '2,000': 4055, 'count!': 4056, 'further\\nand': 4057, 'eliminate': 4058, 'filtering': 4059, 'any\\nnon-alphabetic': 4060, 'items:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 4061, '.isalpha()))\\n16948\\n>>>\\n\\n\\n\\nthis': 4062, 'complicated:': 4063, 'lowercases': 4064, 'purely': 4065, 'alphabetic': 4066, '.\\nperhaps': 4067, 'simpler': 4068, 'lowercase-only': 4069, 'this\\ngives': 4070, 'wrong': 4071, '(why?)': 4072, \".\\ndon't\": 4073, 'confident': 4074, 'yet,\\nsince': 4075, 'explanations': 4076, '.\\n\\n\\n4': 4077, '.3\\xa0\\xa0\\xa0nested': 4078, 'blocks\\nmost': 4079, 'execute': 4080, 'a\\nconditional': 4081, 'statement,': 4082, 'we\\nalready': 4083, 'conditional': 4084, 'tests': 4085, 'in\\nsent7': 4086, '4]': 4087, 'program,': 4088, 'a\\nvariable': 4089, \"'cat'\": 4090, 'the\\nif': 4091, 'checks': 4092, 'len(word)': 4093, 'invoked': 4094, 'the\\nprint': 4095, 'executed,': 4096, '.\\nremember': 4097, 'indent': 4098, 'spaces': 4099, \"'cat'\\n>>>\": 4100, '5:\\n': 4101, \"print('word\": 4102, \"5')\\n\": 4103, '\\nword': 4104, '5\\n>>>\\n\\n\\n\\nwhen': 4105, '\\nin': 4106, 'nested': 4107, '.\\n\\nnote\\nif': 4108, '.7,': 4109, 'following\\nline': 4110, 'above': 4111, 'recognized:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 4112, 'print_function\\n\\n\\n\\n\\nif': 4113, '>=': 4114, '5,\\nto': 4115, '5,\\nthen': 4116, 'time,': 4117, 'executed,\\nand': 4118, 'user:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 4119, '.\\n>>>\\n\\n\\n\\nan': 4120, 'structure\\nbecause': 4121, 'controls': 4122, '.\\nanother': 4123, 'loop': 4124, '.\\ntry': 4125, 'following,': 4126, 'remember': 4127, 'spaces:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 4128, \".']:\\n\": 4129, 'print(word)\\n': 4130, '.\\ncall\\nme\\nishmael\\n': 4131, '.\\n>>>\\n\\n\\n\\nthis': 4132, 'executes': 4133, 'in\\ncircular': 4134, 'fashion': 4135, 'the\\nassignment': 4136, \"'call',\\neffectively\": 4137, 'first\\nitem': 4138, 'then,': 4139, 'displays': 4140, 'word\\nto': 4141, 'next,': 4142, 'statement,\\nand': 4143, 'value\\nto': 4144, 'user,': 4145, 'until\\nevery': 4146, '.4\\xa0\\xa0\\xa0looping': 4147, 'conditions\\nnow': 4148, 'print\\nthe': 4149, 'letter': 4150, 'l': 4151, 'another\\nname': 4152, \"doesn't\\ntry\": 4153, 'sent1:\\n': 4154, \".endswith('l'):\\n\": 4155, 'print(xyzzy)\\n': 4156, '.\\ncall\\nishmael\\n>>>\\n\\n\\n\\nyou': 4157, 'notice': 4158, 'statements\\nhave': 4159, 'line,\\nbefore': 4160, 'fact,': 4161, 'python\\ncontrol': 4162, 'colon\\nindicates': 4163, 'current': 4164, 'relates': 4165, 'the\\nindented': 4166, 'specify': 4167, 'action': 4168, 'if\\nthe': 4169, 'met': 4170, 'elif': 4171, '(else': 4172, 'if)': 4173, 'have\\ncolons': 4174, '.islower():\\n': 4175, 'print(token,': 4176, \"'is\": 4177, \"word')\\n\": 4178, '.istitle():\\n': 4179, 'titlecase': 4180, 'else:\\n': 4181, \"punctuation')\\n\": 4182, '.\\ncall': 4183, 'word\\nme': 4184, 'word\\nishmael': 4185, 'word\\n': 4186, 'punctuation\\n>>>\\n\\n\\n\\nas': 4187, 'see,': 4188, 'small': 4189, 'knowledge,\\nyou': 4190, 'multiline': 4191, \".\\nit's\": 4192, 'pieces,\\ntesting': 4193, 'before\\ncombining': 4194, 'python\\ninteractive': 4195, 'invaluable,': 4196, 'get\\ncomfortable': 4197, 'exploring': 4198, '.\\nfirst,': 4199, 'cie': 4200, 'cei': 4201, 'words,\\nthen': 4202, 'the\\nextra': 4203, 'statement:': 4204, \"end='\": 4205, '(not': 4206, 'default': 4207, 'newline)': 4208, 'tricky': 4209, 'tricky:\\n': 4210, 'print(word,': 4211, \"')\\nancient\": 4212, 'ceiling': 4213, 'conceit': 4214, 'conceited': 4215, 'conceive': 4216, 'conscience\\nconscientious': 4217, 'conscientiously': 4218, 'deceitful': 4219, 'deceive': 4220, '.\\n>>>\\n\\n\\n\\n\\n\\n\\n5\\xa0\\xa0\\xa0automatic': 4221, 'understanding\\n\\nwe': 4222, 'bottom-up,': 4223, 'exploiting': 4224, 'computation\\nby': 4225, 'opportunity\\nnow': 4226, 'nitty-gritty': 4227, 'paint': 4228, 'a\\nbigger': 4229, 'picture': 4230, 'navigate': 4231, 'universe': 4232, 'information\\nlocked': 4233, 'crucial': 4234, 'the\\ngrowth': 4235, 'popularity': 4236, 'web,': 4237, 'shortcomings': 4238, 'skill,': 4239, 'knowledge,': 4240, 'luck,\\nto': 4241, 'answers': 4242, 'as:': 4243, 'tourist': 4244, 'sites': 4245, 'i\\nvisit': 4246, 'philadelphia': 4247, 'pittsburgh': 4248, 'limited': 4249, 'budget?\\nwhat': 4250, 'experts': 4251, 'say': 4252, 'digital': 4253, 'slr': 4254, 'cameras?': 4255, 'what\\npredictions': 4256, 'steel': 4257, 'market': 4258, 'credible': 4259, 'commentators\\nin': 4260, 'week?': 4261, 'automatically\\ninvolves': 4262, 'extraction,\\ninference,': 4263, 'summarization,': 4264, 'carried': 4265, 'scale\\nand': 4266, 'robustness': 4267, 'beyond': 4268, 'capabilities': 4269, 'philosophical': 4270, 'long-standing': 4271, 'intelligence\\nhas': 4272, 'intelligent': 4273, 'machines,': 4274, 'major': 4275, 'behaviour': 4276, 'understanding\\nlanguage': 4277, 'goal': 4278, 'become': 4279, 'mature,': 4280, 'for\\nanalyzing': 4281, 'unrestricted': 4282, 'widespread,': 4283, 'prospect': 4284, 'of\\nnatural': 4285, 're-emerged': 4286, 'plausible': 4287, 'technologies,\\nto': 4288, '.\\n\\n5': 4289, '.1\\xa0\\xa0\\xa0word': 4290, 'disambiguation\\nin': 4291, 'disambiguation': 4292, 'out\\nwhich': 4293, 'consider': 4294, 'the\\nambiguous': 4295, 'serve': 4296, 'dish:\\n\\n': 4297, '(2)\\n': 4298, '.serve:': 4299, 'food': 4300, 'drink;': 4301, 'office;': 4302, 'ball': 4303, 'play\\n\\n': 4304, '.dish:': 4305, 'plate;': 4306, 'meal;': 4307, 'communications': 4308, 'device\\n\\nin': 4309, 'phrase:': 4310, 'served': 4311, 'dish,': 4312, 'you\\ncan': 4313, 'dish': 4314, 'with\\ntheir': 4315, 'meanings': 4316, \"it's\": 4317, 'unlikely': 4318, 'discussion\\nshifted': 4319, 'sports': 4320, 'crockery': 4321, 'invent': 4322, 'bizarre': 4323, 'images,': 4324, 'tennis': 4325, 'pro\\ntaking': 4326, 'her': 4327, 'frustrations': 4328, 'china': 4329, 'tea-set': 4330, 'laid': 4331, 'beside': 4332, 'court': 4333, 'disambiguate': 4334, 'exploiting\\nthe': 4335, 'nearby': 4336, 'related': 4337, 'contextual': 4338, 'effect,': 4339, 'word\\nby,': 4340, 'meanings,': 4341, '.:': 4342, 'by\\nchesterton': 4343, '(agentive': 4344, 'author': 4345, 'book);\\nthe': 4346, 'cup': 4347, 'stove': 4348, '(locative': 4349, 'the\\ncup': 4350, 'is);': 4351, 'submit': 4352, 'friday': 4353, '(temporal': 4354, 'the\\ntime': 4355, 'submitting)': 4356, '.\\nobserve': 4357, '(3c)': 4358, 'italicized': 4359, 'helps': 4360, 'us\\ninterpret': 4361, '(3)\\n': 4362, '.the': 4363, 'lost': 4364, 'searchers': 4365, '(agentive)\\n\\n': 4366, 'mountain': 4367, '(locative)\\n\\n': 4368, 'afternoon': 4369, '(temporal)\\n\\n\\n\\n5': 4370, '.2\\xa0\\xa0\\xa0pronoun': 4371, 'resolution\\na': 4372, 'deeper': 4373, 'whom': 4374, '—\\ni': 4375, 'subjects': 4376, 'verbs': 4377, 'in\\nelementary': 4378, 'school,': 4379, 'harder': 4380, 'thieves': 4381, 'stole': 4382, 'paintings\\nit': 4383, 'stealing': 4384, '.\\nconsider': 4385, '(4c),': 4386, 'determine\\nwhat': 4387, 'sold,': 4388, 'caught,': 4389, '(one': 4390, 'ambiguous)': 4391, '(4)\\n': 4392, 'paintings': 4393, 'subsequently': 4394, 'sold': 4395, 'caught': 4396, '.\\n\\nanswering': 4397, 'antecedent': 4398, 'pronoun': 4399, 'they,\\neither': 4400, 'tackling': 4401, 'problem\\ninclude': 4402, 'anaphora': 4403, 'resolution': 4404, 'noun': 4405, 'phrase\\nrefers': 4406, 'labeling': 4407, 'phrase\\nrelates': 4408, 'verb': 4409, '(as': 4410, 'agent,': 4411, 'patient,': 4412, 'instrument,': 4413, 'on)': 4414, '.\\n\\n\\n5': 4415, '.3\\xa0\\xa0\\xa0generating': 4416, 'output\\nif': 4417, 'understanding,': 4418, 'move': 4419, 'output,': 4420, 'as\\nquestion': 4421, 'case,\\na': 4422, \"user's\": 4423, 'relating': 4424, 'texts:\\n\\n': 4425, '(5)\\n': 4426, '.text:': 4427, '.human:': 4428, 'sold?\\n\\n': 4429, '.machine:': 4430, '.\\n\\nthe': 4431, \"machine's\": 4432, 'demonstrates': 4433, 'they\\nrefers': 4434, 'translate': 4435, 'accurately\\nconveying': 4436, 'original': 4437, 'translating': 4438, 'french,\\nwe': 4439, 'forced': 4440, 'gender': 4441, 'sentence:\\nils': 4442, '(masculine)': 4443, 'found,': 4444, 'elles': 4445, '(feminine)': 4446, 'actually': 4447, 'depends': 4448, '(6)\\n': 4449, '.les': 4450, 'voleurs': 4451, 'ont': 4452, 'volé': 4453, 'les': 4454, 'peintures': 4455, 'ils': 4456, 'été': 4457, 'trouvés': 4458, 'tard': 4459, 'thieves)\\n\\n': 4460, 'trouvées': 4461, 'paintings)\\n\\nin': 4462, 'subject': 4463, 'verb,': 4464, 'the\\nantecedent': 4465, 'establishing': 4466, 'things\\nwe': 4467, '.4\\xa0\\xa0\\xa0machine': 4468, 'translation\\nfor': 4469, '(mt)': 4470, 'grail': 4471, 'understanding,\\nultimately': 4472, 'seeking': 4473, 'high-quality,\\nidiomatic': 4474, '.\\nits': 4475, 'roots': 4476, 'days': 4477, 'cold': 4478, 'war,': 4479, 'promise\\nof': 4480, 'government': 4481, 'sponsorship,\\nand': 4482, '.\\ntoday,': 4483, 'pairs\\nof': 4484, 'languages,': 4485, 'integrated': 4486, 'serious': 4487, 'shortcomings,': 4488, 'which\\nare': 4489, 'starkly': 4490, 'revealed': 4491, 'forth\\nbetween': 4492, 'equilibrium': 4493, 'reached,': 4494, '.:\\n\\n0>': 4495, 'flight': 4496, 'alice': 4497, 'springs?\\n1>': 4498, 'wie': 4499, 'lang': 4500, 'vor': 4501, 'dem': 4502, 'folgenden': 4503, 'flug': 4504, 'zu': 4505, 'springs?\\n2>': 4506, 'jump?\\n3>': 4507, 'springen': 4508, 'sie?\\n4>': 4509, 'jump?\\n5>': 4510, 'lang,': 4511, 'bevor': 4512, 'der': 4513, 'folgende': 4514, 'tun,': 4515, 'sie': 4516, 'springen?\\n6>': 4517, 'long,': 4518, 'does,': 4519, 'jump?\\n7>': 4520, 'tut,': 4521, 'tun': 4522, 'springen?\\n8>': 4523, 'jump?\\n9>': 4524, 'springen?\\n10>': 4525, 'alice,': 4526, 'jump?\\n11>': 4527, 'sprung?\\n12>': 4528, 'leap': 4529, 'you?\\n\\nobserve': 4530, 'translates': 4531, 'springs': 4532, 'english\\nto': 4533, 'german': 4534, '(in': 4535, '1>),': 4536, 'jump\\n(line': 4537, '2)': 4538, 'preposition': 4539, 'translated': 4540, 'corresponding\\ngerman': 4541, 'vor,': 4542, '5)': 4543, '(but': 4544, 'phrasings\\nindicated': 4545, 'leap)': 4546, 'proper': 4547, 'name,\\nand': 4548, 'misinterpreted': 4549, 'grammatical': 4550, 'turn:': 4551, 'http://translationparty': 4552, '.com/\\n\\nmachine': 4553, 'possible\\ntranslations': 4554, '(depending': 4555, 'meaning),': 4556, 'changed\\nin': 4557, 'keeping': 4558, 'target': 4559, '.\\ntoday': 4560, 'difficulties': 4561, 'faced': 4562, 'collecting': 4563, 'massive': 4564, 'of\\nparallel': 4565, 'websites': 4566, 'publish': 4567, 'documents\\nin': 4568, 'possibly\\na': 4569, 'bilingual': 4570, 'dictionary,': 4571, 'sentences,\\na': 4572, 'alignment': 4573, 'once': 4574, 'sentence\\npairs,': 4575, 'phrases,': 4576, 'model\\nthat': 4577, '.5\\xa0\\xa0\\xa0spoken': 4578, 'dialog': 4579, 'systems\\nin': 4580, 'history': 4581, 'intelligence,': 4582, 'one,': 4583, 'namely': 4584, 'turing': 4585, 'test:': 4586, 'system,\\nresponding': 4587, 'input,': 4588, 'naturally': 4589, 'distinguish\\nit': 4590, 'human-generated': 4591, 'response?': 4592, 'contrast,': 4593, \"today's\": 4594, 'commercial': 4595, 'systems\\nare': 4596, 'limited,': 4597, 'narrowly-defined': 4598, 'domains,\\nas': 4599, 'here:\\n\\ns:': 4600, 'you?\\nu:': 4601, 'saving': 4602, 'private': 4603, 'ryan': 4604, 'playing?\\ns:': 4605, 'theater?\\nu:': 4606, 'paramount': 4607, 'theater': 4608, '.\\ns:': 4609, 'playing': 4610, 'theater,': 4611, \"but\\nit's\": 4612, 'madison': 4613, '3:00,': 4614, '5:30,': 4615, '8:00,': 4616, '10:30': 4617, '.\\n\\nyou': 4618, 'driving': 4619, 'or\\ndetails': 4620, 'restaurants': 4621, 'information\\nhad': 4622, 'suitable': 4623, 'question-answer': 4624, 'pairs\\nhad': 4625, 'incorporated': 4626, 'goals:\\nthe': 4627, 'asks': 4628, 'movie': 4629, 'showing': 4630, 'system\\ncorrectly': 4631, 'determines': 4632, 'wants': 4633, 'see\\nthe': 4634, \"probably\\ndidn't\": 4635, 'made,': 4636, 'system\\nneeds': 4637, 'endowed': 4638, 'capability': 4639, 'interact\\nnaturally': 4640, 'asked': 4641, 'private\\nryan': 4642, 'playing?,': 4643, 'unhelpfully': 4644, 'respond': 4645, 'yes': 4646, 'developers': 4647, 'use\\ncontextual': 4648, 'assumptions': 4649, 'logic': 4650, 'ensure': 4651, 'might\\nexpress': 4652, 'requests': 4653, 'handled': 4654, 'that\\nmakes': 4655, 'application': 4656, 'so,': 4657, 'type\\nwhen': 4658, 'me\\nwhen': 4659, 'yield': 4660, 'screening': 4661, 'is\\nenough': 4662, 'service': 4663, 'pipeline': 4664, 'system:\\nspoken': 4665, '(top': 4666, 'left)': 4667, 'analyzed,': 4668, 'recognized,': 4669, 'parsed': 4670, 'and\\ninterpreted': 4671, 'application-specific': 4672, 'actions': 4673, 'right);\\na': 4674, 'response': 4675, 'planned,': 4676, 'realized': 4677, 'suitably\\ninflected': 4678, 'finally': 4679, 'output;': 4680, 'of\\nlinguistic': 4681, 'inform': 4682, 'stage': 4683, '.\\n\\ndialogue': 4684, 'opportunity': 4685, 'mention': 4686, 'the\\ncommonly': 4687, 'assumed': 4688, '.\\n5': 4689, '.\\nalong': 4690, 'diagram,': 4691, 'a\\npipeline': 4692, 'map': 4693, 'via': 4694, 'parsing\\nto': 4695, 'representation': 4696, 'middle,': 4697, 'from\\nright': 4698, 'left,': 4699, 'reverse': 4700, 'converting\\nconcepts': 4701, 'aspects': 4702, 'bottom': 4703, 'diagram': 4704, 'representative': 4705, 'of\\nstatic': 4706, 'information:': 4707, 'repositories': 4708, 'language-related': 4709, 'draw': 4710, 'turn:\\nfor': 4711, 'primitive': 4712, 'having\\na': 4713, 'conversation': 4714, 'chatbot': 4715, 'chatbots,\\nrun': 4716, '.chat': 4717, '.chatbots()': 4718, '.\\n(remember': 4719, '.)\\n\\n\\n\\n5': 4720, '.6\\xa0\\xa0\\xa0textual': 4721, 'entailment\\nthe': 4722, 'brought': 4723, 'public\\nshared': 4724, 'recognizing': 4725, 'textual': 4726, 'entailment': 4727, '(rte)': 4728, 'basic\\nscenario': 4729, 'suppose': 4730, 'evidence': 4731, 'hypothesis:': 4732, 'sandra': 4733, 'goudie': 4734, 'defeated': 4735, 'max': 4736, 'purnell,': 4737, 'and\\nthat': 4738, 'relevant,': 4739, 'example,\\nsandra': 4740, 'elected': 4741, 'parliament': 4742, '2002': 4743, 'elections,\\nnarrowly': 4744, 'winning': 4745, 'seat': 4746, 'coromandel': 4747, 'defeating': 4748, 'labour': 4749, 'candidate\\nmax': 4750, 'purnell': 4751, 'pushing': 4752, 'incumbent': 4753, 'green': 4754, 'mp': 4755, 'jeanette': 4756, 'fitzsimons': 4757, 'into\\nthird': 4758, 'to\\naccept': 4759, 'hypothesis?': 4760, 'conclusion': 4761, 'easily,': 4762, 'with\\nautomated': 4763, 'decision': 4764, 'rte\\nchallenges': 4765, 'allow': 4766, 'competitors': 4767, 'their\\nsystems,': 4768, 'brute': 4769, 'topic\\nwe': 4770, 'chap-data-intensive)': 4771, 'consequently,': 4772, 'some\\nlinguistic': 4773, 'important\\nfor': 4774, 'person': 4775, 'being\\ndefeated': 4776, 'hypothesis,': 4777, 'doing': 4778, 'the\\ntext': 4779, 'illustration': 4780, 'difficulty': 4781, 'consider\\nthe': 4782, 'text-hypothesis': 4783, 'pair:\\n\\n': 4784, '(7)\\n': 4785, 'david': 4786, 'golinkin': 4787, 'editor': 4788, 'eighteen': 4789, 'books,': 4790, '150': 4791, 'responsa,': 4792, 'articles,': 4793, 'sermons': 4794, 'books\\n\\n': 4795, '.hypothesis:': 4796, 'books\\n\\nin': 4797, 'determine': 4798, 'hypothesis': 4799, 'supported': 4800, 'the\\ntext,': 4801, 'needs': 4802, 'background': 4803, 'knowledge:\\n(i)': 4804, 'someone': 4805, 'he/she': 4806, 'that\\nbook;': 4807, '(ii)': 4808, 'not\\nwritten': 4809, '(all': 4810, 'of)': 4811, 'book;': 4812, '(iii)': 4813, 'eighteen\\nbooks,': 4814, 'conclude': 4815, '.7\\xa0\\xa0\\xa0limitations': 4816, 'nlp\\ndespite': 4817, 'research-led': 4818, 'advances': 4819, 'rte,': 4820, 'language\\nsystems': 4821, 'real-world': 4822, 'perform\\ncommon-sense': 4823, 'reasoning': 4824, 'and\\nrobust': 4825, 'manner': 4826, 'wait': 4827, 'artificial\\nintelligence': 4828, 'solved,': 4829, 'meantime': 4830, 'is\\nnecessary': 4831, 'severe': 4832, 'limitations': 4833, 'and\\nknowledge': 4834, 'accordingly,': 4835, 'right\\nfrom': 4836, 'beginning,': 4837, 'progress': 4838, 'that\\nunderstand': 4839, 'superficial': 4840, 'of\\nunrestricted': 4841, '.\\nindeed,': 4842, 'equip': 4843, 'systems,': 4844, 'to\\ncontribute': 4845, 'long-term': 4846, 'aspiration': 4847, '.\\n\\n\\n\\n6\\xa0\\xa0\\xa0summary\\n\\ntexts': 4848, 'represented': 4849, \"lists:\\n['monty',\": 4850, 'indexing,': 4851, 'slicing,\\nand': 4852, 'len()': 4853, 'appearance': 4854, 'text;\\na': 4855, 'sequence\\nof': 4856, 'using\\nlen(set(text))': 4857, 'sorted(set(t))': 4858, 'operate': 4859, '[f(x)': 4860, 'x': 4861, 'text]': 4862, 'derive': 4863, 'vocabulary,': 4864, 'collapsing': 4865, 'distinctions': 4866, 'ignoring': 4867, 'punctuation,\\nwe': 4868, 'set(w': 4869, '.isalpha())': 4870, 't:': 4871, 'character\\nand': 4872, 'executed': 4873, 'character': 4874, 'of\\ncode,': 4875, 'counts\\n(e': 4876, 'appearance)': 4877, 'assigned': 4878, 'reused': 4879, 'keyword,': 4880, 'in\\ndef': 4881, 'mult(x,': 4882, 'y);': 4883, 'y': 4884, 'function,\\nand': 4885, 'placeholders': 4886, 'specifying': 4887, 'more\\narguments': 4888, 'this:': 4889, 'texts(),': 4890, 'mult(3,': 4891, '4),': 4892, 'len(text1)': 4893, '.\\n\\n\\n\\n7\\xa0\\xa0\\xa0further': 4894, 'reading\\nthis': 4895, 'processing,\\nand': 4896, 'linguistics,': 4897, 'mixed': 4898, '.\\nmany': 4899, 'consolidated': 4900, 'to\\nconsult': 4901, '.org/),': 4902, 'links\\nto': 4903, 'links': 4904, 'on\\nsome': 4905, 'nlp-related': 4906, 'wikipedia': 4907, 'collocations,\\nthe': 4908, 'test,': 4909, 'type-token': 4910, 'distinction)': 4911, 'acquaint': 4912, '.org/,\\nincluding': 4913, 'tutorials': 4914, \"beginner's\": 4915, 'guide': 4916, 'http://wiki': 4917, '.org/moin/beginnersguide': 4918, '.\\nmiscellaneous': 4919, 'answered': 4920, 'faq': 4921, 'at\\nhttp://python': 4922, '.org/doc/faq/general/': 4923, 'delve': 4924, 'subscribe': 4925, 'mailing': 4926, 'new\\nreleases': 4927, 'announced': 4928, 'nltk-users': 4929, 'list,\\nwhere': 4930, 'for\\nlanguage': 4931, 'covered': 4932, '5,\\nand': 4933, 'excellent\\nbooks:\\n\\nindurkhya,': 4934, 'nitin': 4935, 'fred': 4936, 'damerau': 4937, '(eds,': 4938, '2010)': 4939, 'handbook': 4940, 'processing\\n(second': 4941, 'edition)': 4942, 'chapman': 4943, 'hall/crc': 4944, '2010': 4945, '(indurkhya': 4946, 'damerau,': 4947, '(dale,': 4948, 'moisl,': 4949, 'somers,': 4950, '2000)\\njurafsky,': 4951, 'daniel': 4952, 'james': 4953, '(2008)': 4954, '(second': 4955, 'prentice': 4956, 'hall': 4957, '.\\n(jurafsky': 4958, 'martin,': 4959, '2008)\\nmitkov,': 4960, 'ruslan': 4961, '(ed,': 4962, '2003)': 4963, 'oxford': 4964, 'press': 4965, '.\\n(second': 4966, '(mitkov,': 4967, '2002)\\n\\nthe': 4968, 'international': 4969, 'organization': 4970, 'acl': 4971, '(http://www': 4972, '.aclweb': 4973, '.org/)': 4974, 'hosts': 4975, 'resources,': 4976, 'including:\\ninformation': 4977, 'regional': 4978, 'conferences': 4979, 'workshops;\\nthe': 4980, 'wiki': 4981, 'resources;\\nand': 4982, 'anthology,': 4983, 'literature\\nfrom': 4984, '50+': 4985, 'years,': 4986, 'fully': 4987, 'indexed': 4988, '.\\nsome': 4989, 'excellent': 4990, 'textbooks': 4991, 'are:\\n[finegan2007]_,': 4992, \"(o'grady\": 4993, 'et': 4994, 'al,': 4995, '2004),': 4996, '(osu,': 4997, '2007)': 4998, 'consult\\nlanguagelog,': 4999, 'popular': 5000, 'blog': 5001, 'occasional': 5002, 'posts': 5003, 'that\\nuse': 5004, '.\\n\\n\\n8\\xa0\\xa0\\xa0exercises\\n\\n\\n☼': 5005, 'calculator,': 5006, 'and\\ntyping': 5007, '12': 5008, '(4': 5009, '1)': 5010, '.\\n\\n☼': 5011, 'alphabet': 5012, '26': 5013, 'power\\n10,': 5014, '**': 5015, '10,': 5016, 'ten-letter': 5017, 'out\\nto': 5018, '141167095653376': 5019, 'hundred-letter': 5020, 'possible?\\n\\n☼': 5021, 'applied': 5022, '20,\\nor': 5023, 'sent1?\\n\\n☼': 5024, 'on\\ncomputing': 5025, 'text2?\\nhow': 5026, 'there?\\n\\n☼': 5027, 'scores': 5028, 'humor\\nand': 5029, 'romance': 5030, 'fiction': 5031, 'is\\nmore': 5032, 'lexically': 5033, 'diverse?\\n\\n☼': 5034, 'protagonists': 5035, 'in\\nsense': 5036, 'sensibility:': 5037, 'elinor,': 5038, 'marianne,': 5039, 'edward,': 5040, 'willoughby': 5041, 'observe': 5042, 'roles': 5043, 'played': 5044, 'males\\nand': 5045, 'females': 5046, 'novel?': 5047, 'couples?\\n\\n☼': 5048, 'expression:': 5049, 'len(set(text4))': 5050, '.\\nstate': 5051, 'purpose': 5052, 'steps\\ninvolved': 5053, '2\\non': 5054, '.\\n\\ndefine': 5055, 'variable,': 5056, '.,\\nmy_string': 5057, \"'my\": 5058, \"string'\": 5059, 'string)': 5060, '.\\nprint': 5061, 'ways,': 5062, 'first\\nby': 5063, 'pressing': 5064, 'enter,': 5065, 'then\\nby': 5066, 'adding': 5067, 'my_string': 5068, 'my_string,': 5069, 'multiplying\\nit': 5070, 'number,': 5071, 'strings\\nare': 5072, 'joined': 5073, 'fix': 5074, 'this?\\n\\n\\n☼': 5075, '[my,': 5076, 'sent]': 5077, 'favorite': 5078, 'saying)': 5079, '.\\n\\nuse': 5080, '.join(my_sent)': 5081, '.\\nuse': 5082, 'split()': 5083, 'form\\nyou': 5084, '.\\n\\n\\n☼': 5085, 'phrase1,\\nphrase2,': 5086, 'combinations': 5087, '(using': 5088, 'operator)\\nto': 5089, 'whole': 5090, 'between\\nlen(phrase1': 5091, 'phrase2)': 5092, 'len(phrase1)': 5093, 'len(phrase2)?\\n\\n☼': 5094, 'expressions,': 5095, 'same\\nvalue': 5096, 'typically': 5097, 'nlp?': 5098, 'why?\\n\\nmonty': 5099, 'python[6:12]\\n[monty,': 5100, 'python][1]\\n\\n\\n☼': 5101, 'represent': 5102, 'where\\neach': 5103, 'sent1[2][2]': 5104, 'do?\\nwhy?': 5105, 'sent3': 5106, 'sent3[1]\\ngives': 5107, \"'the'\": 5108, 'occurrences\\nof': 5109, 'sent3?\\n\\n☼': 5110, '.\\nfind': 5111, '(text5)\\nstarting': 5112, 'alphabetical': 5113, 'list(range(10))': 5114, 'list(range(10,': 5115, '20)),': 5116, '2)),': 5117, 'list(range(20,': 5118, '-2))': 5119, '.\\n\\n◑': 5120, '.index()': 5121, 'sunset': 5122, '.\\nby': 5123, 'trial': 5124, 'that\\ncontains': 5125, 'addition,': 5126, 'operations,': 5127, 'the\\nvocabulary': 5128, 'sent8': 5129, 'lines?\\nwhich': 5130, 'value?': 5131, 'texts?\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 5132, 'sorted(set(w': 5133, 'text1))\\n>>>': 5134, 'set(text1))\\n\\n\\n\\n\\n◑': 5135, 'tests:\\nw': 5136, '.isupper()': 5137, '.islower()?\\n\\n◑': 5138, 'extracts': 5139, 'four-letter': 5140, '(text5)': 5141, '(freqdist),': 5142, 'these\\nwords': 5143, 'decreasing': 5144, 'script': 5145, '(text6)\\nand': 5146, 'text6': 5147, 'that\\nmeet': 5148, 'words:': 5149, '.\\n\\nending': 5150, 'ise\\ncontaining': 5151, 'z\\ncontaining': 5152, 'pt\\nhaving': 5153, 'capital': 5154, 'titlecase)\\n\\n\\n◑': 5155, \"words\\n['she',\": 5156, \"'sells',\": 5157, \"'sea',\": 5158, \"'shells',\": 5159, \"'shore']\": 5160, 'tasks:\\n\\nprint': 5161, 'sh\\nprint': 5162, 'characters\\n\\n\\n◑': 5163, 'do?': 5164, 'sum(len(w)': 5165, 'text1)\\ncan': 5166, 'average': 5167, 'text?\\n\\n◑': 5168, 'vocab_size(text)': 5169, 'single\\nparameter': 5170, 'percent(word,': 5171, 'text)': 5172, 'calculates\\nhow': 5173, 'expresses': 5174, 'result\\nas': 5175, 'vocabularies': 5176, 'following\\npython': 5177, 'set(sent3)': 5178, 'using\\ndifferent': 5179, 'set()': 5180, 'do?\\ncan': 5181, 'this?\\n\\n\\n\\n\\nabout': 5182, 'acst\\n\\n\\n\\ndocutils': 5183, 'messages\\n\\nsystem': 5184, 'message:': 5185, 'error/3': 5186, '(ch01': 5187, '.rst2,': 5188, '1889);': 5189, 'backlink\\nunknown': 5190, 'name:': 5191, 'finegan2007': 5192, 'resources\\n\\n\\n\\n\\n\\n2': 5193, 'resources\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\npractical': 5194, 'uses\\nlarge': 5195, 'python?\\nwhich': 5196, 'helpful': 5197, 'work?\\nhow': 5198, 'repeating': 5199, 'code?\\n\\nthis': 5200, 'present': 5201, 'before\\nexploring': 5202, 'see\\nan': 5203, 'unfamiliar;': 5204, 'see\\nwhat': 5205, 'game': 5206, 'substituting\\nsome': 5207, 'will\\nassociate': 5208, 'hows': 5209, 'whys': 5210, '.\\n\\n1\\xa0\\xa0\\xa0accessing': 5211, 'corpora\\nas': 5212, 'mentioned,': 5213, 'many\\ncorpora': 5214, 'material\\nin': 5215, 'examined': 5216, 'in\\n1': 5217, 'speeches': 5218, 'presidential\\ninaugural': 5219, 'addresses': 5220, 'dozens\\nof': 5221, 'convenience\\nwe': 5222, 'glued': 5223, 'end-to-end': 5224, 'treated': 5225, '.\\n1': 5226, 'that\\nwe': 5227, 'accessed': 5228, 'want\\nto': 5229, 'examines': 5230, 'a\\nvariety': 5231, '.1\\xa0\\xa0\\xa0gutenberg': 5232, 'corpus\\nnltk': 5233, 'project': 5234, 'gutenberg\\nelectronic': 5235, 'archive,': 5236, 'contains\\nsome': 5237, '25,000': 5238, 'electronic': 5239, 'hosted': 5240, 'http://www': 5241, '.gutenberg': 5242, 'begin\\nby': 5243, 'package,\\nthen': 5244, '.corpus': 5245, '.fileids(),': 5246, 'identifiers': 5247, 'corpus:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 5248, \".fileids()\\n['austen-emma\": 5249, \".txt',\": 5250, \"'austen-persuasion\": 5251, \"'austen-sense\": 5252, \"'bible-kjv\": 5253, \".txt',\\n'blake-poems\": 5254, \"'bryant-stories\": 5255, \"'burgess-busterbrown\": 5256, \".txt',\\n'carroll-alice\": 5257, \"'chesterton-ball\": 5258, \"'chesterton-brown\": 5259, \".txt',\\n'chesterton-thursday\": 5260, \"'edgeworth-parents\": 5261, \"'melville-moby_dick\": 5262, \".txt',\\n'milton-paradise\": 5263, \"'shakespeare-caesar\": 5264, \"'shakespeare-hamlet\": 5265, \".txt',\\n'shakespeare-macbeth\": 5266, \"'whitman-leaves\": 5267, \".txt']\\n\\n\\n\\nlet's\": 5268, 'emma': 5269, 'and\\ngive': 5270, 'emma,': 5271, 'contains:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 5272, \".words('austen-emma\": 5273, \".txt')\\n>>>\": 5274, 'len(emma)\\n192427\\n\\n\\n\\n\\nnote\\nin': 5275, 'showed': 5276, 'you\\ncould': 5277, 'carry': 5278, 'concordancing': 5279, '.concordance()': 5280, 'are\\nusing': 5281, 'nine': 5282, 'obtained': 5283, 'from\\nnltk': 5284, '.corpus,': 5285, 'the\\nfollowing': 5286, 'other\\ntasks': 5287, '1:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 5288, '.text(nltk': 5289, \".txt'))\\n>>>\": 5290, '.concordance(surprize)\\n\\n\\n\\n\\nwhen': 5291, 'words()': 5292, 'gutenberg\\nobject': 5293, 'package': 5294, 'cumbersome': 5295, 'provides\\nanother': 5296, 'gutenberg\\n>>>': 5297, 'gutenberg': 5298, \".txt')\\n\\n\\n\\nlet's\": 5299, 'each\\ntext,': 5300, 'fileid': 5301, 'computing\\nstatistics': 5302, 'compact': 5303, 'display,': 5304, 'round\\neach': 5305, 'nearest': 5306, 'integer,': 5307, 'round()': 5308, '.fileids():\\n': 5309, 'num_chars': 5310, 'len(gutenberg': 5311, '.raw(fileid))': 5312, 'num_words': 5313, '.words(fileid))\\n': 5314, 'num_sents': 5315, '.sents(fileid))\\n': 5316, 'num_vocab': 5317, 'len(set(w': 5318, '.words(fileid)))\\n': 5319, 'print(round(num_chars/num_words),': 5320, 'round(num_words/num_sents),': 5321, 'round(num_words/num_vocab),': 5322, 'fileid)\\n': 5323, '25': 5324, 'austen-emma': 5325, '.txt\\n5': 5326, '17': 5327, 'austen-persuasion': 5328, '28': 5329, '22': 5330, 'austen-sense': 5331, '.txt\\n4': 5332, '34': 5333, '79': 5334, 'bible-kjv': 5335, 'blake-poems': 5336, '14': 5337, 'bryant-stories': 5338, '18': 5339, 'burgess-busterbrown': 5340, '13': 5341, 'carroll-alice': 5342, 'chesterton-ball': 5343, '23': 5344, 'chesterton-brown': 5345, 'chesterton-thursday': 5346, '21': 5347, 'edgeworth-parents': 5348, 'melville-moby_dick': 5349, '52': 5350, 'milton-paradise': 5351, 'shakespeare-caesar': 5352, '8': 5353, 'shakespeare-hamlet': 5354, 'shakespeare-macbeth': 5355, '36': 5356, 'whitman-leaves': 5357, '.txt\\n\\n\\n\\nthis': 5358, 'statistics': 5359, 'text:\\naverage': 5360, 'vocabulary\\nitem': 5361, '(our': 5362, 'score)': 5363, 'since\\nit': 5364, 'recurrent': 5365, 'really\\n3': 5366, '.)\\nby': 5367, 'diversity\\nappear': 5368, 'characteristics': 5369, ',\\nnot': 5370, 'raw()': 5371, 'file\\nwithout': 5372, \".raw('blake-poems\": 5373, \".txt'))\\ntells\": 5374, 'sents()': 5375, 'divides': 5376, 'sentences,': 5377, 'words:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 5378, 'macbeth_sentences': 5379, \".sents('shakespeare-macbeth\": 5380, \"macbeth_sentences\\n[['[',\": 5381, \"'tragedie',\": 5382, \"'macbeth',\": 5383, \"'william',\": 5384, \"'shakespeare',\\n'1603',\": 5385, \"']'],\": 5386, \"['actus',\": 5387, \"'primus',\": 5388, \".'],\": 5389, \"macbeth_sentences[1116]\\n['double',\": 5390, \"'double',\": 5391, \"'toile',\": 5392, \"'trouble',\": 5393, \"';',\\n'fire',\": 5394, \"'burne',\": 5395, \"'cauldron',\": 5396, \"'bubble']\\n>>>\": 5397, 'longest_len': 5398, 'max(len(s)': 5399, 'macbeth_sentences)\\n>>>': 5400, '[s': 5401, 'len(s)': 5402, \"longest_len]\\n[['doubtfull',\": 5403, \"'stood',\": 5404, \"'two',\": 5405, \"'spent',\": 5406, \"'swimmers',\": 5407, \"'that',\\n'doe',\": 5408, \"'cling',\": 5409, \"'together',\": 5410, \"'choake',\": 5411, \"'art',\": 5412, \"'the',\\n'mercilesse',\": 5413, \"'macdonwald',\": 5414, '.]]\\n\\n\\n\\n\\nnote\\nmost': 5415, 'methods\\napart': 5416, 'words(),': 5417, 'raw(),': 5418, 'richer\\nlinguistic': 5419, 'corpora,': 5420, 'part-of-speech\\ntags,': 5421, 'tags,': 5422, 'trees,': 5423, 'forth;': 5424, 'these\\nin': 5425, '.\\n\\n\\n\\n1': 5426, '.2\\xa0\\xa0\\xa0web': 5427, 'text\\nalthough': 5428, 'established\\nliterature': 5429, 'formal': 5430, \"nltk's\\nsmall\": 5431, 'firefox': 5432, 'forum,\\nconversations': 5433, 'overheard': 5434, 'york,': 5435, 'pirates': 5436, 'carribean,\\npersonal': 5437, 'advertisements,': 5438, 'reviews:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 5439, 'webtext\\n>>>': 5440, 'webtext': 5441, 'print(fileid,': 5442, '.raw(fileid)[:65],': 5443, \".')\\n\": 5444, '.\\nfirefox': 5445, '.txt': 5446, 'cookie': 5447, 'manager:': 5448, 'cookies': 5449, 'se': 5450, '.\\ngrail': 5451, 'scene': 5452, '[wind]': 5453, '[clop': 5454, 'clop': 5455, 'clop]': 5456, 'king': 5457, 'arthur:': 5458, 'whoa': 5459, 'there!': 5460, '.\\noverheard': 5461, 'white': 5462, 'guy:': 5463, 'evening?': 5464, 'asian': 5465, 'girl': 5466, '.\\npirates': 5467, 'carribean:': 5468, \"man's\": 5469, 'chest,': 5470, 'ted': 5471, 'elliott': 5472, 'terr': 5473, '.\\nsingles': 5474, 'sexy': 5475, 'male,': 5476, 'seeks': 5477, 'attrac': 5478, 'lady,': 5479, 'discreet': 5480, 'encoun': 5481, '.\\nwine': 5482, 'lovely': 5483, 'delicate,': 5484, 'fragrant': 5485, 'rhone': 5486, 'polished': 5487, 'leather': 5488, 'strawb': 5489, '.\\n\\n\\n\\nthere': 5490, 'instant': 5491, 'messaging': 5492, 'sessions,': 5493, 'collected\\nby': 5494, 'naval': 5495, 'detection': 5496, 'internet': 5497, 'predators': 5498, '10,000': 5499, 'posts,': 5500, 'anonymized': 5501, 'replacing': 5502, 'usernames': 5503, 'generic\\nnames': 5504, 'usernnn,': 5505, 'manually': 5506, 'edited': 5507, 'remove': 5508, 'hundred': 5509, 'posts\\ncollected': 5510, 'date,': 5511, 'age-specific': 5512, 'chatroom': 5513, '(teens,': 5514, '20s,': 5515, '30s,': 5516, '40s,': 5517, 'a\\ngeneric': 5518, 'adults': 5519, 'chatroom)': 5520, 'filename': 5521, 'chatroom,\\nand': 5522, 'posts;': 5523, '10-19-20s_706posts': 5524, '.xml': 5525, '706': 5526, 'gathered': 5527, '20s': 5528, 'room': 5529, '10/19/2006': 5530, 'nps_chat\\n>>>': 5531, 'nps_chat': 5532, \".posts('10-19-20s_706posts\": 5533, \".xml')\\n>>>\": 5534, \"chatroom[123]\\n['i',\": 5535, \"'do',\": 5536, \"n't,\": 5537, \"'want',\": 5538, \"'hot',\": 5539, \"'pics',\": 5540, \"'female',\": 5541, \"',',\\n'i',\": 5542, \"'look',\": 5543, \"'mirror',\": 5544, \".']\\n\\n\\n\\n\\n\\n1\": 5545, '.3\\xa0\\xa0\\xa0brown': 5546, 'corpus\\nthe': 5547, 'million-word': 5548, 'electronic\\ncorpus': 5549, '1961': 5550, '500': 5551, 'sources,': 5552, 'sources\\nhave': 5553, 'categorized': 5554, 'genre,': 5555, 'news,': 5556, 'editorial,': 5557, 'genre\\n(for': 5558, 'http://icame': 5559, '.uib': 5560, '.no/brown/bcm-los': 5561, '.html)': 5562, '.\\n\\n\\n\\n\\n\\n\\n\\n\\nid\\nfile\\ngenre\\ndescription\\n\\n\\n\\na16\\nca16\\nnews\\nchicago': 5563, 'tribune:': 5564, 'reportage\\n\\nb02\\ncb02\\neditorial\\nchristian': 5565, 'monitor:': 5566, 'editorials\\n\\nc17\\ncc17\\nreviews\\ntime': 5567, 'magazine:': 5568, 'reviews\\n\\nd12\\ncd12\\nreligion\\nunderwood:': 5569, 'probing': 5570, 'ethics': 5571, 'realtors\\n\\ne36\\nce36\\nhobbies\\nnorling:': 5572, 'renting': 5573, 'car': 5574, 'europe\\n\\nf25\\ncf25\\nlore\\nboroff:': 5575, 'jewish': 5576, 'teenage': 5577, 'culture\\n\\ng22\\ncg22\\nbelles_lettres\\nreiner:': 5578, 'coping': 5579, 'runaway': 5580, 'technology\\n\\nh15\\nch15\\ngovernment\\nus': 5581, 'office': 5582, 'civil': 5583, 'defence': 5584, 'mobilization:': 5585, 'family': 5586, 'fallout': 5587, 'shelter\\n\\nj17\\ncj19\\nlearned\\nmosteller:': 5588, 'statistical': 5589, 'applications\\n\\nk04\\nck04\\nfiction\\nw': 5590, '.b': 5591, 'du': 5592, 'bois:': 5593, 'worlds': 5594, 'color\\n\\nl13\\ncl13\\nmystery\\nhitchens:': 5595, 'footsteps': 5596, 'night\\n\\nm01\\ncm01\\nscience_fiction\\nheinlein:': 5597, 'stranger': 5598, 'strange': 5599, 'land\\n\\nn14\\ncn15\\nadventure\\nfield:': 5600, 'rattlesnake': 5601, 'ridge\\n\\np12\\ncp12\\nromance\\ncallaghan:': 5602, 'passion': 5603, 'rome\\n\\nr06\\ncr06\\nhumor\\nthurber:': 5604, 'future,': 5605, 'any,': 5606, 'comedy\\n\\n\\ntable': 5607, 'corpus\\n\\n\\nwe': 5608, '(where': 5609, 'sentence\\nis': 5610, 'words)': 5611, 'optionally': 5612, 'categories': 5613, 'read:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 5614, 'brown\\n>>>': 5615, \".categories()\\n['adventure',\": 5616, \"'belles_lettres',\": 5617, \"'editorial',\": 5618, \"'fiction',\": 5619, \"'government',\": 5620, \"'hobbies',\\n'humor',\": 5621, \"'learned',\": 5622, \"'lore',\": 5623, \"'mystery',\": 5624, \"'news',\": 5625, \"'religion',\": 5626, \"'reviews',\": 5627, \"'romance',\\n'science_fiction']\\n>>>\": 5628, \".words(categories='news')\\n['the',\": 5629, \"'fulton',\": 5630, \"'county',\": 5631, \"'grand',\": 5632, \"'jury',\": 5633, \".words(fileids=['cg22'])\\n['does',\": 5634, \"'our',\": 5635, \"'society',\": 5636, \"'have',\": 5637, \"'runaway',\": 5638, \".sents(categories=['news',\": 5639, \"'reviews'])\\n[['the',\": 5640, \"'county'\": 5641, \"['the',\": 5642, \"'further'\": 5643, '.]\\n\\n\\n\\nthe': 5644, 'convenient': 5645, 'resource': 5646, 'studying': 5647, 'between\\ngenres,': 5648, 'inquiry': 5649, 'stylistics': 5650, 'modal': 5651, 'step\\nis': 5652, 'to\\nimport': 5653, 'news_text': 5654, \".words(categories='news')\\n>>>\": 5655, '.freqdist(w': 5656, 'news_text)\\n>>>': 5657, 'modals': 5658, \"['can',\": 5659, \"'may',\": 5660, \"'might',\": 5661, \"'must',\": 5662, \"'will']\\n>>>\": 5663, 'm': 5664, 'modals:\\n': 5665, 'print(m': 5666, 'fdist[m],': 5667, \"')\\n\": 5668, '.\\ncan:': 5669, '94': 5670, 'could:': 5671, '87': 5672, 'may:': 5673, '93': 5674, 'might:': 5675, '38': 5676, 'must:': 5677, '53': 5678, 'will:': 5679, '389\\n\\n\\n\\n\\nnote\\nwe': 5680, 'to\\nput': 5681, '.\\n\\n\\nnote\\nyour': 5682, 'turn:\\nchoose': 5683, 'previous\\nexample': 5684, 'wh': 5685, 'what,\\nwhen,': 5686, 'where,': 5687, 'who,': 5688, '.\\n\\nnext,': 5689, \"use\\nnltk's\": 5690, 'are\\npresented': 5691, '2,\\nwhere': 5692, 'unpick': 5693, 'moment,\\nyou': 5694, 'ignore': 5695, 'concentrate': 5696, 'cfd': 5697, '.conditionalfreqdist(\\n': 5698, '(genre,': 5699, 'word)\\n': 5700, '.categories()\\n': 5701, '.words(categories=genre))\\n>>>': 5702, \"['news',\": 5703, \"'hobbies',\": 5704, \"'science_fiction',\": 5705, \"'romance',\": 5706, \"'humor']\\n>>>\": 5707, '.tabulate(conditions=genres,': 5708, 'samples=modals)\\n': 5709, 'will\\n': 5710, '86': 5711, '66': 5712, '389\\n': 5713, 'religion': 5714, '82': 5715, '59': 5716, '78': 5717, '54': 5718, '71\\n': 5719, 'hobbies': 5720, '268': 5721, '58': 5722, '131': 5723, '83': 5724, '264\\nscience_fiction': 5725, '49': 5726, '16\\n': 5727, '74': 5728, '193': 5729, '51': 5730, '45': 5731, '43\\n': 5732, 'humor': 5733, '13\\n\\n\\n\\nobserve': 5734, 'will,\\nwhile': 5735, '.\\nwould': 5736, 'predicted': 5737, 'this?': 5738, 'counts\\nmight': 5739, 'distinguish': 5740, 'chap-data-intensive': 5741, '.4\\xa0\\xa0\\xa0reuters': 5742, 'reuters': 5743, '10,788': 5744, 'documents': 5745, 'totaling': 5746, '.3': 5747, 'classified': 5748, '90': 5749, 'topics,': 5750, 'grouped\\ninto': 5751, 'training': 5752, 'test;': 5753, 'with\\nfileid': 5754, \"'test/14826'\": 5755, 'drawn': 5756, 'for\\ntraining': 5757, 'document,\\nas': 5758, 'reuters\\n>>>': 5759, \".fileids()\\n['test/14826',\": 5760, \"'test/14828',\": 5761, \"'test/14829',\": 5762, \"'test/14832',\": 5763, \".categories()\\n['acq',\": 5764, \"'alum',\": 5765, \"'barley',\": 5766, \"'bop',\": 5767, \"'carcass',\": 5768, \"'castor-oil',\": 5769, \"'cocoa',\\n'coconut',\": 5770, \"'coconut-oil',\": 5771, \"'coffee',\": 5772, \"'copper',\": 5773, \"'copra-cake',\": 5774, \"'corn',\\n'cotton',\": 5775, \"'cotton-oil',\": 5776, \"'cpi',\": 5777, \"'cpu',\": 5778, \"'crude',\": 5779, \"'dfl',\": 5780, \"'dlr',\": 5781, '.]\\n\\n\\n\\nunlike': 5782, 'overlap': 5783, 'with\\neach': 5784, 'other,': 5785, 'story': 5786, 'multiple': 5787, 'documents,': 5788, 'the\\ndocuments': 5789, 'convenience,': 5790, 'the\\ncorpus': 5791, 'accept': 5792, 'fileids': 5793, \".categories('training/9865')\\n['barley',\": 5794, \"'corn',\": 5795, \"'grain',\": 5796, \"'wheat']\\n>>>\": 5797, \".categories(['training/9865',\": 5798, \"'training/9880'])\\n['barley',\": 5799, \"'money-fx',\": 5800, \".fileids('barley')\\n['test/15618',\": 5801, \"'test/15649',\": 5802, \"'test/15676',\": 5803, \"'test/15728',\": 5804, \"'test/15871',\": 5805, \".fileids(['barley',\": 5806, \"'corn'])\\n['test/14832',\": 5807, \"'test/14858',\": 5808, \"'test/15033',\": 5809, \"'test/15043',\": 5810, \"'test/15106',\\n'test/15287',\": 5811, \"'test/15341',\": 5812, \"'test/15618',\": 5813, \"'test/15648',\": 5814, '.]\\n\\n\\n\\nsimilarly,': 5815, 'of\\nfiles': 5816, 'handful': 5817, 'the\\ntitles,': 5818, 'convention': 5819, 'upper': 5820, \".words('training/9865')[:14]\\n['french',\": 5821, \"'free',\": 5822, \"'market',\": 5823, \"'cereal',\": 5824, \"'export',\": 5825, \"'bids',\\n'detailed',\": 5826, \"'french',\": 5827, \"'operators',\": 5828, \"'requested',\": 5829, \"'licences',\": 5830, \"'export']\\n>>>\": 5831, \".words(['training/9865',\": 5832, \"'training/9880'])\\n['french',\": 5833, \".words(categories='barley')\\n['french',\": 5834, \".words(categories=['barley',\": 5835, \"'corn'])\\n['thai',\": 5836, \"'trade',\": 5837, \"'deficit',\": 5838, \"'widens',\": 5839, \"'first',\": 5840, '.]\\n\\n\\n\\n\\n\\n1': 5841, '.5\\xa0\\xa0\\xa0inaugural': 5842, 'corpus\\n\\nin': 5843, 'looked': 5844, 'at\\nthe': 5845, 'corpus,\\nbut': 5846, 'fig-inaugural\\nused': 5847, 'offset': 5848, 'axes;': 5849, 'dimension:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 5850, 'inaugural\\n>>>': 5851, \".fileids()\\n['1789-washington\": 5852, \"'1793-washington\": 5853, \"'1797-adams\": 5854, '[fileid[:4]': 5855, \".fileids()]\\n['1789',\": 5856, \"'1793',\": 5857, \"'1797',\": 5858, \"'1801',\": 5859, \"'1805',\": 5860, \"'1809',\": 5861, \"'1813',\": 5862, \"'1817',\": 5863, \"'1821',\": 5864, '.]\\n\\n\\n\\nnotice': 5865, 'year': 5866, 'year\\nout': 5867, 'filename,': 5868, 'extracted': 5869, 'fileid[:4]': 5870, 'america': 5871, 'citizen': 5872, 'code\\nconverts': 5873, 'corpus\\nto': 5874, ',\\nthen': 5875, 'targets\\namerica': 5876, 'startswith()': 5877, '.\\nthus': 5878, \"american's\": 5879, 'citizens': 5880, 'in\\n2;': 5881, '(target,': 5882, 'fileid[:4])\\n': 5883, '.fileids()\\n': 5884, '.words(fileid)\\n': 5885, \"['america',\": 5886, \"'citizen']\\n\": 5887, '.startswith(target))': 5888, '.plot()\\n\\n\\n\\n\\n\\nfigure': 5889, 'distribution:': 5890, 'address\\ncorpus': 5891, 'counted;': 5892, 'counts\\nare': 5893, 'kept': 5894, 'address;': 5895, 'plotted': 5896, 'trends': 5897, 'observed;': 5898, 'normalized': 5899, '.6\\xa0\\xa0\\xa0annotated': 5900, 'corpora\\nmany': 5901, 'annotations,': 5902, 'pos': 5903, 'tags,\\nnamed': 5904, 'entities,': 5905, 'roles,': 5906, 'provides\\nconvenient': 5907, 'corpora\\nand': 5908, 'samples,': 5909, 'about\\ndownloading': 5910, 'them,': 5911, '.org/data': 5912, 'corpora,\\nplease': 5913, 'howto': 5914, '.org/howto': 5915, '.\\n\\n\\n\\n\\n\\n\\n\\ncorpus\\ncompiler\\ncontents\\n\\n\\n\\nbrown': 5916, 'corpus\\nfrancis,': 5917, 'kucera\\n15': 5918, '.15m': 5919, 'tagged,': 5920, 'categorized\\n\\ncess': 5921, 'treebanks\\nclic-ub\\n1m': 5922, 'tagged': 5923, '(catalan,': 5924, 'spanish)\\n\\nchat-80': 5925, 'files\\npereira': 5926, 'warren\\nworld': 5927, 'geographic': 5928, 'database\\n\\ncmu': 5929, 'pronouncing': 5930, 'dictionary\\ncmu\\n127k': 5931, 'entries\\n\\nconll': 5932, '2000': 5933, 'chunking': 5934, 'data\\nconll\\n270k': 5935, 'chunked\\n\\nconll': 5936, 'entity\\nconll\\n700k': 5937, 'pos-': 5938, 'named-entity-tagged': 5939, '(dutch,': 5940, 'spanish)\\n\\nconll': 5941, '2007': 5942, 'dependency': 5943, 'treebanks': 5944, '(sel)\\nconll\\n150k': 5945, '(basque,': 5946, 'catalan)\\n\\ndependency': 5947, 'treebank\\nnarad\\ndependency': 5948, 'penn': 5949, 'treebank': 5950, 'sample\\n\\nframenet\\nfillmore,': 5951, 'baker': 5952, 'al\\n10k': 5953, 'senses,': 5954, '170k': 5955, 'sentences\\n\\nfloresta': 5956, 'treebank\\ndiana': 5957, 'santos': 5958, 'al\\n9k': 5959, '(portuguese)\\n\\ngazetteer': 5960, 'lists\\nvarious\\nlists': 5961, 'cities': 5962, 'countries\\n\\ngenesis': 5963, 'corpus\\nmisc': 5964, 'sources\\n6': 5965, '200k': 5966, '6': 5967, 'languages\\n\\ngutenberg': 5968, '(selections)\\nhart,': 5969, 'newby,': 5970, 'al\\n18': 5971, '2m': 5972, 'words\\n\\ninaugural': 5973, 'corpus\\ncspan\\nus': 5974, '(1789-present)\\n\\nindian': 5975, 'pos-tagged': 5976, 'corpus\\nkumaran': 5977, 'al\\n60k': 5978, '(bangla,': 5979, 'hindi,': 5980, 'marathi,': 5981, 'telugu)\\n\\nmacmorpho': 5982, 'corpus\\nnilc,': 5983, 'usp,': 5984, 'brazil\\n1m': 5985, '(brazilian': 5986, 'portuguese)\\n\\nmovie': 5987, 'reviews\\npang,': 5988, 'lee\\n2k': 5989, 'polarity': 5990, 'classification\\n\\nnames': 5991, 'corpus\\nkantrowitz,': 5992, 'ross\\n8k': 5993, 'male': 5994, 'female': 5995, 'names\\n\\nnist': 5996, '1999': 5997, 'info': 5998, 'extr': 5999, '(selections)\\ngarofolo\\n63k': 6000, 'newswire': 6001, 'named-entity': 6002, 'sgml': 6003, 'markup\\n\\nnombank\\nmeyers\\n115k': 6004, 'propositions,': 6005, '1400': 6006, 'frames\\n\\nnps': 6007, 'corpus\\nforsyth,': 6008, 'martell\\n10k': 6009, 'im': 6010, 'dialogue-act': 6011, 'tagged\\n\\nopen': 6012, 'wordnet\\nbond': 6013, 'al\\n15': 6014, 'aligned': 6015, 'wordnet\\n\\npp': 6016, 'attachment': 6017, 'corpus\\nratnaparkhi\\n28k': 6018, 'prepositional': 6019, 'modifiers\\n\\nproposition': 6020, 'bank\\npalmer\\n113k': 6021, '3300': 6022, 'frames\\n\\nquestion': 6023, 'classification\\nli,': 6024, 'roth\\n6k': 6025, 'questions,': 6026, 'categorized\\n\\nreuters': 6027, 'corpus\\nreuters\\n1': 6028, '.3m': 6029, '10k': 6030, \"categorized\\n\\nroget's\": 6031, 'thesaurus\\nproject': 6032, 'gutenberg\\n200k': 6033, 'text\\n\\nrte': 6034, 'entailment\\ndagan': 6035, 'al\\n8k': 6036, 'categorized\\n\\nsemcor\\nrus,': 6037, 'mihalcea\\n880k': 6038, 'part-of-speech': 6039, 'tagged\\n\\nsenseval': 6040, 'corpus\\npedersen\\n600k': 6041, 'tagged\\n\\nsentiwordnet\\nesuli,': 6042, 'sebastiani\\nsentiment': 6043, '145k': 6044, 'synonym': 6045, 'sets\\n\\nshakespeare': 6046, '(selections)\\nbosak\\n8': 6047, 'xml': 6048, 'format\\n\\nstate': 6049, 'union': 6050, 'corpus\\ncspan\\n485k': 6051, 'text\\n\\nstopwords': 6052, 'corpus\\nporter': 6053, 'al\\n2,400': 6054, 'stopwords': 6055, 'languages\\n\\nswadesh': 6056, 'corpus\\nwiktionary\\ncomparative': 6057, 'wordlists': 6058, '24': 6059, 'languages\\n\\nswitchboard': 6060, '(selections)\\nldc\\n36': 6061, 'phonecalls,': 6062, 'transcribed,': 6063, 'parsed\\n\\nuniv': 6064, 'decl': 6065, 'rights\\nunited': 6066, 'nations\\n480k': 6067, '300+': 6068, 'languages\\n\\npenn': 6069, '(selections)\\nldc\\n40k': 6070, 'parsed\\n\\ntimit': 6071, '(selections)\\nnist/ldc\\naudio': 6072, 'transcripts': 6073, 'speakers\\n\\nverbnet': 6074, '.1\\npalmer': 6075, 'al\\n5k': 6076, 'verbs,': 6077, 'hierarchically': 6078, 'organized,': 6079, 'wordnet\\n\\nwordlist': 6080, 'corpus\\nopenoffice': 6081, '.org': 6082, 'al\\n960k': 6083, '20k': 6084, 'affixes': 6085, 'languages\\n\\nwordnet': 6086, '(english)\\nmiller,': 6087, 'fellbaum\\n145k': 6088, 'sets\\n\\n\\ntable': 6089, 'nltk:': 6090, 'downloading\\nand': 6091, '.\\n\\n\\n\\n\\n1': 6092, '.7\\xa0\\xa0\\xa0corpora': 6093, 'languages\\nnltk': 6094, 'cases\\nyou': 6095, 'encodings': 6096, 'python\\nbefore': 6097, '.3)': 6098, '.cess_esp': 6099, \".words()\\n['el',\": 6100, \"'grupo',\": 6101, \"'estatal',\": 6102, \"'electricit\\\\xe9_de_france',\": 6103, '.floresta': 6104, \".words()\\n['um',\": 6105, \"'revivalismo',\": 6106, \"'refrescante',\": 6107, \"'o',\": 6108, \"'7_e_meio',\": 6109, '.indian': 6110, \".words('hindi\": 6111, \".pos')\\n['पूर्ण',\": 6112, \"'प्रतिबंध',\": 6113, \"'हटाओ',\": 6114, \"'इराक',\": 6115, \"'संयुक्त',\": 6116, '.udhr': 6117, \".fileids()\\n['abkhaz-cyrillic+abkh',\": 6118, \"'abkhaz-utf8',\": 6119, \"'achehnese-latin1',\": 6120, \"'achuar-shiwiar-latin1',\\n'adja-utf8',\": 6121, \"'afaan_oromo_oromiffa-latin1',\": 6122, \"'afrikaans-latin1',\": 6123, \"'aguaruna-latin1',\\n'akuapem_twi-utf8',\": 6124, \"'albanian_shqip-latin1',\": 6125, \"'amahuaca',\": 6126, \"'amahuaca-latin1',\": 6127, \".words('javanese-latin1')[11:]\\n['saben',\": 6128, \"'umat',\": 6129, \"'manungsa',\": 6130, \"'lair',\": 6131, \"'kanthi',\": 6132, \"'hak',\": 6133, 'udhr,': 6134, 'universal': 6135, 'declaration': 6136, 'rights\\nin': 6137, '300': 6138, 'include\\ninformation': 6139, 'encoding': 6140, 'file,\\nsuch': 6141, 'utf8': 6142, 'latin1': 6143, 'lengths\\nfor': 6144, 'udhr': 6145, '(run': 6146, 'color': 6147, 'plot)': 6148, 'boolean': 6149, 'udhr\\n>>>': 6150, \"['chickasaw',\": 6151, \"'english',\": 6152, \"'german_deutsch',\\n\": 6153, \"'greenlandic_inuktikut',\": 6154, \"'hungarian_magyar',\": 6155, \"'ibibio_efik']\\n>>>\": 6156, '(lang,': 6157, 'len(word))\\n': 6158, 'languages\\n': 6159, '.words(lang': 6160, \"'-latin1'))\\n>>>\": 6161, '.plot(cumulative=true)\\n\\n\\n\\n\\n\\nfigure': 6162, 'distributions:\\nsix': 6163, 'translations': 6164, 'rights': 6165, 'processed;\\nthis': 6166, 'fewer': 6167, 'about\\n80%': 6168, 'ibibio': 6169, '60%': 6170, '25%': 6171, 'inuktitut': 6172, '.\\n\\n\\n\\nnote\\nyour': 6173, 'variable\\nraw_text': 6174, '.raw(language-latin1)': 6175, 'frequency\\ndistribution': 6176, '.freqdist(raw_text)': 6177, '.plot()': 6178, '.\\n\\nunfortunately,': 6179, 'is\\ninsufficient': 6180, 'industrial': 6181, 'developing': 6182, 'individual\\nefforts': 6183, 'piecemeal': 6184, 'no\\nestablished': 6185, 'endangered': 6186, '7\\nfor': 6187, 'suggestions': 6188, '.)\\n\\n\\n1': 6189, '.8\\xa0\\xa0\\xa0text': 6190, 'structure\\nwe': 6191, 'far;': 6192, 'are\\nsummarized': 6193, 'lacks': 6194, 'structure:': 6195, '.\\noften,': 6196, 'correspond': 6197, 'source,': 6198, 'author,': 6199, 'etc': 6200, '.\\nsometimes': 6201, 'overlap,': 6202, 'notably': 6203, 'topical': 6204, 'be\\nrelevant': 6205, 'occasionally,': 6206, 'temporal': 6207, 'structure,\\nnews': 6208, '.3:': 6209, 'corpora:': 6210, 'collection\\nof': 6211, 'isolated': 6212, 'organization;': 6213, 'structured\\ninto': 6214, '(brown': 6215, 'corpus);': 6216, 'categorizations': 6217, 'as\\ntopic': 6218, '(reuters': 6219, 'time\\n(inaugural': 6220, 'corpus)': 6221, '.\\n\\n\\n\\n\\n\\n\\n\\nexample\\ndescription\\n\\n\\n\\nfileids()\\nthe': 6222, 'corpus\\n\\nfileids([categories])\\nthe': 6223, 'categories\\n\\ncategories()\\nthe': 6224, 'corpus\\n\\ncategories([fileids])\\nthe': 6225, 'files\\n\\nraw()\\nthe': 6226, 'corpus\\n\\nraw(fileids=[f1,f2,f3])\\nthe': 6227, 'files\\n\\nraw(categories=[c1,c2])\\nthe': 6228, 'categories\\n\\nwords()\\nthe': 6229, 'corpus\\n\\nwords(fileids=[f1,f2,f3])\\nthe': 6230, 'fileids\\n\\nwords(categories=[c1,c2])\\nthe': 6231, 'categories\\n\\nsents()\\nthe': 6232, 'corpus\\n\\nsents(fileids=[f1,f2,f3])\\nthe': 6233, 'fileids\\n\\nsents(categories=[c1,c2])\\nthe': 6234, 'categories\\n\\nabspath(fileid)\\nthe': 6235, 'disk\\n\\nencoding(fileid)\\nthe': 6236, '(if': 6237, 'known)\\n\\nopen(fileid)\\nopen': 6238, 'stream': 6239, 'file\\n\\nroot\\nif': 6240, 'path': 6241, 'root': 6242, 'locally': 6243, 'corpus\\n\\nreadme()\\nthe': 6244, 'readme': 6245, 'corpus\\n\\n\\ntable': 6246, 'using\\nhelp(nltk': 6247, '.reader)': 6248, \".\\n\\n\\nnltk's\": 6249, 'functionality\\nprovided': 6250, 'illustrate': 6251, 'some\\nof': 6252, 'below:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 6253, '.raw(burgess-busterbrown': 6254, '.txt)\\n>>>': 6255, \"raw[1:20]\\n'the\": 6256, 'adventures': 6257, \"b'\\n>>>\": 6258, '.words(burgess-busterbrown': 6259, \"words[1:20]\\n['the',\": 6260, \"'adventures',\": 6261, \"'buster',\": 6262, \"'bear',\": 6263, \"'thornton',\": 6264, \"'w',\": 6265, \".',\\n'burgess',\": 6266, \"'1920',\": 6267, \"'i',\": 6268, \"'goes',\": 6269, \"'fishing',\": 6270, \"'buster',\\n'bear']\\n>>>\": 6271, 'sents': 6272, '.sents(burgess-busterbrown': 6273, \"sents[1:20]\\n[['i'],\": 6274, \"['buster',\": 6275, \"'fishing'],\": 6276, \"'yawned',\": 6277, \"'as',\\n'he',\": 6278, \"'lay',\": 6279, \"'on',\": 6280, \"'his',\": 6281, \"'comfortable',\": 6282, \"'bed',\": 6283, \"'leaves',\": 6284, \"'watched',\\n'the',\": 6285, \"'early',\": 6286, \"'morning',\": 6287, \"'sunbeams',\": 6288, \"'creeping',\": 6289, \"'through',\": 6290, '.9\\xa0\\xa0\\xa0loading': 6291, 'corpus\\nif': 6292, 'methods,': 6293, \"nltk's\\nplaintextcorpusreader\": 6294, 'system;': 6295, 'directory\\n/usr/share/dict': 6296, 'whatever': 6297, 'location,': 6298, 'of\\ncorpus_root': 6299, 'plaintextcorpusreader': 6300, 'initializer': 6301, '\\ncan': 6302, 'fileids,': 6303, \"['a\": 6304, \"'test/b\": 6305, \".txt'],\\nor\": 6306, 'matches': 6307, \"'[abc]/\": 6308, '.*\\\\': 6309, \".txt'\\n(see\": 6310, 'information\\nabout': 6311, 'regular': 6312, 'expressions)': 6313, 'plaintextcorpusreader\\n>>>': 6314, 'corpus_root': 6315, \"'/usr/share/dict'\": 6316, 'plaintextcorpusreader(corpus_root,': 6317, \".*')\": 6318, \".fileids()\\n['readme',\": 6319, \"'connectives',\": 6320, \"'propernames',\": 6321, \"'web2',\": 6322, \"'web2a',\": 6323, \"'words']\\n>>>\": 6324, \".words('connectives')\\n['the',\": 6325, \"'that',\": 6326, '.]\\n\\n\\n\\nas': 6327, 'local': 6328, '(release': 6329, '3),\\nin': 6330, 'c:\\\\corpora': 6331, 'bracketparsecorpusreader': 6332, 'this\\ncorpus': 6333, 'street\\njournal': 6334, 'component': 6335, 'file_pattern\\nthat': 6336, 'contained': 6337, 'subfolders': 6338, 'slashes)': 6339, 'bracketparsecorpusreader\\n>>>': 6340, 'rc:\\\\corpora\\\\penntreebank\\\\parsed\\\\mrg\\\\wsj': 6341, 'file_pattern': 6342, 'r': 6343, '.*/wsj_': 6344, '.mrg': 6345, 'ptb': 6346, 'bracketparsecorpusreader(corpus_root,': 6347, 'file_pattern)\\n>>>': 6348, \".fileids()\\n['00/wsj_0001\": 6349, \".mrg',\": 6350, \"'00/wsj_0002\": 6351, \"'00/wsj_0003\": 6352, \"'00/wsj_0004\": 6353, 'len(ptb': 6354, '.sents())\\n49208\\n>>>': 6355, \".sents(fileids='20/wsj_2013\": 6356, \".mrg')[19]\\n['the',\": 6357, \"'55-year-old',\": 6358, \"'mr\": 6359, \"'noriega',\": 6360, \"'smooth',\": 6361, \"'the',\\n'shah',\": 6362, \"'iran',\": 6363, \"'well-born',\": 6364, \"'nicaragua',\": 6365, \"'s,\": 6366, \"'anastasio',\\n'somoza',\": 6367, \"'imperial',\": 6368, \"'ferdinand',\": 6369, \"'marcos',\": 6370, \"'philippines',\\n'or',\": 6371, \"'bloody',\": 6372, \"'haiti',\": 6373, \"'baby',\": 6374, \"doc',\": 6375, \"'duvalier',\": 6376, \".']\\n\\n\\n\\n\\n\\n\\n2\\xa0\\xa0\\xa0conditional\": 6377, 'distributions\\nwe': 6378, 'mylist': 6379, 'items,\\nfreqdist(mylist)': 6380, 'each\\nitem': 6381, 'generalize': 6382, 'several\\ncategories,': 6383, 'topic,': 6384, 'etc,': 6385, 'maintain': 6386, 'separate\\nfrequency': 6387, 'category': 6388, 'to\\nstudy': 6389, 'previous\\nsection': 6390, 'achieved': 6391, 'conditionalfreqdist': 6392, 'data\\ntype': 6393, 'of\\nfrequency': 6394, 'the\\ncondition': 6395, '.1\\ndepicts': 6396, 'just\\ntwo': 6397, 'conditions,': 6398, 'distribution)\\n\\n\\n2': 6399, '.1\\xa0\\xa0\\xa0conditions': 6400, 'events\\na': 6401, 'events,\\nsuch': 6402, 'conditional\\nfrequency': 6403, '.\\nso': 6404, ',\\nwe': 6405, 'pairs': 6406, \"[('news',\": 6407, \"'the'),\": 6408, \"('news',\": 6409, \"'fulton'),\": 6410, \"'county'),\": 6411, '\\n\\n\\n\\neach': 6412, '(condition,': 6413, 'event)': 6414, 'the\\nentire': 6415, 'genre),\\nand': 6416, '1,161,192': 6417, 'events': 6418, 'word)': 6419, '.\\n\\n\\n2': 6420, '.2\\xa0\\xa0\\xa0counting': 6421, 'genre\\nin': 6422, 'the\\nbrown': 6423, 'whereas\\nfreqdist()': 6424, 'conditionalfreqdist()\\ntakes': 6425, \".words(categories=genre))\\n\\n\\n\\nlet's\": 6426, 'down,': 6427, ',\\nproducing': 6428, 'genre_word': 6429, '[(genre,': 6430, \"'romance']\": 6431, '.words(categories=genre)]': 6432, 'len(genre_word)\\n170576\\n\\n\\n\\nso,': 6433, 'below,\\npairs': 6434, \"form\\n('news',\": 6435, \"form\\n('romance',\": 6436, \"genre_word[:4]\\n[('news',\": 6437, \"'grand')]\": 6438, '#': 6439, '[_start-genre]\\n>>>': 6440, \"genre_word[-4:]\\n[('romance',\": 6441, \"'afraid'),\": 6442, \"('romance',\": 6443, \"'not'),\": 6444, \"''),\": 6445, \".')]\": 6446, '[_end-genre]\\n\\n\\n\\nwe': 6447, 'conditionalfreqdist,': 6448, 'and\\nsave': 6449, 'usual,': 6450, '.conditionalfreqdist(genre_word)\\n>>>': 6451, '\\n<conditionalfreqdist': 6452, 'conditions>\\n>>>': 6453, \".conditions()\\n['news',\": 6454, \"[_conditions-cfd]\\n\\n\\n\\nlet's\": 6455, 'satisfy': 6456, 'just\\na': 6457, 'distribution:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 6458, \"print(cfd['news'])\\n<freqdist\": 6459, '14394': 6460, '100554': 6461, \"print(cfd['romance'])\\n<freqdist\": 6462, '8452': 6463, '70022': 6464, \"cfd['romance']\": 6465, \".most_common(20)\\n[(',',\": 6466, '3899),': 6467, '3736),': 6468, '2758),': 6469, '1776),': 6470, \"1502),\\n('a',\": 6471, '1335),': 6472, '1186),': 6473, \"('``',\": 6474, '1045),': 6475, '1044),': 6476, \"993),\\n('i',\": 6477, '951),': 6478, '875),': 6479, '702),': 6480, '692),': 6481, \"690),\\n('her',\": 6482, '651),': 6483, '583),': 6484, '573),': 6485, '559),': 6486, \"('she',\": 6487, '496)]\\n>>>': 6488, \"cfd['romance']['could']\\n193\\n\\n\\n\\n\\n\\n2\": 6489, '.3\\xa0\\xa0\\xa0plotting': 6490, 'distributions\\napart': 6491, 'initialize,\\na': 6492, 'tabulation': 6493, 'distribution\\nreproduced': 6494, ',\\nand': 6495, 'occured': 6496, 'exploits': 6497, 'speech,': 6498, '1865-lincoln': 6499, '.txt\\ncontains': 6500, 'generates': 6501, \"('america',\": 6502, \"'1865')\": 6503, 'for\\nevery': 6504, 'instance': 6505, 'lowercased': 6506, 'america\\n—': 6507, 'americans': 6508, 'fileid[:4])': 6509, \"'citizen']\": 6510, '.startswith(target))\\n\\n\\n\\nthe': 6511, 'distribution,\\nreproduced': 6512, 'language\\nand': 6513, 'derived': 6514, \"'-latin1'\": 6515, 'encoding)': 6516, 'len(word))': 6517, \"'-latin1'))\\n\\n\\n\\nin\": 6518, 'plot()': 6519, 'tabulate()': 6520, 'can\\noptionally': 6521, 'conditions=': 6522, 'limit': 6523, 'the\\nsamples': 6524, 'samples=': 6525, 'to\\nload': 6526, 'distribution,': 6527, 'then\\nto': 6528, 'selected': 6529, 'also\\ngives': 6530, 'tabulate': 6531, 'two\\nlanguages,': 6532, '10': 6533, 'interpret': 6534, 'cell': 6535, '1,638': 6536, 'the\\nenglish': 6537, \".tabulate(conditions=['english',\": 6538, \"'german_deutsch'],\\n\": 6539, 'samples=range(10),': 6540, 'cumulative=true)\\n': 6541, '0': 6542, '9\\n': 6543, '185': 6544, '525': 6545, '883': 6546, '997': 6547, '1166': 6548, '1283': 6549, '1440': 6550, '1558': 6551, '1638\\ngerman_deutsch': 6552, '171': 6553, '263': 6554, '614': 6555, '717': 6556, '894': 6557, '1013': 6558, '1110': 6559, '1213': 6560, '1275\\n\\n\\n\\n\\nnote\\nyour': 6561, 'turn:\\nworking': 6562, 'corpus,\\nfind': 6563, 'week': 6564, 'newsworthy,': 6565, 'romantic': 6566, '.\\ndefine': 6567, 'week,': 6568, \".\\n['monday',\": 6569, 'using\\ncfd': 6570, '.tabulate(samples=days)': 6571, \"parameter:\\nsamples=['monday',\": 6572, 'noticed': 6573, 'multi-line': 6574, 'been\\nusing': 6575, 'list\\ncomprehensions,': 6576, 'general,\\nwhen': 6577, 'function,\\nlike': 6578, 'set([w': 6579, 't]),': 6580, 'permitted': 6581, 'omit\\nthe': 6582, 'write:': 6583, 't)': 6584, '.\\n(see': 6585, 'generator': 6586, '.2\\nfor': 6587, '.)\\n\\n\\n2': 6588, '.4\\xa0\\xa0\\xa0generating': 6589, 'bigrams\\nwe': 6590, 'of\\nbigrams': 6591, '(word': 6592, 'pairs)': 6593, 'introducted': 6594, 'in\\n3': 6595, '.)\\nthe': 6596, 'bigrams()': 6597, 'of\\nwords': 6598, 'builds': 6599, 'consecutive': 6600, 'that,': 6601, 'cryptic\\ngenerator': 6602, 'function:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 6603, \"['in',\": 6604, \"'heaven',\\n\": 6605, 'list(nltk': 6606, \".bigrams(sent))\\n[('in',\": 6607, \"'beginning'),\": 6608, \"('beginning',\": 6609, \"'god'),\": 6610, \"('god',\": 6611, \"'created'),\\n('created',\": 6612, \"'heaven'),\": 6613, \"('heaven',\": 6614, \"'and'),\": 6615, \"'the'),\\n('the',\": 6616, \"'earth'),\": 6617, \"('earth',\": 6618, \".')]\\n\\n\\n\\nin\": 6619, '.2,': 6620, 'condition,': 6621, 'one\\nwe': 6622, 'following\\nwords': 6623, 'generate_model()': 6624, 'to\\ngenerate': 6625, '(such': 6626, \"as\\n'living')\": 6627, 'loop,': 6628, 'we\\nprint': 6629, 'reset': 6630, 'likely': 6631, 'max());': 6632, 'next\\ntime': 6633, 'inspecting': 6634, 'approach': 6635, 'text\\ngeneration': 6636, 'tends': 6637, 'stuck': 6638, 'loops;': 6639, 'to\\nrandomly': 6640, 'among': 6641, '.\\n\\n\\n\\n\\n\\xa0\\n\\ndef': 6642, 'generate_model(cfdist,': 6643, 'num=15):\\n': 6644, 'range(num):\\n': 6645, 'cfdist[word]': 6646, '.max()\\n\\ntext': 6647, '.genesis': 6648, \".words('english-kjv\": 6649, \".txt')\\nbigrams\": 6650, '.bigrams(text)\\ncfd': 6651, '.conditionalfreqdist(bigrams)': 6652, '\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 6653, \"cfd['living']\\nfreqdist({'creature':\": 6654, \"'thing':\": 6655, \"'substance':\": 6656, \"',':\": 6657, \".':\": 6658, \"'soul':\": 6659, '1})\\n>>>': 6660, 'generate_model(cfd,': 6661, \"'living')\\nliving\": 6662, 'creature': 6663, 'said': 6664, 'land\\n\\n\\nexample': 6665, '(code_random_text': 6666, '.py):': 6667, 'figure': 6668, 'obtains': 6669, 'bigrams\\nfrom': 6670, 'genesis,': 6671, 'record': 6672, 'which\\nwords': 6673, 'word;': 6674, 'after\\nthe': 6675, 'living,': 6676, 'is\\ncreature;': 6677, 'this\\ndata,': 6678, 'seed': 6679, '.\\n\\nconditional': 6680, '.\\ntheir': 6681, 'commonly-used': 6682, 'summarized': 6683, '.\\n\\n\\n\\n\\n\\n\\nexample\\ndescription\\n\\n\\n\\ncfdist': 6684, 'conditionalfreqdist(pairs)\\ncreate': 6685, 'pairs\\n\\ncfdist': 6686, '.conditions()\\nthe': 6687, 'conditions\\n\\ncfdist[condition]\\nthe': 6688, 'condition\\n\\ncfdist[condition][sample]\\nfrequency': 6689, 'condition\\n\\ncfdist': 6690, 'distribution\\n\\ncfdist': 6691, '.tabulate(samples,': 6692, 'conditions)\\ntabulation': 6693, 'conditions\\n\\ncfdist': 6694, '.plot(samples,': 6695, 'conditions)\\ngraphical': 6696, 'conditions\\n\\ncfdist1': 6697, 'cfdist2\\ntest': 6698, 'cfdist1': 6699, 'cfdist2\\n\\n\\ntable': 6700, 'distributions:': 6701, 'defining,\\naccessing,': 6702, 'counters': 6703, '.\\n\\n\\n\\n\\n\\n3\\xa0\\xa0\\xa0more': 6704, 'reusing': 6705, 'code\\nby': 6706, 'retyped': 6707, 'lot': 6708, 'mess': 6709, 'arrow': 6710, 'keys': 6711, 'so\\nfar': 6712, 'reuse': 6713, 'code:': 6714, 'editors': 6715, '.\\n\\n3': 6716, '.1\\xa0\\xa0\\xa0creating': 6717, 'editor\\nthe': 6718, 'type\\nthem': 6719, 'often,': 6720, 'compose': 6721, 'editor,\\nthen': 6722, 'idle,': 6723, 'do\\nthis': 6724, 'menu': 6725, 'window': 6726, 'and\\nenter': 6727, 'one-line': 6728, \"program:\\n\\nprint('monty\": 6729, \"python')\\n\\nsave\": 6730, '.py,': 6731, 'then\\ngo': 6732, 'menu,': 6733, \".\\n(we'll\": 6734, 'shortly': 6735, 'idle': 6736, 'this:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 6737, '================================': 6738, 'restart': 6739, '================================\\n>>>\\nmonty': 6740, 'python\\n>>>\\n\\n\\n\\nyou': 6741, '.\\nfrom': 6742, 'on,': 6743, 'a\\ntext': 6744, 'ideas\\nusing': 6745, 'revising': 6746, 'ready,': 6747, 'paste': 6748, 'code\\n(minus': 6749, 'prompts)': 6750, 'editor,\\ncontinue': 6751, 'program\\nin': 6752, '.\\ngive': 6753, 'descriptive': 6754, 'separating\\nwords': 6755, 'extension,': 6756, 'monty_python': 6757, '.\\n\\nnote\\nimportant:\\nour': 6758, 'inline': 6759, 'prompts\\nas': 6760, 'interacting': 6761, 'complicated,\\nyou': 6762, 'editor,': 6763, 'prompts,': 6764, 'them\\nfrom': 6765, 'book,\\nwe': 6766, 'prompts': 6767, 'rather\\nthan': 6768, 'couple': 6769, 'prompt;\\nthis': 6770, 'downloadable\\nfrom': 6771, '.\\n\\n\\n\\n3': 6772, '.2\\xa0\\xa0\\xa0functions\\nsuppose': 6773, 'forms\\nof': 6774, 'out\\nthe': 6775, 'plural': 6776, 'singular': 6777, 'this\\nwork': 6778, 'places,': 6779, 'again\\nwhen': 6780, '.\\nrather': 6781, 'over,': 6782, 'more\\nefficient': 6783, 'reliable': 6784, 'localize': 6785, 'well-defined\\ntask,': 6786, 'inputs,': 6787, 'parameters,\\nand': 6788, 'result,': 6789, '1\\n(including': 6790, 'needed': 6791, 'behave': 6792, 'expected):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 6793, 'division\\n>>>': 6794, 'lexical_diversity(text):\\n': 6795, 'len(set(text))\\n\\n\\n\\nwe': 6796, 'is\\nproduced': 6797, 'example,\\nall': 6798, \".\\nhere's\": 6799, 'equivalent': 6800, 'work\\nusing': 6801, 'name\\nfrom': 6802, 'my_text_data': 6803, 'arbitrary': 6804, 'choice:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 6805, 'lexical_diversity(my_text_data):\\n': 6806, 'word_count': 6807, 'len(my_text_data)\\n': 6808, 'len(set(my_text_data))\\n': 6809, 'diversity_score': 6810, 'word_count\\n': 6811, 'diversity_score\\n\\n\\n\\nnotice': 6812, 'lexical_diversity': 6813, 'just\\ndefining': 6814, \"won't\": 6815, 'output!\\nfunctions': 6816, 'invoked):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 6817, 'genesis\\n>>>': 6818, 'kjv': 6819, 'lexical_diversity(kjv)\\n0': 6820, \".06230453042623537\\n\\n\\n\\nlet's\": 6821, 'scenario,': 6822, 'simple\\nfunction': 6823, 'plurals': 6824, 'plural()': 6825, '.1\\ntakes': 6826, 'form,': 6827, 'always\\ncorrect': 6828, \"(we'll\": 6829, '.)\\n\\n\\n\\n\\n\\xa0\\n\\ndef': 6830, 'plural(word):\\n': 6831, \".endswith('y'):\\n\": 6832, 'word[:-1]': 6833, \"'ies'\\n\": 6834, 'word[-1]': 6835, \"'sx'\": 6836, 'word[-2:]': 6837, \"['sh',\": 6838, \"'ch']:\\n\": 6839, \"'es'\\n\": 6840, \".endswith('an'):\\n\": 6841, 'word[:-2]': 6842, \"'en'\\n\": 6843, \"'s'\\n\\n\\n\\n\\n\\xa0\\n\\n>>>\": 6844, \"plural('fairy')\\n'fairies'\\n>>>\": 6845, \"plural('woman')\\n'women'\\n\\n\\nexample\": 6846, '(code_plural': 6847, 'function:': 6848, 'tries': 6849, 'the\\nplural': 6850, 'noun;': 6851, '(define)\\nis': 6852, 'inside\\nparentheses,': 6853, 'colon;': 6854, 'code;': 6855, 'patterns\\nwithin': 6856, 'accordingly;': 6857, 'y,': 6858, 'delete': 6859, 'ies': 6860, 'endswith()': 6861, 'object\\n(e': 6862, '.1)': 6863, 'give\\nthe': 6864, '.3\\xa0\\xa0\\xa0modules\\nover': 6865, 'functions,\\nand': 6866, 'the\\nlatest': 6867, 'use?\\nit': 6868, 'collect': 6869, 'and\\naccess': 6870, 'previously': 6871, 'copies': 6872, 'function(s)': 6873, '(say)': 6874, 'text_proc': 6875, 'importing': 6876, 'file:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 6877, 'plural\\n>>>': 6878, \"plural('wish')\\nwishes\\n>>>\": 6879, \"plural('fan')\\nfen\\n\\n\\n\\nour\": 6880, 'obviously': 6881, 'of\\nfan': 6882, 'fans': 6883, '.\\ninstead': 6884, 'can\\nsimply': 6885, 'edit': 6886, 'existing': 6887, 'every\\nstage,': 6888, 'confusion': 6889, 'about\\nwhich': 6890, 'definitions': 6891, 'python\\nmodule': 6892, \".\\nnltk's\": 6893, 'module,\\nand': 6894, 'is\\nan': 6895, 'sometimes\\ncalled': 6896, '.\\n\\ncaution!\\nif': 6897, 'python\\ncode,': 6898, '.py:': 6899, 'imported': 6900, 'in\\nplace': 6901, 'imports': 6902, 'modules,': 6903, 'python\\nfirst': 6904, 'looks': 6905, 'directory': 6906, '(folder)': 6907, '.\\n\\n\\n\\n\\n4\\xa0\\xa0\\xa0lexical': 6908, 'resources\\na': 6909, 'lexicon,': 6910, 'resource,': 6911, 'and/or': 6912, 'along\\nwith': 6913, '.\\nlexical': 6914, 'secondary': 6915, 'enriched': 6916, 'help\\nof': 6917, 'my_text,': 6918, 'then\\nvocab': 6919, 'sorted(set(my_text))': 6920, 'my_text,\\nwhile': 6921, 'word_freq': 6922, 'freqdist(my_text)': 6923, 'both\\nof': 6924, 'concordance\\nlike': 6925, '1\\ngives': 6926, 'preparation': 6927, 'dictionary': 6928, 'terminology': 6929, 'lexicons': 6930, 'entry': 6931, 'consists': 6932, 'headword': 6933, '(also': 6934, 'lemma)\\nalong': 6935, 'sense\\ndefinition': 6936, 'homonyms': 6937, 'lexicon': 6938, 'terminology:': 6939, 'entries': 6940, 'lemmas\\nhaving': 6941, '(homonyms),': 6942, 'speech\\nand': 6943, 'gloss': 6944, '.\\nsophisticated': 6945, 'across\\nthe': 6946, 'resources\\nincluded': 6947, '.1\\xa0\\xa0\\xa0wordlist': 6948, 'corpora\\n\\nnltk': 6949, '/usr/share/dict/words': 6950, 'unix,': 6951, 'by\\nsome': 6952, 'spell': 6953, 'checkers': 6954, 'mis-spelt\\nwords': 6955, 'unusual_words(text):\\n': 6956, 'text_vocab': 6957, '.isalpha())\\n': 6958, 'english_vocab': 6959, '.words': 6960, '.words())\\n': 6961, 'english_vocab\\n': 6962, 'sorted(unusual)\\n\\n>>>': 6963, 'unusual_words(nltk': 6964, \".words('austen-sense\": 6965, \".txt'))\\n['abbeyland',\": 6966, \"'abhorred',\": 6967, \"'abilities',\": 6968, \"'abounded',\": 6969, \"'abridgement',\": 6970, \"'abused',\": 6971, \"'abuses',\\n'accents',\": 6972, \"'accepting',\": 6973, \"'accommodations',\": 6974, \"'accompanied',\": 6975, \"'accounted',\": 6976, \"'accounts',\\n'accustomary',\": 6977, \"'aches',\": 6978, \"'acknowledging',\": 6979, \"'acknowledgment',\": 6980, \"'acknowledgments',\": 6981, '.nps_chat': 6982, \".words())\\n['aaaaaaaaaaaaaaaaa',\": 6983, \"'aaahhhh',\": 6984, \"'abortions',\": 6985, \"'abou',\": 6986, \"'abourted',\": 6987, \"'abs',\": 6988, \"'ack',\\n'acros',\": 6989, \"'actualy',\": 6990, \"'adams',\": 6991, \"'adds',\": 6992, \"'adduser',\": 6993, \"'adjusts',\": 6994, \"'adoted',\": 6995, \"'adreniline',\\n'ads',\": 6996, \"'adults',\": 6997, \"'afe',\": 6998, \"'affairs',\": 6999, \"'affari',\": 7000, \"'affects',\": 7001, \"'afk',\": 7002, \"'agaibn',\": 7003, \"'ages',\": 7004, '.]\\n\\n\\nexample': 7005, '(code_unusual': 7006, 'computes': 7007, 'text,\\nthen': 7008, 'removes': 7009, 'wordlist,\\nleaving': 7010, 'mis-spelt': 7011, '.\\n\\nthere': 7012, 'stopwords,': 7013, 'high-frequency\\nwords': 7014, 'the,': 7015, 'sometimes\\nwant': 7016, 'filter': 7017, 'stopwords\\nusually': 7018, 'content,': 7019, 'presence': 7020, 'fails\\nto': 7021, 'stopwords\\n>>>': 7022, \".words('english')\\n['i',\": 7023, \"'my',\": 7024, \"'myself',\": 7025, \"'we',\": 7026, \"'ours',\": 7027, \"'ourselves',\": 7028, \"'your',\": 7029, \"'yours',\\n'yourself',\": 7030, \"'yourselves',\": 7031, \"'he',\": 7032, \"'him',\": 7033, \"'himself',\": 7034, \"'she',\": 7035, \"'her',\": 7036, \"'hers',\\n'herself',\": 7037, \"'its',\": 7038, \"'itself',\": 7039, \"'them',\": 7040, \"'theirs',\": 7041, \"'themselves',\\n'what',\": 7042, \"'which',\": 7043, \"'who',\": 7044, \"'whom',\": 7045, \"'this',\": 7046, \"'these',\": 7047, \"'those',\": 7048, \"'am',\": 7049, \"'are',\\n'was',\": 7050, \"'were',\": 7051, \"'been',\": 7052, \"'being',\": 7053, \"'has',\": 7054, \"'having',\": 7055, \"'does',\\n'did',\": 7056, \"'doing',\": 7057, \"'but',\": 7058, \"'if',\": 7059, \"'because',\": 7060, \"'until',\\n'while',\": 7061, \"'at',\": 7062, \"'with',\": 7063, \"'about',\": 7064, \"'against',\": 7065, \"'into',\\n'through',\": 7066, \"'during',\": 7067, \"'before',\": 7068, \"'after',\": 7069, \"'above',\": 7070, \"'below',\": 7071, \"'up',\": 7072, \"'down',\\n'in',\": 7073, \"'out',\": 7074, \"'off',\": 7075, \"'over',\": 7076, \"'under',\": 7077, \"'again',\": 7078, \"'further',\": 7079, \"'then',\": 7080, \"'once',\": 7081, \"'here',\\n'there',\": 7082, \"'when',\": 7083, \"'where',\": 7084, \"'how',\": 7085, \"'any',\": 7086, \"'both',\": 7087, \"'each',\": 7088, \"'few',\": 7089, \"'more',\\n'most',\": 7090, \"'some',\": 7091, \"'such',\": 7092, \"'no',\": 7093, \"'nor',\": 7094, \"'only',\": 7095, \"'own',\": 7096, \"'same',\": 7097, \"'so',\\n'than',\": 7098, \"'too',\": 7099, \"'very',\": 7100, \"'s',\": 7101, \"'t',\": 7102, \"'just',\": 7103, \"'don',\": 7104, \"'should',\": 7105, \"'now']\\n\\n\\n\\nlet's\": 7106, 'the\\nstopwords': 7107, 'content_fraction(text):\\n': 7108, '.stopwords': 7109, \".words('english')\\n\": 7110, 'stopwords]\\n': 7111, 'len(content)': 7112, 'len(text)\\n': 7113, 'content_fraction(nltk': 7114, '.reuters': 7115, '.words())\\n0': 7116, '.7364374824583169\\n\\n\\n\\nthus,': 7117, 'lexical\\nresource': 7118, 'puzzle:': 7119, 'grid': 7120, 'randomly': 7121, 'chosen': 7122, 'for\\ncreating': 7123, 'letters;': 7124, 'puzzle': 7125, 'wordlist': 7126, 'solving': 7127, 'puzzles,': 7128, '.\\nour': 7129, 'iterates': 7130, 'and,': 7131, 'meets\\nthe': 7132, 'obligatory': 7133, '\\nand': 7134, 'constraints': 7135, '(and': 7136, \"we'll\\nonly\": 7137, 'six': 7138, 'here)': 7139, 'trickier': 7140, 'candidate': 7141, 'solutions': 7142, 'the\\nsupplied': 7143, 'especially': 7144, 'letters\\nappear': 7145, 'twice': 7146, '(here,': 7147, 'v)': 7148, 'equal\\nto': 7149, 'puzzle_letters': 7150, \".freqdist('egivrvonl')\\n>>>\": 7151, \"'r'\\n>>>\": 7152, '.words()\\n>>>': 7153, '.freqdist(w)': 7154, 'puzzle_letters]': 7155, \"\\n['glover',\": 7156, \"'gorlin',\": 7157, \"'govern',\": 7158, \"'grovel',\": 7159, \"'ignore',\": 7160, \"'involver',\": 7161, \"'lienor',\\n'linger',\": 7162, \"'longer',\": 7163, \"'lovering',\": 7164, \"'noiler',\": 7165, \"'overling',\": 7166, \"'region',\": 7167, \"'renvoi',\\n'revolving',\": 7168, \"'ringle',\": 7169, \"'roving',\": 7170, \"'violer',\": 7171, \"'virole']\\n\\n\\n\\none\": 7172, '8,000': 7173, 'appear\\nin': 7174, 'ambiguous': 7175, 'gender:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 7176, '.names\\n>>>': 7177, \".fileids()\\n['female\": 7178, \"'male\": 7179, \".txt']\\n>>>\": 7180, 'male_names': 7181, \".words('male\": 7182, 'female_names': 7183, \".words('female\": 7184, \"female_names]\\n['abbey',\": 7185, \"'abbie',\": 7186, \"'abby',\": 7187, \"'addie',\": 7188, \"'adrian',\": 7189, \"'adrien',\": 7190, \"'ajay',\": 7191, \"'alex',\": 7192, \"'alexis',\\n'alfie',\": 7193, \"'ali',\": 7194, \"'alix',\": 7195, \"'allie',\": 7196, \"'allyn',\": 7197, \"'andie',\": 7198, \"'andrea',\": 7199, \"'andy',\": 7200, \"'angel',\\n'angie',\": 7201, \"'ariel',\": 7202, \"'ashley',\": 7203, \"'aubrey',\": 7204, \"'augustine',\": 7205, \"'austin',\": 7206, \"'averil',\": 7207, '.]\\n\\n\\n\\nit': 7208, 'almost': 7209, '.4,\\nproduced': 7210, 'name[-1]': 7211, 'letter\\nof': 7212, '(fileid,': 7213, 'name[-1])\\n': 7214, '.words(fileid))\\n>>>': 7215, '.4:': 7216, 'names\\nending': 7217, 'alphabet;': 7218, 'a,': 7219, 'i\\nare': 7220, 'female;': 7221, 'h': 7222, 'equally': 7223, 'female;\\nnames': 7224, 'k,': 7225, 'o,': 7226, 'r,': 7227, 's,': 7228, '.\\n\\n\\n\\n4': 7229, '.2\\xa0\\xa0\\xa0a': 7230, 'dictionary\\na': 7231, 'richer': 7232, 'spreadsheet),': 7233, 'word\\nplus': 7234, 'cmu': 7235, 'pronouncing\\ndictionary': 7236, 'for\\nuse': 7237, 'synthesizers': 7238, '.cmudict': 7239, '.entries()\\n>>>': 7240, 'len(entries)\\n133737\\n>>>': 7241, 'entries[42371:42379]:\\n': 7242, 'print(entry)\\n': 7243, \".\\n('fir',\": 7244, \"['f',\": 7245, \"'er1'])\\n('fire',\": 7246, \"'ay1',\": 7247, \"'er0'])\\n('fire',\": 7248, \"'r'])\\n('firearm',\": 7249, \"'er0',\": 7250, \"'aa2',\": 7251, \"'r',\": 7252, \"'m'])\\n('firearm',\": 7253, \"'m'])\\n('firearms',\": 7254, \"'m',\": 7255, \"'z'])\\n('firearms',\": 7256, \"'z'])\\n('fireball',\": 7257, \"'b',\": 7258, \"'ao2',\": 7259, \"'l'])\\n\\n\\n\\nfor\": 7260, 'phonetic\\ncodes': 7261, 'labels': 7262, 'contrastive': 7263, 'sound': 7264, '—\\nknown': 7265, 'fire': 7266, 'pronunciations\\n(in': 7267, 'english):\\nthe': 7268, 'one-syllable': 7269, 'f': 7270, 'ay1': 7271, 'two-syllable': 7272, 'er0': 7273, 'arpabet,\\ndescribed': 7274, 'detail': 7275, 'http://en': 7276, '.wikipedia': 7277, '.org/wiki/arpabet\\n\\neach': 7278, 'parts,': 7279, 'can\\nprocess': 7280, 'individually': 7281, 'entries:,': 7282, 'replace\\nentry': 7283, 'pron': 7284, 'the\\nentry,': 7285, 'entry:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 7286, 'entries:': 7287, 'len(pron)': 7288, '3:': 7289, 'ph1,': 7290, 'ph2,': 7291, 'ph3': 7292, 'ph1': 7293, \"'p'\": 7294, \"'t':\\n\": 7295, '.\\npait': 7296, 'ey1': 7297, 'pat': 7298, 'ae1': 7299, 'pate': 7300, 'patt': 7301, 'peart': 7302, 'er1': 7303, 'peat': 7304, 'iy1': 7305, 'peet': 7306, 'peete': 7307, 'pert': 7308, 'er1\\npet': 7309, 'eh1': 7310, 'pete': 7311, 'pett': 7312, 'piet': 7313, 'piette': 7314, 'pit': 7315, 'ih1': 7316, 'pitt': 7317, 'pot': 7318, 'aa1': 7319, 'pote': 7320, 'ow1\\npott': 7321, 'pout': 7322, 'aw1': 7323, 'puett': 7324, 'uw1': 7325, 'purt': 7326, 'uh1': 7327, 'putt': 7328, 'ah1\\n\\n\\n\\nthe': 7329, 'scans': 7330, 'looking': 7331, 'pronunciation': 7332, 'of\\nthree': 7333, 'true,': 7334, 'assigns': 7335, 'contents\\nof': 7336, 'ph2': 7337, 'unusual\\nform': 7338, 'list\\ncomprehension': 7339, 'finds': 7340, 'syllable\\nsounding': 7341, 'nicks': 7342, 'rhyming': 7343, 'syllable': 7344, \"['n',\": 7345, \"'ih0',\": 7346, \"'k',\": 7347, \"'s']\\n>>>\": 7348, 'pron[-4:]': 7349, \"syllable]\\n[atlantic's,\": 7350, \"'audiotronics',\": 7351, \"'avionics',\": 7352, \"'beatniks',\": 7353, \"'calisthenics',\": 7354, \"'centronics',\\n'chamonix',\": 7355, \"'chetniks',\": 7356, \"clinic's,\": 7357, \"'clinics',\": 7358, \"'conics',\": 7359, \"'cryogenics',\\n'cynics',\": 7360, \"'diasonics',\": 7361, \"dominic's,\": 7362, \"'ebonics',\": 7363, \"'electronics',\": 7364, \"electronics',\": 7365, 'spelt': 7366, 'ways:': 7367, 'nics,': 7368, 'niks,': 7369, 'nix,\\neven': 7370, \"ntic's\": 7371, 'silent': 7372, 't,': 7373, \"atlantic's\": 7374, 'other\\nmismatches': 7375, 'summarize': 7376, 'work?\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 7377, 'w,': 7378, 'pron[-1]': 7379, \"'m'\": 7380, 'w[-1]': 7381, \"'n']\\n['autumn',\": 7382, \"'column',\": 7383, \"'condemn',\": 7384, \"'damn',\": 7385, \"'goddamn',\": 7386, \"'hymn',\": 7387, \"'solemn']\\n>>>\": 7388, 'sorted(set(w[:2]': 7389, 'pron[0]': 7390, \"'n'\": 7391, 'w[0]': 7392, \"'n'))\\n['gn',\": 7393, \"'kn',\": 7394, \"'mn',\": 7395, \"'pn']\\n\\n\\n\\nthe\": 7396, 'represent\\nprimary': 7397, 'stress': 7398, '(1),': 7399, '(2)': 7400, '(0)': 7401, 'digits\\nand': 7402, 'scan': 7403, 'stress(pron):\\n': 7404, '[char': 7405, 'phone': 7406, 'char': 7407, '.isdigit()]\\n>>>': 7408, 'stress(pron)': 7409, \"['0',\": 7410, \"'1',\": 7411, \"'0',\": 7412, \"'2',\": 7413, \"'0']]\\n['abbreviated',\": 7414, \"'abbreviated',\": 7415, \"'abbreviating',\": 7416, \"'accelerated',\": 7417, \"'accelerating',\\n'accelerator',\": 7418, \"'accelerators',\": 7419, \"'accentuated',\": 7420, \"'accentuating',\": 7421, \"'accommodated',\\n'accommodating',\": 7422, \"'accommodative',\": 7423, \"'accumulated',\": 7424, \"'accumulating',\": 7425, \"'accumulative',\": 7426, \"'0']]\\n['abbreviation',\": 7427, \"'abbreviations',\": 7428, \"'abomination',\": 7429, \"'abortifacient',\": 7430, \"'abortifacients',\\n'academicians',\": 7431, \"'accommodation',\": 7432, \"'accreditation',\": 7433, \"'accreditations',\\n'accumulation',\": 7434, \"'accumulations',\": 7435, \"'acetylcholine',\": 7436, \"'adjudication',\": 7437, '.]\\n\\n\\n\\n\\nnote\\na': 7438, 'subtlety': 7439, 'our\\nuser-defined': 7440, 'stress()': 7441, 'doubly-nested': 7442, \".\\nthere's\": 7443, 'minimally-contrasting\\nsets': 7444, 'p-words': 7445, 'p3': 7446, \"[(pron[0]+'-'+pron[2],\": 7447, '(word,': 7448, 'pron)': 7449, 'entries\\n': 7450, '3]': 7451, '.conditionalfreqdist(p3)\\n>>>': 7452, 'template': 7453, 'sorted(cfd': 7454, '.conditions()):\\n': 7455, 'len(cfd[template])': 7456, '10:\\n': 7457, 'sorted(cfd[template])\\n': 7458, 'wordstring': 7459, '.join(words)\\n': 7460, 'print(template,': 7461, 'wordstring[:70]': 7462, '.)\\n': 7463, '.\\np-ch': 7464, 'patch': 7465, 'pautsch': 7466, 'peach': 7467, 'perch': 7468, 'petsch': 7469, 'petsche': 7470, 'piche': 7471, 'piech': 7472, 'pietsch': 7473, 'pitch': 7474, '.\\np-k': 7475, 'pac': 7476, 'pack': 7477, 'paek': 7478, 'paik': 7479, 'pak': 7480, 'pake': 7481, 'paque': 7482, 'peak': 7483, 'peake': 7484, 'pech': 7485, 'peck': 7486, 'peek': 7487, 'perc': 7488, 'perk': 7489, '.\\np-l': 7490, 'pahl': 7491, 'pail': 7492, 'paille': 7493, 'pal': 7494, 'pale': 7495, 'pall': 7496, 'paul': 7497, 'paule': 7498, 'paull': 7499, 'peal': 7500, 'peale': 7501, 'pearl': 7502, '.\\np-n': 7503, 'paign': 7504, 'pain': 7505, 'paine': 7506, 'pan': 7507, 'pane': 7508, 'pawn': 7509, 'payne': 7510, 'peine': 7511, 'pen': 7512, 'penh': 7513, 'pin': 7514, 'pine': 7515, 'pinn': 7516, '.\\np-p': 7517, 'paap': 7518, 'paape': 7519, 'pap': 7520, 'pape': 7521, 'papp': 7522, 'paup': 7523, 'peep': 7524, 'pep': 7525, 'pip': 7526, 'pipe': 7527, 'pipp': 7528, 'poop': 7529, 'pop': 7530, 'pope': 7531, '.\\np-r': 7532, 'paar': 7533, 'par': 7534, 'pare': 7535, 'parr': 7536, 'pear': 7537, 'peer': 7538, 'pier': 7539, 'poor': 7540, 'poore': 7541, 'por': 7542, 'pore': 7543, 'porr': 7544, 'pour': 7545, '.\\np-s': 7546, 'pasts': 7547, 'peace': 7548, 'pearse': 7549, 'pease': 7550, 'perce': 7551, 'pers': 7552, 'perse': 7553, 'pesce': 7554, 'piss': 7555, '.\\np-t': 7556, 'pait': 7557, 'pet': 7558, 'piett': 7559, '.\\np-uw1': 7560, 'peru': 7561, 'peugh': 7562, 'pew': 7563, 'plew': 7564, 'plue': 7565, 'prew': 7566, 'pru': 7567, 'prue': 7568, 'prugh': 7569, 'pshew': 7570, 'pugh': 7571, '.\\n\\n\\n\\nrather': 7572, 'iterating': 7573, 'it\\nby': 7574, 'data\\nstructure,': 7575, 'key\\n(such': 7576, \"'fire')\": 7577, 'prondict': 7578, '.dict()\\n>>>': 7579, \"prondict['fire']\": 7580, \"\\n[['f',\": 7581, \"'er0'],\": 7582, \"'r']]\\n>>>\": 7583, \"prondict['blog']\": 7584, '<module>\\nkeyerror:': 7585, \"'blog'\\n>>>\": 7586, \"[['b',\": 7587, \"'l',\": 7588, \"'aa1',\": 7589, \"'g']]\": 7590, \"prondict['blog']\\n[['b',\": 7591, \"'g']]\\n\\n\\n\\nif\": 7592, 'non-existent': 7593, 'keyerror': 7594, 'an\\ninteger': 7595, 'producing': 7596, 'indexerror': 7597, 'missing': 7598, 'dictionary,\\nso': 7599, 'tweak': 7600, '\\n(this': 7601, 'effect': 7602, 'corpus;': 7603, 'it,\\nblog': 7604, 'absent)': 7605, 'having\\nsome': 7606, '(like': 7607, 'nouns),': 7608, 'mapping': 7609, 'text-to-speech': 7610, 'word\\nof': 7611, \"['natural',\": 7612, \"'language',\": 7613, \"'processing']\\n>>>\": 7614, '[ph': 7615, 'ph': 7616, \"prondict[w][0]]\\n['n',\": 7617, \"'ae1',\": 7618, \"'ch',\": 7619, \"'ah0',\": 7620, \"'ng',\": 7621, \"'g',\": 7622, \"'jh',\\n'p',\": 7623, \"'eh0',\": 7624, \"'ng']\\n\\n\\n\\n\\n\\n\\n4\": 7625, '.3\\xa0\\xa0\\xa0comparative': 7626, 'wordlists\\nanother': 7627, 'tabular': 7628, 'comparative': 7629, 'swadesh': 7630, 'wordlists,': 7631, '200': 7632, 'identified': 7633, 'iso': 7634, '639': 7635, 'two-letter': 7636, 'swadesh\\n>>>': 7637, \".fileids()\\n['be',\": 7638, \"'bg',\": 7639, \"'bs',\": 7640, \"'ca',\": 7641, \"'cs',\": 7642, \"'cu',\": 7643, \"'de',\": 7644, \"'en',\": 7645, \"'es',\": 7646, \"'fr',\": 7647, \"'hr',\": 7648, \"'la',\": 7649, \"'mk',\\n'nl',\": 7650, \"'pl',\": 7651, \"'pt',\": 7652, \"'ro',\": 7653, \"'ru',\": 7654, \"'sk',\": 7655, \"'sl',\": 7656, \"'sr',\": 7657, \"'sw',\": 7658, \"'uk']\\n>>>\": 7659, \".words('en')\\n['i',\": 7660, \"'you\": 7661, '(singular),': 7662, \"thou',\": 7663, \"(plural)',\": 7664, \"'that',\\n'here',\": 7665, \"'there',\": 7666, \"'what',\": 7667, \"'many',\": 7668, \"'some',\\n'few',\": 7669, \"'one',\": 7670, \"'three',\": 7671, \"'four',\": 7672, \"'five',\": 7673, \"'big',\": 7674, \"'long',\": 7675, \"'wide',\": 7676, '.]\\n\\n\\n\\nwe': 7677, 'cognate': 7678, 'entries()': 7679, 'method,\\nspecifying': 7680, 'into\\na': 7681, 'dict()': 7682, '3)': 7683, 'fr2en': 7684, \".entries(['fr',\": 7685, \"'en'])\\n>>>\": 7686, \"fr2en\\n[('je',\": 7687, \"'i'),\": 7688, \"('tu,\": 7689, \"vous',\": 7690, \"thou'),\": 7691, \"('il',\": 7692, \"'he'),\": 7693, 'dict(fr2en)\\n>>>': 7694, \"translate['chien']\\n'dog'\\n>>>\": 7695, \"translate['jeter']\\n'throw'\\n\\n\\n\\nwe\": 7696, 'translator': 7697, 'source': 7698, 'german-english': 7699, 'spanish-english': 7700, 'a\\ndictionary': 7701, 'dict(),': 7702, 'update': 7703, 'dictionary\\nwith': 7704, 'mappings:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 7705, 'de2en': 7706, \".entries(['de',\": 7707, \"'en'])\": 7708, 'german-english\\n>>>': 7709, 'es2en': 7710, \".entries(['es',\": 7711, 'spanish-english\\n>>>': 7712, '.update(dict(de2en))\\n>>>': 7713, '.update(dict(es2en))\\n>>>': 7714, \"translate['hund']\\n'dog'\\n>>>\": 7715, \"translate['perro']\\n'dog'\\n\\n\\n\\nwe\": 7716, 'germanic': 7717, 'languages:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 7718, \"['en',\": 7719, \"'nl',\": 7720, \"'la']\\n>>>\": 7721, '[139,': 7722, '140,': 7723, '141,': 7724, '142]:\\n': 7725, 'print(swadesh': 7726, '.entries(languages)[i])\\n': 7727, \".\\n('say',\": 7728, \"'sagen',\": 7729, \"'zeggen',\": 7730, \"'decir',\": 7731, \"'dire',\": 7732, \"'dizer',\": 7733, \"'dicere')\\n('sing',\": 7734, \"'singen',\": 7735, \"'zingen',\": 7736, \"'cantar',\": 7737, \"'chanter',\": 7738, \"'canere')\\n('play',\": 7739, \"'spielen',\": 7740, \"'spelen',\": 7741, \"'jugar',\": 7742, \"'jouer',\": 7743, \"'jogar,\": 7744, \"brincar',\": 7745, \"'ludere')\\n('float',\": 7746, \"'schweben',\": 7747, \"'zweven',\": 7748, \"'flotar',\": 7749, \"'flotter',\": 7750, \"'flutuar,\": 7751, \"boiar',\": 7752, \"'fluctuare')\\n\\n\\n\\n\\n\\n4\": 7753, '.4\\xa0\\xa0\\xa0shoebox': 7754, 'lexicons\\nperhaps': 7755, 'tool': 7756, 'linguists': 7757, 'data\\nis': 7758, 'toolbox,': 7759, 'shoebox': 7760, 'replaces\\nthe': 7761, \"linguist's\": 7762, 'traditional': 7763, 'cards': 7764, '.\\ntoolbox': 7765, '.sil': 7766, '.org/computing/toolbox/': 7767, 'entries,\\nwhere': 7768, 'fields': 7769, '.\\nmost': 7770, 'optional': 7771, 'repeatable,': 7772, 'of\\nlexical': 7773, 'spreadsheet': 7774, 'rotokas': 7775, 'entry,\\nfor': 7776, 'kaa': 7777, 'gag:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 7778, 'toolbox\\n>>>': 7779, \".entries('rotokas\": 7780, \".dic')\\n[('kaa',\": 7781, \"[('ps',\": 7782, \"'v'),\": 7783, \"('pt',\": 7784, \"'a'),\": 7785, \"('ge',\": 7786, \"'gag'),\": 7787, \"('tkp',\": 7788, \"'nek\": 7789, \"pas'),\\n('dcsv',\": 7790, \"'true'),\": 7791, \"('vx',\": 7792, \"'1'),\": 7793, \"('sc',\": 7794, \"'???'),\": 7795, \"('dt',\": 7796, \"'29/oct/2005'),\\n('ex',\": 7797, \"'apoka\": 7798, 'ira': 7799, 'kaaroi': 7800, 'aioa-ia': 7801, 'reoreopaoro': 7802, \".'),\\n('xp',\": 7803, \"'kaikai\": 7804, 'pas': 7805, 'nek': 7806, 'bilong': 7807, 'apoka': 7808, 'bikos': 7809, 'em': 7810, 'kaikai': 7811, 'na': 7812, 'toktok': 7813, \".'),\\n('xe',\": 7814, 'gagging': 7815, \".')]),\": 7816, '.]\\n\\n\\n\\nentries': 7817, 'consist': 7818, 'attribute-value': 7819, \"('ps',\": 7820, \"'v')\\nto\": 7821, \"'v'\": 7822, '(verb),': 7823, \"'gag')\\nto\": 7824, 'gloss-into-english': 7825, \"'gag'\": 7826, 'contain\\nan': 7827, 'tok': 7828, 'pisin': 7829, 'loose': 7830, 'them\\nat': 7831, 'island': 7832, 'bougainville,': 7833, 'papua': 7834, 'guinea': 7835, 'contributed': 7836, 'stuart': 7837, 'robinson': 7838, '.\\nrotokas': 7839, 'notable': 7840, 'inventory': 7841, 'phonemes': 7842, '(contrastive': 7843, 'sounds),\\nhttp://en': 7844, '.org/wiki/rotokas_language\\n\\n\\n\\n\\n5\\xa0\\xa0\\xa0wordnet\\n\\nwordnet': 7845, 'semantically-oriented': 7846, 'english,\\nsimilar': 7847, 'thesaurus': 7848, 'wordnet,': 7849, '155,287': 7850, 'words\\nand': 7851, '117,659': 7852, 'by\\nlooking': 7853, 'synonyms': 7854, '.1\\xa0\\xa0\\xa0senses': 7855, 'synonyms\\n\\n\\nconsider': 7856, 'replace': 7857, 'motorcar': 7858, 'automobile,\\nto': 7859, '(1b),': 7860, 'stays': 7861, 'pretty': 7862, 'same:\\n\\n': 7863, '.benz': 7864, 'credited': 7865, 'invention': 7866, 'automobile': 7867, '.\\n\\nsince': 7868, 'remained': 7869, 'unchanged,': 7870, 'can\\nconclude': 7871, 'meaning,': 7872, 'wordnet:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 7873, 'wn\\n>>>': 7874, 'wn': 7875, \".synsets('motorcar')\\n[synset('car\": 7876, '.n': 7877, \".01')]\\n\\n\\n\\nthus,\": 7878, '.01,\\nthe': 7879, 'entity': 7880, '.01': 7881, 'synset,\\nor': 7882, 'synonymous': 7883, 'lemmas):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 7884, \".synset('car\": 7885, \".01')\": 7886, \".lemma_names()\\n['car',\": 7887, \"'auto',\": 7888, \"'automobile',\": 7889, \"'machine',\": 7890, \"'motorcar']\\n\\n\\n\\neach\": 7891, 'synset': 7892, 'signify\\na': 7893, 'train': 7894, 'carriage,': 7895, 'gondola,': 7896, 'elevator': 7897, 'interested\\nin': 7898, 'synsets\\nalso': 7899, 'prose': 7900, 'sentences:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 7901, \".definition()\\n'a\": 7902, 'motor': 7903, 'vehicle': 7904, 'wheels;': 7905, 'propelled': 7906, 'internal': 7907, 'combustion': 7908, \"engine'\\n>>>\": 7909, \".examples()\\n['he\": 7910, \"work']\\n\\n\\n\\nalthough\": 7911, 'humans': 7912, 'synset,\\nthe': 7913, 'ambiguity,': 7914, 'as\\ncar': 7915, '.automobile,': 7916, '.motorcar,': 7917, 'pairing': 7918, 'lemma': 7919, 'lemmas': 7920, ',\\nlook': 7921, ',\\nget': 7922, '.lemmas()': 7923, \"\\n[lemma('car\": 7924, \".car'),\": 7925, \"lemma('car\": 7926, \".auto'),\": 7927, \".automobile'),\\nlemma('car\": 7928, \".machine'),\": 7929, \".motorcar')]\\n>>>\": 7930, \".lemma('car\": 7931, \".automobile')\": 7932, \"\\nlemma('car\": 7933, \".automobile')\\n>>>\": 7934, '.synset()': 7935, \"\\nsynset('car\": 7936, \".01')\\n>>>\": 7937, '.name()': 7938, \"\\n'automobile'\\n\\n\\n\\nunlike\": 7939, 'motorcar,': 7940, 'unambiguous': 7941, 'one\\nsynset,': 7942, 'ambiguous,': 7943, 'five': 7944, 'synsets:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 7945, \".synsets('car')\\n[synset('car\": 7946, \".01'),\": 7947, \"synset('car\": 7948, \".02'),\": 7949, \".03'),\": 7950, \".04'),\\nsynset('cable_car\": 7951, \".01')]\\n>>>\": 7952, \".synsets('car'):\\n\": 7953, 'print(synset': 7954, '.lemma_names())\\n': 7955, \".\\n['car',\": 7956, \"'motorcar']\\n['car',\": 7957, \"'railcar',\": 7958, \"'railway_car',\": 7959, \"'railroad_car']\\n['car',\": 7960, \"'gondola']\\n['car',\": 7961, \"'elevator_car']\\n['cable_car',\": 7962, \"'car']\\n\\n\\n\\nfor\": 7963, 'involving': 7964, 'car\\nas': 7965, \".lemmas('car')\\n[lemma('car\": 7966, '.02': 7967, '.03': 7968, \".car'),\\nlemma('car\": 7969, '.04': 7970, \"lemma('cable_car\": 7971, \".car')]\\n\\n\\n\\n\\nnote\\nyour\": 7972, 'turn:\\nwrite': 7973, 'senses': 7974, 'this\\nword': 7975, '.\\n\\n\\n\\n5': 7976, '.2\\xa0\\xa0\\xa0the': 7977, 'hierarchy\\nwordnet': 7978, 'synsets': 7979, 'abstract': 7980, 'always\\nhave': 7981, 'hierarchy': 7982, 'entity,': 7983, 'state,': 7984, 'called\\nunique': 7985, 'beginners': 7986, 'others,': 7987, 'gas': 7988, 'guzzler': 7989, 'and\\nhatchback,': 7990, 'concept\\nhierarchy': 7991, 'hierarchy:': 7992, 'nodes': 7993, 'synsets;\\nedges': 7994, 'hypernym/hyponym': 7995, 'relation,': 7996, 'relation': 7997, 'between\\nsuperordinate': 7998, 'subordinate': 7999, '.\\n\\nwordnet': 8000, 'motorcar,\\nwe': 8001, 'specific;\\nthe': 8002, '(immediate)': 8003, 'hyponyms': 8004, 'types_of_motorcar': 8005, '.hyponyms()\\n>>>': 8006, \"types_of_motorcar[0]\\nsynset('ambulance\": 8007, 'sorted(lemma': 8008, \".lemmas())\\n['model_t',\": 8009, \"'s\": 8010, '.u': 8011, '.v': 8012, \"'suv',\": 8013, \"'stanley_steamer',\": 8014, \"'ambulance',\": 8015, \"'beach_waggon',\\n'beach_wagon',\": 8016, \"'bus',\": 8017, \"'cab',\": 8018, \"'compact',\": 8019, \"'compact_car',\": 8020, \"'convertible',\\n'coupe',\": 8021, \"'cruiser',\": 8022, \"'electric',\": 8023, \"'electric_automobile',\": 8024, \"'electric_car',\\n'estate_car',\": 8025, \"'gas_guzzler',\": 8026, \"'hack',\": 8027, \"'hardtop',\": 8028, \"'hatchback',\": 8029, \"'heap',\\n'horseless_carriage',\": 8030, \"'hot-rod',\": 8031, \"'hot_rod',\": 8032, \"'jalopy',\": 8033, \"'jeep',\": 8034, \"'landrover',\\n'limo',\": 8035, \"'limousine',\": 8036, \"'loaner',\": 8037, \"'minicar',\": 8038, \"'minivan',\": 8039, \"'pace_car',\": 8040, \"'patrol_car',\\n'phaeton',\": 8041, \"'police_car',\": 8042, \"'police_cruiser',\": 8043, \"'prowl_car',\": 8044, \"'race_car',\": 8045, \"'racer',\\n'racing_car',\": 8046, \"'roadster',\": 8047, \"'runabout',\": 8048, \"'saloon',\": 8049, \"'secondhand_car',\": 8050, \"'sedan',\\n'sport_car',\": 8051, \"'sport_utility',\": 8052, \"'sport_utility_vehicle',\": 8053, \"'sports_car',\": 8054, \"'squad_car',\\n'station_waggon',\": 8055, \"'station_wagon',\": 8056, \"'stock_car',\": 8057, \"'subcompact',\": 8058, \"'subcompact_car',\\n'taxi',\": 8059, \"'taxicab',\": 8060, \"'tourer',\": 8061, \"'touring_car',\": 8062, \"'two-seater',\": 8063, \"'used-car',\": 8064, \"'waggon',\\n'wagon']\\n\\n\\n\\nwe\": 8065, 'visiting': 8066, 'hypernyms': 8067, 'words\\nhave': 8068, 'paths,': 8069, 'paths': 8070, 'because\\nwheeled_vehicle': 8071, 'container': 8072, \".hypernyms()\\n[synset('motor_vehicle\": 8073, '.hypernym_paths()\\n>>>': 8074, 'len(paths)\\n2\\n>>>': 8075, '[synset': 8076, \"paths[0]]\\n['entity\": 8077, \".01',\": 8078, \"'physical_entity\": 8079, \"'object\": 8080, \"'whole\": 8081, \".02',\": 8082, \"'artifact\": 8083, \".01',\\n'instrumentality\": 8084, \".03',\": 8085, \"'container\": 8086, \"'wheeled_vehicle\": 8087, \".01',\\n'self-propelled_vehicle\": 8088, \"'motor_vehicle\": 8089, \"'car\": 8090, \".01']\\n>>>\": 8091, \"paths[1]]\\n['entity\": 8092, \"'conveyance\": 8093, \"'vehicle\": 8094, \".01']\\n\\n\\n\\nwe\": 8095, 'hypernyms)': 8096, \".root_hypernyms()\\n[synset('entity\": 8097, \".01')]\\n\\n\\n\\n\\nnote\\nyour\": 8098, 'browser:': 8099, '.app': 8100, '.wordnet()': 8101, '.\\nexplore': 8102, 'hypernym': 8103, 'hyponym': 8104, '.3\\xa0\\xa0\\xa0more': 8105, 'relations\\nhypernyms': 8106, 'relations': 8107, 'relate': 8108, 'one\\nsynset': 8109, 'is-a': 8110, 'their\\ncomponents': 8111, '(meronyms)': 8112, '(holonyms)': 8113, 'parts': 8114, 'tree': 8115, 'trunk,': 8116, 'crown,': 8117, 'on;\\nthe': 8118, 'part_meronyms()': 8119, 'substance': 8120, 'heartwood': 8121, 'sapwood;\\nthe': 8122, 'substance_meronyms()': 8123, 'trees': 8124, 'forms': 8125, 'forest;': 8126, 'member_holonyms():\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 8127, \".synset('tree\": 8128, \".part_meronyms()\\n[synset('burl\": 8129, \"synset('crown\": 8130, \".07'),\": 8131, \"synset('limb\": 8132, \".02'),\\nsynset('stump\": 8133, \"synset('trunk\": 8134, \".substance_meronyms()\\n[synset('heartwood\": 8135, \"synset('sapwood\": 8136, \".member_holonyms()\\n[synset('forest\": 8137, \".01')]\\n\\n\\n\\nto\": 8138, 'intricate': 8139, 'get,': 8140, 'mint,': 8141, 'which\\nhas': 8142, 'closely-related': 8143, 'mint': 8144, 'of\\nmint': 8145, '.05': 8146, \".synsets('mint',\": 8147, '.noun):\\n': 8148, '.definition())\\n': 8149, '.\\nbatch': 8150, '.02:': 8151, '(often': 8152, \"`of')\": 8153, 'extent\\nmint': 8154, 'north': 8155, 'temperate': 8156, 'plant': 8157, 'genus': 8158, 'mentha': 8159, 'aromatic': 8160, 'and\\n': 8161, 'mauve': 8162, 'flowers\\nmint': 8163, '.03:': 8164, 'plants\\nmint': 8165, '.04:': 8166, 'fresh': 8167, 'candied\\nmint': 8168, '.05:': 8169, 'candy': 8170, 'flavored': 8171, 'oil\\nmint': 8172, '.06:': 8173, 'money': 8174, 'coined': 8175, 'authority': 8176, 'government\\n>>>': 8177, \".synset('mint\": 8178, \".04')\": 8179, \".part_holonyms()\\n[synset('mint\": 8180, \".02')]\\n>>>\": 8181, \".substance_holonyms()\\n[synset('mint\": 8182, \".05')]\\n\\n\\n\\nthere\": 8183, 'relationships': 8184, 'stepping,\\nso': 8185, 'entails': 8186, 'stepping': 8187, 'entailments:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 8188, \".synset('walk\": 8189, \".entailments()\\n[synset('step\": 8190, \".synset('eat\": 8191, \".entailments()\\n[synset('chew\": 8192, \"synset('swallow\": 8193, \".synset('tease\": 8194, \".03')\": 8195, \".entailments()\\n[synset('arouse\": 8196, \"synset('disappoint\": 8197, \".01')]\\n\\n\\n\\nsome\": 8198, 'lemmas,': 8199, 'antonymy:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 8200, \".lemma('supply\": 8201, \".supply')\": 8202, \".antonyms()\\n[lemma('demand\": 8203, \".demand')]\\n>>>\": 8204, \".lemma('rush\": 8205, \".rush')\": 8206, \".antonyms()\\n[lemma('linger\": 8207, \".linger')]\\n>>>\": 8208, \".lemma('horizontal\": 8209, '.a': 8210, \".horizontal')\": 8211, \".antonyms()\\n[lemma('inclined\": 8212, \".inclined'),\": 8213, \"lemma('vertical\": 8214, \".vertical')]\\n>>>\": 8215, \".lemma('staccato\": 8216, '.r': 8217, \".staccato')\": 8218, \".antonyms()\\n[lemma('legato\": 8219, \".legato')]\\n\\n\\n\\nyou\": 8220, 'relations,': 8221, 'defined\\non': 8222, 'synset,': 8223, 'dir(),': 8224, 'dir(wn': 8225, \".synset('harmony\": 8226, \".02'))\": 8227, '.4\\xa0\\xa0\\xa0semantic': 8228, 'similarity\\n\\nwe': 8229, 'traverse\\nthe': 8230, '.\\nknowing': 8231, 'semantically': 8232, 'related\\nis': 8233, 'indexing': 8234, 'so\\nthat': 8235, 'match': 8236, 'documents\\ncontaining': 8237, 'limousine': 8238, '.\\nrecall': 8239, 'link': 8240, 'it\\nto': 8241, 'common\\n(cf': 8242, 'low\\ndown': 8243, \".synset('right_whale\": 8244, 'orca': 8245, \".synset('orca\": 8246, 'minke': 8247, \".synset('minke_whale\": 8248, 'tortoise': 8249, \".synset('tortoise\": 8250, 'novel': 8251, \".synset('novel\": 8252, \".lowest_common_hypernyms(minke)\\n[synset('baleen_whale\": 8253, \".lowest_common_hypernyms(orca)\\n[synset('whale\": 8254, \".lowest_common_hypernyms(tortoise)\\n[synset('vertebrate\": 8255, \".lowest_common_hypernyms(novel)\\n[synset('entity\": 8256, \".01')]\\n\\n\\n\\nof\": 8257, 'baleen': 8258, 'so),\\nwhile': 8259, 'vertebrate': 8260, 'quantify': 8261, 'generality': 8262, 'depth': 8263, 'synset:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 8264, \".synset('baleen_whale\": 8265, '.min_depth()\\n14\\n>>>': 8266, \".synset('whale\": 8267, \".02')\": 8268, '.min_depth()\\n13\\n>>>': 8269, \".synset('vertebrate\": 8270, '.min_depth()\\n8\\n>>>': 8271, \".synset('entity\": 8272, '.min_depth()\\n0\\n\\n\\n\\nsimilarity': 8273, 'measures': 8274, 'synsets\\nwhich': 8275, 'incorporate': 8276, 'insight': 8277, 'example,\\npath_similarity': 8278, 'score': 8279, '0–1': 8280, 'shortest': 8281, 'connects': 8282, 'hypernym\\nhierarchy': 8283, '(-1': 8284, 'returned': 8285, 'be\\nfound)': 8286, 'comparing': 8287, 'similarity': 8288, 'scores,': 8289, 'whale\\nto': 8290, 'orca,': 8291, 'tortoise,': 8292, '.\\nalthough': 8293, 'much,': 8294, 'decrease': 8295, 'as\\nwe': 8296, 'sea': 8297, 'creatures': 8298, 'inanimate': 8299, '.path_similarity(minke)\\n0': 8300, '.25\\n>>>': 8301, '.path_similarity(orca)\\n0': 8302, '.16666666666666666\\n>>>': 8303, '.path_similarity(tortoise)\\n0': 8304, '.07692307692307693\\n>>>': 8305, '.path_similarity(novel)\\n0': 8306, '.043478260869565216\\n\\n\\n\\n\\nnote\\nseveral': 8307, 'available;': 8308, 'help(wn)\\nfor': 8309, 'verbnet,': 8310, 'hierarhical': 8311, '.verbnet': 8312, '.\\n\\n\\n\\n\\n6\\xa0\\xa0\\xa0summary\\n\\na': 8313, 'corpora,\\ne': 8314, '.brown': 8315, 'categorized,': 8316, 'topic;': 8317, 'the\\ncategories': 8318, 'distributions,\\neach': 8319, 'frequencies,\\ngiven': 8320, 'entered': 8321, 'editor,\\nsaved': 8322, 'code,\\nand': 8323, 'necessary': 8324, 'object\\nname': 8325, 'period': 8326, '.funct(y),\\ne': 8327, '.isalpha()': 8328, 'v,\\ntype': 8329, 'help(v)': 8330, '.\\nwordnet': 8331, 'default,': 8332, \"using\\npython's\": 8333, 'reading\\nextra': 8334, 'freely\\navailable': 8335, 'howto,': 8336, '.org/howto,': 8337, 'documented': 8338, 'extensively': 8339, '.\\nsignificant': 8340, 'sources': 8341, 'published': 8342, '(ldc)': 8343, 'agency': 8344, '(elra)': 8345, 'speech\\ncorpora': 8346, 'non-commercial': 8347, 'licences': 8348, 'to\\nbe': 8349, 'licenses': 8350, 'available\\n(but': 8351, 'higher': 8352, 'fee)': 8353, 'brat,\\nand': 8354, 'http://brat': 8355, '.nlplab': 8356, 'olac': 8357, 'metadata,': 8358, 'homepage': 8359, '.language-archives': 8360, 'list\\nfor': 8361, 'discussions': 8362, 'archives\\nor': 8363, 'posting': 8364, \"world's\": 8365, 'ethnologue,': 8366, '.ethnologue': 8367, '.com/': 8368, '.\\nof': 8369, '7,000': 8370, 'dozen': 8371, 'touched': 8372, 'this\\narea': 8373, '(biber,': 8374, 'conrad,': 8375, 'reppen,': 8376, '1998),': 8377, '(mcenery,': 8378, '2006),': 8379, '(meyer,': 8380, '2002),': 8381, '(sampson': 8382, 'mccarthy,': 8383, '2005),': 8384, '(scott': 8385, 'tribble,': 8386, '2006)': 8387, '.\\nfurther': 8388, 'readings': 8389, 'quantitative': 8390, 'are:\\n(baayen,': 8391, '2008),': 8392, '(gries,': 8393, '2009),': 8394, '(woods,': 8395, 'fletcher,': 8396, 'hughes,': 8397, '1986)': 8398, 'description': 8399, '(fellbaum,': 8400, '1998)': 8401, 'research\\nin': 8402, 'psycholinguistics,': 8403, 'retrieval': 8404, '.\\nwordnets': 8405, 'documented\\nat': 8406, '.globalwordnet': 8407, 'measures,': 8408, '(budanitsky': 8409, 'hirst,': 8410, '.\\nother': 8411, 'phonetics': 8412, 'semantics,\\nand': 8413, '(jurafsky': 8414, '.\\n\\n\\n8\\xa0\\xa0\\xa0exercises\\n\\n☼': 8415, '.\\nreview': 8416, 'addition,\\nmultiplication,': 8417, 'sorting': 8418, '.\\n☼': 8419, 'have?': 8420, 'types?\\n☼': 8421, 'reader': 8422, '.words()': 8423, 'corpus\\nreader': 8424, '.webtext': 8425, 'state': 8426, 'addresses,': 8427, 'the\\nstate_union': 8428, 'men,': 8429, 'women,\\nand': 8430, 'happened': 8431, 'time?\\n☼': 8432, 'holonym-meronym': 8433, 'nouns': 8434, 'relation,\\nso': 8435, 'use:\\nmember_meronyms(),': 8436, 'part_meronyms(),': 8437, 'substance_meronyms(),\\nmember_holonyms(),': 8438, 'part_holonyms(),': 8439, 'substance_holonyms()': 8440, 'object\\ncalled': 8441, 'spanish\\nin': 8442, 'arise': 8443, 'approach?\\ncan': 8444, 'suggest': 8445, 'problem?\\n☼': 8446, 'strunk': 8447, \"white's\": 8448, 'style,\\nthe': 8449, 'sentence,\\nmeans': 8450, 'extent,': 8451, 'not\\nnevertheless': 8452, 'usage:\\nhowever': 8453, 'advise': 8454, 'him,': 8455, 'thinks': 8456, 'best': 8457, '.\\n(http://www': 8458, '.bartleby': 8459, '.com/141/strunk3': 8460, '.html)\\nuse': 8461, 'word\\nin': 8462, 'considering': 8463, '.\\nsee': 8464, 'languagelog': 8465, 'fossilized': 8466, 'prejudices': 8467, \"'however'\\nat\": 8468, 'http://itre': 8469, '.cis': 8470, '.upenn': 8471, '.edu/~myl/languagelog/archives/001913': 8472, '.html\\n◑': 8473, 'males\\nvs': 8474, '(cf': 8475, '.4)': 8476, '.\\n◑': 8477, 'them,\\nin': 8478, 'richness,': 8479, 'you\\nfind': 8480, 'the\\ntwo': 8481, 'sensibility?\\n◑': 8482, 'bbc': 8483, 'article:': 8484, \"uk's\": 8485, 'vicky': 8486, 'pollards': 8487, \"'left\": 8488, \"behind'\": 8489, 'http://news': 8490, '.bbc': 8491, '.co': 8492, '.uk/1/hi/education/6173441': 8493, '.stm': 8494, 'article': 8495, 'statistic': 8496, 'teen': 8497, 'language:\\nthe': 8498, 'used,': 8499, 'yeah,': 8500, 'no,': 8501, 'third': 8502, 'third\\nof': 8503, 'sources?': 8504, 'statistic?\\nread': 8505, 'languagelog,': 8506, '.edu/~myl/languagelog/archives/003993': 8507, 'impressionistic': 8508, 'understanding\\nof': 8509, 'closed': 8510, 'that\\nexhibit': 8511, 'genres?\\n◑': 8512, 'pronunciations\\nfor': 8513, 'contain?': 8514, 'fraction\\nof': 8515, 'pronunciation?\\n◑': 8516, 'hyponyms?\\nyou': 8517, \".all_synsets('n')\": 8518, 'supergloss(s)': 8519, 'argument\\nand': 8520, 'concatenation': 8521, 'token/type': 8522, 'ratios),': 8523, '(nltk': 8524, '.categories())': 8525, '.\\nwhich': 8526, 'lowest': 8527, '(greatest': 8528, 'type)?\\nis': 8529, 'expected?\\n◑': 8530, 'words\\nof': 8531, 'bigrams\\n(pairs': 8532, 'adjacent': 8533, 'omitting': 8534, 'genre,\\nlike': 8535, '.\\nchoose': 8536, 'presence\\n(or': 8537, 'absence)': 8538, 'findings': 8539, 'word_freq()': 8540, 'section\\nof': 8541, 'arguments,': 8542, 'syllables': 8543, 'text,\\nmaking': 8544, 'hedge(text)': 8545, \"word\\n'like'\": 8546, '.\\n★': 8547, \"zipf's\": 8548, 'law:\\nlet': 8549, 'that\\nall': 8550, 'ranked': 8551, 'frequency,\\nwith': 8552, 'law': 8553, 'the\\nfrequency': 8554, 'inversely': 8555, 'proportional': 8556, 'rank\\n(i': 8557, '×': 8558, 'constant': 8559, 'k)': 8560, '50th': 8561, 'most\\ncommon': 8562, 'the\\n150th': 8563, '.\\nwrite': 8564, 'word\\nfrequency': 8565, 'against': 8566, 'rank': 8567, 'pylab': 8568, '.plot': 8569, 'do\\nyou': 8570, 'confirm': 8571, 'law?': 8572, '(hint:': 8573, 'logarithmic': 8574, 'scale)': 8575, 'extreme': 8576, 'line?\\ngenerate': 8577, '.choice(abcdefg': 8578, '),\\ntaking': 8579, 'string\\nconcatenation': 8580, 'accumulate': 8581, '(very)\\nlong': 8582, 'tokenize': 8583, 'zipf\\nplot': 8584, \"of\\nzipf's\": 8585, 'light': 8586, 'this?\\n\\n\\n★': 8587, 'to\\ndo': 8588, 'tasks:\\nstore': 8589, 'randomly\\nchoose': 8590, '.choice()': 8591, '(you': 8592, '.)\\nselect': 8593, 'corpus,\\nor': 8594, 'translation,': 8595, 'train\\nthe': 8596, 'you\\nmay': 8597, 'intelligible\\nis': 8598, 'strengths': 8599, 'weaknesses': 8600, 'of\\ngenerating': 8601, 'experiment\\nwith': 8602, 'hybrid': 8603, 'observations': 8604, '.\\n\\n\\n★': 8605, 'find_language()': 8606, 'string\\nas': 8607, 'argument,': 8608, 'that\\nstring': 8609, 'searches\\nto': 8610, 'latin-1': 8611, 'branching': 8612, 'factor': 8613, 'hierarchy?\\ni': 8614, 'the\\nhypernym': 8615, 'average?\\nyou': 8616, 'polysemy': 8617, 'dog': 8618, 'senses\\nwith:': 8619, 'len(wn': 8620, \".synsets('dog',\": 8621, \"'n'))\": 8622, '.\\ncompute': 8623, 'nouns,': 8624, 'adjectives': 8625, 'and\\nadverbs': 8626, 'predefined': 8627, 'score\\nthe': 8628, '.\\nrank': 8629, 'close': 8630, 'ranking': 8631, 'here,\\nan': 8632, 'experimentally\\nby': 8633, '(miller': 8634, 'charles,': 8635, '1998):\\ncar-automobile,': 8636, 'gem-jewel,': 8637, 'journey-voyage,': 8638, 'boy-lad,\\ncoast-shore,': 8639, 'asylum-madhouse,': 8640, 'magician-wizard,': 8641, 'midday-noon,\\nfurnace-stove,': 8642, 'food-fruit,': 8643, 'bird-cock,': 8644, 'bird-crane,': 8645, 'tool-implement,\\nbrother-monk,': 8646, 'lad-brother,': 8647, 'crane-implement,': 8648, 'journey-car,\\nmonk-oracle,': 8649, 'cemetery-woodland,': 8650, 'food-rooster,': 8651, 'coast-hill,\\nforest-graveyard,': 8652, 'shore-woodland,': 8653, 'monk-slave,': 8654, 'coast-forest,\\nlad-wizard,': 8655, 'chord-smile,': 8656, 'glass-magician,': 8657, 'rooster-voyage,': 8658, 'noon-string': 8659, 'acstch03': 8660, '.rst2\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3\\xa0\\xa0\\xa0processing': 8661, 'text\\nthe': 8662, 'undoubtedly': 8663, 'convenient\\nto': 8664, 'explore,': 8665, 'saw\\nin': 8666, 'sources\\nin': 8667, 'mind,': 8668, 'questions:\\n\\nhow': 8669, 'and\\nfrom': 8670, 'unlimited': 8671, 'of\\nlanguage': 8672, 'material?\\nhow': 8673, 'and\\npunctuation': 8674, 'of\\nanalysis': 8675, 'chapters?\\nhow': 8676, 'output\\nand': 8677, 'file?\\n\\nin': 8678, 'covering\\nkey': 8679, 'nlp,': 8680, 'tokenization': 8681, 'stemming': 8682, 'consolidate': 8683, 'and\\nlearn': 8684, 'since\\nso': 8685, 'html': 8686, 'format,': 8687, 'also\\nsee': 8688, 'dispense': 8689, 'markup': 8690, '.\\n\\nnote\\nimportant:\\nfrom': 8691, 'onwards,': 8692, 'assume': 8693, 'you\\nbegin': 8694, 'session': 8695, 'import\\nstatements:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 8696, 'only\\n>>>': 8697, 're,': 8698, 'pprint\\n>>>': 8699, 'word_tokenize\\n\\n\\n\\n\\n\\n3': 8700, '.1\\xa0\\xa0\\xa0accessing': 8701, 'disk\\n\\nelectronic': 8702, 'books\\na': 8703, 'catalog': 8704, 'at\\nhttp://www': 8705, '.org/catalog/,': 8706, 'url': 8707, 'ascii': 8708, '90%': 8709, 'it\\nincludes': 8710, 'catalan,': 8711, 'chinese,': 8712, 'dutch,\\nfinnish,': 8713, 'french,': 8714, 'german,': 8715, 'italian,': 8716, 'portuguese': 8717, 'spanish': 8718, '(with': 8719, 'than\\n100': 8720, 'each)': 8721, '.\\ntext': 8722, '2554': 8723, 'crime': 8724, 'punishment,\\nand': 8725, 'urllib': 8726, 'request\\n>>>': 8727, '.org/files/2554/2554-0': 8728, '.txt\\n>>>': 8729, 'request': 8730, '.urlopen(url)\\n>>>': 8731, '.read()': 8732, \".decode('utf8')\\n>>>\": 8733, 'type(raw)\\n<class': 8734, \"'str'>\\n>>>\": 8735, 'len(raw)\\n1176893\\n>>>': 8736, \"raw[:75]\\n'the\": 8737, 'ebook': 8738, 'punishment,': 8739, 'fyodor': 8740, \"dostoevsky\\\\r\\\\n'\\n\\n\\n\\n\\nnote\\nthe\": 8741, 'read()': 8742, 'downloads': 8743, 'proxy': 8744, 'detected': 8745, 'python,\\nyou': 8746, 'manually,': 8747, 'urlopen,': 8748, 'proxies': 8749, \"{'http':\": 8750, \"'http://www\": 8751, '.someproxy': 8752, \".com:3128'}\\n>>>\": 8753, '.proxyhandler(proxies)\\n\\n\\n\\n\\nthe': 8754, '1,176,893': 8755, '.\\n(we': 8756, 'type(raw)': 8757, 'book,\\nincluding': 8758, 'as\\nwhitespace,': 8759, 'breaks': 8760, '\\\\r': 8761, '\\\\n\\nin': 8762, 'file,': 8763, 'the\\nspecial': 8764, 'carriage': 8765, 'feed': 8766, 'must\\nhave': 8767, 'machine)': 8768, 'language\\nprocessing,': 8769, 'into\\nwords': 8770, 'punctuation,': 8771, 'tokenization,': 8772, 'word_tokenize(raw)\\n>>>': 8773, 'type(tokens)\\n<class': 8774, \"'list'>\\n>>>\": 8775, 'len(tokens)\\n254354\\n>>>': 8776, \"tokens[:10]\\n['the',\": 8777, \"'project',\": 8778, \"'gutenberg',\": 8779, \"'ebook',\": 8780, \"'crime',\": 8781, \"'punishment',\": 8782, \"'by']\\n\\n\\n\\nnotice\": 8783, 'the\\nearlier': 8784, 'this\\nlist,': 8785, 'operations\\nlike': 8786, 'slicing:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 8787, '.text(tokens)\\n>>>': 8788, 'type(text)\\n<class': 8789, \"'nltk\": 8790, '.text': 8791, \".text'>\\n>>>\": 8792, \"text[1024:1062]\\n['chapter',\": 8793, \"'exceptionally',\": 8794, \"'evening',\": 8795, \"'in',\\n\": 8796, \"'july',\": 8797, \"'young',\": 8798, \"'man',\": 8799, \"'came',\": 8800, \"'garret',\": 8801, \"'lodged',\": 8802, \"'place',\": 8803, \"'walked',\": 8804, \"'slowly',\\n\": 8805, \"'hesitation',\": 8806, \"'towards',\": 8807, \"'k\": 8808, \"'bridge',\": 8809, '.collocations()\\nkaterina': 8810, 'ivanovna;': 8811, 'pyotr': 8812, 'petrovitch;': 8813, 'pulcheria': 8814, 'alexandrovna;': 8815, 'avdotya\\nromanovna;': 8816, 'rodion': 8817, 'romanovitch;': 8818, 'marfa': 8819, 'petrovna;': 8820, 'sofya': 8821, 'semyonovna;': 8822, 'old\\nwoman;': 8823, 'gutenberg-tm;': 8824, 'porfiry': 8825, 'amalia': 8826, 'ivanovna;\\ngreat': 8827, 'deal;': 8828, 'nikodim': 8829, 'fomitch;': 8830, 'young': 8831, 'man;': 8832, 'ilya': 8833, \"n't\": 8834, 'know;\\nproject': 8835, 'gutenberg;': 8836, 'dmitri': 8837, 'prokofitch;': 8838, 'andrey': 8839, 'semyonovitch;': 8840, 'hay': 8841, 'market\\n\\n\\n\\nnotice': 8842, 'header': 8843, 'the\\nname': 8844, 'scanned': 8845, 'and\\ncorrected': 8846, 'license,': 8847, 'information\\nappears': 8848, 'footer': 8849, 'reliably': 8850, 'detect\\nwhere': 8851, 'ends,': 8852, 'resort': 8853, 'manual\\ninspection': 8854, 'beginning\\nand': 8855, 'end,': 8856, 'trimming': 8857, 'else:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 8858, '.find(part': 8859, 'i)\\n5338\\n>>>': 8860, '.rfind(end': 8861, \"gutenberg's\": 8862, 'crime)\\n1157743\\n>>>': 8863, 'raw[5338:1157743]': 8864, 'i)\\n0\\n\\n\\n\\nthe': 8865, 'find()': 8866, 'rfind()': 8867, '(reverse': 8868, 'find)': 8869, 'get\\nthe': 8870, 'overwrite': 8871, 'slice,': 8872, 'begins\\nwith': 8873, 'including)\\nthe': 8874, 'marks': 8875, 'brush': 8876, 'reality': 8877, 'web:\\ntexts': 8878, 'unwanted': 8879, 'material,\\nand': 8880, '.\\n\\n\\ndealing': 8881, 'html\\nmuch': 8882, 'browser': 8883, 'local\\nfile,': 8884, 'easiest': 8885, 'before,\\nusing': 8886, 'urlopen': 8887, 'fun': 8888, 'story\\ncalled': 8889, 'blondes': 8890, 'die': 8891, 'urban': 8892, 'legend\\npassed': 8893, 'fact:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 8894, '.uk/2/hi/health/2284783': 8895, '.stm\\n>>>': 8896, '.urlopen(url)': 8897, \"html[:60]\\n'<!doctype\": 8898, '-//w3c//dtd': 8899, \"transitional//en'\\n\\n\\n\\nyou\": 8900, 'print(html)': 8901, 'glory,\\nincluding': 8902, 'meta': 8903, 'image': 8904, 'map,': 8905, 'javascript,': 8906, 'forms,': 8907, 'tables': 8908, 'beautifulsoup,\\navailable': 8909, '.crummy': 8910, '.com/software/beautifulsoup/:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 8911, 'bs4': 8912, 'beautifulsoup\\n>>>': 8913, 'beautifulsoup(html,': 8914, \"'html\": 8915, \".parser')\": 8916, '.get_text()\\n>>>': 8917, \"tokens\\n['bbc',\": 8918, \"'|',\": 8919, \"'health',\": 8920, \"'blondes',\": 8921, \"'to,\": 8922, \"'die',\": 8923, '.]\\n\\n\\n\\nthis': 8924, 'concerning': 8925, 'site': 8926, 'navigation': 8927, 'related\\nstories': 8928, 'the\\ncontent': 8929, 'interest,': 8930, 'initialize': 8931, 'tokens[110:390]\\n>>>': 8932, \".concordance('gene')\\ndisplaying\": 8933, 'matches:\\nhey': 8934, 'gene': 8935, 'next\\nblonde': 8936, 'hair': 8937, 'caused': 8938, 'recessive': 8939, 'child': 8940, 'blond\\nhave': 8941, 'blonde': 8942, 'g\\nere': 8943, 'disadvantage': 8944, 'chance': 8945, 'disappear\\ndes': 8946, 'disappear': 8947, 'thin\\n\\n\\n\\n\\n\\nprocessing': 8948, 'engine': 8949, 'results\\nthe': 8950, 'thought': 8951, 'unannotated': 8952, 'large\\nquantity': 8953, 'advantage\\nof': 8954, 'size:': 8955, 'of\\ndocuments,': 8956, 'you\\nare': 8957, 'furthermore,': 8958, 'specific\\npatterns,': 8959, 'smaller\\nexample,': 8960, 'run\\non': 8961, 'advantage': 8962, 'are\\nvery': 8963, 'for\\nquickly': 8964, 'checking': 8965, 'theory,': 8966, 'reasonable': 8967, '.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ngoogle': 8968, 'hits\\nadore\\nlove\\nlike\\nprefer\\n\\n\\n\\nabsolutely\\n289,000\\n905,000\\n16,200\\n644\\n\\ndefinitely\\n1,460\\n51,000\\n158,000\\n62,600\\n\\nratio\\n198:1\\n18:1\\n1:10\\n1:97\\n\\n\\ntable': 8969, 'google': 8970, 'hits': 8971, 'collocations:': 8972, 'collocations\\ninvolving': 8973, 'absolutely': 8974, 'definitely,': 8975, 'adore,': 8976, 'prefer': 8977, '.\\n(liberman,': 8978, '2005)': 8979, '.\\n\\n\\nunfortunately,': 8980, 'allowable': 8981, 'severely': 8982, 'restricted': 8983, '.\\nunlike': 8984, 'for\\narbitrarily': 8985, 'patterns,': 8986, 'generally\\nonly': 8987, 'of\\nwords,': 8988, 'wildcards': 8989, 'give\\ninconsistent': 8990, 'results,': 8991, 'figures': 8992, 'used\\nat': 8993, 'geographical': 8994, 'regions': 8995, 'been\\nduplicated': 8996, 'sites,': 8997, 'boosted': 8998, 'unpredictably,\\nbreaking': 8999, 'pattern-based': 9000, 'locating': 9001, 'problem\\nwhich': 9002, 'ameliorated': 9003, 'apis)': 9004, 'turn:\\nsearch': 9005, '(inside': 9006, 'quotes)': 9007, 'large\\ncount,': 9008, 'collocation\\nin': 9009, 'english?\\n\\n\\n\\nprocessing': 9010, 'rss': 9011, 'feeds\\n\\n\\nthe': 9012, 'blogosphere': 9013, 'registers': 9014, 'parser,\\navailable': 9015, 'https://pypi': 9016, '.org/pypi/feedparser,': 9017, 'content\\nof': 9018, 'blog,': 9019, 'feedparser\\n>>>': 9020, 'llog': 9021, 'feedparser': 9022, '.parse(http://languagelog': 9023, '.ldc': 9024, '.edu/nll/?feed=atom)\\n>>>': 9025, \"llog['feed']['title']\\n'language\": 9026, \"log'\\n>>>\": 9027, 'len(llog': 9028, '.entries)\\n15\\n>>>': 9029, 'post': 9030, '.entries[2]\\n>>>': 9031, \".title\\nhe's\": 9032, 'my': 9033, 'bf\\n>>>': 9034, '.content[0]': 9035, '.value\\n>>>': 9036, \"content[:70]\\n'<p>today\": 9037, 'chatting': 9038, \"f'\\n>>>\": 9039, 'beautifulsoup(content,': 9040, \"word_tokenize(raw)\\n['today',\": 9041, \"'was',\": 9042, \"'chatting',\": 9043, \"'visiting',\\n'graduate',\": 9044, \"'students',\": 9045, \"'prc',\": 9046, \"'thinking',\": 9047, \"'i',\\n'was',\": 9048, \"'au',\": 9049, \"'courant',\": 9050, \"'mentioned',\": 9051, \"'expression',\\n'dui4xiang4',\": 9052, \"'\\\\u5c0d\\\\u8c61',\": 9053, \"'boy',\": 9054, \"'/',\": 9055, \"'girl',\": 9056, \"'friend',\": 9057, \"'',\": 9058, '.]\\n\\n\\n\\nwith': 9059, 'work,': 9060, 'posts,\\nand': 9061, '.\\n\\n\\n\\nreading': 9062, 'files\\n\\nin': 9063, 'open()': 9064, 'function,\\nfollowed': 9065, '.txt,': 9066, \"open('document\": 9067, '.read()\\n\\n\\n\\n\\nnote\\nyour': 9068, 'turn:\\ncreate': 9069, 'plain': 9070, 'typing\\nthe': 9071, 'window,': 9072, 'as\\ndocument': 9073, 'offers': 9074, 'pop-up': 9075, 'box': 9076, \".txt'),\": 9077, 'then\\ninspect': 9078, 'print(f': 9079, '.read())': 9080, '.\\n\\nvarious': 9081, 'gone': 9082, \"couldn't\": 9083, 'an\\nerror': 9084, \".txt')\\ntraceback\": 9085, 'last):\\nfile': 9086, '<pyshell#7>,': 9087, '-toplevel-\\nf': 9088, \".txt')\\nioerror:\": 9089, '[errno': 9090, '2]': 9091, 'directory:': 9092, \"'document\": 9093, \".txt'\\n\\n\\n\\nto\": 9094, 'really': 9095, 'the\\nright': 9096, 'directory,': 9097, \"idle's\": 9098, 'menu;\\nthis': 9099, 'where\\nidle': 9100, 'alternative': 9101, 'current\\ndirectory': 9102, 'python:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 9103, 'os\\n>>>': 9104, 'os': 9105, \".listdir('\": 9106, \".')\\n\\n\\n\\nanother\": 9107, 'file\\nis': 9108, 'newline': 9109, 'conventions,': 9110, 'operating': 9111, 'controlling': 9112, 'how\\nthe': 9113, 'opened:': 9114, \"'ru')\": 9115, \"—\\n'r'\": 9116, 'default),': 9117, \"and\\n'u'\": 9118, 'universal,': 9119, 'lets': 9120, 'different\\nconventions': 9121, 'marking': 9122, 'newlines': 9123, '.\\nassuming': 9124, \".read()\\n'time\": 9125, 'flies': 9126, '.\\\\nfruit': 9127, 'banana': 9128, \".\\\\n'\\n\\n\\n\\nrecall\": 9129, \"'\\\\n'\": 9130, 'newlines;': 9131, 'this\\nis': 9132, 'keyboard': 9133, 'loop:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 9134, \"'ru')\\n>>>\": 9135, 'f:\\n': 9136, 'print(line': 9137, '.strip())\\ntime': 9138, '.\\nfruit': 9139, '.\\n\\n\\n\\nhere': 9140, 'strip()': 9141, 'simply\\nhave': 9142, '.data': 9143, '.find()': 9144, '.\\nthen': 9145, 'demonstrated': 9146, 'above:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 9147, \".find('corpora/gutenberg/melville-moby_dick\": 9148, 'open(path,': 9149, '.read()\\n\\n\\n\\n\\n\\nextracting': 9150, 'pdf,': 9151, 'msword': 9152, 'binary': 9153, 'formats\\nascii': 9154, 'formats': 9155, 'binary\\nformats': 9156, 'pdf': 9157, 'opened': 9158, 'specialized\\nsoftware': 9159, 'third-party': 9160, 'libraries': 9161, 'pypdf': 9162, 'pywin32\\nprovide': 9163, 'to\\nthese': 9164, 'multi-column': 9165, 'particularly\\nchallenging': 9166, 'once-off': 9167, 'conversion': 9168, 'documents,\\nit': 9169, 'text\\nto': 9170, 'drive,': 9171, \"google's\": 9172, 'document,\\nwhich': 9173, '.\\n\\n\\ncapturing': 9174, 'input\\nsometimes': 9175, 'capture': 9176, 'inputs': 9177, 'she': 9178, 'is\\ninteracting': 9179, 'user\\nto': 9180, 'input()': 9181, 'can\\nmanipulate': 9182, 'input(enter': 9183, ')\\nenter': 9184, 'exceptionally': 9185, 'hot': 9186, 'evening': 9187, 'july\\n>>>': 9188, 'print(you': 9189, 'typed,': 9190, 'len(word_tokenize(s)),': 9191, '.)\\nyou': 9192, '.\\n\\n\\n\\n\\n\\nthe': 9193, 'pipeline\\n3': 9194, 'section,': 9195, 'process\\nof': 9196, 'step,': 9197, 'normalization,\\nwill': 9198, 'discussed': 9199, '.)\\n\\n\\nfigure': 9200, 'pipeline:': 9201, 'content,\\nremove': 9202, 'characters;\\nthis': 9203, 'tokenized': 9204, 'converted': 9205, 'object;\\nwe': 9206, \".\\n\\nthere's\": 9207, 'properly,': 9208, 'be\\nclear': 9209, 'mentions': 9210, 'type\\nof': 9211, 'type(x),': 9212, 'type(1)': 9213, '<int>\\nsince': 9214, 'integer': 9215, 'strip': 9216, 'markup,\\nwe': 9217, 'dealing': 9218, '<str>': 9219, '.2):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 9220, \".txt')\": 9221, '.read()\\n>>>': 9222, \"'str'>\\n\\n\\n\\nwhen\": 9223, '(of': 9224, 'words),': 9225, '<list>\\ntype': 9226, 'normalizing': 9227, 'lists:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 9228, 'tokens]\\n>>>': 9229, 'type(words)\\n<class': 9230, 'sorted(set(words))\\n>>>': 9231, 'type(vocab)\\n<class': 9232, \"'list'>\\n\\n\\n\\nthe\": 9233, '.\\nso,': 9234, 'append': 9235, 'string:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 9236, \".append('blog')\\n>>>\": 9237, \".append('blog')\\ntraceback\": 9238, '<module>\\nattributeerror:': 9239, \"'str'\": 9240, 'attribute': 9241, \"'append'\\n\\n\\n\\n\\nsimilarly,\": 9242, 'concatenate': 9243, 'with\\nlists,': 9244, 'query': 9245, \"'who\": 9246, \"knows?'\\n>>>\": 9247, 'beatles': 9248, \"['john',\": 9249, \"'paul',\": 9250, \"'george',\": 9251, \"'ringo']\\n>>>\": 9252, 'beatles\\ntraceback': 9253, '<module>\\ntypeerror:': 9254, \"'list'\": 9255, 'objects\\n\\n\\n\\n\\n\\n\\n3': 9256, '.2\\xa0\\xa0\\xa0strings:': 9257, \"level\\nit's\": 9258, 'studiously': 9259, 'avoiding\\nso': 9260, 'far': 9261, 'focused': 9262, \"didn't\\nlook\": 9263, 'interface': 9264, 'ignore\\nthe': 9265, 'and\\nof': 9266, 'fundamental\\ndata': 9267, 'strings\\nin': 9268, 'detail,': 9269, 'connection': 9270, '.\\n\\nbasic': 9271, 'strings\\nstrings': 9272, '\\nor': 9273, 'double': 9274, 'quote,': 9275, 'backslash-escape\\nthe': 9276, 'quote': 9277, 'literal': 9278, 'intended,\\nor': 9279, '.\\notherwise,': 9280, '\\nwill': 9281, 'interpreted': 9282, 'interpreter\\nwill': 9283, 'report': 9284, \"monty\\n'monty\": 9285, 'circus': 9286, 'flying': 9287, 'circus\\nmonty': 9288, 'circus\\n>>>': 9289, \"python\\\\'s\": 9290, \"circus'\": 9291, \"circus'\\n\": 9292, 'syntax\\n\\n\\n\\nsometimes': 9293, 'various\\nways': 9294, 'is\\njoined': 9295, 'backslash': 9296, 'couplet': 9297, 'thee': 9298, \"summer's\": 9299, 'day?\\\\\\n': 9300, 'thou': 9301, 'temperate:': 9302, 'print(couplet)\\nshall': 9303, 'day?thou': 9304, 'temperate:\\n>>>': 9305, '(rough': 9306, 'winds': 9307, 'shake': 9308, 'darling': 9309, 'buds': 9310, 'may,\\n': 9311, 'lease': 9312, 'hath': 9313, 'date:)': 9314, 'print(couplet)\\nrough': 9315, 'may,and': 9316, 'date:\\n\\n\\n\\nunfortunately': 9317, 'between\\nthe': 9318, 'sonnet': 9319, 'triple-quoted\\nstring': 9320, 'day?\\n': 9321, 'day?\\nthou': 9322, \"'''rough\": 9323, \"date:'''\\n>>>\": 9324, 'may,\\nand': 9325, 'date:\\n\\n\\n\\nnow': 9326, '.\\nfirst': 9327, 'operation,': 9328, 'pasted': 9329, 'that\\nconcatenation': 9330, 'multiply': 9331, \"'very'\": 9332, \"\\n'veryveryvery'\\n>>>\": 9333, \"\\n'veryveryvery'\\n\\n\\n\\n\\nnote\\nyour\": 9334, '.\\nbe': 9335, 'which\\nis': 9336, 'whitespace': 9337, 'character,': 9338, '[1,': 9339, '1]\\n>>>': 9340, \"['\": 9341, '(7': 9342, 'i)': 9343, 'a]\\n>>>': 9344, 'b:\\n': 9345, \"print(line)\\n\\n\\n\\n\\n\\nwe've\": 9346, 'to\\nstrings,': 9347, 'use\\nsubtraction': 9348, \"'y'\\ntraceback\": 9349, 'unsupported': 9350, 'operand': 9351, 'type(s)': 9352, '-:': 9353, \"'str'\\n>>>\": 9354, '2\\ntraceback': 9355, '/:': 9356, \"'int'\\n\\n\\n\\nthese\": 9357, 'messages': 9358, 'we\\nhave': 9359, 'muddle': 9360, 'told\\nthat': 9361, 'subtraction': 9362, '-)': 9363, 'to\\nobjects': 9364, 'str': 9365, '(strings),': 9366, 'told': 9367, 'that\\ndivision': 9368, 'int': 9369, 'operands': 9370, '.\\n\\n\\nprinting': 9371, 'strings\\nso': 9372, 'or\\nsee': 9373, 'calculation,': 9374, 'name\\ninto': 9375, 'variable\\nusing': 9376, 'statement:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 9377, 'print(monty)\\nmonty': 9378, 'python\\n\\n\\n\\nnotice': 9379, 'quotation': 9380, 'inspect\\na': 9381, 'prints\\nthe': 9382, 'string,\\nthe': 9383, 'the\\ninterpreter': 9384, 'see\\nquotation': 9385, 'line\\nin': 9386, \"'holy\": 9387, \"grail'\\n>>>\": 9388, 'print(monty': 9389, 'grail)\\nmonty': 9390, 'pythonholy': 9391, 'grail\\n>>>': 9392, 'print(monty,': 9393, 'grail\\n\\n\\n\\n\\n\\naccessing': 9394, 'characters\\nas': 9395, 'lists,': 9396, 'indexed,': 9397, 'letters)': 9398, \"monty[0]\\n'm'\\n>>>\": 9399, \"monty[3]\\n't'\\n>>>\": 9400, \"monty[5]\\n'\": 9401, \"'\\n\\n\\n\\nas\": 9402, 'monty[20]\\ntraceback': 9403, 'range\\n\\n\\n\\n\\nagain': 9404, 'negative': 9405, 'strings,\\nwhere': 9406, '-1': 9407, '.\\npositive': 9408, 'to\\nany': 9409, '12,\\nindexes': 9410, '-7': 9411, 'space)': 9412, '.\\n(notice': 9413, 'len(monty)': 9414, 'monty[-1]': 9415, \"\\n'n'\\n>>>\": 9416, \"'\\n>>>\": 9417, \"monty[-7]\\n'\": 9418, \"'\\n\\n\\n\\nwe\": 9419, 'loops': 9420, 'iterate': 9421, 'characters\\nin': 9422, \"'\\nparameter,\": 9423, \"'colorless\": 9424, 'ideas': 9425, 'sleep': 9426, \"furiously'\\n>>>\": 9427, 'sent:\\n': 9428, 'print(char,': 9429, '.\\nc': 9430, 'o': 9431, 'd': 9432, 'y\\n\\n\\n\\nwe': 9433, 'case\\ndistinction': 9434, 'lowercase,': 9435, 'non-alphabetic': 9436, 'characters:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 9437, \".raw('melville-moby_dick\": 9438, '.freqdist(ch': 9439, 'ch': 9440, '.isalpha())\\n>>>': 9441, \".most_common(5)\\n[('e',\": 9442, '117092),': 9443, \"('t',\": 9444, '87996),': 9445, \"('a',\": 9446, '77916),': 9447, \"('o',\": 9448, '69326),': 9449, \"('n',\": 9450, '65617)]\\n>>>': 9451, '(char,': 9452, 'count)': 9453, \".most_common()]\\n['e',\": 9454, \"'n',\": 9455, \"'h',\": 9456, \"'d',\": 9457, \"'u',\": 9458, \"'c',\": 9459, \"'w',\\n'f',\": 9460, \"'p',\": 9461, \"'y',\": 9462, \"'v',\": 9463, \"'q',\": 9464, \"'j',\": 9465, \"'x',\": 9466, \"'z']\\n\\n\\n\\n\\n\\n\\n[sb]explain\": 9467, 'tuple': 9468, 'unpacking': 9469, 'somewhere?\\n\\n\\nthis': 9470, 'alphabet,': 9471, 'letters\\nlisted': 9472, 'complicated': 9473, 'below)': 9474, 'visualize': 9475, 'relative': 9476, 'identifying\\nthe': 9477, '.\\n\\n\\naccessing': 9478, 'substrings\\n\\n\\nfigure': 9479, 'slicing:': 9480, 'positive': 9481, 'and\\nnegative': 9482, 'indexes;': 9483, 'substrings': 9484, '[m,n]': 9485, 'continuous': 9486, 'pull': 9487, 'for\\nfurther': 9488, 'notation\\nwe': 9489, '.2)': 9490, '6,\\nup': 9491, 'including)': 9492, '10:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 9493, \"monty[6:10]\\n'pyth'\\n\\n\\n\\nhere\": 9494, \"'h'\": 9495, 'correspond\\nto': 9496, 'monty[6]': 9497, 'monty[9]': 9498, 'monty[10]': 9499, 'because\\na': 9500, 'finishes': 9501, 'rule': 9502, 'starting\\nfrom': 9503, 'stopping': 9504, 'applies;\\nhere': 9505, 'stop': 9506, \"monty[-12:-7]\\n'monty'\\n\\n\\n\\nas\": 9507, 'slices,': 9508, 'value,': 9509, 'start\\nof': 9510, 'end\\nof': 9511, \"monty[:5]\\n'monty'\\n>>>\": 9512, \"monty[6:]\\n'python'\\n\\n\\n\\nwe\": 9513, 'operator,': 9514, \"'and\": 9515, \"different'\\n>>>\": 9516, \"'thing'\": 9517, 'phrase:\\n': 9518, \"print('found\": 9519, \"thing')\\nfound\": 9520, 'thing\\n\\n\\n\\nwe': 9521, 'find():\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 9522, \".find('python')\\n6\\n\\n\\n\\n\\nnote\\nyour\": 9523, 'obviously\\nnot': 9524, 'text!)\\n\\n\\n\\nmore': 9525, 'strings\\npython': 9526, 'summary,': 9527, 'operations\\nwe': 9528, 'yet,': 9529, 'type\\nhelp(str)': 9530, '.\\n\\n\\n\\n\\n\\n\\nmethod\\nfunctionality\\n\\n\\n\\ns': 9531, '.find(t)\\nindex': 9532, 'found)\\n\\ns': 9533, '.rfind(t)\\nindex': 9534, '.index(t)\\nlike': 9535, '.find(t)': 9536, 'raises': 9537, 'valueerror': 9538, 'found\\n\\ns': 9539, '.rindex(t)\\nlike': 9540, '.rfind(t)': 9541, '.join(text)\\ncombine': 9542, 'glue\\n\\ns': 9543, '.split(t)\\nsplit': 9544, 'wherever': 9545, '(whitespace': 9546, 'default)\\n\\ns': 9547, '.splitlines()\\nsplit': 9548, 'line\\n\\ns': 9549, '.lower()\\na': 9550, '.upper()\\nan': 9551, 'uppercased': 9552, '.title()\\na': 9553, 'titlecased': 9554, '.strip()\\na': 9555, 'trailing': 9556, 'whitespace\\n\\ns': 9557, '.replace(t,': 9558, 'u)\\nreplace': 9559, 'instances': 9560, 's\\n\\n\\ntable': 9561, 'methods:': 9562, 'tests\\nshown': 9563, '.2;': 9564, 'list\\n\\n\\n\\n\\nthe': 9565, 'them\\napart': 9566, 'together\\nby': 9567, 'concatenating': 9568, \"query[2]\\n'o'\\n>>>\": 9569, \"beatles[2]\\n'george'\\n>>>\": 9570, \"query[:2]\\n'wh'\\n>>>\": 9571, \"beatles[:2]\\n['john',\": 9572, \"'paul']\\n>>>\": 9573, \"don't\\nwho\": 9574, 'knows?': 9575, \"don't\\n>>>\": 9576, \"'brian'\\ntraceback\": 9577, 'str)': 9578, 'list\\n>>>': 9579, \"['brian']\\n['john',\": 9580, \"'ringo',\": 9581, \"'brian']\\n\\n\\n\\nwhen\": 9582, 'file\\nfor': 9583, 'string\\ncorresponding': 9584, 'to\\nprocess': 9585, 'the\\nindividual': 9586, 'the\\ngranularity': 9587, 'big': 9588, 'or\\nsmall': 9589, 'like:': 9590, 'paragraphs,': 9591, 'sentences,\\nphrases,': 9592, 'we\\ncan': 9593, 'contain,': 9594, 'and\\ncorrespondingly': 9595, 'downstream': 9596, '.\\nconsequently,': 9597, 'nlp\\ncode': 9598, '(3': 9599, '.7)': 9600, '.\\nconversely,': 9601, 'terminal,\\nwe': 9602, '.9)': 9603, '.\\nlists': 9604, 'power': 9605, 'elements:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 9606, 'beatles[0]': 9607, 'john': 9608, 'lennon\\n>>>': 9609, 'del': 9610, 'beatles[-1]\\n>>>': 9611, \"beatles\\n['john\": 9612, \"lennon',\": 9613, \"'george']\\n\\n\\n\\non\": 9614, 'hand': 9615, 'string\\n—': 9616, '0th': 9617, \"'f'\": 9618, 'get:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 9619, 'query[0]': 9620, \"'f'\\ntraceback\": 9621, '?\\ntypeerror:': 9622, 'assignment\\n\\n\\n\\n': 9623, 'immutable': 9624, 'a\\nstring': 9625, 'mutable,\\nand': 9626, 'modified': 9627, 'lists\\nsupport': 9628, 'turn:\\nconsolidate': 9629, 'on\\nstrings': 9630, '.\\n\\n\\n\\n\\n3': 9631, '.3\\xa0\\xa0\\xa0text': 9632, 'unicode\\nour': 9633, 'deal': 9634, 'and\\ndifferent': 9635, 'english-speaking': 9636, 'ascii,\\npossibly': 9637, 'realizing': 9638, 'europe': 9639, 'use\\none': 9640, 'extended': 9641, 'latin': 9642, 'ø': 9643, 'danish': 9644, 'norwegian,': 9645, 'ő': 9646, 'hungarian,\\nñ': 9647, 'breton,': 9648, 'ň': 9649, 'czech': 9650, 'and\\nslovak': 9651, 'overview': 9652, 'use\\nunicode': 9653, 'non-ascii': 9654, '.\\n\\nwhat': 9655, 'unicode?\\nunicode': 9656, 'each\\ncharacter': 9657, 'code\\npoints': 9658, '\\\\uxxxx,': 9659, 'xxxx': 9660, 'number\\nin': 9661, '4-digit': 9662, 'hexadecimal': 9663, 'unicode': 9664, 'normal': 9665, 'terminal,\\nthey': 9666, 'encoded': 9667, 'bytes': 9668, '(such\\nas': 9669, 'latin-2)': 9670, 'byte': 9671, 'point,': 9672, 'a\\nsmall': 9673, 'subset': 9674, 'unicode,': 9675, 'encodings\\n(such': 9676, 'utf-8)': 9677, 'of\\nunicode': 9678, 'encoding,': 9679, 'some\\nmechanism': 9680, 'into\\nunicode': 9681, 'decoding': 9682, 'conversely,': 9683, 'a\\nfile': 9684, 'terminal,': 9685, 'suitable\\nencoding': 9686, 'encoding,\\nand': 9687, 'encoding\\n\\nfrom': 9688, 'perspective,': 9689, 'entities': 9690, 'glyphs': 9691, 'a\\nscreen': 9692, 'paper': 9693, 'font': 9694, '.\\n\\n\\nextracting': 9695, \"files\\nlet's\": 9696, 'it\\nis': 9697, 'polish-lat2': 9698, 'suggests,': 9699, 'snippet': 9700, 'polish': 9701, '(from': 9702, 'wikipedia;': 9703, 'see\\nhttp://pl': 9704, '.org/wiki/biblioteka_pruska)': 9705, 'latin-2,\\nalso': 9706, 'iso-8859-2': 9707, 'locates': 9708, 'the\\nfile': 9709, \".find('corpora/unicode_samples/polish-lat2\": 9710, \".txt')\\n\\n\\n\\nthe\": 9711, 'data\\ninto': 9712, 'encoded\\nform': 9713, 'to\\nspecify': 9714, 'open\\nour': 9715, 'file\\nwith': 9716, \"'latin2'\": 9717, \"encoding='latin2')\\n>>>\": 9718, '.strip()\\n': 9719, 'print(line)\\npruska': 9720, 'biblioteka': 9721, 'państwowa': 9722, 'jej': 9723, 'dawne': 9724, 'zbiory': 9725, 'znane': 9726, 'pod': 9727, 'nazwą\\nberlinka': 9728, 'skarb': 9729, 'kultury': 9730, 'sztuki': 9731, 'niemieckiej': 9732, 'przewiezione': 9733, 'przez\\nniemców': 9734, 'koniec': 9735, 'ii': 9736, 'wojny': 9737, 'światowej': 9738, 'dolny': 9739, 'śląsk,': 9740, 'zostały\\nodnalezione': 9741, 'po': 9742, '1945': 9743, 'terytorium': 9744, 'polski': 9745, 'trafiły': 9746, 'biblioteki\\njagiellońskiej': 9747, 'krakowie,': 9748, 'obejmują': 9749, 'ponad': 9750, 'tys': 9751, 'zabytkowych\\narchiwaliów,': 9752, '.in': 9753, 'manuskrypty': 9754, 'goethego,': 9755, 'mozarta,': 9756, 'beethovena,': 9757, 'bacha': 9758, '.\\n\\n\\n\\nif': 9759, 'underlying': 9760, 'codepoints)': 9761, 'characters,\\nthen': 9762, 'two-digit': 9763, '\\\\xxx\\nand': 9764, 'four-digit': 9765, '\\\\uxxxx': 9766, 'representations:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 9767, \".encode('unicode_escape'))\\nb'pruska\": 9768, 'pa\\\\\\\\u0144stwowa': 9769, \"nazw\\\\\\\\u0105'\\nb'berlinka\": 9770, \"przez'\\nb'niemc\\\\\\\\xf3w\": 9771, '\\\\\\\\u015bwiatowej': 9772, '\\\\\\\\u015al\\\\\\\\u0105sk,': 9773, \"zosta\\\\\\\\u0142y'\\nb'odnalezione\": 9774, 'trafi\\\\\\\\u0142y': 9775, \"biblioteki'\\nb'jagiello\\\\\\\\u0144skiej\": 9776, 'obejmuj\\\\\\\\u0105': 9777, \"zabytkowych'\\nb'archiwali\\\\\\\\xf3w,\": 9778, \".'\\n\\n\\n\\nthe\": 9779, 'escape': 9780, 'string\\npreceded': 9781, '\\\\u': 9782, '\\\\u0144': 9783, 'relevant\\nunicode': 9784, 'dislayed': 9785, 'screen': 9786, 'glyph\\nń': 9787, 'see\\n\\\\xf3,': 9788, 'corresponds': 9789, 'glyph': 9790, 'ó,': 9791, 'the\\n128-255': 9792, 'utf-8': 9793, 'can\\ninclude': 9794, 'editor\\nthat': 9795, '.\\narbitrary': 9796, 'the\\n\\\\uxxxx': 9797, 'ordinal': 9798, 'ord()': 9799, 'example:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 9800, \"ord('ń')\\n324\\n\\n\\n\\nthe\": 9801, 'digit': 9802, '324': 9803, '0144': 9804, '(type': 9805, 'hex(324)': 9806, 'this),\\nand': 9807, 'appropriate': 9808, 'nacute': 9809, \"'\\\\u0144'\\n>>>\": 9810, \"nacute\\n'ń'\\n\\n\\n\\n\\nnote\\nthere\": 9811, 'factors': 9812, 'determining': 9813, 'rendered\\non': 9814, 'encoding,\\nbut': 9815, 'failing': 9816, 'you\\nexpected,': 9817, 'fonts\\ninstalled': 9818, 'configure': 9819, 'locale\\nto': 9820, 'render': 9821, 'print(nacute': 9822, \".encode('utf8'))\\nin\": 9823, 'ń': 9824, 'terminal': 9825, 'inside\\na': 9826, \".encode('utf8')\\nb'\\\\xc5\\\\x84'\\n\\n\\n\\nthe\": 9827, 'unicodedata': 9828, 'unicode\\ncharacters': 9829, 'the\\nthird': 9830, 'their\\nutf-8': 9831, 'sequence,': 9832, 'the\\nstandard': 9833, 'prefixing': 9834, 'hex': 9835, 'with\\nu+),': 9836, 'unicodedata\\n>>>': 9837, \"encoding='latin2')\": 9838, '.readlines()\\n>>>': 9839, 'lines[2]\\n>>>': 9840, \".encode('unicode_escape'))\\nb'niemc\\\\\\\\xf3w\": 9841, \"zosta\\\\\\\\u0142y\\\\\\\\n'\\n>>>\": 9842, 'line:': 9843, 'ord(c)': 9844, '127:\\n': 9845, \"print('{}\": 9846, 'u+{:04x}': 9847, \"{}'\": 9848, '.format(c': 9849, \".encode('utf8'),\": 9850, 'ord(c),': 9851, \".name(c)))\\nb'\\\\xc3\\\\xb3'\": 9852, 'u+00f3': 9853, \"acute\\nb'\\\\xc5\\\\x9b'\": 9854, 'u+015b': 9855, \"acute\\nb'\\\\xc5\\\\x9a'\": 9856, 'u+015a': 9857, \"acute\\nb'\\\\xc4\\\\x85'\": 9858, 'u+0105': 9859, \"ogonek\\nb'\\\\xc5\\\\x82'\": 9860, 'u+0142': 9861, 'stroke\\n\\n\\n\\nif': 9862, 'replace\\nc': 9863, \".encode('utf8')\": 9864, 'c,': 9865, 'utf-8,\\nyou': 9866, 'following:\\n\\nó': 9867, 'acute\\nś': 9868, 'acute\\ną': 9869, 'ogonek\\nł': 9870, 'stroke\\n\\nalternatively,': 9871, \"'utf8'\": 9872, 'the\\nexample': 9873, \"'latin2',\": 9874, 're\\nmodule': 9875, 're': 9876, '\\\\w': 9877, 'word\\ncharacter,': 9878, 'cf': 9879, \".find('zosta\\\\u0142y')\\n54\\n>>>\": 9880, '.lower()\\n>>>': 9881, \"line\\n'niemców\": 9882, \"zostały\\\\n'\\n>>>\": 9883, \".encode('unicode_escape')\\nb'niemc\\\\\\\\xf3w\": 9884, '\\\\\\\\u015bl\\\\\\\\u0105sk,': 9885, 're\\n>>>': 9886, \".search('\\\\u015b\\\\w*',\": 9887, 'line)\\n>>>': 9888, \".group()\\n'\\\\u015bwiatowej'\\n\\n\\n\\nnltk\": 9889, 'tokenizers': 9890, \"word_tokenize(line)\\n['niemców',\": 9891, \"'pod',\": 9892, \"'koniec',\": 9893, \"'ii',\": 9894, \"'wojny',\": 9895, \"'światowej',\": 9896, \"'na',\": 9897, \"'dolny',\": 9898, \"'śląsk',\": 9899, \"'zostały']\\n\\n\\n\\n\\n\\nusing\": 9900, 'python\\nif': 9901, 'local\\nencoding,': 9902, 'methods\\nfor': 9903, 'inputting': 9904, 'editing': 9905, 'this,\\nyou': 9906, \"'#\": 9907, '-*-': 9908, 'coding:': 9909, '<coding>': 9910, \"-*-'\": 9911, 'the\\nfirst': 9912, \"'latin-1',\": 9913, \"'big5'\": 9914, \"'utf-8'\": 9915, 'idle:': 9916, 'literals': 9917, 'editor;\\nthis': 9918, 'requires': 9919, 'preferences;\\nhere': 9920, 'courier': 9921, 'ce': 9922, 'use\\nencoded': 9923, '.4\\xa0\\xa0\\xa0regular': 9924, 'detecting': 9925, 'patterns\\nmany': 9926, 'matching': 9927, 'ed': 9928, \"using\\nendswith('ed')\": 9929, 'tests\\nin': 9930, '.\\nregular': 9931, 'flexible\\nmethod': 9932, 'describing': 9933, '.\\n\\nnote\\nthere': 9934, 'introductions': 9935, 'expressions,\\norganized': 9936, 'searching\\ntext': 9937, 'expressions\\nat': 9938, 'stages': 9939, 'adopt\\na': 9940, 'problem-based': 9941, 'are\\nneeded': 9942, 'mark\\nregular': 9943, 'chevrons': 9944, '«patt»': 9945, '.\\n\\nto': 9946, 're\\nlibrary': 9947, 'using:': 9948, \"search;\\nwe'll\": 9949, '(4)': 9950, 'preprocess': 9951, \".words('en')\": 9952, '.islower()]\\n\\n\\n\\n\\nusing': 9953, \"meta-characters\\nlet's\": 9954, '«ed$»': 9955, '.search(p,': 9956, 's)': 9957, 'found\\nsomewhere': 9958, 'dollar': 9959, 'a\\nspecial': 9960, 'matches\\nthe': 9961, \".search('ed$',\": 9962, \"w)]\\n['abaissed',\": 9963, \"'abandoned',\": 9964, \"'abased',\": 9965, \"'abashed',\": 9966, \"'abatised',\": 9967, \"'abed',\": 9968, \"'aborted',\": 9969, 'wildcard': 9970, 'symbol': 9971, '.\\nsuppose': 9972, 'crossword': 9973, '8-letter': 9974, 'word\\nwith': 9975, 'j': 9976, 'sixth': 9977, 'period:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 9978, \".search('^\": 9979, '.j': 9980, '.t': 9981, \".$',\": 9982, \"w)]\\n['abjectly',\": 9983, \"'adjuster',\": 9984, \"'dejected',\": 9985, \"'dejectly',\": 9986, \"'injector',\": 9987, \"'majestic',\": 9988, '.]\\n\\n\\n\\n\\nnote\\nyour': 9989, 'turn:\\nthe': 9990, 'caret': 9991, '^': 9992, '$': 9993, 'out\\nboth': 9994, 'these,': 9995, '«': 9996, '.»?\\n\\nfinally,': 9997, 'specifies': 9998, '«^e-?mail$»': 9999, 'email': 10000, 'e-mail': 10001, 'spelling)\\nin': 10002, 'sum(1': 10003, \".search('^e-?mail$',\": 10004, 'w))': 10005, '.\\n\\n\\nranges': 10006, 'closures\\n\\n\\nfigure': 10007, '.5:': 10008, 't9:': 10009, 'keys\\n\\nthe': 10010, 't9': 10011, 'mobile': 10012, '.5)': 10013, 'that\\nare': 10014, 'keystrokes': 10015, 'textonyms': 10016, 'hole': 10017, 'golf': 10018, 'pressing\\nthe': 10019, '4653': 10020, 'words\\ncould': 10021, 'sequence?': 10022, 'expression\\n«^[ghi][mno][jlk][def]$»:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 10023, \".search('^[ghi][mno][jlk][def]$',\": 10024, \"w)]\\n['gold',\": 10025, \"'golf',\": 10026, \"'hold',\": 10027, \"'hole']\\n\\n\\n\\nthe\": 10028, '«^[ghi]»,': 10029, 'g,': 10030, 'h,': 10031, 'expression,\\n«[mno]»,': 10032, 'constrains': 10033, 'be\\nm,': 10034, 'n,': 10035, 'fourth': 10036, 'constrained': 10037, '.\\nonly': 10038, 'significant,': 10039, 'we\\ncould': 10040, '«^[hig][nom][ljk][fed]$»': 10041, 'matched': 10042, 'same\\nwords': 10043, 'turn:\\nlook': 10044, 'finger-twisters,': 10045, 'part\\nof': 10046, 'number-pad': 10047, '«^[ghijklmno]+$»,': 10048, 'or\\nmore': 10049, 'concisely,': 10050, '«^[g-o]+$»,': 10051, 'words\\nthat': 10052, 'center': 10053, 'row,': 10054, '«^[a-fj-o]+$»\\nwill': 10055, 'top-right': 10056, 'corner': 10057, \"mean?\\n\\nlet's\": 10058, 'bit': 10059, 'to\\nindividual': 10060, 'letters:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 10061, 'chat_words': 10062, '.words()))\\n>>>': 10063, \".search('^m+i+n+e+$',\": 10064, \"w)]\\n['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee',\": 10065, \"'miiiiiinnnnnnnnnneeeeeeee',\": 10066, \"'mine',\\n'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']\\n>>>\": 10067, \".search('^[ha]+$',\": 10068, \"w)]\\n['a',\": 10069, \"'aaaaaaaaaaaaaaaaa',\": 10070, \"'ah',\": 10071, \"'ahah',\": 10072, \"'ahahah',\": 10073, \"'ahh',\\n'ahhahahaha',\": 10074, \"'ahhh',\": 10075, \"'ahhhh',\": 10076, \"'ahhhhhh',\": 10077, \"'ahhhhhhhhhhhhhh',\": 10078, \"'ha',\": 10079, \"'haaa',\\n'hah',\": 10080, \"'haha',\": 10081, \"'hahaaa',\": 10082, \"'hahah',\": 10083, \"'hahaha',\": 10084, \"'hahahaa',\": 10085, \"'hahahah',\": 10086, \"'hahahaha',\": 10087, 'item,\\nwhich': 10088, 'm,': 10089, '[fed]': 10090, '[d-f]': 10091, '*,': 10092, '«^m*i*n*e*$»': 10093, 'using\\n«^m+i+n+e+$»,': 10094, 'all,\\ne': 10095, 'me,': 10096, 'min,': 10097, 'mmmmm': 10098, 'referred': 10099, 'kleene': 10100, 'closures,\\nor': 10101, 'closures': 10102, 'first\\ncharacter': 10103, 'for\\nexample': 10104, '«[^aeiouaeiou]»': 10105, 'vowel': 10106, 'non-vowel\\ncharacters': 10107, '«^[^aeiouaeiou]+$»': 10108, 'these:\\n:):):),': 10109, 'grrr,': 10110, 'cyb3r': 10111, 'zzzzzzzz': 10112, 'includes\\nnon-alphabetic': 10113, 'pattern,': 10114, 'illustrating': 10115, 'symbols:\\n\\\\,': 10116, '{},': 10117, '(),': 10118, '|:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 10119, 'wsj': 10120, 'sorted(set(nltk': 10121, '.treebank': 10122, \".search('^[0-9]+\\\\\": 10123, \".[0-9]+$',\": 10124, \"w)]\\n['0\": 10125, \".0085',\": 10126, \"'0\": 10127, \".05',\": 10128, \".1',\": 10129, \".16',\": 10130, \".2',\": 10131, \".25',\": 10132, \".28',\": 10133, \".3',\": 10134, \".4',\": 10135, \".5',\\n'0\": 10136, \".50',\": 10137, \".54',\": 10138, \".56',\": 10139, \".60',\": 10140, \".7',\": 10141, \".82',\": 10142, \".84',\": 10143, \".9',\": 10144, \".95',\": 10145, \".99',\\n'1\": 10146, \"'1\": 10147, \".125',\": 10148, \".14',\": 10149, \".1650',\": 10150, \".17',\": 10151, \".18',\": 10152, \".19',\": 10153, \".search('^[a-z]+\\\\$$',\": 10154, \"w)]\\n['c$',\": 10155, \"'us$']\\n>>>\": 10156, \".search('^[0-9]{4}$',\": 10157, \"w)]\\n['1614',\": 10158, \"'1637',\": 10159, \"'1787',\": 10160, \"'1901',\": 10161, \"'1903',\": 10162, \"'1917',\": 10163, \"'1925',\": 10164, \"'1929',\": 10165, \"'1933',\": 10166, \".search('^[0-9]+-[a-z]{3,5}$',\": 10167, \"w)]\\n['10-day',\": 10168, \"'10-lap',\": 10169, \"'10-year',\": 10170, \"'100-share',\": 10171, \"'12-point',\": 10172, \"'12-year',\": 10173, \".search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$',\": 10174, \"w)]\\n['black-and-white',\": 10175, \"'bread-and-butter',\": 10176, \"'father-in-law',\": 10177, \"'machine-gun-toting',\\n'savings-and-loan']\\n>>>\": 10178, \".search('(ed|ing)$',\": 10179, \"w)]\\n['62%-owned',\": 10180, \"'absorbed',\": 10181, \"'according',\": 10182, \"'adopting',\": 10183, \"'advanced',\": 10184, \"'advancing',\": 10185, 'turn:\\nstudy': 10186, 'the\\n\\\\,': 10187, 'before\\nyou': 10188, 'is\\ndeprived': 10189, 'powers': 10190, 'special,': 10191, '\\\\': 10192, 'braced': 10193, '{3,5},': 10194, 'repeats': 10195, '.\\nparentheses': 10196, 'operator:': 10197, 'disjunction)': 10198, '«w(i|e|ai|oo)t»,': 10199, 'wit,\\nwet,': 10200, 'wait,': 10201, 'woot': 10202, 'instructive': 10203, 'when\\nyou': 10204, 'for\\n«ed|ing$»': 10205, 'meta-characters': 10206, '.\\n\\n\\n\\n\\n\\n\\noperator\\nbehavior\\n\\n\\n\\n': 10207, '.\\nwildcard,': 10208, 'character\\n\\n^abc\\nmatches': 10209, 'abc': 10210, 'string\\n\\nabc$\\nmatches': 10211, 'string\\n\\n[abc]\\nmatches': 10212, 'characters\\n\\n[a-z0-9]\\nmatches': 10213, 'characters\\n\\ned|ing|s\\nmatches': 10214, '(disjunction)\\n\\n*\\nzero': 10215, 'a*,': 10216, '[a-z]*': 10217, 'closure)\\n\\n+\\none': 10218, 'a+,': 10219, '[a-z]+\\n\\n?\\nzero': 10220, 'optional),': 10221, 'a?,': 10222, '[a-z]?\\n\\n{n}\\nexactly': 10223, 'non-negative': 10224, 'integer\\n\\n{n,}\\nat': 10225, 'repeats\\n\\n{,n}\\nno': 10226, 'repeats\\n\\n{m,n}\\nat': 10227, 'repeats\\n\\na(b|c)+\\nparentheses': 10228, 'operators\\n\\n\\ntable': 10229, 'meta-characters,': 10230, 'wildcards,': 10231, 'closures\\n\\n\\nto': 10232, 'will\\ninterpret': 10233, 'specially': 10234, '\\\\b': 10235, 'the\\nbackspace': 10236, 'containing\\nbackslash,': 10237, 'string\\nat': 10238, 'all,': 10239, 'that\\nit': 10240, \"r'\\\\band\\\\b'\\ncontains\": 10241, 'library\\nas': 10242, 'boundaries': 10243, 'backspace': 10244, 'habit': 10245, \"r'\": 10246, 'expressions\\n—': 10247, 'about\\nthese': 10248, 'complications': 10249, '.5\\xa0\\xa0\\xa0useful': 10250, 'expressions\\nthe': 10251, 'w\\nthat': 10252, 'regexp': 10253, '.search(regexp,': 10254, 'w)': 10255, '.\\napart': 10256, 'use\\nregular': 10257, '.\\n\\nextracting': 10258, 'pieces\\nthe': 10259, '.findall()': 10260, '(find': 10261, 'all)': 10262, '(non-overlapping)\\nmatches': 10263, 'vowels': 10264, 'them:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 10265, \"'supercalifragilisticexpialidocious'\\n>>>\": 10266, \".findall(r'[aeiou]',\": 10267, \"word)\\n['u',\": 10268, \"'e',\": 10269, \"'u']\\n>>>\": 10270, 'len(re': 10271, \"word))\\n16\\n\\n\\n\\nlet's\": 10272, 'sequences': 10273, 'frequency:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 10274, 'fd': 10275, '.freqdist(vs': 10276, 'wsj\\n': 10277, 'vs': 10278, \".findall(r'[aeiou]{2,}',\": 10279, 'word))\\n>>>': 10280, \".most_common(12)\\n[('io',\": 10281, '549),': 10282, \"('ea',\": 10283, '476),': 10284, \"('ie',\": 10285, '331),': 10286, \"('ou',\": 10287, '329),': 10288, \"('ai',\": 10289, '261),': 10290, \"('ia',\": 10291, \"253),\\n('ee',\": 10292, '217),': 10293, \"('oo',\": 10294, '174),': 10295, \"('ua',\": 10296, '109),': 10297, \"('au',\": 10298, '106),': 10299, \"('ue',\": 10300, '105),': 10301, \"('ui',\": 10302, '95)]\\n\\n\\n\\n\\nnote\\nyour': 10303, 'turn:\\nin': 10304, 'w3c': 10305, 'date': 10306, 'dates': 10307, '2009-12-31': 10308, '.\\nreplace': 10309, 'expression,\\nin': 10310, \"'2009-12-31'\": 10311, 'integers\\n[2009,': 10312, '12,': 10313, '31]:\\n[int(n)': 10314, '.findall(?,': 10315, \"'2009-12-31')]\\n\\n\\n\\ndoing\": 10316, 'pieces\\nonce': 10317, \"there's\\ninteresting\": 10318, 'pieces,': 10319, 'glue': 10320, 'or\\nplot': 10321, 'noted': 10322, 'redundant,': 10323, 'still\\neasy': 10324, 'word-internal': 10325, 'example,\\ndeclaration': 10326, 'becomes': 10327, 'dclrtn,': 10328, 'inalienable': 10329, 'inlnble,\\nretaining': 10330, 'expression\\nin': 10331, 'sequences,': 10332, 'consonants;\\neverything': 10333, 'three-way': 10334, 'disjunction': 10335, 'left-to-right,\\nif': 10336, 'regular\\nexpression': 10337, 'matching\\npieces,': 10338, \"''\": 10339, '.join()': 10340, '.9': 10341, 'for\\nmore': 10342, \"r'^[aeiouaeiou]+|[aeiouaeiou]+$|[^aeiouaeiou]'\\n>>>\": 10343, 'compress(word):\\n': 10344, '.findall(regexp,': 10345, '.join(pieces)\\n': 10346, 'english_udhr': 10347, \".words('english-latin1')\\n>>>\": 10348, 'print(nltk': 10349, '.tokenwrap(compress(w)': 10350, 'english_udhr[:75]))\\nunvrsl': 10351, 'dclrtn': 10352, 'hmn': 10353, 'rghts': 10354, 'prmble': 10355, 'whrs': 10356, 'rcgntn': 10357, 'inhrnt': 10358, 'dgnty': 10359, 'eql': 10360, 'inlnble': 10361, 'mmbrs': 10362, 'fmly': 10363, 'fndtn\\nof': 10364, 'frdm': 10365, 'jstce': 10366, 'pce': 10367, 'wrld': 10368, 'dsrgrd': 10369, 'cntmpt': 10370, 'fr': 10371, 'hmn\\nrghts': 10372, 'hve': 10373, 'rsltd': 10374, 'brbrs': 10375, 'acts': 10376, 'whch': 10377, 'outrgd': 10378, 'cnscnce': 10379, 'mnknd': 10380, 'advnt': 10381, 'bngs': 10382, 'shll': 10383, 'enjy': 10384, 'spch': 10385, 'and\\n\\n\\n\\nnext,': 10386, 'frequency\\ndistributions': 10387, 'consonant-vowel': 10388, 'sequences\\nfrom': 10389, 'rotokas,': 10390, 'ka': 10391, 'si': 10392, 'of\\nthese': 10393, 'pair,': 10394, 'pair:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 10395, 'rotokas_words': 10396, '.toolbox': 10397, \".words('rotokas\": 10398, \".dic')\\n>>>\": 10399, 'cvs': 10400, '[cv': 10401, 'cv': 10402, \".findall(r'[ptksvr][aeiou]',\": 10403, 'w)]\\n>>>': 10404, '.conditionalfreqdist(cvs)\\n>>>': 10405, '.tabulate()\\n': 10406, 'u\\nk': 10407, '418': 10408, '148': 10409, '420': 10410, '173\\np': 10411, '31': 10412, '105': 10413, '51\\nr': 10414, '187': 10415, '63': 10416, '84': 10417, '89': 10418, '79\\ns': 10419, '1\\nt': 10420, '47': 10421, '37\\nv': 10422, '27': 10423, '48': 10424, '49\\n\\n\\n\\nexamining': 10425, 'rows': 10426, 'partial\\ncomplementary': 10427, 'not\\ndistinct': 10428, 'conceivably': 10429, 'drop\\ns': 10430, 'rule\\nthat': 10431, 'pronounced': 10432, 'by\\ni': 10433, 'su,': 10434, \"kasuari,\\n'cassowary'\": 10435, 'borrowed': 10436, '.)\\nif': 10437, 'behind': 10438, 'table,\\nit': 10439, 'index,': 10440, 'allowing': 10441, \"cv_index['su']\": 10442, 'us\\nall': 10443, 'su': 10444, 'cv_word_pairs': 10445, '[(cv,': 10446, 'rotokas_words\\n': 10447, 'cv_index': 10448, '.index(cv_word_pairs)\\n>>>': 10449, \"cv_index['su']\\n['kasuari']\\n>>>\": 10450, \"cv_index['po']\\n['kaapo',\": 10451, \"'kaapopato',\": 10452, \"'kaipori',\": 10453, \"'kaiporipie',\": 10454, \"'kaiporivira',\": 10455, \"'kapo',\": 10456, \"'kapoa',\\n'kapokao',\": 10457, \"'kapokapo',\": 10458, \"'kapokapoa',\": 10459, \"'kapokapora',\": 10460, 'turn,': 10461, 'every\\nsubstring': 10462, '«[ptksvr][aeiou]»': 10463, 'kasuari,': 10464, 'ka,': 10465, 'ri': 10466, '.\\ntherefore,': 10467, \"('ka',\": 10468, \"'kasuari'),\\n('su',\": 10469, \"'kasuari')\": 10470, \"('ri',\": 10471, 'using\\nnltk': 10472, '.index(),': 10473, 'converts': 10474, '.\\n\\n\\nfinding': 10475, 'stems\\nwhen': 10476, 'engine,': 10477, 'mind': 10478, 'notice)\\nif': 10479, 'having\\ndifferent': 10480, 'endings': 10481, 'laptops': 10482, 'laptop': 10483, 'versa': 10484, 'lemma)': 10485, 'endings,': 10486, 'just\\ndeal': 10487, 'stems': 10488, 'stem': 10489, 'simple-minded\\napproach': 10490, 'strips': 10491, 'suffix:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 10492, 'stem(word):\\n': 10493, 'suffix': 10494, \"['ing',\": 10495, \"'ly',\": 10496, \"'ed',\": 10497, \"'ious',\": 10498, \"'ies',\": 10499, \"'ive',\": 10500, \"'ment']:\\n\": 10501, '.endswith(suffix):\\n': 10502, 'word[:-len(suffix)]\\n': 10503, 'word\\n\\n\\n\\nalthough': 10504, 'ultimately': 10505, 'stemmers,': 10506, 'interesting\\nto': 10507, 'is\\nto': 10508, 'suffixes': 10509, 'parentheses\\nin': 10510, \".findall(r'^\": 10511, \".*(ing|ly|ed|ious|ies|ive|es|s|ment)$',\": 10512, \"'processing')\\n['ing']\\n\\n\\n\\nhere,\": 10513, 'gave': 10514, 'expression\\nmatched': 10515, 'function,\\nto': 10516, 'disjunction,': 10517, 'output,\\nwe': 10518, '?:,\\nwhich': 10519, 'arcane': 10520, 'subtleties': 10521, \".*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$',\": 10522, \"'processing')\\n['processing']\\n\\n\\n\\nhowever,\": 10523, \"we'd\": 10524, 'parenthesize': 10525, 'expression:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 10526, \".findall(r'^(\": 10527, \".*)(ing|ly|ed|ious|ies|ive|es|s|ment)$',\": 10528, \"'processing')\\n[('process',\": 10529, \"'ing')]\\n\\n\\n\\nthis\": 10530, 'promising,': 10531, 'different\\nword,': 10532, 'processes:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 10533, \"'processes')\\n[('processe',\": 10534, \"'s')]\\n\\n\\n\\nthe\": 10535, 'incorrectly': 10536, '-s': 10537, 'of\\nan': 10538, '-es': 10539, 'subtlety:': 10540, 'star': 10541, 'operator\\nis': 10542, 'greedy': 10543, '.*': 10544, 'consume': 10545, 'input\\nas': 10546, 'non-greedy': 10547, '*?,\\nwe': 10548, 'want:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 10549, \".*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$',\": 10550, \"'processes')\\n[('process',\": 10551, \"'es')]\\n\\n\\n\\nthis\": 10552, 'suffix,': 10553, 'the\\nsecond': 10554, 'optional:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 10555, \".*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$',\": 10556, \"'language')\\n[('language',\": 10557, \"'')]\\n\\n\\n\\nthis\": 10558, '(can': 10559, 'spot': 10560, 'them?)': 10561, 'stemming,': 10562, \"r'^(\": 10563, \".*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\\n\": 10564, 'stem,': 10565, 'word)[0]\\n': 10566, 'stem\\n': 10567, 'dennis:': 10568, 'listen,': 10569, 'women': 10570, 'lying': 10571, 'ponds': 10572, 'swords\\n': 10573, 'supreme': 10574, 'executive': 10575, 'derives': 10576, 'from\\n': 10577, 'mandate': 10578, 'masses,': 10579, 'farcical': 10580, 'aquatic': 10581, 'ceremony': 10582, '[stem(t)': 10583, \"tokens]\\n['dennis',\": 10584, \"'listen',\": 10585, \"'strange',\": 10586, \"'women',\": 10587, \"'pond',\": 10588, \"'distribut',\\n'sword',\": 10589, \"'basi',\": 10590, \"'system',\": 10591, \"'supreme',\\n'execut',\": 10592, \"'power',\": 10593, \"'deriv',\": 10594, \"'mandate',\": 10595, \"'mass',\": 10596, \"',',\\n'not',\": 10597, \"'farcical',\": 10598, \"'aquatic',\": 10599, \"'ceremony',\": 10600, \".']\\n\\n\\n\\nnotice\": 10601, 'is\\nand': 10602, 'non-words': 10603, 'distribut': 10604, 'deriv,': 10605, 'these\\nare': 10606, 'acceptable': 10607, '.\\n\\n\\nsearching': 10608, 'text\\nyou': 10609, 'tokens)': 10610, '<a>': 10611, '<man>': 10612, 'all\\ninstances': 10613, 'angle': 10614, 'boundaries,\\nand': 10615, '(behaviors': 10616, 'unique\\nto': 10617, 'findall()': 10618, 'texts)': 10619, 'include\\n<': 10620, '.*>': 10621, 'token,': 10622, 'the\\nmatched': 10623, 'monied)': 10624, 'monied': 10625, 'man)\\nis': 10626, 'three-word': 10627, 'bro\\n': 10628, 'gutenberg,': 10629, '.text(gutenberg': 10630, \".words('melville-moby_dick\": 10631, '.findall(r<a>': 10632, '(<': 10633, '.*>)': 10634, '<man>)': 10635, '\\nmonied;': 10636, 'nervous;': 10637, 'dangerous;': 10638, 'white;': 10639, 'pious;': 10640, 'queer;': 10641, 'good;\\nmature;': 10642, 'cape;': 10643, 'great;': 10644, 'wise;': 10645, 'butterless;': 10646, 'fiendish;\\npale;': 10647, 'furious;': 10648, 'better;': 10649, 'certain;': 10650, 'complete;': 10651, 'dismasted;': 10652, 'younger;': 10653, 'brave;\\nbrave;': 10654, 'brave;': 10655, 'brave\\n>>>': 10656, '.text(nps_chat': 10657, '.words())\\n>>>': 10658, '.findall(r<': 10659, '<bro>)': 10660, '\\nyou': 10661, 'bro;': 10662, 'twizted': 10663, 'bro\\n>>>': 10664, '.findall(r<l': 10665, '.*>{3,})': 10666, '\\nlol': 10667, 'lol;': 10668, 'lmao': 10669, 'la': 10670, 'la;': 10671, 'la\\nla': 10672, 'love;': 10673, '.;': 10674, 'la\\n\\n\\n\\n\\nnote\\nyour': 10675, 'substitutions': 10676, '.re_show(p,': 10677, 'annotates': 10678, 'where\\npattern': 10679, 'matched,': 10680, '.nemo()': 10681, 'graphical\\ninterface': 10682, 'practice,': 10683, '.\\n\\n\\nit': 10684, 'phenomenon': 10685, \"we're\\nstudying\": 10686, 'tied': 10687, 'cases,': 10688, 'creativity\\nwill': 10689, 'for\\nexpressions': 10690, 'ys': 10691, 'discover\\nhypernyms': 10692, '5):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 10693, 'hobbies_learned': 10694, '.text(brown': 10695, \".words(categories=['hobbies',\": 10696, \"'learned']))\\n>>>\": 10697, '.findall(r<\\\\w*>': 10698, '<and>': 10699, '<other>': 10700, '<\\\\w*s>)\\nspeed': 10701, 'activities;': 10702, 'water': 10703, 'liquids;': 10704, 'tomb': 10705, 'other\\nlandmarks;': 10706, 'statues': 10707, 'monuments;': 10708, 'pearls': 10709, 'jewels;\\ncharts': 10710, 'items;': 10711, 'roads': 10712, 'features;': 10713, 'other\\nobjects;': 10714, 'military': 10715, 'areas;': 10716, 'demands': 10717, 'factors;\\nabstracts': 10718, 'compilations;': 10719, 'iron': 10720, 'metals\\n\\n\\n\\nwith': 10721, 'store\\nof': 10722, 'taxonomy': 10723, 'objects,': 10724, 'for\\nany': 10725, 'manual': 10726, 'labor': 10727, 'usually\\ncontain': 10728, 'positives,': 10729, 'exclude': 10730, 'result:': 10731, 'suggests\\nthat': 10732, 'demand': 10733, 'factor,': 10734, 'this\\nsentence': 10735, 'wage': 10736, 'nevertheless,': 10737, 'could\\nconstruct': 10738, 'ontology': 10739, 'correcting\\nthe': 10740, 'common\\nway': 10741, 'in\\n11': 10742, '.\\n\\nsearching': 10743, 'suffers': 10744, 'negatives,\\ni': 10745, 'risky': 10746, 'to\\nconclude': 10747, 'corpus\\njust': 10748, \"didn't\": 10749, 'discover\\ninformation': 10750, '.\\n\\n\\n\\n\\n\\n3': 10751, '.6\\xa0\\xa0\\xa0normalizing': 10752, 'text\\nin': 10753, 'before\\ndoing': 10754, 'lower(),': 10755, 'distinction': 10756, 'affixes,': 10757, 'resulting': 10758, 'dictionary,\\na': 10759, 'lemmatization': 10760, 'section:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 10761, 'word_tokenize(raw)\\n\\n\\n\\n\\nstemmers\\nnltk': 10762, 'off-the-shelf': 10763, 'need\\na': 10764, 'stemmer': 10765, 'preference': 10766, 'crafting': 10767, 'own\\nusing': 10768, 'irregular': 10769, 'porter': 10770, 'lancaster': 10771, 'stemmers': 10772, 'stripping': 10773, 'lying\\n(mapping': 10774, 'lie),': 10775, '.porterstemmer()\\n>>>': 10776, '.lancasterstemmer()\\n>>>': 10777, '[porter': 10778, '.stem(t)': 10779, \"tokens]\\n['denni',\": 10780, \"'strang',\": 10781, \"'lie',\": 10782, \"'pond',\\n'distribut',\": 10783, \"'sword',\": 10784, \"'govern',\\n'\": 10785, \"'suprem',\": 10786, \"'execut',\": 10787, \"'mandat',\": 10788, \"'from',\\n'the',\": 10789, \"'farcic',\": 10790, \"'aquat',\": 10791, \"'ceremoni',\": 10792, '[lancaster': 10793, \"tokens]\\n['den',\": 10794, \"'list',\": 10795, \"'wom',\": 10796, \"'lying',\": 10797, \"'bas',\": 10798, \"'suprem',\\n'execut',\": 10799, \"'pow',\": 10800, \"'der',\": 10801, \"'mand',\": 10802, \"'not',\\n'from',\": 10803, \"'som',\": 10804, \"'farc',\": 10805, \"'aqu',\": 10806, \".']\\n\\n\\n\\nstemming\": 10807, 'well-defined': 10808, 'process,': 10809, 'best\\nsuits': 10810, '(illustrated': 10811, '.6,': 10812, 'oriented\\nprogramming': 10813, 'formatting\\ntechniques': 10814, '.9,': 10815, 'enumerate()': 10816, 'function\\nto': 10817, '.\\n\\n\\n\\n\\n\\xa0\\n\\nclass': 10818, 'indexedtext(object):\\n\\n': 10819, '__init__(self,': 10820, 'stemmer,': 10821, 'text):\\n': 10822, 'self': 10823, '._text': 10824, 'text\\n': 10825, '._stemmer': 10826, 'stemmer\\n': 10827, '._index': 10828, '.index((self': 10829, '._stem(word),': 10830, 'i)\\n': 10831, '(i,': 10832, 'enumerate(text))\\n\\n': 10833, 'concordance(self,': 10834, 'width=40):\\n': 10835, '._stem(word)\\n': 10836, 'wc': 10837, 'int(width/4)': 10838, 'context\\n': 10839, '._index[key]:\\n': 10840, 'lcontext': 10841, '.join(self': 10842, '._text[i-wc:i])\\n': 10843, 'rcontext': 10844, '._text[i:i+wc])\\n': 10845, 'ldisplay': 10846, \"'{:>{width}}'\": 10847, '.format(lcontext[-width:],': 10848, 'width=width)\\n': 10849, 'rdisplay': 10850, \"'{:{width}}'\": 10851, '.format(rcontext[:width],': 10852, 'print(ldisplay,': 10853, 'rdisplay)\\n\\n': 10854, '_stem(self,': 10855, 'word):\\n': 10856, '.stem(word)': 10857, '.lower()\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 10858, \".words('grail\": 10859, 'indexedtext(porter,': 10860, 'grail)\\n>>>': 10861, \".concordance('lie')\\nr\": 10862, 'dennis': 10863, 'listen': 10864, 'swords': 10865, 'no\\n': 10866, 'beat': 10867, 'brave': 10868, 'retreat': 10869, 'robin': 10870, 'lies': 10871, 'minstrel': 10872, '[': 10873, 'singing': 10874, ']': 10875, 'bravest': 10876, 'of\\n': 10877, 'nay': 10878, 'lie': 10879, 'oh': 10880, 'wounded': 10881, '!\\ndoctors': 10882, 'immediately': 10883, 'clap': 10884, 'piglet': 10885, 'well\\nere': 10886, 'danger': 10887, 'cave': 10888, 'gorge': 10889, 'eternal': 10890, 'peril': 10891, 'which\\n': 10892, 'tim': 10893, 'caerbannog': 10894, '--\\nh': 10895, 'lived': 10896, 'fifty': 10897, 'men': 10898, 'strewn': 10899, 'lair': 10900, 'k\\nnot': 10901, 'fight': 10902, 'til': 10903, 't\\n\\n\\nexample': 10904, '(code_stemmer_indexing': 10905, '.6:': 10906, 'stemmer\\n\\n\\n\\nlemmatization\\nthe': 10907, 'lemmatizer': 10908, 'slower': 10909, 'lying,': 10910, 'woman': 10911, 'wnl': 10912, '.wordnetlemmatizer()\\n>>>': 10913, '[wnl': 10914, '.lemmatize(t)': 10915, \"'woman',\": 10916, \"'pond',\\n'distributing',\": 10917, \"'basis',\": 10918, \"'of',\\n'government',\": 10919, \"'supreme',\": 10920, \"'executive',\": 10921, \"'derives',\": 10922, \"'a',\\n'mandate',\": 10923, \"'farcical',\\n'aquatic',\": 10924, \".']\\n\\n\\n\\nthe\": 10925, 'valid': 10926, 'headwords)': 10927, '.\\n\\nnote\\nanother': 10928, 'normalization': 10929, 'non-standard': 10930, 'words\\nincluding': 10931, 'abbreviations,': 10932, 'dates,': 10933, 'tokens\\nto': 10934, 'decimal': 10935, 'be\\nmapped': 10936, 'acronym': 10937, 'mapped': 10938, 'aaa': 10939, 'keeps': 10940, 'improves': 10941, 'accuracy': 10942, 'many\\nlanguage': 10943, 'modeling': 10944, '.7\\xa0\\xa0\\xa0regular': 10945, 'tokenizing': 10946, 'text\\ntokenization': 10947, 'cutting': 10948, 'into\\nidentifiable': 10949, 'units': 10950, 'constitute': 10951, 'to\\ndelay': 10952, 'tokenized,\\nand': 10953, 'expressions,\\nyou': 10954, 'to\\nhave': 10955, '.\\n\\nsimple': 10956, 'tokenization\\nthe': 10957, \"alice's\": 10958, 'wonderland:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 10959, \"'when\": 10960, \"i'm\": 10961, \"duchess,'\": 10962, 'herself,': 10963, 'hopeful': 10964, 'tone\\n': 10965, 'though),': 10966, \"'i\": 10967, 'pepper': 10968, 'kitchen': 10969, 'soup': 10970, 'very\\n': 10971, 'without--maybe': 10972, \"hot-tempered,'\": 10973, '.\\n\\n\\n\\nwe': 10974, 'match\\nany': 10975, 'results\\nin': 10976, '\\\\n': 10977, 'character;': 10978, 'spaces,': 10979, 'tabs,': 10980, \".split(r'\": 10981, 'raw)': 10982, \"\\n['when,\": 10983, \"i'm,\": 10984, \"duchess,',\": 10985, \"'herself,',\": 10986, \"'(not',\": 10987, \"'in',\\n'a',\": 10988, \"'hopeful',\": 10989, \"'tone\\\\nthough),',\": 10990, \"'i,\": 10991, \"won't,\": 10992, \"'pepper',\\n'in',\": 10993, \"'kitchen',\": 10994, \"'all\": 10995, \"'soup',\": 10996, \"'does',\": 10997, \"'very\\\\nwell',\": 10998, \"'without--maybe',\\nit's,\": 10999, \"'always',\": 11000, \"'pepper',\": 11001, \"'makes',\": 11002, \"'people',\": 11003, \".split(r'[\": 11004, \"\\\\t\\\\n]+',\": 11005, \"'tone',\": 11006, \"'though),',\": 11007, \"'well',\": 11008, '«[': 11009, '\\\\t\\\\n]+»': 11010, 'space,': 11011, '(\\\\t)\\nor': 11012, '(\\\\n)': 11013, 'carriage-return': 11014, 'and\\nform-feed': 11015, 'built-in\\nre': 11016, 'abbreviation,': 11017, '\\\\s,': 11018, 'above\\nstatement': 11019, 'rewritten': 11020, \".split(r'\\\\s+',\": 11021, '.\\n\\nnote\\nimportant:\\nremember': 11022, 'prefix': 11023, 'r\\n(meaning': 11024, 'raw),': 11025, 'instructs': 11026, 'python\\ninterpreter': 11027, 'literally,': 11028, 'than\\nprocessing': 11029, 'backslashed': 11030, '.\\n\\nsplitting': 11031, \"'(not'\": 11032, \"'herself,'\": 11033, 'a\\ncharacter': 11034, '[a-za-z0-9_]': 11035, 'complement': 11036, '\\\\w,': 11037, 'characters\\nother': 11038, 'underscore': 11039, 'simple\\nregular': 11040, 'character:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 11041, \".split(r'\\\\w+',\": 11042, \"raw)\\n['',\": 11043, \"'duchess',\": 11044, \"'herself',\": 11045, \"'won',\": 11046, \"'without',\\n'maybe',\": 11047, \"'tempered',\\n'']\\n\\n\\n\\nobserve\": 11048, 'understand\\nwhy,': 11049, \"'xx'\": 11050, \".split('x'))\": 11051, 'strings,\\nwith': 11052, \".findall(r'\\\\w+',\": 11053, 'extend': 11054, 'expression\\nto': 11055, 'wider': 11056, '«\\\\w+|\\\\s\\\\w*»': 11057, 'any\\nnon-whitespace': 11058, '(\\\\s': 11059, '\\\\s)': 11060, 'by\\nfurther': 11061, 'following\\nletters': 11062, \"'s)\": 11063, 'punctuation\\ncharacters': 11064, \".findall(r'\\\\w+|\\\\s\\\\w*',\": 11065, \"raw)\\n['when,\": 11066, \"'m,\": 11067, \"',',\\n'(not',\": 11068, \"'t,\\n'have',\": 11069, \"'does',\\n'very',\": 11070, \"'-maybe',\": 11071, \"'that',\\n'makes',\": 11072, \"'-tempered',\": 11073, \".']\\n\\n\\n\\nlet's\": 11074, '\\\\w+': 11075, 'hyphens': 11076, 'apostrophes:': 11077, \"«\\\\w+([-']\\\\w+)*»\": 11078, \"[-']\\\\w+;\\nit\": 11079, 'hot-tempered': 11080, '?:': 11081, \".)\\nwe'll\": 11082, 'separate\\nfrom': 11083, 'print(re': 11084, \".findall(r\\\\w+(?:[-']\\\\w+)*|'|[-\": 11085, '.(]+|\\\\s\\\\w*,': 11086, \"raw))\\n[',\": 11087, \"',',\\n'(',\": 11088, \"'i',\\nwon't,\": 11089, \"'soup',\\n'does',\": 11090, \"'--',\": 11091, \"'maybe',\": 11092, \"it's,\": 11093, \"'pepper',\\n'that',\": 11094, \"'hot-tempered',\": 11095, '«[-': 11096, '.(]+»': 11097, 'causes': 11098, 'hyphen,\\nellipsis,': 11099, 'parenthesis': 11100, 'separately': 11101, 'have\\nseen': 11102, '.\\n\\n\\n\\n\\n\\n\\nsymbol\\nfunction\\n\\n\\n\\n\\\\b\\nword': 11103, 'boundary': 11104, '(zero': 11105, 'width)\\n\\n\\\\d\\nany': 11106, '(equivalent': 11107, '[0-9])\\n\\n\\\\d\\nany': 11108, 'non-digit': 11109, '[^0-9])\\n\\n\\\\s\\nany': 11110, '\\\\t\\\\n\\\\r\\\\f\\\\v])\\n\\n\\\\s\\nany': 11111, 'non-whitespace': 11112, '[^': 11113, '\\\\t\\\\n\\\\r\\\\f\\\\v])\\n\\n\\\\w\\nany': 11114, 'alphanumeric': 11115, '[a-za-z0-9_])\\n\\n\\\\w\\nany': 11116, 'non-alphanumeric': 11117, '[^a-za-z0-9_])\\n\\n\\\\t\\nthe': 11118, 'character\\n\\n\\\\n\\nthe': 11119, 'character\\n\\n\\ntable': 11120, \"symbols\\n\\n\\n\\n\\nnltk's\": 11121, 'tokenizer\\nthe': 11122, '.regexp_tokenize()': 11123, \"(as\\nwe've\": 11124, 'tokenization)': 11125, '.regexp_tokenize()\\nis': 11126, 'avoids': 11127, 'treatment': 11128, 'readability': 11129, 'lines\\nand': 11130, 'comment': 11131, '(?x)': 11132, 'verbose': 11133, 'flag\\ntells': 11134, 'embedded': 11135, \"'that\": 11136, 'poster-print': 11137, 'costs': 11138, '$12': 11139, '.40': 11140, \".'\\n>>>\": 11141, \"r'''(?x)\": 11142, 'regexps\\n': 11143, '(?:[a-z]\\\\': 11144, '.)+': 11145, '.\\n': 11146, '\\\\w+(?:-\\\\w+)*': 11147, 'hyphens\\n': 11148, '\\\\$?\\\\d+(?:\\\\': 11149, '.\\\\d+)?%?': 11150, 'currency': 11151, 'percentages,': 11152, '.40,': 11153, '82%\\n': 11154, '.\\\\': 11155, 'ellipsis\\n': 11156, '[][': 11157, \".,;'?():-_`]\": 11158, 'tokens;': 11159, '[\\n': 11160, \"'''\\n>>>\": 11161, '.regexp_tokenize(text,': 11162, \"pattern)\\n['that',\": 11163, \"'u\": 11164, \"'poster-print',\": 11165, \"'costs',\": 11166, \"'$12\": 11167, \".40',\": 11168, \".']\\n\\n\\n\\nwhen\": 11169, 'flag,': 11170, 'match\\na': 11171, '\\\\s': 11172, 'regexp_tokenize()': 11173, 'gaps': 11174, 'gaps\\nbetween': 11175, '.\\n\\nnote\\nwe': 11176, 'tokenizer': 11177, 'a\\nwordlist,': 11178, 'reporting': 11179, 'wordlist,\\nusing': 11180, 'set(tokens)': 11181, '.difference(wordlist)': 11182, 'to\\nlowercase': 11183, '.\\n\\n\\n\\nfurther': 11184, 'issues': 11185, 'tokenization\\ntokenization': 11186, 'turns': 11187, '.\\nno': 11188, 'solution': 11189, 'across-the-board,': 11190, 'we\\nmust': 11191, 'decide': 11192, 'tokenized,': 11193, 'tokenizer\\nwith': 11194, 'high-quality': 11195, 'gold-standard)': 11196, 'corpus\\ncollection': 11197, 'raw\\nwall': 11198, 'journal': 11199, '.treebank_raw': 11200, '.raw())': 11201, '.words())': 11202, 'contractions,': 11203, 'meaning\\nof': 11204, 'normalize': 11205, 'this\\nform': 11206, 'forms:': 11207, 'not)': 11208, 'lookup': 11209, '.8\\xa0\\xa0\\xa0segmentation\\nthis': 11210, 'discusses': 11211, '.\\ntokenization': 11212, 'segmentation': 11213, 'problem,\\nwhich': 11214, 'radically': 11215, 'far\\nin': 11216, '.\\n\\nsentence': 11217, 'segmentation\\nmanipulating': 11218, 'presupposes\\nthe': 11219, 'divide': 11220, 'have\\nseen,': 11221, 'per\\nsentence': 11222, 'len(nltk': 11223, '.sents())\\n20': 11224, '.250994070456922\\n\\n\\n\\nin': 11225, 'before\\ntokenizing': 11226, 'segment': 11227, 'facilitates\\nthis': 11228, 'punkt': 11229, 'segmenter': 11230, '(kiss': 11231, 'strunk,': 11232, 'segmenting': 11233, \"segmenter's\": 11234, 'output):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 11235, \".raw('chesterton-thursday\": 11236, '.sent_tokenize(text)\\n>>>': 11237, 'pprint': 11238, \".pprint(sents[79:89])\\n['nonsense!',\\n\": 11239, \"'said\": 11240, 'gregory,': 11241, 'rational': 11242, 'anyone': 11243, 'else\\\\nattempted': 11244, 'paradox': 11245, \".',\\n\": 11246, \"'why\": 11247, 'clerks': 11248, 'navvies': 11249, \"the\\\\n'\\n\": 11250, \"'railway\": 11251, 'trains': 11252, 'sad': 11253, 'tired,': 11254, \"tired?',\\n\": 11255, 'will\\\\ntell': 11256, \"'it\": 11257, \"'it\\\\n'\\n\": 11258, \"ticket\\\\n'\\n\": 11259, \"'for\": 11260, 'reach': 11261, \"have\\\\n'\\n\": 11262, \"'passed\": 11263, 'sloane': 11264, 'station': 11265, \"be\\\\n'\\n\": 11266, \"'victoria,\": 11267, 'victoria': 11268, \"'oh,\": 11269, 'wild': 11270, \"rapture!',\\n\": 11271, \"'oh,\\\\n'\\n\": 11272, \"'their\": 11273, 'eyes': 11274, 'stars': 11275, 'souls': 11276, 'eden,': 11277, \"next\\\\n'\\n\": 11278, \"'station\": 11279, 'unaccountably': 11280, \"street!',\\n\": 11281, 'unpoetical,': 11282, 'replied': 11283, 'poet': 11284, 'syme': 11285, 'mr': 11286, 'lucian': 11287, 'gregory': 11288, 'individual\\nstrings': 11289, '.\\nsentence': 11290, 'abbreviations,\\nand': 11291, 'periods': 11292, 'simultaneously': 11293, 'abbreviation': 11294, 'terminate': 11295, 'sentence,\\nas': 11296, 'acronyms': 11297, 'segmentation,': 11298, '.\\n\\n\\nword': 11299, 'segmentation\\nfor': 11300, 'there\\nis': 11301, 'visual': 11302, 'three-character': 11303, 'string:': 11304, '爱国人\\n(ai4': 11305, 'love': 11306, 'guo2': 11307, 'country,': 11308, 'ren2': 11309, 'person)': 11310, 'could\\nbe': 11311, '爱国': 11312, '人,': 11313, 'country-loving': 11314, 'person\\nor': 11315, '爱': 11316, '国人,': 11317, 'country-person': 11318, 'arises': 11319, 'the\\nhearer': 11320, 'particularly': 11321, 'challenging': 11322, \"don't\\nknow\": 11323, 'advance': 11324, 'learner,\\nsuch': 11325, 'hearing': 11326, 'utterances': 11327, 'parent': 11328, 'removed:\\n\\n': 11329, '.doyouseethekitty\\n\\n': 11330, '.seethedoggy\\n\\n': 11331, '.doyoulikethekitty\\n\\n': 11332, '.likethedoggy\\n\\nour': 11333, 'problem:': 11334, 'find\\na': 11335, 'by\\nannotating': 11336, 'or\\nnot': 11337, 'word-break': 11338, '(an': 11339, 'used\\nheavily': 11340, '.)': 11341, 'learner': 11342, 'utterance': 11343, 'breaks,\\nsince': 11344, 'pauses': 11345, 'representation,\\nincluding': 11346, 'segmentations:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 11347, 'doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\\n>>>': 11348, 'seg1': 11349, '0000000000000001000000000010000000000000000100000000000\\n>>>': 11350, 'seg2': 11351, '0100100100100001001001000010100100010010000100010010000\\n\\n\\n\\nobserve': 11352, 'zeros': 11353, 'they\\nare': 11354, 'shorter': 11355, 'length\\nn': 11356, 'broken': 11357, 'places': 11358, 'segment()': 11359, 'can\\nget': 11360, 'segmented': 11361, 'segment(text,': 11362, 'segs):\\n': 11363, '[]\\n': 11364, '0\\n': 11365, 'range(len(segs)):\\n': 11366, 'segs[i]': 11367, \"'1':\\n\": 11368, '.append(text[last:i+1])\\n': 11369, 'i+1\\n': 11370, '.append(text[last:])\\n': 11371, 'words\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 11372, '0100100100100001001001000010100100010010000100010010000\\n>>>': 11373, \"seg1)\\n['doyouseethekitty',\": 11374, \"'seethedoggy',\": 11375, \"'doyoulikethekitty',\": 11376, \"'likethedoggy']\\n>>>\": 11377, \"seg2)\\n['do',\": 11378, \"'see',\": 11379, \"'kitty',\": 11380, \"'doggy',\": 11381, \"'you',\\n'like',\": 11382, \"'doggy']\\n\\n\\nexample\": 11383, '(code_segment': 11384, '.7:': 11385, 'reconstruct': 11386, 'representation:\\nseg1': 11387, 'final\\nsegmentations': 11388, 'hypothetical': 11389, 'child-directed': 11390, 'speech;\\nthe': 11391, 'reproduce': 11392, 'the\\nsegmented': 11393, '.\\n\\nnow': 11394, 'causes\\nthe': 11395, 'acquiring': 11396, '.\\ngiven': 11397, '(brent,': 11398, '1995),': 11399, 'objective': 11400, 'function,\\na': 11401, 'scoring': 11402, 'optimize,': 11403, '(number': 11404, 'extra\\ndelimiter': 11405, 'to\\nreconstruct': 11406, '.8': 11407, '.8:': 11408, 'calculation': 11409, 'segmentation\\nof': 11410, '(on': 11411, 'left),': 11412, 'derivation\\ntable': 11413, 'reconstructed,': 11414, 'total\\nup': 11415, '(including': 11416, 'boundary\\nmarker)': 11417, 'derivation,': 11418, 'quality': 11419, 'segmentation;': 11420, 'smaller': 11421, 'implement': 11422, 'evaluate(text,': 11423, 'segs)\\n': 11424, 'text_size': 11425, 'len(words)\\n': 11426, 'lexicon_size': 11427, 'sum(len(word)': 11428, 'set(words))\\n': 11429, 'lexicon_size\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 11430, 'seg3': 11431, '0000100100000011001000000110000100010000001100010000001\\n>>>': 11432, \"seg3)\\n['doyou',\": 11433, \"'thekitt',\": 11434, \"'thedogg',\": 11435, \"'doyou',\": 11436, \"'like',\\n\": 11437, \"'y']\\n>>>\": 11438, 'seg3)\\n47\\n>>>': 11439, 'seg2)\\n48\\n>>>': 11440, 'seg1)\\n64\\n\\n\\nexample': 11441, '(code_evaluate': 11442, '.9:': 11443, 'cost': 11444, 'reconstructing': 11445, 'text\\n\\nthe': 11446, 'minimizes': 11447, 'objective\\nfunction,': 11448, '.10': 11449, 'like\\nthekitty,': 11450, '.\\n\\n\\n\\n\\n\\xa0\\n\\nfrom': 11451, 'randint\\n\\ndef': 11452, 'flip(segs,': 11453, 'pos):\\n': 11454, 'segs[:pos]': 11455, 'str(1-int(segs[pos]))': 11456, 'segs[pos+1:]\\n\\ndef': 11457, 'flip_n(segs,': 11458, 'n):\\n': 11459, 'range(n):\\n': 11460, 'segs': 11461, 'randint(0,': 11462, 'len(segs)-1))\\n': 11463, 'segs\\n\\ndef': 11464, 'anneal(text,': 11465, 'segs,': 11466, 'iterations,': 11467, 'cooling_rate):\\n': 11468, 'temperature': 11469, 'float(len(segs))\\n': 11470, '.5:\\n': 11471, 'best_segs,': 11472, 'range(iterations):\\n': 11473, 'round(temperature))\\n': 11474, 'guess)\\n': 11475, 'best:\\n': 11476, 'best,': 11477, 'best_segs': 11478, 'score,': 11479, 'guess\\n': 11480, 'best_segs\\n': 11481, 'cooling_rate\\n': 11482, 'print(evaluate(text,': 11483, 'segs),': 11484, 'segs))\\n': 11485, 'print()\\n': 11486, 'segs\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 11487, 'seg1,': 11488, '5000,': 11489, '.2)\\n61': 11490, \"['doyouseetheki',\": 11491, \"'tty',\": 11492, \"'thedoggy',\": 11493, \"'doyouliketh',\": 11494, \"'ekittylike',\": 11495, \"'thedoggy']\\n59\": 11496, \"['doy',\": 11497, \"'ouseetheki',\": 11498, \"'ttysee',\": 11499, \"'doy',\": 11500, \"'ulikethekittylike',\": 11501, \"'thedoggy']\\n57\": 11502, \"['doyou',\": 11503, \"'seetheki',\": 11504, \"'liketh',\": 11505, \"'thedoggy']\\n55\": 11506, \"'seethekit',\": 11507, \"'tysee',\": 11508, \"'likethekittylike',\": 11509, \"'thedoggy']\\n54\": 11510, \"'thekitty',\": 11511, \"'thedoggy']\\n52\": 11512, \"'seethekittysee',\": 11513, \"'thedoggy']\\n43\": 11514, \"'thedoggy']\\n'0000100100000001001000000010000100010000000100010000000'\\n\\n\\nexample\": 11515, '(code_anneal': 11516, '.10:': 11517, 'non-deterministic': 11518, 'simulated': 11519, 'annealing:': 11520, 'searching\\nwith': 11521, 'segmentations': 11522, 'only;': 11523, 'perturb': 11524, 'ones\\nproportional': 11525, 'temperature;': 11526, 'iteration': 11527, 'temperature\\nis': 11528, 'lowered': 11529, 'perturbation': 11530, 'reduced': 11531, 'this\\nsearch': 11532, 'algorithm': 11533, 'non-deterministic,': 11534, 'different\\nresult': 11535, '.\\n\\nwith': 11536, 'reasonable\\ndegree': 11537, \"that\\ndon't\": 11538, '.9\\xa0\\xa0\\xa0formatting:': 11539, 'strings\\noften': 11540, 'meets': 11541, 'criterion,': 11542, 'summary': 11543, 'statistic\\nsuch': 11544, 'word-count': 11545, 'tagger': 11546, 'program\\nto': 11547, 'result;': 11548, 'forms,\\nor': 11549, 'reformatting': 11550, 'linguistic,\\ntextual': 11551, 'numerical,\\nit': 11552, 'preferable': 11553, 'about\\na': 11554, '.\\n\\nfrom': 11555, 'strings\\nthe': 11556, 'convert\\nthese': 11557, 'join()': 11558, 'method,': 11559, 'specify\\nthe': 11560, 'silly': 11561, \"['we',\": 11562, \"'called',\": 11563, \"'tortoise',\": 11564, \"'taught',\": 11565, \"'us',\": 11566, \".join(silly)\\n'we\": 11567, 'him': 11568, \"';'\": 11569, \".join(silly)\\n'we;called;him;tortoise;because;he;taught;us;\": 11570, \".join(silly)\\n'wecalledhimtortoisebecausehetaughtus\": 11571, \".'\\n\\n\\n\\nso\": 11572, '.join(silly)': 11573, 'means:': 11574, 'and\\nconcatenate': 11575, 'spacer': 11576, '(many': 11577, 'counter-intuitive': 11578, 'calling': 11579, 'text\\n—': 11580, 'enjoys': 11581, 'privileges': 11582, '.\\n\\n\\nstrings': 11583, 'formats\\nwe': 11584, 'object:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 11585, 'hello\\n': 11586, 'world\\n>>>': 11587, 'print(word)\\ncat\\n>>>': 11588, 'print(sentence)\\nhello\\nworld\\n>>>': 11589, \"word\\n'cat'\\n>>>\": 11590, \"sentence\\n'hello\\\\nworld'\\n\\n\\n\\nthe\": 11591, 'attempt': 11592, 'human-readable': 11593, 'naming': 11594, 'string\\nthat': 11595, 'recreate': 11596, 'benefit': 11597, 'give\\nus': 11598, 'clue': 11599, 'of\\ncharacters': 11600, 'reader,': 11601, 'because\\nwe': 11602, 'export': 11603, 'use\\nin': 11604, '.\\nformatted': 11605, 'and\\npre-specified': 11606, 'fdist\\nwe': 11607, 'do:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 11608, \".freqdist(['dog',\": 11609, \"'cat',\": 11610, \"'dog',\": 11611, \"'snake',\": 11612, \"'cat'])\\n>>>\": 11613, 'sorted(fdist):\\n': 11614, \"'->',\": 11615, 'fdist[word],': 11616, \"end=';\": 11617, \"')\\ncat\": 11618, '->': 11619, '4;': 11620, 'snake': 11621, '1;\\n\\n\\n\\nprint': 11622, 'alternating': 11623, 'constants': 11624, 'and\\nmaintain': 11625, 'formatting': 11626, \"print('{}->{};'\": 11627, '.format(word,': 11628, 'fdist[word]),': 11629, \"')\\ncat->3;\": 11630, 'dog->4;': 11631, 'snake->1;\\n\\n\\n\\nto': 11632, 'the\\nformat': 11633, '(by': 11634, 'be\\nyour': 11635, 'usual': 11636, \"'{}->{};'\": 11637, '.format': 11638, \"('cat',\": 11639, \"3)\\n'cat->3;'\\n\\n\\n\\nthe\": 11640, 'curly': 11641, \"'{}'\": 11642, 'replacement\\nfield:': 11643, 'a\\nplaceholder': 11644, 'are\\npassed': 11645, '.format()': 11646, 'embed': 11647, \"'{}'\\ninside\": 11648, 'format()': 11649, 'with\\nappropriate': 11650, 'containing\\nreplacement': 11651, 'unpack': 11652, 'this\\nbehavior': 11653, 'close:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 11654, \"'{}->'\": 11655, \".format('cat')\\n'cat->'\\n>>>\": 11656, \".format(3)\\n'3'\\n>>>\": 11657, '{}': 11658, \"now'\": 11659, \".format('coffee')\\n'i\": 11660, 'coffee': 11661, \"now'\\n\\n\\n\\nwe\": 11662, 'placeholders,': 11663, 'method\\nmust': 11664, \"'{}\": 11665, \"('lee',\": 11666, \"'sandwich',\": 11667, \"lunch')\\n'lee\": 11668, 'sandwich': 11669, \"lunch'\\n>>>\": 11670, \"('sandwich',\": 11671, \"lunch')\\ntraceback\": 11672, \"lunch')\\nindexerror:\": 11673, 'range\\n\\n\\n\\narguments': 11674, 'consumed': 11675, 'any\\nsuperfluous': 11676, '.\\n\\nsystem': 11677, '(ch03': 11678, '2262)\\nunexpected': 11679, \"sandwich'\\n\\n\\n\\nthe\": 11680, 'which\\nrefers': 11681, \"like\\n'from\": 11682, \"{}'\\nis\": 11683, \"'from\": 11684, '{0}': 11685, \"{1}',\": 11686, 'get\\nnon-default': 11687, 'orders:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 11688, '{1}': 11689, \"{0}'\": 11690, \".format('a',\": 11691, \"'b')\\n'from\": 11692, \"a'\\n\\n\\n\\nwe\": 11693, 'indirectly': 11694, \"here's\\nan\": 11695, \"'lee\": 11696, \"now'\\n>>>\": 11697, \"['sandwich',\": 11698, \"'spam\": 11699, \"fritter',\": 11700, \"'pancake']\\n>>>\": 11701, 'snack': 11702, 'menu:\\n': 11703, 'print(template': 11704, '.format(snack))\\n': 11705, '.\\nlee': 11706, 'now\\nlee': 11707, 'spam': 11708, 'fritter': 11709, 'pancake': 11710, 'now\\n\\n\\n\\n\\n\\nlining': 11711, 'up\\n\\nso': 11712, 'generated': 11713, 'width\\non': 11714, 'screen)': 11715, 'padding': 11716, 'given\\nwidth': 11717, \"':'\": 11718, '{:6}\\nspecifies': 11719, 'is\\npadded': 11720, 'right-justified': 11721, ',\\nbut': 11722, 'specifier': 11723, \"'<'\": 11724, 'option': 11725, 'left-justified': 11726, \"'{:6}'\": 11727, '.format(41)': 11728, \"\\n'\": 11729, \"41'\\n>>>\": 11730, \"'{:<6}'\": 11731, \"\\n'41\": 11732, \"'\\n\\n\\n\\nstrings\": 11733, \"'>'\": 11734, '2310)\\nunexpected': 11735, \".format('dog')\": 11736, \"\\n'dog\": 11737, \"'{:>6}'\": 11738, \"dog'\\n\\n\\n\\nother\": 11739, 'precision\\nof': 11740, 'numbers;': 11741, '{:': 11742, '.4f}': 11743, 'four\\ndigits': 11744, 'floating\\npoint': 11745, 'math\\n>>>': 11746, \"'{:\": 11747, \".4f}'\": 11748, '.format(math': 11749, \".pi)\\n'3\": 11750, \".1416'\\n\\n\\n\\nthe\": 11751, 'smart': 11752, \"a\\n'%'\": 11753, 'specification,': 11754, 'percentage;': 11755, 'count,': 11756, '3205,': 11757, '9375\\n>>>': 11758, '.4%}': 11759, '.format(total,': 11760, \"total)\\n'accuracy\": 11761, '9375': 11762, \".1867%'\\n\\n\\n\\nan\": 11763, 'saw\\ndata': 11764, 'tabulated': 11765, 'ourselves,': 11766, 'exercising': 11767, 'control\\nof': 11768, 'headings': 11769, 'column': 11770, 'widths,': 11771, '.11': 11772, 'separation': 11773, 'work,\\nand': 11774, 'tabulate(cfdist,': 11775, 'categories):\\n': 11776, \"print('{:16}'\": 11777, \".format('category'),\": 11778, \"')\": 11779, 'headings\\n': 11780, 'words:\\n': 11781, \"print('{:>6}'\": 11782, '.format(word),': 11783, 'categories:\\n': 11784, '.format(category),': 11785, 'heading\\n': 11786, \"print('{:6}'\": 11787, '.format(cfdist[category][word]),': 11788, 'cell\\n': 11789, 'print()': 11790, 'row\\n\\n>>>': 11791, 'tabulate(cfd,': 11792, 'modals,': 11793, 'genres)\\ncategory': 11794, 'will\\nnews': 11795, '389\\nreligion': 11796, '71\\nhobbies': 11797, '16\\nromance': 11798, '43\\nhumor': 11799, '13\\n\\n\\nexample': 11800, '(code_modal_tabulate': 11801, '.11:': 11802, 'corpus\\n\\nrecall': 11803, 'listing': 11804, \"string\\n'{:{width}}'\": 11805, 'bound': 11806, 'in\\nformat()': 11807, \".format('monty\": 11808, \"python',\": 11809, \"width=15)\\n'monty\": 11810, 'customize': 11811, 'be\\njust': 11812, 'accommodate': 11813, 'using\\nwidth': 11814, 'max(len(w)': 11815, '.\\n\\n\\nwriting': 11816, 'file\\nwe': 11817, 'following\\ncode': 11818, 'opens': 11819, 'writing,': 11820, 'saves': 11821, 'program\\noutput': 11822, 'output_file': 11823, \"open('output\": 11824, \"'w')\\n>>>\": 11825, 'set(nltk': 11826, 'sorted(words):\\n': 11827, 'file=output_file)\\n\\n\\n\\nwhen': 11828, 'non-text': 11829, 'len(words)\\n2789\\n>>>': 11830, \"str(len(words))\\n'2789'\\n>>>\": 11831, 'print(str(len(words)),': 11832, 'file=output_file)\\n\\n\\n\\n\\ncaution!\\nyou': 11833, 'like\\noutput': 11834, 'identical': 11835, 'case\\ndistinctions,': 11836, '.\\n\\n\\n\\ntext': 11837, 'wrapping\\nwhen': 11838, 'text-like,': 11839, 'tabular,\\nit': 11840, 'wrap': 11841, 'displayed\\nconveniently': 11842, 'overflows': 11843, 'line,\\nand': 11844, \"'done',\": 11845, \"',',\\n\": 11846, 'saying:\\n': 11847, \"'('\": 11848, 'str(len(word))': 11849, \"'),',\": 11850, \"')\\nafter\": 11851, '(5),': 11852, '(3),': 11853, '(2),': 11854, '(4),': 11855, '(1),\\n\\n\\n\\nwe': 11856, 'textwrap': 11857, 'clarity': 11858, 'onto': 11859, 'line:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 11860, 'fill\\n>>>': 11861, '[{}': 11862, 'saying]\\n>>>': 11863, '.join(pieces)\\n>>>': 11864, 'wrapped': 11865, 'fill(output)\\n>>>': 11866, 'print(wrapped)\\nafter': 11867, 'more\\n(4),': 11868, '(1),\\n\\n\\n\\nnotice': 11869, 'linebreak': 11870, 'could\\nredefine': 11871, 'spaces,\\ne': 11872, \"'%s_(%d),',\": 11873, 'wrapped,\\nwe': 11874, \".replace('_',\": 11875, '.10\\xa0\\xa0\\xa0summary\\n\\nin': 11876, 'potentially\\nlong': 11877, 'formatting,': 11878, 'we\\ntypically': 11879, 'quotes:': 11880, 'indexes,': 11881, \"zero:\\n'monty\": 11882, \"python'[0]\": 11883, 'is\\nfound': 11884, '.\\nsubstrings': 11885, 'notation:': 11886, \"python'[1:5]\\ngives\": 11887, 'onty': 11888, 'omitted,': 11889, 'the\\nsubstring': 11890, 'string;': 11891, 'omitted,\\nthe': 11892, '.\\nstrings': 11893, 'lists:': 11894, \"gives\\n['monty',\": 11895, \"strings:\\n'/'\": 11896, \"'python'])\": 11897, \"'monty/python'\": 11898, \"open('input\": 11899, \".decode('utf8')\": 11900, 'open(f)': 11901, 'writing\\noutput_file': 11902, \"'w'),\": 11903, 'file=output_file)': 11904, '.\\ntexts': 11905, 'headers,': 11906, 'footers,': 11907, 'markup),\\nthat': 11908, '—\\nsuch': 11909, 'inadequate': 11910, 'it\\nbundles': 11911, '.word_tokenize()': 11912, '.\\nlemmatization': 11913, 'maps': 11914, 'appeared,': 11915, 'appears)\\nto': 11916, 'canonical': 11917, 'citation': 11918, 'lexeme': 11919, 'appear)': 11920, 'specifying\\npatterns': 11921, 'use\\nre': 11922, 'backslash,': 11923, 'to\\npreprocess': 11924, 'prefix:': 11925, \"r'regexp'\": 11926, '\\\\n,': 11927, 'on\\na': 11928, '(newline': 11929, 'character);': 11930, 'used\\nbefore': 11931, '\\\\|,': 11932, '\\\\$,\\nthese': 11933, 'lose': 11934, '%': 11935, 'arg_tuple': 11936, 'a\\nformat': 11937, 'specifiers\\nlike': 11938, '%-6s': 11939, '%0': 11940, '.2d': 11941, '.11\\xa0\\xa0\\xa0further': 11942, 'materials\\nat': 11943, '(for': 11944, 'support,\\nexplaining': 11945, 'various\\noperating': 11946, '.)\\nfor': 11947, 'the\\ntokenization,': 11948, 'howtos': 11949, '.\\nchapters': 11950, 'advanced\\nmaterial': 11951, 'morphology': 11952, 'extensive\\ndiscussion': 11953, '(mertz,': 11954, '(sproat': 11955, '2001)\\nthere': 11956, 'references': 11957, 'and\\ntheoretical': 11958, 'introductory\\ntutorial': 11959, 'python,\\nsee': 11960, \"kuchling's\": 11961, 'howto,\\nhttp://www': 11962, '.amk': 11963, '.ca/python/howto/regex/': 11964, 'manual\\nin': 11965, 'major\\nprogramming': 11966, '(friedl,': 11967, '2002)': 11968, 'presentations': 11969, '2008),\\nand': 11970, 'discussions\\nof': 11971, 'facilities': 11972, 'handling': 11973, 'are:\\n\\nned': 11974, 'batchelder,': 11975, 'http://nedbatchelder': 11976, '.com/text/unipain': 11977, '.html\\nunicode': 11978, 'documentation,\\nhttp://docs': 11979, '.org/3/howto/unicode': 11980, '.html\\ndavid': 11981, 'beazley,': 11982, 'i/o,\\nhttp://pyvideo': 11983, '.org/video/289/pycon-2010--mastering-python-3-i-o\\njoel': 11984, 'spolsky,': 11985, 'absolute': 11986, 'minimum': 11987, 'developer\\nabsolutely,': 11988, 'positively': 11989, 'sets\\n(no': 11990, 'excuses!),': 11991, '.joelonsoftware': 11992, '.com/articles/unicode': 11993, '.html\\n\\nthe': 11994, 'chinese': 11995, 'sighan,\\nthe': 11996, 'processing\\nhttp://sighan': 11997, 'text\\nfollows': 11998, '1995);': 11999, 'acquisition': 12000, '(niyogi,': 12001, '.\\ncollocations': 12002, 'multiword': 12003, 'meaning\\nand': 12004, 'alone,\\ne': 12005, '(baldwin': 12006, 'kim,': 12007, '.\\nsimulated': 12008, 'annealing': 12009, 'heuristic': 12010, 'finding\\na': 12011, 'approximation': 12012, 'optimum': 12013, 'discrete': 12014, 'space,\\nbased': 12015, 'analogy': 12016, 'metallurgy': 12017, 'technique': 12018, 'discovering': 12019, 'search\\npatterns': 12020, '(hearst,': 12021, '1992)': 12022, '.12\\xa0\\xa0\\xa0exercises\\n\\n☼': 12023, \"'colorless'\": 12024, 'statement\\nthat': 12025, 'colourless': 12026, 'and\\nconcatenation': 12027, 'morphological': 12028, 'on\\nwords': 12029, \"'dogs'[:-1]\": 12030, 'of\\ndogs,': 12031, 'leaving': 12032, 'the\\naffixes': 12033, \"(we've\": 12034, 'inserted': 12035, 'affix': 12036, 'boundary,': 12037, 'strings):\\ndish-es,': 12038, 'run-ning,': 12039, 'nation-ality,': 12040, 'un-do,\\npre-heat': 12041, 'string?\\n\\n☼': 12042, 'following\\nreturns': 12043, 'slice:': 12044, 'monty[6:11:2]': 12045, 'direction:': 12046, 'monty[10:5:-2]\\ntry': 12047, 'monty[::-1]?\\nexplain': 12048, 'regular\\nexpressions': 12049, '.\\n\\n[a-za-z]+\\n[a-z][a-z]*\\np[aeiou]{,2}t\\n\\\\d+(\\\\': 12050, '.\\\\d+)?\\n([^aeiou][aeiou][^aeiou])*\\n\\\\w+|[^\\\\w\\\\s]+\\n\\ntest': 12051, '.re_show()': 12052, 'strings:\\n\\n\\na': 12053, 'determiner': 12054, '(assume': 12055, 'an,': 12056, 'the\\nare': 12057, 'determiners)': 12058, 'arithmetic': 12059, 'integers,': 12060, 'and\\nmultiplication,': 12061, '2*3+8': 12062, '.\\n\\n\\n\\n☼': 12063, 'returns\\nthe': 12064, 'url,': 12065, 'request\\nand': 12066, \".urlopen('http://nltk\": 12067, \".org/')\": 12068, '.\\n\\n☼\\nsave': 12069, 'load(f)\\nthat': 12070, 'sole': 12071, 'string\\ncontaining': 12072, 'tokenizes\\nthe': 12073, 'multi-line\\nregular': 12074, 'comments,': 12075, 'monetary': 12076, 'amounts;': 12077, 'dates;': 12078, 'names\\nof': 12079, 'organizations': 12080, 'rewrite': 12081, 'comprehension:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 12082, \"'gave',\": 12083, \"'john',\": 12084, \"'newspaper']\\n>>>\": 12085, '[]\\n>>>': 12086, 'word_len': 12087, '.append(word_len)\\n>>>': 12088, \"result\\n[('the',\": 12089, '3),': 12090, \"('dog',\": 12091, \"('gave',\": 12092, \"('john',\": 12093, \"('newspaper',\": 12094, '9)]\\n\\n\\n\\n\\n☼': 12095, 'choosing': 12096, \"'s'\": 12097, 'string\\nwith': 12098, 'argument,\\ne': 12099, 'versus': 12100, \".split('\": 12101, \"')?\": 12102, 'happens\\nwhen': 12103, 'consecutive\\nspace': 12104, 'tabs': 12105, 'spaces?': 12106, 'you\\nwill': 12107, \"'\\\\t'\": 12108, '.)\\n\\n☼': 12109, '.\\nexperiment': 12110, '.sort()': 12111, 'sorted(words)': 12112, 'difference?\\n\\n☼': 12113, 'integers': 12114, 'prompt:': 12115, 'converting': 12116, 'using\\nint(3)': 12117, 'str(3)': 12118, 'file\\ncalled': 12119, 'prog': 12120, 'following\\n(note': 12121, 'filename):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 12122, 'monty\\n>>>': 12123, 'monty\\n\\n\\n\\nthis': 12124, 'try\\nimport': 12125, 'prog,': 12126, 'to\\nevaluate': 12127, '.monty': 12128, '%6s': 12129, '%-6s\\nare': 12130, 'characters?\\n\\n◑': 12131, 'of\\nall': 12132, 'wh-word': 12133, '(wh-words': 12134, 'english\\nare': 12135, 'clauses': 12136, 'exclamations:\\nwho,': 12137, 'which,': 12138, 'what,': 12139, 'print\\nthem': 12140, 'duplicated': 12141, 'punctuation?\\n\\n◑': 12142, '(made': 12143, 'up)': 12144, 'frequencies,': 12145, 'each\\nline': 12146, 'integer,\\ne': 12147, 'fuzzy': 12148, 'open(filename)': 12149, '.readlines()': 12150, 'split(),': 12151, 'and\\nconvert': 12152, 'int()': 12153, \"[['fuzzy',\": 12154, '53],': 12155, 'webpage': 12156, 'weather': 12157, 'forecast': 12158, 'top\\ntemperature': 12159, 'town': 12160, 'city': 12161, 'today': 12162, 'unknown()': 12163, 'argument,\\nand': 12164, 'unknown': 12165, 'letters\\n(using': 12166, '.findall())': 12167, 'occur\\nin': 12168, '.words)': 12169, 'categorize': 12170, 'words\\nmanually': 12171, 'url\\nhttp://news': 12172, '.uk/': 12173, 'suggested\\nabove': 12174, 'of\\nnon-textual': 12175, 'there,': 12176, 'javascript': 12177, 'may\\nalso': 12178, 'properly\\npreserved': 12179, 'improve': 12180, 'the\\nextraction': 12181, 'such\\na': 12182, \"n't?\\nexplain\": 12183, 'work:': 12184, \"«n't|\\\\w+»\": 12185, 'hack3r,': 12186, 'expressions\\nand': 12187, 'substitution,': 12188, 'where\\ne': 12189, '→': 12190, '3,\\ni': 12191, '1,\\no': 12192, '0,\\nl': 12193, '|,\\ns': 12194, '5,\\n': 12195, '5w33t!,\\nate': 12196, '.\\nnormalize': 12197, '.\\nadd': 12198, 'map\\ns': 12199, 'values:': 12200, 'word-initial': 12201, 's,\\nand': 12202, 'pig': 12203, 'transformation': 12204, 'follows:': 12205, 'consonant': 12206, 'cluster)\\nthat': 12207, 'end,\\nthen': 12208, 'ay,': 12209, 'ingstray,\\nidle': 12210, 'idleay': 12211, '.org/wiki/pig_latin\\n\\nwrite': 12212, '.\\nextend': 12213, 'preserve': 12214, 'qu': 12215, 'together\\n(i': 12216, 'ietquay),': 12217, 'y\\nis': 12218, 'yellow)': 12219, 'style)': 12220, '.\\n\\n\\n◑': 12221, 'harmony': 12222, 'hungarian),\\nextract': 12223, 'bigram': 12224, 'choice()': 12225, 'which\\nrandomly': 12226, 'chooses': 12227, 'choice(aehh': 12228, ')': 12229, 'will\\nproduce': 12230, 'being\\ntwice': 12231, 'expression\\nthat': 12232, 'the\\nstring': 12233, 'aehh': 12234, 'this\\nexpression': 12235, 'concatenate\\nthem': 12236, 'like\\nuncontrolled': 12237, 'sneezing': 12238, 'maniacal': 12239, 'laughter:': 12240, 'haha': 12241, 'ee': 12242, 'heheeh': 12243, 'eha': 12244, 'numeric': 12245, 'medline': 12246, 'cortisol': 12247, 'fractions': 12248, 'these\\nsera': 12249, '.53': 12250, '+/-': 12251, '.15%': 12252, '.16': 12253, '.23%,': 12254, 'respectively': 12255, '.\\nshould': 12256, 'three\\nwords?': 12257, 'compound': 12258, 'word?': 12259, 'should\\nwe': 12260, 'point\\nfive': 12261, 'three,': 12262, 'fifteen': 12263, 'percent?': 12264, \"that\\nit's\": 12265, \"wouldn't\": 12266, 'dictionary?\\ndiscuss': 12267, 'possibilities': 12268, 'domains\\nthat': 12269, 'motivate': 12270, 'answers?\\n\\n◑': 12271, 'a\\ntext,': 12272, 'purposes': 12273, 'selecting': 12274, 'difficulty\\nfor': 12275, 'learners': 12276, 'let': 12277, 'define\\nμw': 12278, 'and\\nμs': 12279, '(ari)': 12280, 'text\\nis': 12281, 'be:\\n4': 12282, '.71': 12283, 'μw': 12284, '.5': 12285, 'μs': 12286, '.43': 12287, 'ari': 12288, 'including\\nsection': 12289, '(lore)': 12290, '(learned)': 12291, 'that\\nnltk': 12292, 'while\\nnltk': 12293, '.sents()': 12294, 'calling\\nthe': 12295, 'stemmer\\nand': 12296, \"list\\n['after',\": 12297, \"'more',\\n'is',\": 12298, \".']\": 12299, 'the\\nlength': 12300, 'hint:': 12301, 'the\\nempty': 12302, 'lengths,': 12303, '[]': 12304, 'time\\nthrough': 12305, \"string:\\n'newly\": 12306, 'formed': 12307, 'bland': 12308, 'inexpressible': 12309, \"infuriating\\nway'\": 12310, 'legitimate': 12311, 'interpretation': 12312, 'that\\nbilingual': 12313, 'english-spanish': 12314, 'speakers': 12315, \"chomsky's\\nfamous\": 12316, 'nonsense': 12317, 'phrase,': 12318, 'colorless': 12319, 'furiously\\naccording': 12320, 'wikipedia)': 12321, 'tasks:\\n\\nsplit': 12322, 'per\\nword,': 12323, 'save\\nthis': 12324, '.\\nextract': 12325, 'join\\nthem': 12326, \"'eoldrnnnna'\": 12327, '.\\ncombine': 12328, '.\\nmake': 12329, 'with\\nwhitespace': 12330, 'index()': 12331, \"'inexpressible'\": 12332, \".index('e')\": 12333, 'substring,': 12334, \".index('re')?\\ndefine\": 12335, '.index()\\nto': 12336, 'exercise': 12337, 'to\\nbuild': 12338, 'not\\nincluding)': 12339, 'nationality': 12340, 'canadian': 12341, 'and\\naustralian': 12342, 'canada': 12343, 'australia\\n(see': 12344, '.org/wiki/list_of_adjectival_forms_of_place_names)': 12345, 'can\\nand': 12346, 'can,': 12347, 'this\\nphenomenon': 12348, 'method\\nfor': 12349, '.\\nhttp://itre': 12350, '.edu/~myl/languagelog/archives/002733': 12351, '.html\\n\\n◑': 12352, 'lolcat': 12353, 'genesis,\\naccessible': 12354, \".words('lolcat\": 12355, 'lolspeak': 12356, '.lolcatbible': 12357, '.com/index': 12358, '.php?title=how_to_speak_lolcat': 12359, 'into\\ncorresponding': 12360, '.sub()': 12361, 'substitution\\nusing': 12362, 'help(re': 12363, '.sub)': 12364, 'consulting\\nthe': 12365, '.sub': 12366, 'code\\nto': 12367, 'tags': 12368, '.\\n\\n★': 12369, 'been\\nsplit': 12370, 'line-break': 12371, 'split,': 12372, 'long-\\\\nterm': 12373, '.\\n\\nwrite': 12374, 'identifies': 12375, 'are\\nhyphenated': 12376, 'the\\n\\\\n': 12377, 'remain': 12378, 'hyphenated\\nonce': 12379, 'removed,': 12380, \"'encyclo-\\\\npedia'?x\\n\\n\\n★\": 12381, 'soundex': 12382, 'this\\nalgorithm': 12383, 'respective\\nreading': 12384, '.\\ne': 12385, 'rural': 12386, '.abc)': 12387, 'comprehension:\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 12388, \"['attribution',\": 12389, \"'confabulation',\": 12390, \"'elocution',\\n\": 12391, \"'sequoia',\": 12392, \"'tenacious',\": 12393, \"'unidirectional']\\n>>>\": 12394, 'vsequences': 12395, 'set()\\n>>>': 12396, 'word:\\n': 12397, \"'aeiou':\\n\": 12398, '.append(char)\\n': 12399, \".add(''\": 12400, '.join(vowels))\\n>>>': 12401, \"sorted(vsequences)\\n['aiuio',\": 12402, \"'eaiou',\": 12403, \"'eouio',\": 12404, \"'euoia',\": 12405, \"'oauaio',\": 12406, \"'uiieioa']\\n\\n\\n\\n\\n\\n\\n★\": 12407, '.6,\\nindexing': 12408, 'synset,\\ne': 12409, \".synsets('dog')[0]\": 12410, '.offset': 12411, 'the\\noffset': 12412, 'ancestors': 12413, 'hierarchy)': 12414, 'the\\nuniversal': 12415, '.udhr),\\nand': 12416, 'correlation': 12417, 'functionality\\n(nltk': 12418, '.freqdist,': 12419, '.spearman_correlation),\\ndevelop': 12420, 'guesses': 12421, 'unseen': 12422, 'simplicity,': 12423, 'few\\nlanguages': 12424, 'discovers\\ncases': 12425, 'similarity\\nbetween': 12426, 'crude\\napproach;': 12427, '.)\\n\\n★': 12428, 'words\\n(sproat': 12429, '2001),': 12430, '.\\n\\n\\n\\n\\n\\nabout': 12431, 'acstch04': 12432, '.rst2\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4\\xa0\\xa0\\xa0writing': 12433, 'programs\\nby': 12434, 'language\\nfor': 12435, 'may\\nstill': 12436, 'wrestling': 12437, \"we'll\\naddress\": 12438, 'well-structured,': 12439, 'easily?\\nhow': 12440, 'blocks': 12441, 'assignment?\\nwhat': 12442, 'pitfalls': 12443, 'them?\\n\\nalong': 12444, 'programming\\nconstructs,': 12445, 'natural\\nand': 12446, 'concise': 12447, 'and\\nexercises': 12448, 'introduce': 12449, 'material)': 12450, '.\\nreaders': 12451, 'carefully\\nand': 12452, 'necessary;\\nexperienced': 12453, 'skim': 12454, 'programming\\nconcepts': 12455, 'dictated': 12456, 'revert': 12457, 'more\\nconventional': 12458, 'for\\na': 12459, '.1\\xa0\\xa0\\xa0back': 12460, 'basics\\n\\nassignment\\nassignment': 12461, 'seem': 12462, 'elementary': 12463, 'concept,': 12464, 'not\\ndeserving': 12465, 'surprising': 12466, 'subtleties\\nhere': 12467, 'fragment:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 12468, 'foo': 12469, \"'monty'\\n>>>\": 12470, \"'python'\": 12471, \"bar\\n'monty'\\n\\n\\n\\nthis\": 12472, 'behaves': 12473, 'above\\ncode': 12474, ',\\nthe': 12475, \"'monty')\": 12476, '.\\nthat': 12477, 'foo,': 12478, 'overwrite\\nfoo': 12479, 'value\\nof': 12480, 'affected': 12481, '.\\nassignment': 12482, 'not\\nalways': 12483, 'particular,\\nthe': 12484, 'a\\nreference': 12485, 'example,\\n': 12486, 'see\\nthat': 12487, \"'python']\\n>>>\": 12488, 'foo[1]': 12489, \"'bodkin'\": 12490, \"bar\\n['monty',\": 12491, \"'bodkin']\\n\\n\\n\\n\\n\\nfigure\": 12492, 'memory:': 12493, 'reference\\nthe': 12494, 'memory;': 12495, 'updating': 12496, 'bar,\\nand': 12497, 'the\\nvariable,': 12498, 'to\\nknow': 12499, '.1,': 12500, '3133': 12501, '(which': 12502, 'is\\nitself': 12503, 'locations': 12504, 'holding': 12505, 'strings)': 12506, 'reference\\n3133': 12507, 'gets': 12508, 'copied': 12509, 'extends': 12510, 'as\\nparameter': 12511, 'more,': 12512, '[empty,': 12513, 'empty,': 12514, 'empty]\\n>>>': 12515, 'nested\\n[[],': 12516, '[],': 12517, '[]]\\n>>>': 12518, 'nested[1]': 12519, \".append('python')\\n>>>\": 12520, \"nested\\n[['python'],\": 12521, \"['python'],\": 12522, \"['python']]\\n\\n\\n\\nobserve\": 12523, 'turn:\\nuse': 12524, '[[]]': 12525, 'the\\nelements': 12526, 'id()': 12527, 'identifier': 12528, 'that\\nid(nested[0]),': 12529, 'id(nested[1]),': 12530, 'id(nested[2])': 12531, 'are\\nall': 12532, 'list,\\nit': 12533, 'propagate': 12534, 'others:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 12535, '3\\n>>>': 12536, \"['monty']\\n>>>\": 12537, \"['monty'],\": 12538, \"['python']]\\n\\n\\n\\n\\n\\n\\n\\nwe\": 12539, 'we\\nmodified': 12540, 'containing\\nthree': 12541, \"['python']\": 12542, 'overwrote': 12543, \"['monty']\": 12544, \"wasn't\": 12545, 'changed,': 12546, 'referenced': 12547, 'in\\nour': 12548, 'between\\nmodifying': 12549, 'reference,': 12550, 'overwriting': 12551, '.\\n\\nnote\\nimportant:\\nto': 12552, 'bar,': 12553, 'write\\nbar': 12554, 'foo[:]': 12555, 'references,': 12556, '.deepcopy()': 12557, '.\\n\\n\\n\\nequality\\npython': 12558, 'identity': 12559, 'to\\nverify': 12560, 'create\\na': 12561, 'demonstrate\\nthat': 12562, '==,': 12563, 'also\\nthat': 12564, '5\\n>>>': 12565, \"['python']\\n>>>\": 12566, 'snake_nest': 12567, '[python]': 12568, 'size\\n>>>': 12569, 'snake_nest[0]': 12570, 'snake_nest[1]': 12571, 'snake_nest[2]': 12572, 'snake_nest[3]': 12573, 'snake_nest[4]\\ntrue\\n>>>': 12574, 'snake_nest[4]\\ntrue\\n\\n\\n\\nnow': 12575, 'not\\nall': 12576, 'identical:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 12577, 'random\\n>>>': 12578, '.choice(range(size))\\n>>>': 12579, 'snake_nest[position]': 12580, \"snake_nest\\n[['python'],\": 12581, \"['python']]\\n>>>\": 12582, 'snake_nest[4]\\nfalse\\n\\n\\n\\nyou': 12583, 'pairwise': 12584, 'interloper,\\nbut': 12585, 'easier:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 12586, '[id(snake)': 12587, 'snake_nest]\\n[4557855488,': 12588, '4557854763,': 12589, '4557855488,': 12590, '4557855488]\\n\\n\\n\\nthis': 12591, 'reveals': 12592, 'try\\nrunning': 12593, 'list,\\nand': 12594, 'interloper': 12595, '.\\n\\n\\nhaving': 12596, 'equality': 12597, 'the\\ntype-token': 12598, 'distinction,': 12599, '.\\n\\n\\nconditionals\\nin': 12600, 'a\\nnonempty': 12601, 'evaluated': 12602, 'or\\nlist': 12603, 'evaluates': 12604, \"['cat',\": 12605, \"['dog'],\": 12606, 'mixed:\\n': 12607, 'element:\\n': 12608, 'print(element)\\n': 12609, \".\\ncat\\n['dog']\\n\\n\\n\\nthat\": 12610, 'len(element)': 12611, '0:': 12612, \".\\nwhat's\": 12613, '.elif': 12614, 'opposed': 12615, 'using\\na': 12616, 'row?': 12617, 'following\\nsituation:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 12618, 'animals': 12619, \"'dog']\\n>>>\": 12620, 'animals:\\n': 12621, 'print(1)\\n': 12622, \"'dog'\": 12623, 'print(2)\\n': 12624, '.\\n1\\n\\n\\n\\nsince': 12625, 'clause': 12626, 'satisfied,': 12627, 'never\\ntries': 12628, 'clause,': 12629, 'out\\n2': 12630, 'replaced': 12631, 'we\\nwould': 12632, 'clause\\npotentially': 12633, 'bare': 12634, 'clause;': 12635, 'when\\nit': 12636, 'is\\nsatisfied,': 12637, 'was\\nnot': 12638, 'all()': 12639, 'any()': 12640, 'sequence)': 12641, 'to\\ncheck': 12642, 'meet': 12643, 'condition:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 12644, \"['no',\": 12645, \"'good',\": 12646, \"'fish',\": 12647, \"'anywhere',\": 12648, \"'porpoise',\": 12649, 'all(len(w)': 12650, 'sent)\\nfalse\\n>>>': 12651, 'any(len(w)': 12652, 'sent)\\ntrue\\n\\n\\n\\n\\n\\n\\n4': 12653, '.2\\xa0\\xa0\\xa0sequences\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nso': 12654, 'object:': 12655, 'another\\nkind': 12656, '.\\ntuples': 12657, 'comma': 12658, 'enclosed\\nusing': 12659, 'the\\nprevious': 12660, 'since\\nthere': 12661, 'tuples': 12662, 'number\\nof': 12663, 'sliced': 12664, \"'walk',\": 12665, \"'fem',\": 12666, \"t\\n('walk',\": 12667, '3)\\n>>>': 12668, 't[0]': 12669, \"\\n'walk'\\n>>>\": 12670, 't[1:]': 12671, \"\\n('fem',\": 12672, 'len(t)': 12673, '\\n3\\n\\n\\n\\n\\ncaution!\\ntuples': 12674, 'more\\ngeneral': 12675, 'syntax,': 12676, 'grouping': 12677, \"'snark'\": 12678, 'a\\ntrailing': 12679, 'comma,': 12680, \"'snark',\": 12681, 'special\\ncase,': 12682, '()': 12683, \".\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nlet's\": 12684, 'directly,': 12685, 'length\\noperation': 12686, 'type:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 12687, 'turned': 12688, \"spectroroute'\\n>>>\": 12689, \"['i',\": 12690, \"'turned',\": 12691, \"'spectroroute']\\n>>>\": 12692, \"'turned')\\n>>>\": 12693, 'raw[2],': 12694, 'text[3],': 12695, \"pair[1]\\n('t',\": 12696, 'raw[-3:],': 12697, 'text[-3:],': 12698, \"pair[-3:]\\n('ute',\": 12699, \"['off',\": 12700, \"'spectroroute'],\": 12701, \"'turned'))\\n>>>\": 12702, 'len(raw),': 12703, 'len(text),': 12704, 'len(pair)\\n(29,': 12705, '2)\\n\\n\\n\\nnotice': 12706, 'computed': 12707, 'a\\nsingle': 12708, 'line,': 12709, 'commas': 12710, 'comma-separated': 12711, 'expressions\\nare': 12712, 'the\\nparentheses': 12713, 'ambiguity': 12714, 'a\\ntuple,': 12715, 'this\\nway,': 12716, 'implicitly': 12717, 'aggregating': 12718, '.\\n\\noperating': 12719, 'types\\nwe': 12720, 'ways,\\nas': 12721, '.\\n\\n\\n\\n\\n\\n\\npython': 12722, 'expression\\ncomment\\n\\n\\n\\nfor': 12723, 's\\niterate': 12724, 's\\n\\nfor': 12725, 'sorted(s)\\niterate': 12726, 'order\\n\\nfor': 12727, 'set(s)\\niterate': 12728, 'reversed(s)\\niterate': 12729, 'reverse\\n\\nfor': 12730, 'set(s)': 12731, '.difference(t)\\niterate': 12732, 't\\n\\n\\ntable': 12733, 'sequences\\n\\n\\nthe': 12734, 'combined\\nin': 12735, 'ways;': 12736, 'sorted\\nin': 12737, 'reverse,': 12738, 'reversed(sorted(set(s)))': 12739, 'randomize': 12740, 'over\\nthem,': 12741, '.shuffle(s)': 12742, 'example,\\ntuple(s)': 12743, 'tuple,': 12744, 'and\\nlist(s)': 12745, 'the\\njoin()': 12746, '.join(words)': 12747, 'a\\nsequence': 12748, 'sorted())': 12749, 'iteration,': 12750, \"'red\": 12751, 'lorry,': 12752, 'yellow': 12753, 'lorry': 12754, '.freqdist(text)\\n>>>': 12755, \"sorted(fdist)\\n[',',\": 12756, \"'red',\": 12757, \"'lorry',\": 12758, \"'yellow']\\n>>>\": 12759, 'fdist:\\n': 12760, 'print(key': 12761, 'fdist[key],': 12762, '.\\nlorry:': 12763, 'red:': 12764, '1;': 12765, ',:': 12766, 'yellow:': 12767, '2\\n\\n\\n\\nin': 12768, 're-arrange': 12769, 'the\\ncontents': 12770, 'parentheses\\nbecause': 12771, 'precedence': 12772, 'words[2],': 12773, 'words[3],': 12774, 'words[4]': 12775, 'words[4],': 12776, 'words[2]\\n>>>': 12777, \"words\\n['i',\": 12778, \"'spectroroute',\": 12779, \"'off']\\n\\n\\n\\nthis\": 12780, 'such\\ntasks': 12781, '(notice': 12782, 'a\\ntemporary': 12783, 'tmp)': 12784, 'tmp': 12785, 'words[2]': 12786, 'words[3]\\n>>>': 12787, 'words[3]': 12788, 'words[4]\\n>>>': 12789, 'tmp\\n\\n\\n\\nas': 12790, 'reversed()\\nthat': 12791, 'rearrange': 12792, 'that\\nmodify': 12793, 'handy': 12794, 'zip()': 12795, 'takes\\nthe': 12796, 'zips': 12797, 'enumerate(s)': 12798, \"['noun',\": 12799, \"'verb',\": 12800, \"'prep',\": 12801, \"'det',\": 12802, \"'noun']\\n>>>\": 12803, 'zip(words,': 12804, 'tags)\\n<zip': 12805, '.>\\n>>>': 12806, 'list(zip(words,': 12807, \"tags))\\n[('i',\": 12808, \"'noun'),\": 12809, \"('turned',\": 12810, \"'verb'),\": 12811, \"('off',\": 12812, \"'prep'),\\n('the',\": 12813, \"'det'),\": 12814, \"('spectroroute',\": 12815, \"'noun')]\\n>>>\": 12816, 'list(enumerate(words))\\n[(0,': 12817, \"'turned'),\": 12818, \"'off'),\": 12819, '(3,': 12820, \"'spectroroute')]\\n\\n\\n\\n\\nnote\\nit\": 12821, 'perform\\ncomputation': 12822, 'lazy': 12823, 'evaluation)': 12824, '<zip': 12825, '0x10d005448>': 12826, 'be\\nevaluated': 12827, 'putting': 12828, 'sequence,\\nlike': 12829, 'list(x),': 12830, '.\\n\\nfor': 12831, 'cut': 12832, 'it\\non': 12833, '10%': 12834, 'to\\ncut': 12835, 'int(0': 12836, 'len(text))': 12837, 'training_data,': 12838, 'test_data': 12839, 'text[:cut],': 12840, 'text[cut:]': 12841, 'training_data': 12842, '\\ntrue\\n>>>': 12843, 'len(training_data)': 12844, 'len(test_data)': 12845, '\\n9': 12846, '.0\\n\\n\\n\\nwe': 12847, 'during': 12848, 'duplicated\\n': 12849, 'ratio': 12850, 'sizes': 12851, 'what\\nwe': 12852, '.\\n\\n\\ncombining': 12853, \"types\\nlet's\": 12854, 'types,': 12855, 'by\\ntheir': 12856, \"spectroroute'\": 12857, 'wordlens': 12858, '[(len(word),': 12859, 'words]': 12860, '.join(w': 12861, '(_,': 12862, 'wordlens)': 12863, \"\\n'i\": 12864, \"spectroroute'\\n\\n\\n\\n\\neach\": 12865, ',\\nwhere': 12866, 'length)': 12867, 'the\\nword,': 12868, \"'the')\": 12869, 'sort()': 12870, '\\nto': 12871, 'sort': 12872, 'in-place': 12873, 'discard': 12874, 'length\\ninformation': 12875, '.\\n(the': 12876, 'variable,\\nbut': 12877, 'will\\nnot': 12878, '.)\\nwe': 12879, 'commonalities': 12880, 'types,\\nbut': 12881, 'their\\nroles': 12882, 'end:': 12883, 'is\\ntypical': 12884, 'and\\nproducing': 12885, 'the\\nmiddle,': 12886, 'of\\nobjects': 12887, 'type,': 12888, 'often\\nuse': 12889, 'contrast,\\na': 12890, 'of\\nfixed': 12891, 'record,\\na': 12892, 'some\\ngetting': 12893, 'to,\\nso': 12894, \"['di:',\": 12895, \"'d@']),\\n\": 12896, \"['qf',\": 12897, \"'o:f'])\\n\": 12898, ']\\n\\n\\n\\nhere,': 12899, 'a\\ncollection': 12900, '—\\nof': 12901, 'predetermined': 12902, 'a\\ntuple': 12903, 'different\\ninterpretations,': 12904, 'orthographic': 12905, 'speech,\\nand': 12906, 'pronunciations': 12907, '(represented': 12908, 'sampa': 12909, 'computer-readable\\nphonetic': 12910, '.phon': 12911, '.ucl': 12912, '.ac': 12913, '.uk/home/sampa/)': 12914, '(why?)\\n\\nnote\\na': 12915, 'whether\\nthe': 12916, 'example,\\na': 12917, 'interpretation,\\nand': 12918, 'tag': 12919, \"('grail',\": 12920, \"'noun');\\na\": 12921, \"('noun',\": 12922, \"'grail')\": 12923, 'is\\nnot': 12924, \"['venetian',\": 12925, \"'blind'];\\na\": 12926, \"['blind',\": 12927, \"'venetian']\": 12928, 'different,': 12929, 'the\\ninterpretation': 12930, 'unchanged': 12931, 'of\\nusage': 12932, 'difference:': 12933, 'python,\\nlists': 12934, 'mutable,': 12935, 'other\\nwords,': 12936, 'modified,': 12937, 'modification': 12938, '.sort()\\n>>>': 12939, 'lexicon[1]': 12940, \"'vbd',\": 12941, \"['t3:nd',\": 12942, \"'t3`nd'])\\n>>>\": 12943, 'lexicon[0]\\n\\n\\n\\n\\nnote\\nyour': 12944, 'turn:\\nconvert': 12945, 'tuple(lexicon),\\nthen': 12946, 'of\\nthem': 12947, '.\\n\\n\\n\\ngenerator': 12948, \"expressions\\nwe've\": 12949, 'heavy': 12950, 'comprehensions,': 12951, 'readable\\nprocessing': 12952, \"'''when\": 12953, 'humpty': 12954, 'dumpty': 12955, 'scornful': 12956, 'tone,\\n': 12957, \".'''\\n>>>\": 12958, \"word_tokenize(text)]\\n['``',\": 12959, \"'use',\": 12960, \"'word',\": 12961, \"'humpty',\": 12962, \"'dumpty',\": 12963, '.]\\n\\n\\n\\nsuppose': 12964, 'above\\nexpression': 12965, 'max([w': 12966, 'word_tokenize(text)])': 12967, \"\\n'word'\\n>>>\": 12968, 'max(w': 12969, 'word_tokenize(text))': 12970, \"\\n'word'\\n\\n\\n\\nthe\": 12971, 'notational': 12972, 'convenience:\\nin': 12973, 'situations,': 12974, 'storage': 12975, 'allocated\\nbefore': 12976, 'max()': 12977, 'is\\nvery': 12978, 'slow': 12979, 'streamed': 12980, 'calling\\nfunction': 12981, 'latest': 12982, 'lexicographic': 12983, 'stream\\nof': 12984, '.3\\xa0\\xa0\\xa0questions': 12985, 'style\\nprogramming': 12986, 'art': 12987, 'undisputed': 12988, 'bible': 12989, 'programming,\\na': 12990, '2,500': 12991, 'multi-volume': 12992, 'donald': 12993, 'knuth,': 12994, 'called\\nthe': 12995, 'on\\nliterate': 12996, 'humans,': 12997, 'computers,\\nmust': 12998, 'of\\nprogramming': 12999, 'ramifications': 13000, 'readability\\nof': 13001, 'layout,': 13002, 'procedural': 13003, 'declarative': 13004, 'style,\\nand': 13005, '.\\n\\npython': 13006, 'coding': 13007, 'style\\nwhen': 13008, 'subtle': 13009, 'choices': 13010, 'names,\\nspacing,': 13011, 'by\\nother': 13012, 'people,': 13013, 'needless': 13014, 'harder\\nto': 13015, 'therefore,': 13016, 'designers': 13017, 'python\\nlanguage': 13018, 'available\\nat': 13019, '.org/dev/peps/pep-0008/': 13020, 'consistency,\\nfor': 13021, 'maximizing': 13022, 'recommendations': 13023, 'refer\\nreaders': 13024, '.\\n\\ncode': 13025, 'layout': 13026, 'that\\nwhen': 13027, 'you\\navoid': 13028, 'by\\ndifferent': 13029, 'messed': 13030, '.\\nlines': 13031, '80': 13032, 'long;': 13033, 'can\\nbreak': 13034, 'brackets,': 13035, 'braces,': 13036, 'because\\npython': 13037, 'braces,\\nyou': 13038, 'broken:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 13039, '(len(syllables)': 13040, 'len(syllables[2])': 13041, 'syllables[2][2]': 13042, '[aeiou]': 13043, 'syllables[2][3]': 13044, 'syllables[1][3]):\\n': 13045, 'process(syllables)\\n>>>': 13046, 'len(syllables)': 13047, '\\\\\\n': 13048, 'syllables[1][3]:\\n': 13049, 'process(syllables)\\n\\n\\n\\n\\nnote\\ntyping': 13050, 'chore': 13051, 'programming\\neditors': 13052, 'indent\\ncode': 13053, 'highlight': 13054, 'errors': 13055, 'errors)': 13056, 'python-aware': 13057, 'editors,': 13058, 'see\\nhttp://wiki': 13059, '.org/moin/pythoneditors': 13060, '.\\n\\n\\n\\nprocedural': 13061, 'style\\n\\n\\n\\n\\n\\n\\nwe': 13062, 'different\\nways,': 13063, 'implications': 13064, 'efficiency': 13065, 'influencing\\nprogram': 13066, 'following\\nprogram': 13067, '0\\n>>>': 13068, 'tokens:\\n': 13069, 'len(token)\\n>>>': 13070, 'count\\n4': 13071, '.401545438271973\\n\\n\\n\\n\\nin': 13072, 'track': 13073, 'the\\nnumber': 13074, 'low-level': 13075, 'machine\\ncode,': 13076, 'cpu': 13077, \"cpu's\": 13078, 'registers,': 13079, 'accumulating': 13080, 'values\\nat': 13081, 'stages,': 13082, 'meaningless': 13083, 'dictating\\nthe': 13084, 'thing:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 13085, 'sum(len(t)': 13086, 'tokens)\\n>>>': 13087, 'print(total': 13088, 'len(tokens))\\n4': 13089, '.401': 13090, '.\\n\\n\\n\\n\\nthe': 13091, 'lengths,\\nwhile': 13092, 'complete,': 13093, 'which\\ncan': 13094, 'understood': 13095, 'high-level': 13096, 'like:\\ntotal': 13097, '.\\nimplementation': 13098, 'constitutes\\nprogramming': 13099, 'level;': 13100, 'word_list': 13101, 'len(tokens):\\n': 13102, 'len(word_list)': 13103, 'word_list[j]': 13104, 'tokens[i]:\\n': 13105, 'tokens[i]': 13106, 'word_list[j-1]:\\n': 13107, '.insert(j,': 13108, 'tokens[i])\\n': 13109, '.\\n\\n\\n\\nthe': 13110, 'instantly': 13111, 'recognizable:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 13112, 'sorted(set(tokens))\\n\\n\\n\\n\\nanother': 13113, 'printing\\na': 13114, 'counter': 13115, 'enumerate(),': 13116, 'which\\nprocesses': 13117, 's[i])': 13118, '(0,': 13119, 's[0])': 13120, 'enumerate': 13121, 'key-value': 13122, '(rank,': 13123, 'count))': 13124, 'rank+1': 13125, '1,\\nas': 13126, '.freqdist(nltk': 13127, 'most_common_words': 13128, '.most_common()]\\n>>>': 13129, 'rank,': 13130, 'enumerate(most_common_words):\\n': 13131, '.freq(word)\\n': 13132, 'print(%3d': 13133, '%6': 13134, '.2f%%': 13135, '%s': 13136, '(rank': 13137, '100,': 13138, 'word))\\n': 13139, '.25:\\n': 13140, 'break\\n': 13141, '.40%': 13142, 'the\\n': 13143, '.42%': 13144, ',\\n': 13145, '.67%': 13146, '.78%': 13147, '.19%': 13148, 'to\\n': 13149, '.29%': 13150, 'a\\n': 13151, '.97%': 13152, \"in\\n\\n\\n\\nit's\": 13153, 'tempting': 13154, 'value\\nseen': 13155, 'longest': 13156, \".words('milton-paradise\": 13157, \"''\\n>>>\": 13158, 'text:\\n': 13159, 'len(longest):\\n': 13160, 'word\\n>>>': 13161, \"longest\\n'unextinguishable'\\n\\n\\n\\nhowever,\": 13162, 'transparent': 13163, 'comprehensions,\\nboth': 13164, 'now:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 13165, 'maxlen': 13166, 'max(len(word)': 13167, 'text)\\n>>>': 13168, \"maxlen]\\n['unextinguishable',\": 13169, \"'transubstantiate',\": 13170, \"'inextinguishable',\": 13171, \"'incomprehensible']\\n\\n\\n\\nnote\": 13172, 'want)': 13173, 'solutions,\\nthe': 13174, 'overhead': 13175, 'pass\\nthrough': 13176, 'instantaneous': 13177, 'concerns': 13178, 'about\\nprogram': 13179, 'cryptic': 13180, 'solution\\nwill': 13181, '.\\n\\n\\nsome': 13182, 'counters\\n\\n\\nthere': 13183, 'successive': 13184, 'overlapping': 13185, 'n-grams\\nfrom': 13186, '[sent[i:i+n]': 13187, \"range(len(sent)-n+1)]\\n[['the',\": 13188, \"'gave'],\\n\": 13189, \"['dog',\": 13190, \"'john'],\\n\": 13191, \"['gave',\": 13192, \"'the'],\\n\": 13193, \"'newspaper']]\\n\\n\\n\\nit\": 13194, 'nltk\\nsupports': 13195, 'bigrams(text)': 13196, 'trigrams(text),': 13197, 'and\\na': 13198, 'ngrams(text,': 13199, 'n)': 13200, 'in\\nbuilding': 13201, 'columns,\\nwhere': 13202, '7\\n>>>': 13203, '[[set()': 13204, 'range(n)]': 13205, 'range(m)]\\n>>>': 13206, 'array[2][5]': 13207, \".add('alice')\\n>>>\": 13208, '.pprint(array)\\n[[set(),': 13209, 'set()],\\n': 13210, '[set(),': 13211, \"{'alice'},\": 13212, 'set()]]\\n\\n\\n\\nobserve': 13213, 'used\\nanywhere': 13214, 'syntactically\\ncorrect': 13215, 'usage,': 13216, 'observe\\nthat': 13217, \"['very'\": 13218, 'range(3)]': 13219, 'list\\ncontaining': 13220, 'sight': 13221, 'incorrect': 13222, 'multiplication,\\nfor': 13223, '[[set()]': 13224, 'n]': 13225, 'm\\n>>>': 13226, '.add(7)\\n>>>': 13227, '.pprint(array)\\n[[{7},': 13228, '{7},': 13229, '{7}],\\n': 13230, '[{7},': 13231, '{7}]]\\n\\n\\n\\n\\n\\niteration': 13232, 'device': 13233, 'adopt': 13234, 'elegant': 13235, 'alternatives,\\nas': 13236, '.4\\xa0\\xa0\\xa0functions:': 13237, 'foundation': 13238, 'programming\\n\\n\\n\\nfunctions': 13239, 'code,\\nas': 13240, 'steps:': 13241, 'in,': 13242, 'normalizing\\nwhitespace,': 13243, 'a\\nfunction,': 13244, 'get_text(),': 13245, '.\\n\\n\\n\\n\\n\\xa0\\n\\nimport': 13246, 're\\ndef': 13247, 'get_text(file):\\n': 13248, 'open(file)': 13249, '.read()\\n': 13250, \".sub(r'<\": 13251, \".*?>',\": 13252, 'text)\\n': 13253, \".sub('\\\\s+',\": 13254, 'text\\n\\n\\nexample': 13255, '(code_get_text': 13256, 'file\\n\\nnow,': 13257, 'cleaned-up': 13258, 'call\\nget_text()': 13259, 'return\\na': 13260, '.:\\ncontents': 13261, 'get_text(test': 13262, 'of\\nsteps': 13263, 'more\\nimportantly,': 13264, 'whenever': 13265, 'cleaned-up\\ntext': 13266, 'clutter': 13267, 'we\\nsimply': 13268, 'get_text()': 13269, 'semantic\\ninterpretation': 13270, 'docstring': 13271, 'the\\npurpose': 13272, 'programmer\\nwho': 13273, 'loaded': 13274, 'file:\\n\\n|': 13275, 'help(get_text)\\n|': 13276, 'get_text': 13277, '__main__:\\n|\\n|': 13278, 'get_text(file)\\n|': 13279, 'reusable': 13280, 'they\\nalso': 13281, 'tested,': 13282, 'risk': 13283, 'forget': 13284, 'bug': 13285, 'calls': 13286, 'increased': 13287, 'reliability': 13288, 'author\\nof': 13289, 'behave\\ntransparently': 13290, 'summarize,': 13291, 'captures': 13292, 'performs\\na': 13293, 'details,\\nto': 13294, 'bigger': 13295, 'picture,': 13296, 'the\\nmechanics': 13297, '.\\n\\nfunction': 13298, 'outputs\\nwe': 13299, \"function's\": 13300, 'parameters,\\nthe': 13301, 'parenthesized': 13302, 'following\\nthe': 13303, 'repeat(msg,': 13304, 'num):': 13305, '.join([msg]': 13306, 'num)\\n>>>': 13307, 'repeat(monty,': 13308, \"\\n'monty\": 13309, \"python'\\n\\n\\n\\nwe\": 13310, 'msg': 13311, 'num\\n': 13312, '3\\n;': 13313, 'fill': 13314, 'and\\nprovide': 13315, 'num': 13316, 'monty():\\n': 13317, 'python\\n>>>': 13318, \"monty()\\n'monty\": 13319, \"python'\\n\\n\\n\\na\": 13320, 'communicates': 13321, 'statement,\\nas': 13322, 'replaced\\nwith': 13323, '.:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 13324, 'repeat(monty(),': 13325, \"3)\\n'monty\": 13326, \"repeat('monty\": 13327, 'result,\\nmodifying': 13328, 'function\\n(such': 13329, 'procedures': 13330, 'languages)': 13331, '.\\n\\n\\n\\n\\nconsider': 13332, 'dangerous': 13333, 'could\\nuse': 13334, 'parameter\\n(my_sort1()),': 13335, '(my_sort2()),\\nnot': 13336, '(my_sort3())': 13337, 'my_sort1(mylist):': 13338, 'good:': 13339, 'value\\n': 13340, 'my_sort2(mylist):': 13341, 'touch': 13342, 'sorted(mylist)\\n>>>': 13343, 'my_sort3(mylist):': 13344, 'bad:': 13345, 'it\\n': 13346, '.sort()\\n': 13347, 'mylist\\n\\n\\n\\n\\n\\nparameter': 13348, 'passing\\nback': 13349, 'values,\\nbut': 13350, 'same\\nis': 13351, 'is\\nknown': 13352, 'call-by-value)': 13353, 'set_up()': 13354, 'parameters,\\nboth': 13355, 'string\\nto': 13356, 'unchanged,\\nwhile': 13357, 'changed:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 13358, 'set_up(word,': 13359, 'properties):\\n': 13360, \"'lolcat'\\n\": 13361, \".append('noun')\\n\": 13362, '5\\n': 13363, 'set_up(w,': 13364, 'p)\\n>>>': 13365, \"w\\n''\\n>>>\": 13366, \"p\\n['noun']\\n\\n\\n\\nnotice\": 13367, 'p),': 13368, 'to\\na': 13369, 'is\\nidentical': 13370, 'w\\n>>>': 13371, \"'lolcat'\\n>>>\": 13372, \"w\\n''\\n\\n\\n\\nlet's\": 13373, 'empty\\nlist)': 13374, 'properties,\\nso': 13375, 'properties,': 13376, 'also\\nreflected': 13377, 'also\\nassigned': 13378, '5);': 13379, 'this\\ndid': 13380, 'but\\ncreated': 13381, 'p\\n>>>': 13382, \".append('noun')\\n>>>\": 13383, \"p\\n['noun']\\n\\n\\n\\nthus,\": 13384, 'call-by-value': 13385, 'passing,\\nit': 13386, '.\\n\\n\\nvariable': 13387, 'scope\\nfunction': 13388, 'new,': 13389, 'function,\\nthe': 13390, 'not\\nvisible': 13391, 'behavior\\nmeans': 13392, 'concerned': 13393, 'about\\ncollisions': 13394, 'body\\nof': 13395, 'resolve\\nthe': 13396, 'respect': 13397, 'global\\nname': 13398, 'succeed,': 13399, 'is\\nthe': 13400, 'lgb': 13401, 'resolution:': 13402, 'local,\\nthen': 13403, 'global,': 13404, '.\\n\\ncaution!\\na': 13405, 'enable': 13406, 'global': 13407, 'the\\nglobal': 13408, 'be\\navoided': 13409, 'defining': 13410, 'variables\\ninside': 13411, 'introduces': 13412, 'dependencies': 13413, 'context\\nand': 13414, 'limits': 13415, 'portability': 13416, 'reusability)': 13417, 'inputs\\nand': 13418, 'outputs': 13419, '.\\n\\n\\n\\nchecking': 13420, 'types\\npython': 13421, 'declare': 13422, 'program,\\nand': 13423, 'flexible\\nabout': 13424, 'expect\\na': 13425, 'expressed\\nas': 13426, 'iterator,': 13427, 'is\\noutside': 13428, 'discussion)': 13429, 'defensive': 13430, 'warnings': 13431, 'functions\\nhave': 13432, 'tag()\\nfunction': 13433, 'tag(word):\\n': 13434, \"['a',\": 13435, \"'all']:\\n\": 13436, \"'det'\\n\": 13437, \"'noun'\\n\": 13438, \"tag('the')\\n'det'\\n>>>\": 13439, \"tag('knight')\\n'noun'\\n>>>\": 13440, \"tag(['tis,\": 13441, \"'scratch'])\": 13442, \"\\n'noun'\\n\\n\\n\\nthe\": 13443, 'sensible': 13444, \"'knight',\\nbut\": 13445, 'passed': 13446, 'fails': 13447, 'to\\ncomplain,': 13448, 'clearly': 13449, 'to\\nensure': 13450, 'tag()': 13451, 'using\\nif': 13452, 'type(word)': 13453, 'str,': 13454, 'simply\\nreturn': 13455, 'slight': 13456, 'improvement,': 13457, 'because\\nthe': 13458, 'diagnostic\\nvalue': 13459, 'program\\nmay': 13460, 'diagnostic\\nreturn': 13461, 'be\\npropagated': 13462, 'unpredictable': 13463, 'consequences': 13464, 'has\\ntype': 13465, 'solution,': 13466, 'assert': 13467, 'basestring\\ntype': 13468, 'generalizes': 13469, 'isinstance(word,': 13470, 'basestring),': 13471, 'string\\n': 13472, \"'noun'\\n\\n\\n\\nif\": 13473, 'fails,': 13474, 'ignored,\\nsince': 13475, 'halts': 13476, 'execution': 13477, '.\\nadditionally,': 13478, 'assertions': 13479, 'logical': 13480, 'errors,': 13481, 'function\\nusing': 13482, 'docstrings': 13483, '.\\n\\n\\n\\nfunctional': 13484, 'decomposition\\nwell-structured': 13485, 'grows': 13486, '10-20': 13487, 'a\\ngreat': 13488, 'more\\nfunctions,': 13489, 'analogous': 13490, 'essay': 13491, 'expressing': 13492, '.\\n\\n\\n\\n\\n\\nfunctions': 13493, 'abstraction': 13494, '.\\nthey': 13495, 'single,': 13496, 'action,\\nand': 13497, '.\\n(compare': 13498, 'of\\ngo': 13499, 'fetch': 13500, '.)\\nwhen': 13501, 'level\\nof': 13502, 'abstraction,': 13503, 'transparent,': 13504, 'load_corpus()\\n>>>': 13505, 'analyze(data)\\n>>>': 13506, 'present(results)\\n\\n\\n\\nappropriate': 13507, 'maintainable': 13508, 'reimplement': 13509, 'function\\n—': 13510, '—\\nwithout': 13511, 'freq_words': 13512, 'updates': 13513, 'is\\npassed': 13514, 'parameter,': 13515, 'the\\nn': 13516, 'request\\nfrom': 13517, 'beautifulsoup\\n\\ndef': 13518, 'freq_words(url,': 13519, \".decode('utf8')\\n\": 13520, '.get_text()\\n': 13521, 'word_tokenize(raw):\\n': 13522, 'freqdist[word': 13523, '.lower()]': 13524, '.most_common(n):\\n': 13525, '[word]\\n': 13526, 'print(result)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 13527, 'constitution': 13528, '.archives': 13529, '.gov/exhibits/charters/constitution_transcript': 13530, '.html\\n>>>': 13531, '.freqdist()\\n>>>': 13532, 'freq_words(constitution,': 13533, 'fd,': 13534, \"30)\\n['the',\": 13535, \"'shall',\": 13536, \"'states',\\n'or',\": 13537, \"'united',\": 13538, \"'state',\": 13539, \"'=',\": 13540, \"'president',\\n'all',\": 13541, \"'congress']\\n\\n\\nexample\": 13542, '(code_freq_words1': 13543, 'poorly': 13544, 'words\\n\\nthis': 13545, 'side-effects:': 13546, 'second\\nparameter,': 13547, 'elsewhere': 13548, 'we\\ninitialize': 13549, 'freqdist()': 13550, 'place\\nit': 13551, 'populated),': 13552, 'the\\ncalling': 13553, 'it\\nshould': 13554, 'refactor': 13555, 'simplify': 13556, 'dropping': 13557, '.freqdist(word': 13558, 'word_tokenize(text))\\n': 13559, '_)': 13560, '.most_common(n)]\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 13561, '(code_freq_words2': 13562, 'well-designed': 13563, 'words\\n\\nthe': 13564, 'usability': 13565, 'improved': 13566, '_': 13567, 'other\\nvariable': 13568, 'signals': 13569, 'use\\nfor': 13570, 'holds': 13571, '.\\n\\n\\n\\ndocumenting': 13572, 'functions\\nif': 13573, 'decomposing': 13574, 'provide\\nthis': 13575, 'statement\\nshould': 13576, 'implemented;': 13577, 're-implement': 13578, 'this\\nstatement': 13579, 'adequate': 13580, 'triple-quoted': 13581, 'line,\\nsince': 13582, 'functionality\\n(see': 13583, '.org/dev/peps/pep-0257/': 13584, 'docstring\\nconventions)': 13585, '.\\n\\ndocstrings': 13586, 'block,': 13587, 'tested': 13588, 'automatically\\nusing': 13589, 'docutils': 13590, '.\\ndocstrings': 13591, 'return\\ntype': 13592, 'minimum,': 13593, 'uses\\nthe': 13594, 'sphinx': 13595, 'format\\ncan': 13596, 'richly': 13597, 'structured\\napi': 13598, 'certain\\nfields': 13599, 'param': 13600, 'be\\nclearly': 13601, 'illustrates\\na': 13602, 'accuracy(reference,': 13603, 'test):\\n': 13604, 'values,\\n': 13605, 'indexes\\n': 13606, '{0<i<=len(test)}': 13607, 'c{test[i]': 13608, 'reference[i]}': 13609, \"accuracy(['adj',\": 13610, \"'n'],\": 13611, \"'adj'])\\n\": 13612, '.5\\n\\n': 13613, ':param': 13614, 'reference:': 13615, 'ordered': 13616, 'values\\n': 13617, ':type': 13618, 'list\\n': 13619, 'corresponding\\n': 13620, ':return:': 13621, 'score\\n': 13622, ':rtype:': 13623, 'float\\n': 13624, ':raises': 13625, 'valueerror:': 13626, 'length\\n': 13627, '\\n\\n': 13628, 'len(reference)': 13629, 'len(test):\\n': 13630, 'raise': 13631, 'valueerror(lists': 13632, 'num_correct': 13633, 'x,': 13634, 'zip(reference,': 13635, 'y:\\n': 13636, 'float(num_correct)': 13637, 'len(reference)\\n\\n\\nexample': 13638, '(code_sphinx': 13639, 'docstring,': 13640, 'summary,\\na': 13641, 'explanation,': 13642, 'markup\\nspecifying': 13643, 'exceptions': 13644, '.\\n\\n\\n\\n\\n4': 13645, '.5\\xa0\\xa0\\xa0doing': 13646, 'functions\\nthis': 13647, 'features,': 13648, '.\\n\\nfunctions': 13649, 'arguments\\nso': 13650, 'like\\nstrings,': 13651, 'as\\nan': 13652, 'show,\\nwe': 13653, 'user-defined': 13654, 'last_letter()\\nas': 13655, \"['take',\": 13656, \"'care',\": 13657, \"'sense',\": 13658, \"'the',\\n\": 13659, \"'sounds',\": 13660, \"'take',\": 13661, \"'themselves',\": 13662, 'extract_property(prop):\\n': 13663, '[prop(word)': 13664, 'sent]\\n': 13665, 'extract_property(len)\\n[4,': 13666, 'last_letter(word):\\n': 13667, 'word[-1]\\n>>>': 13668, \"extract_property(last_letter)\\n['e',\": 13669, \"'f',\": 13670, 'last_letter': 13671, 'be\\npassed': 13672, 'dictionaries': 13673, 'parentheses\\nare': 13674, 'invoking': 13675, 'function;\\nwhen': 13676, 'treating': 13677, 'arguments\\nto': 13678, 'lambda': 13679, 'supposing': 13680, 'there\\nwas': 13681, 'last_letter()': 13682, 'places,\\nand': 13683, 'equivalently': 13684, 'extract_property(lambda': 13685, 'w:': 13686, \"w[-1])\\n['e',\": 13687, \".']\\n\\n\\n\\nour\": 13688, 'latter': 13689, 'sorted),\\nit': 13690, 'cmp()': 13691, 'supply': 13692, 'decreasing\\nlength': 13693, \"sorted(sent)\\n[',',\": 13694, \"'sounds',\\n'take',\": 13695, 'sorted(sent,': 13696, \"cmp)\\n[',',\": 13697, 'y:': 13698, 'cmp(len(y),': 13699, \"len(x)))\\n['themselves',\": 13700, \"'care',\\n'the',\": 13701, \".']\\n\\n\\n\\n\\n\\naccumulative\": 13702, 'functions\\nthese': 13703, 'initializing': 13704, 'storage,': 13705, 'over\\ninput': 13706, 'returning': 13707, 'structure\\nor': 13708, 'aggregated': 13709, 'result)': 13710, 'an\\nempty': 13711, 'shown\\nin': 13712, 'search1()': 13713, 'search1(substring,': 13714, 'words):\\n': 13715, '.append(word)\\n': 13716, 'result\\n\\ndef': 13717, 'search2(substring,': 13718, 'word\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 13719, \"search1('zz',\": 13720, '.words()):\\n': 13721, 'print(item,': 13722, 'end=': 13723, \")\\ngrizzlies'\": 13724, 'fizzled': 13725, 'rizzuto': 13726, 'huzzahs': 13727, 'dazzler': 13728, 'jazz': 13729, 'pezza': 13730, \"search2('zz',\": 13731, '.\\n\\n\\nexample': 13732, '(code_search_examples': 13733, 'list\\n\\nthe': 13734, 'search2()': 13735, 'called,': 13736, 'yield\\nstatement': 13737, 'does\\nany': 13738, 'another\\nword,': 13739, 'continued': 13740, 'stopped,': 13741, 'until\\nthe': 13742, 'encounters': 13743, 'is\\ntypically': 13744, 'efficient,': 13745, 'is\\nrequired': 13746, 'allocate': 13747, 'additional\\nmemory': 13748, 'above)': 13749, 'sophisticated': 13750, 'produces\\nall': 13751, 'permutations': 13752, 'permutations()\\nfunction': 13753, 'permutations(seq):\\n': 13754, 'len(seq)': 13755, '1:\\n': 13756, 'seq\\n': 13757, 'perm': 13758, 'permutations(seq[1:]):\\n': 13759, 'range(len(perm)+1):\\n': 13760, 'perm[:i]': 13761, 'seq[0:1]': 13762, 'perm[i:]\\n': 13763, \"list(permutations(['police',\": 13764, \"'buffalo']))\": 13765, \"\\n[['police',\": 13766, \"'buffalo'],\": 13767, \"['fish',\": 13768, \"'police',\": 13769, \"'buffalo'],\\n\": 13770, \"'buffalo',\": 13771, \"'police'],\": 13772, \"['police',\": 13773, \"'fish'],\\n\": 13774, \"['buffalo',\": 13775, \"'fish'],\": 13776, \"'police']]\\n\\n\\n\\n\\nnote\\nthe\": 13777, 'recursion,\\ndiscussed': 13778, 'is\\nuseful': 13779, 'grammar': 13780, '(8': 13781, '.\\n\\n\\n\\nhigher-order': 13782, 'functions\\npython': 13783, 'higher-order': 13784, 'standard\\nfeatures': 13785, 'functional': 13786, 'haskell': 13787, 'alongside': 13788, 'expression\\nusing': 13789, 'is_content_word()\\nwhich': 13790, 'filter(),\\nwhich': 13791, 'applies': 13792, 'contained\\nin': 13793, 'retains': 13794, 'which\\nthe': 13795, 'is_content_word(word):\\n': 13796, 'list(filter(is_content_word,': 13797, \"sent))\\n['take',\": 13798, \"'themselves']\\n>>>\": 13799, \"is_content_word(w)]\\n['take',\": 13800, \"'themselves']\\n\\n\\n\\nanother\": 13801, 'map(),': 13802, 'the\\nextract_property()': 13803, 'news\\nsection': 13804, 'comprehension\\ncalculation:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 13805, 'list(map(len,': 13806, \".sents(categories='news')))\\n>>>\": 13807, 'sum(lengths)': 13808, 'len(lengths)\\n21': 13809, '.75081116158339\\n>>>': 13810, '[len(sent)': 13811, \".sents(categories='news')]\\n>>>\": 13812, '.75081116158339\\n\\n\\n\\nin': 13813, 'is_content_word()\\nand': 13814, 'list(map(lambda': 13815, 'len(filter(lambda': 13816, 'c:': 13817, 'aeiou,': 13818, 'w)),': 13819, 'sent))\\n[2,': 13820, '0,': 13821, '0]\\n>>>': 13822, '[len(c': 13823, 'aeiou)': 13824, 'sent]\\n[2,': 13825, '0]\\n\\n\\n\\nthe': 13826, 'the\\nsolutions': 13827, 'favored': 13828, 'former\\napproach': 13829, 'throughout': 13830, '.\\n\\n\\nnamed': 13831, 'arguments\\nwhen': 13832, 'confused': 13833, 'the\\ncorrect': 13834, 'assign\\nthem': 13835, 'calling\\nprogram': 13836, \"repeat(msg='<empty>',\": 13837, 'num=1):\\n': 13838, 'num\\n>>>': 13839, \"repeat(num=3)\\n'<empty><empty><empty>'\\n>>>\": 13840, \"repeat(msg='alice')\\n'alice'\\n>>>\": 13841, 'repeat(num=5,': 13842, \"msg='alice')\\n'alicealicealicealicealice'\\n\\n\\n\\n\\n\\nthese\": 13843, 'unnamed': 13844, 'takes\\nan': 13845, '*args': 13846, 'and\\nan': 13847, '**kwargs': 13848, '.\\n(dictionaries': 13849, 'generic(*args,': 13850, '**kwargs):\\n': 13851, 'print(args)\\n': 13852, 'print(kwargs)\\n': 13853, 'generic(1,': 13854, 'african': 13855, 'swallow,': 13856, 'monty=python)\\n(1,': 13857, \"'african\": 13858, \"swallow')\\n{'monty':\": 13859, \"'python'}\\n\\n\\n\\nwhen\": 13860, 'aspect': 13861, 'which\\noperates': 13862, '*song': 13863, \"that\\nthere's\": 13864, 'song': 13865, \"[['four',\": 13866, \"'calling',\": 13867, \"'birds'],\\n\": 13868, \"['three',\": 13869, \"'hens'],\\n\": 13870, \"['two',\": 13871, \"'turtle',\": 13872, \"'doves']]\\n>>>\": 13873, 'list(zip(song[0],': 13874, 'song[1],': 13875, \"song[2]))\\n[('four',\": 13876, \"'two'),\": 13877, \"('calling',\": 13878, \"'turtle'),\": 13879, \"('birds',\": 13880, \"'hens',\": 13881, \"'doves')]\\n>>>\": 13882, \"list(zip(*song))\\n[('four',\": 13883, \"'doves')]\\n\\n\\n\\nit\": 13884, 'convenient\\nshorthand,': 13885, 'song[0],': 13886, 'song[2]': 13887, 'function\\ndefinition,': 13888, 'freq_words(file,': 13889, 'min=1,': 13890, 'num=10):\\n': 13891, 'word_tokenize(text)\\n': 13892, '.freqdist(t': 13893, 'min)\\n': 13894, '.most_common(num)\\n>>>': 13895, 'fw': 13896, \"freq_words('ch01\": 13897, \".rst',\": 13898, 'min=4,': 13899, 'num=10)\\n>>>': 13900, 'num=10,': 13901, 'min=4)\\n\\n\\n\\na': 13902, 'side-effect': 13903, 'optionality': 13904, 'happy': 13905, \"value:\\nfreq_words('ch01\": 13906, 'min=4),': 13907, 'reports': 13908, 'its\\nprogress': 13909, 'set:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 13910, 'verbose=false):\\n': 13911, 'freqdist()\\n': 13912, 'verbose:': 13913, 'print(opening,': 13914, 'file)\\n': 13915, 'print(read': 13916, '%d': 13917, 'len(file))\\n': 13918, 'word_tokenize(text):\\n': 13919, 'min:\\n': 13920, 'freqdist[word]': 13921, '.n()': 13922, 'print(': 13923, 'sep=)\\n': 13924, 'print\\n': 13925, '.most_common(num)\\n\\n\\n\\n\\ncaution!\\ntake': 13926, 'mutable': 13927, 'debugging': 13928, '.\\n\\n\\ncaution!\\nif': 13929, 'to\\nclose': 13930, 'will\\nclose': 13931, 'open(lexicon': 13932, '.txt)': 13933, 'data\\n\\n\\n\\n\\n\\n\\n\\n4': 13934, '.6\\xa0\\xa0\\xa0program': 13935, 'development\\nprogramming': 13936, 'skill': 13937, 'acquired': 13938, 'of\\nexperience': 13939, 'key\\nhigh-level': 13940, 'abilities': 13941, 'design': 13942, 'manifestation': 13943, 'in\\nstructured': 13944, 'familiarity\\nwith': 13945, 'diagnostic': 13946, 'trouble-shooting': 13947, 'which\\ndoes': 13948, 'exhibit': 13949, 'describes': 13950, 'and\\nhow': 13951, 'organize': 13952, 'multi-module': 13953, 'various\\nkinds': 13954, 'development,': 13955, 'can\\ndo': 13956, 'still,': 13957, '.\\n\\nstructure': 13958, 'module\\nthe': 13959, 'logically-related': 13960, 'functions\\ntogether': 13961, 'facilitate': 13962, 'be\\nkept': 13963, 'separators,\\nor': 13964, 'extn': 13965, '.inf': 13966, 'updated,\\nyou': 13967, 'could\\ncontain': 13968, 'as\\nsyntax': 13969, 'as\\nplotting': 13970, 'some\\nexamples': 13971, 'emulate': 13972, 'your\\nsystem': 13973, '__file__': 13974, '.metrics': 13975, '.distance': 13976, \".__file__\\n'/usr/lib/python2\": 13977, '.5/site-packages/nltk/metrics/distance': 13978, \".pyc'\\n\\n\\n\\nthis\": 13979, 'compiled': 13980, '.pyc': 13981, \"and\\nyou'll\": 13982, 'same\\ndirectory': 13983, '.\\nalternatively,': 13984, 'web\\nat': 13985, 'http://code': 13986, '.com/p/nltk/source/browse/trunk/nltk/nltk/metrics/distance': 13987, '.\\nlike': 13988, 'distance': 13989, 'comment\\nlines': 13990, 'title': 13991, '.\\n(since': 13992, 'distributed,': 13993, 'the\\ncode': 13994, 'available,': 13995, 'copyright': 13996, '.)\\nnext': 13997, 'module-level': 13998, 'when\\nsomeone': 13999, 'help(nltk': 14000, '.distance)': 14001, '.\\n\\n\\n#': 14002, 'toolkit:': 14003, 'metrics\\n#\\n#': 14004, '(c)': 14005, '2001-2019': 14006, 'project\\n#': 14007, 'author:': 14008, '<edloper@gmail': 14009, '.com>\\n#': 14010, '<stevenbird1@gmail': 14011, 'tom': 14012, 'lippincott': 14013, '<tom@cs': 14014, '.columbia': 14015, '.edu>\\n#': 14016, 'url:': 14017, '<http://nltk': 14018, '.org/>\\n#': 14019, '.txt\\n#\\n\\n\\ndistance': 14020, 'metrics': 14021, '.\\n\\ncompute': 14022, '(usually': 14023, 'metrics,': 14024, 'requirements:\\n\\n1': 14025, 'd(a,': 14026, 'a)': 14027, '0\\n2': 14028, 'b)': 14029, '0\\n3': 14030, 'c)': 14031, 'd(b,': 14032, 'c)\\n\\n\\nafter': 14033, 'module,\\nthen': 14034, 'variables,\\nfollowed': 14035, 'most\\nof': 14036, 'classes,': 14037, 'block\\nof': 14038, 'object-oriented': 14039, '.\\n(most': 14040, 'demo()': 14041, '.)\\n\\nnote\\nsome': 14042, '_helper(),\\nsince': 14043, 'hide': 14044, 'one,\\nusing': 14045, 'idiom:': 14046, 'externally': 14047, '__all__': 14048, \"['edit_distance',\": 14049, \"'jaccard_distance']\": 14050, '.\\n\\n\\n\\nmulti-module': 14051, 'programs\\nsome': 14052, 'loading': 14053, 'from\\na': 14054, 'stable': 14055, 'visualizations': 14056, 'functions\\nfrom': 14057, 'scenario': 14058, 'depicted': 14059, 'program:': 14060, 'my_program': 14061, 'modules;': 14062, 'localized': 14063, 'while\\ncommon': 14064, 'visualization': 14065, '.\\n\\nby': 14066, 'dividing': 14067, 'to\\naccess': 14068, 'elsewhere,': 14069, 'simple\\nand': 14070, 'growing': 14071, 'involving\\na': 14072, 'designing': 14073, 'a\\ncomplex': 14074, 'engineering': 14075, '.\\n\\n\\nsources': 14076, 'error\\nmastery': 14077, 'problem-solving': 14078, 'to\\ndraw': 14079, 'trivial': 14080, 'as\\na': 14081, 'mis-placed': 14082, 'bugs': 14083, 'damage\\nthey': 14084, 'creep': 14085, 'unnoticed,': 14086, 'later\\nwhen': 14087, '.\\nsometimes,': 14088, 'fixing': 14089, 'another,': 14090, 'impression\\nthat': 14091, 'reassurance': 14092, 'are\\nspontaneous': 14093, 'fault': 14094, '.\\nflippancy': 14095, 'aside,': 14096, 'for\\nit': 14097, 'faulty': 14098, 'algorithm,': 14099, 'or\\neven': 14100, 'examples\\nof': 14101, 'unexpected': 14102, '.01,': 14103, 'three\\ncomponents': 14104, 'initially\\ndecomposed': 14105, \"split('\": 14106, \".')\": 14107, 'broke': 14108, 'phd,': 14109, 'synset\\nname': 14110, '.d': 14111, \"rsplit('\": 14112, 'splits,': 14113, 'rightmost': 14114, 'intact': 14115, 'tested\\nthe': 14116, 'released,': 14117, 'weeks': 14118, 'detected\\nthe': 14119, '.com/p/nltk/issues/detail?id=297)': 14120, 'the\\nauthors': 14121, 'antonyms': 14122, 'though\\nthe': 14123, 'database': 14124, 'antonym': 14125, 'misunderstanding': 14126, 'itself:': 14127, 'for\\nlemmas,': 14128, 'misunderstanding\\nof': 14129, '.com/p/nltk/issues/detail?id=98)': 14130, '.\\n\\n\\nthird,': 14131, 'assumption': 14132, 'relative\\nscope': 14133, '.%s': 14134, '.%02d': 14135, 'run-time\\nerror': 14136, 'typeerror:': 14137, 'percent': 14138, 'than\\nthe': 14139, 'to\\nforce': 14140, 'are\\ndefining': 14141, 'a\\ngiven': 14142, 'initial\\nvalue': 14143, 'parameter:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 14144, 'find_words(text,': 14145, 'wordlength,': 14146, 'result=[]):\\n': 14147, 'wordlength:\\n': 14148, 'result\\n>>>': 14149, \"find_words(['omg',\": 14150, \"'teh',\": 14151, \"'lolcat',\": 14152, \"'sitted',\": 14153, \"'mat'],\": 14154, \"\\n['omg',\": 14155, \"'mat']\\n>>>\": 14156, \"['ur'])\": 14157, \"\\n['ur',\": 14158, \"'on']\\n>>>\": 14159, \"'mat',\": 14160, \"'omg',\": 14161, \"'mat']\\n\\n\\n\\nthe\": 14162, 'find_words()': 14163, 'three-letter\\nwords': 14164, 'result,\\na': 14165, 'one-element': 14166, \"['ur'],\": 14167, 'expected,': 14168, 'the\\nother': 14169, '\\nwe': 14170, 'result!\\neach': 14171, 'will\\nsimply': 14172, 'call,': 14173, \"program's\\nbehavior\": 14174, 'default\\nvalue': 14175, 'is\\ncreated': 14176, 'once,': 14177, 'loads': 14178, '.\\n\\n\\ndebugging': 14179, 'techniques\\nsince': 14180, 'assumptions,\\nthe': 14181, '.\\nlocalize': 14182, 'variables,': 14183, 'progressed': 14184, 'exception': 14185, 'run-time': 14186, 'stack': 14187, 'trace,\\npinpointing': 14188, 'reduce': 14189, 'smallest\\nsize': 14190, 'line\\nof': 14191, 'to\\nrecreate': 14192, 'situation': 14193, 'some\\nvariables': 14194, 'copy-paste': 14195, 'offending': 14196, 'session\\nand': 14197, 'reading\\nsome': 14198, 'purport': 14199, 'do\\nthe': 14200, 'explaining': 14201, 'to\\nsomeone': 14202, 'else,': 14203, 'debugger': 14204, 'monitor': 14205, 'execution\\nof': 14206, 'breakpoints),\\nand': 14207, 'pdb\\n>>>': 14208, 'mymodule\\n>>>': 14209, 'pdb': 14210, \".run('mymodule\": 14211, \".myfunction()')\\n\\n\\n\\nit\": 14212, '(pdb)': 14213, 'instructions\\nto': 14214, '.\\ntyping': 14215, 'and\\nstop': 14216, 'function\\nand': 14217, 'similar,\\nbut': 14218, 'stops': 14219, 'the\\nbreak': 14220, 'breakpoints': 14221, 'type\\ncontinue': 14222, 'continue': 14223, 'breakpoint': 14224, '.\\ntype': 14225, 'find_words()\\nfunction': 14226, 'arose': 14227, 'was\\ncalled': 14228, ',\\nusing': 14229, 'smallest': 14230, 'the\\ndebugger': 14231, \"find_words(['cat'],\": 14232, \"\\n['cat']\\n>>>\": 14233, \".run(find_words(['dog'],\": 14234, '3))': 14235, '\\n>': 14236, '<string>(1)<module>()\\n(pdb)': 14237, 'step\\n--call--\\n>': 14238, '<stdin>(1)find_words()\\n(pdb)': 14239, 'args\\ntext': 14240, \"['dog']\\nwordlength\": 14241, '3\\nresult': 14242, \"['cat']\\n\\n\\n\\nhere\": 14243, 'debugger:': 14244, 'inside\\nthe': 14245, 'args': 14246, 'parameters)': 14247, \"['cat'],\": 14248, 'not\\nthe': 14249, 'helped': 14250, 'problem,\\nprompting': 14251, '.\\n\\n\\ndefensive': 14252, 'programming\\nin': 14253, 'debugging,': 14254, 'adopt\\nsome': 14255, 'habits': 14256, '20-line\\nprogram': 14257, 'bottom-up': 14258, 'of\\nsmall': 14259, 'these\\npieces': 14260, 'unit,': 14261, 'works\\nas': 14262, 'code,\\nspecifying': 14263, 'assert(isinstance(text,': 14264, 'list))': 14265, 'your\\ncode': 14266, 'assertionerror\\nand': 14267, 'immediate': 14268, 'notification': 14269, 'bug,': 14270, 'bugfix': 14271, 're-running': 14272, \"isn't\": 14273, 'fixed,': 14274, 'trap': 14275, 'changing\\nthe': 14276, 'magically': 14277, 'change,': 14278, 'articulate': 14279, 'what\\nis': 14280, 'undo': 14281, 'change\\nif': 14282, 'resolved': 14283, 'functionality,': 14284, 'bugs,\\nit': 14285, 'suite': 14286, 'regression': 14287, 'testing,': 14288, 'detect\\nsituations': 14289, 'regresses': 14290, 'unintended': 14291, 'breaking': 14292, 'that\\nused': 14293, 'framework\\nin': 14294, 'file\\nof': 14295, 'like\\nan': 14296, 'session,': 14297, 'seen\\nmany': 14298, 'finds,\\nand': 14299, 'original\\nfile': 14300, 'mismatch,': 14301, 'actual\\nvalues': 14302, 'at\\nhttp://docs': 14303, '.org/library/doctest': 14304, 'its\\nvalue': 14305, 'for\\nensuring': 14306, 'sync': 14307, 'strategy': 14308, 'to\\nset': 14309, 'clearly,': 14310, 'function\\nnames,': 14311, 'into\\nfunctions': 14312, 'well-documented': 14313, '.7\\xa0\\xa0\\xa0algorithm': 14314, 'design\\nthis': 14315, 'algorithmic': 14316, 'adapting\\nan': 14317, 'are\\nseveral': 14318, 'alternatives,': 14319, 'knowledge\\nabout': 14320, '.\\nwhole': 14321, 'introduce\\nsome': 14322, 'elaborate': 14323, 'prevalent\\nin': 14324, 'divide-and-conquer': 14325, 'attack': 14326, 'n/2,\\nsolve': 14327, 'problems,': 14328, 'pile': 14329, 'card': 14330, 'splitting': 14331, 'people\\nto': 14332, '(they': 14333, 'turn)': 14334, 'piles': 14335, 'back,': 14336, 'merge': 14337, 'divide-and-conquer:': 14338, 'array,': 14339, 'and\\nsort': 14340, '(recursively);': 14341, 'whole\\nlist': 14342, '(again': 14343, 'recursively);': 14344, '.\\n\\nanother': 14345, 'open\\nthe': 14346, 'somewhere': 14347, 'middle': 14348, 'current\\npage': 14349, 'first\\nhalf;': 14350, 'called\\nbinary': 14351, 'splits': 14352, 'design,': 14353, 'problem\\nby': 14354, 'transforming': 14355, 'duplicate': 14356, 'pre-sort\\nthe': 14357, 'elements\\nare': 14358, '.\\n\\nrecursion\\nthe': 14359, 'property:\\nto': 14360, 'and\\nthen': 14361, 'n/2': 14362, 'recursion': 14363, 'simplifies': 14364, 'problem,\\nand': 14365, 'instances\\nof': 14366, 'solution\\nfor': 14367, 'to\\ncalculate': 14368, '(n=1),': 14369, 'is\\njust': 14370, 'two\\nwords,': 14371, 'three\\nwords': 14372, 'words,\\nthere': 14373, '1\\nways': 14374, 'factorial': 14375, 'factorial1(n):\\n': 14376, '*=': 14377, '(i+1)\\n': 14378, 'result\\n\\n\\n\\nhowever,': 14379, 'recursive': 14380, 'problem,\\nbased': 14381, 'observation': 14382, 'to\\nconstruct': 14383, 'orderings': 14384, 'then\\nfor': 14385, 'ordering,': 14386, 'can\\ninsert': 14387, 'word:': 14388, 'start,': 14389, 'n-2\\nboundaries': 14390, 'base': 14391, 'single\\nword,': 14392, 'ordering': 14393, 'factorial2(n):\\n': 14394, 'factorial2(n-1)\\n\\n\\n\\n\\nthese': 14395, 'iteration\\nwhile': 14396, 'deeply-nested': 14397, 'the\\nwordnet': 14398, 'rooted': 14399, 'the\\nsize': 14400, 'together\\n(we': 14401, 'itself)': 14402, 'following\\nfunction': 14403, 'size1()': 14404, 'work;': 14405, 'size1():\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 14406, 'size1(s):\\n': 14407, 'sum(size1(child)': 14408, '.hyponyms())\\n\\n\\n\\nwe': 14409, 'iterative': 14410, 'processes\\nthe': 14411, 'layers': 14412, 'layer': 14413, 'the\\nhyponyms': 14414, 'layer\\nby': 14415, 'maintains': 14416, 'size2(s):\\n': 14417, '[s]': 14418, 'layer:\\n': 14419, 'len(layer)': 14420, '[h': 14421, '.hyponyms()]': 14422, 'total\\n\\n\\n\\nnot': 14423, 'longer,': 14424, 'forces': 14425, 'procedurally,': 14426, 'happening': 14427, 'ourselves\\nthat': 14428, 'import\\nstatement,': 14429, 'abbreviate': 14430, 'wn:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 14431, \".synset('dog\": 14432, 'size1(dog)\\n190\\n>>>': 14433, 'size2(dog)\\n190\\n\\n\\n\\nas': 14434, 'recursion,': 14435, 'construct\\na': 14436, 'trie': 14437, 'used\\nfor': 14438, 'name\\nis': 14439, 'retrieval)': 14440, 'trie\\ncontained': 14441, 'trie,': 14442, \"trie['c']\": 14443, 'smaller\\ntrie': 14444, 'held': 14445, '.\\n4': 14446, 'trie,\\nusing': 14447, '(3)': 14448, 'chien': 14449, '(french': 14450, 'dog),\\nwe': 14451, 'recursively': 14452, 'hien\\ninto': 14453, 'sub-trie': 14454, 'continues\\nuntil': 14455, 'we\\nstore': 14456, 'dog)': 14457, 'insert(trie,': 14458, 'key,': 14459, 'value):\\n': 14460, 'key:\\n': 14461, 'key[0],': 14462, 'key[1:]\\n': 14463, 'trie:\\n': 14464, 'trie[first]': 14465, '{}\\n': 14466, 'insert(trie[first],': 14467, 'rest,': 14468, 'value)\\n': 14469, \"trie['value']\": 14470, 'value\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 14471, '{}\\n>>>': 14472, \"'chat',\": 14473, \"'cat')\\n>>>\": 14474, \"'chien',\": 14475, \"'dog')\\n>>>\": 14476, \"'chair',\": 14477, \"'flesh')\\n>>>\": 14478, \"'chic',\": 14479, \"'stylish')\\n>>>\": 14480, 'dict(trie)': 14481, 'nicer': 14482, 'printing\\n>>>': 14483, \"trie['c']['h']['a']['t']['value']\\n'cat'\\n>>>\": 14484, '.pprint(trie,': 14485, \"width=40)\\n{'c':\": 14486, \"{'h':\": 14487, \"{'a':\": 14488, \"{'t':\": 14489, \"{'value':\": 14490, \"'cat'}},\\n\": 14491, \"{'i':\": 14492, \"{'r':\": 14493, \"'flesh'}}},\\n\": 14494, \"'i':\": 14495, \"{'e':\": 14496, \"{'n':\": 14497, \"'dog'}}}\\n\": 14498, \"{'c':\": 14499, \"'stylish'}}}}}\\n\\n\\nexample\": 14500, '(code_trie': 14501, 'trie:': 14502, 'dictionary\\nstructure;': 14503, 'nesting': 14504, 'prefix,\\nand': 14505, 'continuations': 14506, '.\\n\\n\\ncaution!\\ndespite': 14507, 'simplicity': 14508, 'be\\npushed': 14509, 'stack,': 14510, 'completed,': 14511, 'execution\\ncan': 14512, 'reason,': 14513, 'iterative\\nsolutions': 14514, '.\\n\\n\\n\\nspace-time': 14515, 'tradeoffs\\nwe': 14516, 'significantly': 14517, 'speed': 14518, 'auxiliary\\ndata': 14519, 'implements': 14520, 'simple\\ntext': 14521, 'it\\nprovides': 14522, 'faster': 14523, 'raw(file):\\n': 14524, 'contents)\\n': 14525, 'contents\\n\\ndef': 14526, 'snippet(doc,': 14527, 'term):\\n': 14528, \"'*30\": 14529, 'raw(doc)': 14530, \"'*30\\n\": 14531, '.index(term)\\n': 14532, 'text[pos-30:pos+30]\\n\\nprint(building': 14533, '.)\\nfiles': 14534, '.movie_reviews': 14535, '.abspaths()\\nidx': 14536, '.index((w,': 14537, 'f)': 14538, 'raw(f)': 14539, '.split())\\n\\nquery': 14540, \"''\\nwhile\": 14541, 'quit:\\n': 14542, 'input(query>': 14543, 'raw_input()': 14544, '2\\n': 14545, 'idx:\\n': 14546, 'doc': 14547, 'idx[query]:\\n': 14548, 'print(snippet(doc,': 14549, 'query))\\n': 14550, 'print(not': 14551, 'found)\\n\\n\\nexample': 14552, '(code_search_documents': 14553, 'system\\n\\na': 14554, 'space-time': 14555, 'tradeoff': 14556, 'corpus\\nwith': 14557, 'invert': 14558, 'its\\nidentifier': 14559, 'preprocessed,': 14560, '.\\nany': 14561, '.11\\nfor': 14562, '.\\n\\n\\n\\n\\n\\n\\xa0\\n\\ndef': 14563, 'preprocess(tagged_corpus):\\n': 14564, 'set()\\n': 14565, 'tagged_corpus:\\n': 14566, '.add(word)\\n': 14567, '.add(tag)\\n': 14568, 'wm': 14569, 'dict((w,': 14570, 'enumerate(words))\\n': 14571, 'tm': 14572, 'dict((t,': 14573, 'enumerate(tags))\\n': 14574, '[[(wm[w],': 14575, 'tm[t])': 14576, '(w,': 14577, 'tagged_corpus]\\n\\n\\nexample': 14578, '(code_strings_to_ints': 14579, 'integers\\n\\nanother': 14580, 'maintaining': 14581, 'an\\nexisting': 14582, 'membership\\nof': 14583, 'membership': 14584, 'the\\ncorresponding': 14585, 'claim': 14586, 'timeit': 14587, 'timer': 14588, 'times,': 14589, 'setup': 14590, 'executed\\nonce': 14591, 'simulate': 14592, 'of\\n100,000': 14593, '\\nof': 14594, 'random\\nitem': 14595, '50%': 14596, 'timer\\n>>>': 14597, '100000\\n>>>': 14598, 'setup_list': 14599, 'random;': 14600, 'range(%d)': 14601, 'setup_set': 14602, 'set(range(%d))': 14603, '.randint(0,': 14604, '%d)': 14605, '(vocab_size': 14606, 'print(timer(statement,': 14607, 'setup_list)': 14608, '.timeit(1000))\\n2': 14609, '.78092288971\\n>>>': 14610, 'setup_set)': 14611, '.timeit(1000))\\n0': 14612, '.0037260055542\\n\\n\\n\\nperforming': 14613, '1000': 14614, 'seconds,\\nwhile': 14615, 'mere': 14616, '.0037': 14617, 'seconds,\\nor': 14618, 'orders': 14619, 'magnitude': 14620, 'faster!\\n\\n\\ndynamic': 14621, 'programming\\ndynamic': 14622, 'algorithms\\nwhich': 14623, \"term\\n'programming'\": 14624, 'expect,\\nto': 14625, 'planning': 14626, 'scheduling': 14627, 'a\\nproblem': 14628, 'sub-problems': 14629, 'computing\\nsolutions': 14630, 'repeatedly,': 14631, 'a\\nlookup': 14632, 'remainder': 14633, 'programming,\\nbut': 14634, 'parsing': 14635, '.\\npingala': 14636, '5th': 14637, 'century': 14638, '.c': 14639, '.,\\nand': 14640, 'wrote': 14641, 'treatise': 14642, 'sanskrit': 14643, 'prosody': 14644, 'chandas': 14645, 'shastra': 14646, '.\\nvirahanka': 14647, '6th': 14648, 'meter\\nof': 14649, 'syllables,': 14650, 'marked': 14651, 'unit': 14652, 'while\\nlong': 14653, 'l,': 14654, 'meter': 14655, 'v4': 14656, '{ll,': 14657, 'ssl,': 14658, 'sls,\\nlss,': 14659, 'ssss}': 14660, 'two\\nsubsets,': 14661, 'with\\ns,': 14662, '(1)': 14663, '(1)\\nv4': 14664, '=\\n': 14665, 'll,': 14666, 'lss\\n': 14667, 'prefixed': 14668, 'v2': 14669, '{l,': 14670, 'ss}\\n': 14671, 'sls,': 14672, 'ssss\\n': 14673, 'v3': 14674, '{sl,': 14675, 'ls,': 14676, 'sss}\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\ndef': 14677, 'virahanka1(n):\\n': 14678, '0:\\n': 14679, '[s]\\n': 14680, 'virahanka1(n-1)]\\n': 14681, '[l': 14682, 'virahanka1(n-2)]\\n': 14683, 'l\\n\\ndef': 14684, 'virahanka2(n):\\n': 14685, '[[],': 14686, '[s]]\\n': 14687, 'range(n-1):\\n': 14688, 'lookup[i+1]]\\n': 14689, 'lookup[i]]\\n': 14690, '.append(s': 14691, 'l)\\n': 14692, 'lookup[n]\\n\\ndef': 14693, 'virahanka3(n,': 14694, 'lookup={0:[],': 14695, '1:[s]}):\\n': 14696, 'lookup:\\n': 14697, 'virahanka3(n-1)]\\n': 14698, 'virahanka3(n-2)]\\n': 14699, 'lookup[n]': 14700, 'l\\n': 14701, 'lookup[n]\\n\\nfrom': 14702, 'memoize\\n@memoize\\ndef': 14703, 'virahanka4(n):\\n': 14704, 'virahanka4(n-1)]\\n': 14705, 'virahanka4(n-2)]\\n': 14706, 'l\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 14707, \"virahanka1(4)\\n['ssss',\": 14708, \"'ssl',\": 14709, \"'sls',\": 14710, \"'lss',\": 14711, \"'ll']\\n>>>\": 14712, \"virahanka2(4)\\n['ssss',\": 14713, \"virahanka3(4)\\n['ssss',\": 14714, \"virahanka4(4)\\n['ssss',\": 14715, \"'ll']\\n\\n\\nexample\": 14716, '.12': 14717, '(code_virahanka': 14718, '.12:': 14719, 'meter:': 14720, '(i)': 14721, 'recursive;': 14722, 'programming;\\n(iii)': 14723, 'top-down': 14724, 'programming;': 14725, '(iv)': 14726, 'memoization': 14727, 'observation,': 14728, 'called\\nvirahanka1()': 14729, 'meters,': 14730, 'compute\\nv3': 14731, 'v3,\\nwe': 14732, 'v1': 14733, 'call\\nstructure': 14734, '(2)\\nas': 14735, 'problem,': 14736, 'but\\nit': 14737, 'wasteful': 14738, 'large:\\nto': 14739, 'v20': 14740, 'technique,': 14741, '4,181': 14742, 'times;\\nand': 14743, 'v40': 14744, '63,245,986': 14745, 'times!\\na': 14746, 'table\\nand': 14747, 'values,': 14748, 'virahanka2()': 14749, 'a\\ndynamic': 14750, 'filling': 14751, 'a\\ntable': 14752, '(called': 14753, 'lookup)': 14754, 'the\\nproblem,': 14755, 'crucially,': 14756, 'each\\nsub-problem': 14757, 'solved': 14758, 'smaller\\nproblems': 14759, 'the\\nbottom-up': 14760, 'unfortunately': 14761, 'applications,': 14762, 'it\\nmay': 14763, 'for\\nsolving': 14764, 'wasted': 14765, 'avoided\\nusing': 14766, 'is\\nillustrated': 14767, 'virahanka3()': 14768, 'approach,': 14769, 'avoids\\nthe': 14770, 'wastage': 14771, 'virahanka1()': 14772, 'has\\npreviously': 14773, 'not,': 14774, 'result\\nrecursively': 14775, 'stores': 14776, 'return\\nthe': 14777, 'virahanka4(),\\nis': 14778, 'decorator': 14779, 'memoize,\\nwhich': 14780, 'housekeeping': 14781, 'done\\nby': 14782, 'cluttering': 14783, 'previous\\ncall': 14784, 'parameters,\\nit': 14785, 'recalculating': 14786, '.\\n(this': 14787, 'encounter': 14788, '.8\\xa0\\xa0\\xa0a': 14789, 'libraries\\npython': 14790, 'libraries,': 14791, 'specialized': 14792, 'extend\\nthe': 14793, 'realize': 14794, 'power\\nof': 14795, '.\\n\\nmatplotlib\\npython': 14796, 'sophisticated\\nplotting': 14797, 'matlab-style': 14798, 'interface,': 14799, 'from\\nhttp://matplotlib': 14800, '.sourceforge': 14801, '.net/': 14802, 'print\\nstatements': 14803, 'lined': 14804, 'columns': 14805, 'display\\nnumerical': 14806, 'detect\\npatterns': 14807, 'numbers\\nshowing': 14808, 'classified\\nby': 14809, '.13': 14810, 'graphical\\nformat': 14811, '.14': 14812, 'display)': 14813, 'arange\\nfrom': 14814, 'pyplot\\n\\ncolors': 14815, \"'rgbcmyk'\": 14816, 'red,': 14817, 'green,': 14818, 'blue,': 14819, 'cyan,': 14820, 'magenta,': 14821, 'yellow,': 14822, 'black\\n\\ndef': 14823, 'bar_chart(categories,': 14824, 'counts):\\n': 14825, 'chart': 14826, 'category\\n': 14827, 'ind': 14828, 'arange(len(words))\\n': 14829, '(len(categories)': 14830, '1)\\n': 14831, 'bar_groups': 14832, 'range(len(categories)):\\n': 14833, 'bars': 14834, 'pyplot': 14835, '.bar(ind+c*width,': 14836, 'counts[categories[c]],': 14837, 'width,\\n': 14838, 'color=colors[c': 14839, 'len(colors)])\\n': 14840, '.append(bars)\\n': 14841, '.xticks(ind+width,': 14842, 'words)\\n': 14843, '.legend([b[0]': 14844, 'bar_groups],': 14845, 'categories,': 14846, \"loc='upper\": 14847, \"left')\\n\": 14848, \".ylabel('frequency')\\n\": 14849, \".title('frequency\": 14850, \"genre')\\n\": 14851, '.show()\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 14852, \"'adventure']\\n>>>\": 14853, 'cfdist': 14854, 'genres\\n': 14855, '.words(categories=genre)\\n': 14856, 'modals)\\n': 14857, 'genres:\\n': 14858, 'counts[genre]': 14859, '[cfdist[genre][word]': 14860, 'modals]\\n>>>': 14861, 'bar_chart(genres,': 14862, 'counts)\\n\\n\\nexample': 14863, '(code_modal_plot': 14864, '.13:': 14865, 'corpus\\n\\n\\n\\nfigure': 14866, '.14:': 14867, 'this\\nvisualization': 14868, '.\\n\\n\\nfrom': 14869, 'have\\nalmost': 14870, 'fly': 14871, 'visitors': 14872, 'specify\\nsearch': 14873, 'dynamically': 14874, 'generated\\nvisualization': 14875, 'agg': 14876, 'backend': 14877, 'matplotlib,\\nwhich': 14878, 'raster': 14879, '(pixel)': 14880, 'images': 14881, 'the\\nresult': 14882, '.show(),': 14883, 'file\\nusing': 14884, '.savefig()': 14885, 'filename\\nthen': 14886, 'directs': 14887, 'use,': 14888, 'pyplot\\n>>>': 14889, \"use('agg')\": 14890, \".savefig('modals\": 14891, \".png')\": 14892, \"print('content-type:\": 14893, \"text/html')\\n>>>\": 14894, 'print()\\n>>>': 14895, \"print('<html><body>')\\n>>>\": 14896, \"print('<img\": 14897, 'src=modals': 14898, \".png/>')\\n>>>\": 14899, \"print('</body></html>')\\n\\n\\n\\n\\n\\nnetworkx\\nthe\": 14900, 'networkx': 14901, 'edges,': 14902, 'is\\navailable': 14903, 'https://networkx': 14904, '.lanl': 14905, '.gov/': 14906, '.\\nnetworkx': 14907, 'to\\nvisualize': 14908, 'we\\nintroduced': 14909, '.15\\ninitializes': 14910, 'traverses\\nthe': 14911, 'traversal': 14912, ',\\napplying': 14913, 'in\\n4': 14914, 'nx\\nimport': 14915, 'matplotlib\\nfrom': 14916, 'wn\\n\\ndef': 14917, 'traverse(graph,': 14918, 'node):\\n': 14919, '.depth[node': 14920, '.name]': 14921, 'node': 14922, '.shortest_path_distance(start)\\n': 14923, '.hyponyms():\\n': 14924, '.add_edge(node': 14925, '.name,': 14926, '.name)': 14927, 'child)': 14928, '\\n\\ndef': 14929, 'hyponym_graph(start):\\n': 14930, 'nx': 14931, '.graph()': 14932, '.depth': 14933, 'traverse(g,': 14934, 'start)\\n': 14935, 'g\\n\\ndef': 14936, 'graph_draw(graph):\\n': 14937, '.draw_graphviz(graph,\\n': 14938, 'node_size': 14939, '[16': 14940, '.degree(n)': 14941, 'graph],\\n': 14942, 'node_color': 14943, '[graph': 14944, '.depth[n]': 14945, 'with_labels': 14946, 'false)\\n': 14947, '.pyplot': 14948, 'hyponym_graph(dog)\\n>>>': 14949, 'graph_draw(graph)\\n\\n\\nexample': 14950, '.15': 14951, '(code_networkx': 14952, '.15:': 14953, 'libraries\\n\\n\\n\\nfigure': 14954, '.16:': 14955, 'matplotlib:': 14956, 'displayed,': 14957, 'darkest': 14958, 'middle);\\nnode': 14959, 'node,': 14960, '.01;': 14961, 'produced\\nby': 14962, '.\\n\\n\\n\\ncsv\\nlanguage': 14963, 'tabulations,': 14964, 'study,': 14965, 'linguistic\\nfeatures': 14966, 'csv': 14967, 'format:\\n\\nsleep,': 14968, 'sli:p,': 14969, '.i,': 14970, '.\\nwalk,': 14971, 'wo:k,': 14972, '.intr,': 14973, 'lifting': 14974, 'setting': 14975, 'foot': 14976, '.\\nwake,': 14977, 'weik,': 14978, 'intrans,': 14979, 'cease': 14980, 'sleep\\n\\nwe': 14981, '.csv': 14982, 'csv\\n>>>': 14983, 'input_file': 14984, '.csv,': 14985, 'rb)': 14986, '.reader(input_file):': 14987, \"print(row)\\n['sleep',\": 14988, \"'sli:p',\": 14989, \"'v\": 14990, \".i',\": 14991, \"'a\": 14992, \".']\\n['walk',\": 14993, \"'wo:k',\": 14994, \".intr',\": 14995, \"'progress\": 14996, \".']\\n['wake',\": 14997, \"'weik',\": 14998, \"'intrans',\": 14999, \"'cease\": 15000, \"sleep']\\n\\n\\n\\neach\": 15001, 'numerical\\ndata,': 15002, 'using\\nint()': 15003, 'float()': 15004, '.\\n\\n\\nnumpy\\nthe': 15005, '.\\nnumpy': 15006, 'multi-dimensional': 15007, 'access:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 15008, 'array\\n>>>': 15009, 'cube': 15010, 'array([': 15011, '[[0,0,0],': 15012, '[1,1,1],': 15013, '[2,2,2]],\\n': 15014, '[[3,3,3],': 15015, '[4,4,4],': 15016, '[5,5,5]],\\n': 15017, '[[6,6,6],': 15018, '[7,7,7],': 15019, '[8,8,8]]': 15020, '])\\n>>>': 15021, 'cube[1,1,1]\\n4\\n>>>': 15022, 'cube[2]': 15023, '.transpose()\\narray([[6,': 15024, '8],\\n': 15025, '[6,': 15026, '8]])\\n>>>': 15027, 'cube[2,1:]\\narray([[7,': 15028, '7],\\n': 15029, '[8,': 15030, '8]])\\n\\n\\n\\nnumpy': 15031, 'linear': 15032, 'algebra': 15033, 'perform\\nsingular': 15034, 'decomposition': 15035, 'matrix,': 15036, 'used\\nin': 15037, 'latent': 15038, 'implicit\\nconcepts': 15039, 'linalg\\n>>>': 15040, 'a=array([[4,0],': 15041, '[3,-5]])\\n>>>': 15042, 'u,s,vt': 15043, 'linalg': 15044, '.svd(a)\\n>>>': 15045, 'u\\narray([[-0': 15046, '.4472136': 15047, '-0': 15048, '.89442719],\\n': 15049, '[-0': 15050, '.89442719,': 15051, ']])\\n>>>': 15052, 's\\narray([': 15053, '.32455532,': 15054, '.16227766])\\n>>>': 15055, 'vt\\narray([[-0': 15056, '.70710678,': 15057, '.70710678],\\n': 15058, \".70710678]])\\n\\n\\n\\nnltk's\": 15059, 'clustering': 15060, '.cluster': 15061, 'arrays,\\nand': 15062, 'k-means': 15063, 'gaussian': 15064, 'clustering,\\ngroup': 15065, 'agglomerative': 15066, 'dendrogram': 15067, 'details,': 15068, '.cluster)': 15069, '.\\n\\n\\nother': 15070, 'libraries\\nthere': 15071, 'the\\nhelp': 15072, 'http://pypi': 15073, 'relational': 15074, 'databases': 15075, 'mysql-python)\\nand': 15076, 'pylucene)': 15077, 'formats\\nsuch': 15078, 'msword,': 15079, '(pypdf,': 15080, 'pywin32,': 15081, '.etree),\\nrss': 15082, 'feeds': 15083, 'feedparser),\\nand': 15084, 'mail': 15085, 'imaplib,': 15086, 'email)': 15087, \".9\\xa0\\xa0\\xa0summary\\n\\npython's\": 15088, 'references;\\ne': 15089, 'operation\\non': 15090, 'b,': 15091, 'objects,\\nwhile': 15092, 'distinction\\nparallels': 15093, '.\\nstrings,': 15094, 'supporting\\ncommon': 15095, 'sorted(),': 15096, 'and\\nmembership': 15097, 'compact,\\nreadable': 15098, 'manually-incremented': 15099, 'usually\\nunnecessary;': 15100, 'enumerated,': 15101, 'essential': 15102, 'abstraction:': 15103, 'concepts\\nto': 15104, 'passing,': 15105, 'scope,': 15106, 'namespace:': 15107, 'visible\\noutside': 15108, 'declared': 15109, '.\\nmodules': 15110, 'variables\\nand': 15111, 'visible': 15112, '.\\ndynamic': 15113, 'nlp\\nthat': 15114, 'computations': 15115, 'avoid\\nunnecessary': 15116, 'recomputation': 15117, '.10\\xa0\\xa0\\xa0further': 15118, 'python,\\nand': 15119, 'scratched': 15120, 'surface,': 15121, 'for\\nthis': 15122, 'to\\nunderstand': 15123, '.org/library/functions': 15124, 'and\\nhttp://docs': 15125, '.org/library/stdtypes': 15126, 'generators': 15127, 'importance': 15128, 'efficiency;\\nfor': 15129, 'iterators,': 15130, 'topic,\\nsee': 15131, '.org/library/itertools': 15132, '.\\nconsult': 15133, 'multimedia': 15134, 'processing,\\nincluding': 15135, '(guzdial,': 15136, 'aware': 15137, 'that\\nyour': 15138, 'version\\nof': 15139, 'easily\\ncheck': 15140, 'have,': 15141, 'sys;': 15142, 'sys': 15143, '.version': 15144, '.\\nversion-specific': 15145, '.org/doc/versions/': 15146, '.\\nalgorithm': 15147, 'rich': 15148, 'some\\ngood': 15149, '(harel,': 15150, '(levitin,': 15151, '(knuth,': 15152, '.\\nuseful': 15153, 'guidance': 15154, 'provided\\nin': 15155, '(hunt': 15156, 'thomas,': 15157, '2000)': 15158, '(mcconnell,': 15159, '2004)': 15160, '.11\\xa0\\xa0\\xa0exercises\\n\\n☼': 15161, 'facility': 15162, 'help(str),': 15163, 'help(list),': 15164, 'help(tuple)': 15165, 'flanked': 15166, 'underscore;': 15167, 'shows,': 15168, 'something\\nmore': 15169, '.__getitem__(y)': 15170, 'long-winded\\nway': 15171, 'x[y]': 15172, 'tuples\\nand': 15173, 'on\\ntuples': 15174, 'generates\\na': 15175, \"['is',\": 15176, \"'nlp',\": 15177, \"'fun',\": 15178, \"'?']\": 15179, 'use\\na': 15180, 'words[1]': 15181, 'words[2])\\nand': 15182, 'temporary': 15183, 'transform': 15184, \"['nlp',\": 15185, \"'!']\": 15186, 'transformation\\nusing': 15187, 'cmp,': 15188, 'help(cmp)': 15189, 'operators?\\n\\n☼': 15190, 'sliding': 15191, 'n-grams\\nbehave': 15192, 'limiting': 15193, 'cases:': 15194, 'len(sent)?\\n\\n☼': 15195, 'pointed': 15196, 'to\\nfalse': 15197, 'a\\nboolean': 15198, 'non-boolean': 15199, 'boolean\\ncontexts,': 15200, 'inequality': 15201, \".\\n'monty'\": 15202, \"'z'\": 15203, \"'a'?\\ntry\": 15204, 'prefix,': 15205, \"'montague'\": 15206, '.\\nread': 15207, 'lexicographical': 15208, 'is\\ngoing': 15209, \".\\n('monty',\": 15210, \"('monty',\": 15211, 'expected?\\n\\n☼': 15212, 'a\\nstring,': 15213, 'normalizes': 15214, 'single\\nspace': 15215, '.\\n\\ndo': 15216, 'join()\\ndo': 15217, 'substitutions\\n\\n\\n☼': 15218, 'helper': 15219, 'function\\ncmp_len': 15220, 'cmp': 15221, 'sent1\\nand': 15222, 'sent1[:]': 15223, '.\\nmodify': 15224, 'to\\nrepresent': 15225, 'assign\\ntext2': 15226, 'text1[:],': 15227, 'words,\\ne': 15228, 'text1[1][1]': 15229, '.\\nexplain': 15230, '.\\nload': 15231, 'deepcopy()': 15232, 'deepcopy),\\nconsult': 15233, 'any\\nobject': 15234, 'n-by-m': 15235, 'list\\nmultiplication,': 15236, 'word_table': 15237, \"[['']\": 15238, 'word_table[1][2]': 15239, 'hello?\\nexplain': 15240, 'range()\\nto': 15241, 'two-dimensional': 15242, 'called\\nword_vowels': 15243, 'word_vowels[l][v]': 15244, 'novel10(text)': 15245, 'that\\nappeared': 15246, 'string,\\nsplits': 15247, \"the\\nword's\": 15248, 'frequency,': 15249, 'gematria,': 15250, 'for\\nmapping': 15251, 'hidden': 15252, 'of\\ntexts': 15253, '(http://en': 15254, '.org/wiki/gematria,': 15255, 'http://essenes': 15256, '.net/gemcal': 15257, '.htm)': 15258, 'gematria()': 15259, 'sums': 15260, 'letter_vals:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 15261, 'letter_vals': 15262, \"{'a':1,\": 15263, \"'b':2,\": 15264, \"'c':3,\": 15265, \"'d':4,\": 15266, \"'e':5,\": 15267, \"'f':80,\": 15268, \"'g':3,\": 15269, \"'h':8,\\n\": 15270, \"'i':10,\": 15271, \"'j':10,\": 15272, \"'k':20,\": 15273, \"'l':30,\": 15274, \"'m':40,\": 15275, \"'n':50,\": 15276, \"'o':70,\": 15277, \"'p':80,\": 15278, \"'q':100,\\n\": 15279, \"'r':200,\": 15280, \"'s':300,\": 15281, \"'t':400,\": 15282, \"'u':6,\": 15283, \"'v':6,\": 15284, \"'w':800,\": 15285, \"'x':60,\": 15286, \"'y':10,\": 15287, \"'z':7}\\n\\n\\n\\n\\nprocess\": 15288, '.state_union)': 15289, 'document,': 15290, 'how\\nmany': 15291, '666': 15292, 'decode()': 15293, 'gematria': 15294, 'equivalents,': 15295, '.\\n\\n\\n\\n◑': 15296, 'shorten(text,': 15297, 'n\\nmost': 15298, 'it?\\n\\n◑': 15299, 'someone\\nto': 15300, 'pronunciations;': 15301, 'whatever\\nproperties': 15302, 'entries)': 15303, 'sorts': 15304, 'for\\nproximity': 15305, 'synsets\\nminke_whale': 15306, '.01,\\nsort': 15307, 'shortest_path_distance()': 15308, 'right_whale': 15309, '(containing': 15310, 'duplicates)': 15311, 'and\\nreturns': 15312, 'chair,': 15313, 'chair': 15314, 'output\\nlist': 15315, 'arguments\\nand': 15316, '.\\ncan': 15317, '.difference()?\\n\\n◑': 15318, 'itemgetter()': 15319, \"python's\\nstandard\": 15320, 'itemgetter)': 15321, 'list\\nwords': 15322, 'calling:\\nsorted(words,': 15323, 'key=itemgetter(1)),': 15324, 'sorted(words,': 15325, 'key=itemgetter(-1))': 15326, 'lookup(trie,': 15327, 'key)': 15328, 'trie,\\nand': 15329, 'uniquely\\ndetermined': 15330, 'vanguard': 15331, 'vang-,\\nso': 15332, \"'vang')\": 15333, \"'vanguard'))\": 15334, 'linkage': 15335, '2006))': 15336, 'keywords': 15337, \"from\\nnltk's\": 15338, 'shakespeare': 15339, 'package,': 15340, 'networks': 15341, 'levenshtein': 15342, '.edit_distance()': 15343, 'programming?': 15344, 'or\\ntop-down': 15345, 'approach?\\n[see': 15346, 'http://norvig': 15347, '.com/spell-correct': 15348, '.html]\\n\\n◑': 15349, 'catalan': 15350, 'combinatorial': 15351, 'mathematics,\\nincluding': 15352, '(6)': 15353, 'series\\ncan': 15354, 'c0': 15355, 'and\\ncn+1': 15356, 'σ0': 15357, '(cicn-i)': 15358, 'nth': 15359, 'cn': 15360, 'n\\nincreases': 15361, '.\\n\\n\\n★\\nreproduce': 15362, '(zhao': 15363, 'zobel,': 15364, 'authorship': 15365, 'identification': 15366, 'gender-specific': 15367, 'choice,': 15368, 'can\\nreproduce': 15369, '.clintoneast': 15370, '.com/articles/words': 15371, '.php\\n\\n★': 15372, 'alphabetically\\nsorted': 15373, '.:\\n\\nchair:': 15374, \"'flesh'\\n---t:\": 15375, \"'cat'\\n--ic:\": 15376, \"'stylish'\\n---en:\": 15377, \"'dog'\\n\\n\\n★\": 15378, 'recursive\\nfunction': 15379, 'uniqueness': 15380, 'in\\neach': 15381, 'discarding': 15382, 'compression': 15383, 'this\\ngive?': 15384, 'text?\\n\\n★': 15385, 'justify\\nthe': 15386, 'width,': 15387, 'be\\napproximately': 15388, 'evenly': 15389, 'can\\nbegin': 15390, 'extractive': 15391, 'summarization': 15392, 'tool,': 15393, 'the\\nsentences': 15394, 'highest': 15395, 'use\\nsum': 15396, 'n\\nhighest-scoring': 15397, 'the\\ndesign': 15398, 'double\\nsorting': 15399, '.\\n\\n★\\nread': 15400, 'orientation': 15401, 'visualize\\na': 15402, 'different\\nsemantic': 15403, '.org/anthology/p97-1023\\n\\n★\\ndesign': 15404, 'statistically': 15405, 'improbable\\nphrases': 15406, '.\\nhttp://www': 15407, '.amazon': 15408, '.com/gp/search-inside/sipshelp': 15409, '.html\\n\\n★': 15410, 'brute-force': 15411, 'for\\ndiscovering': 15412, 'squares,': 15413, 'crossword\\nin': 15414, 'entry\\nin': 15415, 'discussion,': 15416, 'see\\nhttp://itre': 15417, '.edu/~myl/languagelog/archives/002679': 15418, '.html\\n\\n\\n\\n\\n\\nabout': 15419, 'acst5': 15420, 'words\\n\\n\\n\\n\\n\\n5': 15421, 'words\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nback': 15422, 'verbs,\\nadjectives,': 15423, 'adverbs': 15424, 'just\\nthe': 15425, 'grammarians,': 15426, 'analysis\\nof': 15427, 'processing?\\nwhat': 15428, 'categories?\\nhow': 15429, 'class?\\n\\nalong': 15430, 'including\\nsequence': 15431, 'labeling,': 15432, 'n-gram': 15433, 'models,': 15434, 'evaluation': 15435, 'techniques\\nare': 15436, 'which\\nto': 15437, 'typical\\nnlp': 15438, 'pipeline,': 15439, 'classifying': 15440, 'and\\nlabeling': 15441, 'accordingly': 15442, 'tagging,\\npos-tagging,': 15443, 'speech\\nare': 15444, 'tags\\nused': 15445, 'tagset': 15446, 'emphasis\\nin': 15447, '.\\n\\n1\\xa0\\xa0\\xa0using': 15448, 'tagger\\na': 15449, 'tagger,': 15450, 'pos-tagger,': 15451, 'attaches': 15452, 'a\\npart': 15453, \"(don't\": 15454, 'nltk):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 15455, 'word_tokenize(and': 15456, 'different)\\n>>>': 15457, \".pos_tag(text)\\n[('and',\": 15458, \"'cc'),\": 15459, \"('now',\": 15460, \"'rb'),\": 15461, \"'in'),\": 15462, \"('something',\": 15463, \"'nn'),\\n('completely',\": 15464, \"('different',\": 15465, \"'jj')]\\n\\n\\n\\nhere\": 15466, 'cc,': 15467, 'coordinating': 15468, 'conjunction;\\nnow': 15469, 'rb,': 15470, 'adverbs;\\nfor': 15471, 'preposition;\\nsomething': 15472, 'nn,': 15473, 'jj,': 15474, 'adjective': 15475, '.\\n\\nnote\\nnltk': 15476, 'tag,': 15477, 'queried': 15478, '.help': 15479, \".upenn_tagset('rb'),\": 15480, 'regular\\nexpression,': 15481, \".upenn_tagset('nn\": 15482, 'documentation,\\nsee': 15483, '.???': 15484, '.readme(),': 15485, 'substituting': 15486, 'name\\nof': 15487, 'homonyms:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 15488, 'word_tokenize(they': 15489, 'refuse': 15490, 'permit)\\n>>>': 15491, \".pos_tag(text)\\n[('they',\": 15492, \"'prp'),\": 15493, \"('refuse',\": 15494, \"'vbp'),\": 15495, \"'to'),\": 15496, \"('permit',\": 15497, \"'vb'),\": 15498, \"('us',\": 15499, \"'prp'),\\n('to',\": 15500, \"('obtain',\": 15501, \"'dt'),\": 15502, \"'nn'),\": 15503, \"'nn')]\\n\\n\\n\\nnotice\": 15504, 'a\\npresent': 15505, 'tense': 15506, '(vbp)': 15507, '(nn)': 15508, 'deny,': 15509, 'trash': 15510, 'homophones)': 15511, 'pronounce\\nthe': 15512, 'reason,\\ntext-to-speech': 15513, 'pos-tagging': 15514, '.)\\n\\nnote\\nyour': 15515, 'turn:\\nmany': 15516, 'ski': 15517, 'race,': 15518, 'nouns\\nor': 15519, 'of\\nothers?': 15520, 'commonplace': 15521, 'put\\nthe': 15522, 'or\\nthink': 15523, 'if\\nit': 15524, 'uses\\nof': 15525, 'pos-tagger': 15526, '.\\n\\nlexical': 15527, 'nn': 15528, 'have\\ntheir': 15529, 'obscure': 15530, 'what\\njustification': 15531, 'introducing': 15532, 'distribution\\nof': 15533, 'involving\\nwoman': 15534, 'noun),': 15535, 'bought': 15536, 'verb),\\nover': 15537, 'preposition),': 15538, 'determiner)': 15539, '.similar()': 15540, 'contexts\\nw1w': 15541, 'w2,\\nthen': 15542, \"w'\": 15543, 'context,\\ni': 15544, \"w1w'w2\": 15545, '.text(word': 15546, \".similar('woman')\\nbuilding\": 15547, 'word-context': 15548, '.\\nman': 15549, 'house': 15550, 'boy': 15551, 'job\\nstate': 15552, 'war': 15553, 'question\\n>>>': 15554, \".similar('bought')\\nmade\": 15555, 'heard': 15556, 'got\\nset': 15557, 'felt': 15558, 'told\\n>>>': 15559, \".similar('over')\\nin\": 15560, 'through\\nabout': 15561, 'is\\n>>>': 15562, \".similar('the')\\na\": 15563, 'no\\nsome': 15564, 'and\\n\\n\\n\\nobserve': 15565, 'nouns;\\nsearching': 15566, 'mostly': 15567, 'verbs;\\nsearching': 15568, 'prepositions;\\nsearching': 15569, 'determiners': 15570, '$150,000\\nworth': 15571, 'clothes': 15572, 'scrobbling': 15573, 'verb,\\nwith': 15574, 'scrobble,\\nand': 15575, '.\\n\\n\\n2\\xa0\\xa0\\xa0tagged': 15576, 'corpora\\n\\n2': 15577, '.1\\xa0\\xa0\\xa0representing': 15578, 'tokens\\nby': 15579, 'string\\nrepresentation': 15580, 'str2tuple():\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 15581, 'tagged_token': 15582, '.tag': 15583, \".str2tuple('fly/nn')\\n>>>\": 15584, \"tagged_token\\n('fly',\": 15585, \"'nn')\\n>>>\": 15586, \"tagged_token[0]\\n'fly'\\n>>>\": 15587, \"tagged_token[1]\\n'nn'\\n\\n\\n\\nwe\": 15588, 'first\\nstep': 15589, 'word/tag': 15590, 'convert\\neach': 15591, 'str2tuple())': 15592, \"'''\\n\": 15593, 'the/at': 15594, 'grand/jj': 15595, 'jury/nn': 15596, 'commented/vbd': 15597, 'on/in': 15598, 'a/at': 15599, 'number/nn': 15600, 'of/in\\n': 15601, 'other/ap': 15602, 'topics/nns': 15603, ',/,': 15604, 'among/in': 15605, 'them/ppo': 15606, 'atlanta/np': 15607, 'and/cc\\n': 15608, 'fulton/np-tl': 15609, 'county/nn-tl': 15610, 'purchasing/vbg': 15611, 'departments/nns': 15612, 'which/wdt': 15613, 'it/pps\\n': 15614, 'said/vbd': 15615, '``/``': 15616, 'are/ber': 15617, 'well/ql': 15618, 'operated/vbn': 15619, 'and/cc': 15620, 'follow/vb': 15621, 'generally/rb\\n': 15622, 'accepted/vbn': 15623, 'practices/nns': 15624, 'inure/vb': 15625, 'to/in': 15626, 'best/jjt\\n': 15627, 'interest/nn': 15628, 'of/in': 15629, 'both/abx': 15630, 'governments/nns': 15631, \"''/''\": 15632, './': 15633, '[nltk': 15634, '.str2tuple(t)': 15635, \".split()]\\n[('the',\": 15636, \"'at'),\": 15637, \"('grand',\": 15638, \"'jj'),\": 15639, \"('jury',\": 15640, \"('commented',\": 15641, \"'vbd'),\\n('on',\": 15642, \"('number',\": 15643, \".')]\\n\\n\\n\\n\\n\\n2\": 15644, '.2\\xa0\\xa0\\xa0reading': 15645, 'corpora\\nseveral': 15646, 'for\\ntheir': 15647, 'you\\nopened': 15648, 'editor:\\n\\nthe/at': 15649, 'grand/jj-tl': 15650, 'jury/nn-tl\\nsaid/vbd': 15651, 'friday/nr': 15652, 'an/at': 15653, 'investigation/nn': 15654, \"atlanta's/np$\\nrecent/jj\": 15655, 'primary/nn': 15656, 'election/nn': 15657, 'produced/vbd': 15658, 'no/at\\nevidence/nn': 15659, 'that/cs': 15660, 'any/dti': 15661, 'irregularities/nns': 15662, 'took/vbd\\nplace/nn': 15663, \"you\\ndon't\": 15664, 'above,\\nthe': 15665, 'uppercase,': 15666, 'has\\nbecome': 15667, \".tagged_words()\\n[('the',\": 15668, \"('fulton',\": 15669, \"'np-tl'),\": 15670, \".tagged_words(tagset='universal')\\n[('the',\": 15671, '.]\\n\\n\\n\\nwhenever': 15672, 'interface\\nwill': 15673, 'tagged_words()': 15674, 'format\\nillustrated': 15675, \".tagged_words())\\n[('now',\": 15676, \"('im',\": 15677, \"('left',\": 15678, \"'vbd'),\": 15679, '.conll2000': 15680, \".tagged_words()\\n[('confidence',\": 15681, \".tagged_words()\\n[('pierre',\": 15682, \"'nnp'),\": 15683, \"('vinken',\": 15684, \"(',',\": 15685, \"','),\": 15686, '.]\\n\\n\\n\\nnot': 15687, 'tags;': 15688, 'the\\ntagset': 15689, 'readme()': 15690, 'methods\\nmentioned': 15691, '.\\ninitially': 15692, 'tagsets,\\nso': 15693, 'tagset:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 15694, \".tagged_words(tagset='universal')\\n[('pierre',\": 15695, \".'),\": 15696, '.]\\n\\n\\n\\ntagged': 15697, 'nltk,\\nincluding': 15698, 'portuguese,': 15699, 'spanish,': 15700, 'dutch': 15701, 'structure\\nsuch': 15702, '.sinica_treebank': 15703, \".tagged_words()\\n[('ä',\": 15704, \"'neu'),\": 15705, \"('åæ',\": 15706, \"'nad'),\": 15707, \"('åç',\": 15708, \"'nba'),\": 15709, \".tagged_words()\\n[('মহিষের',\": 15710, \"('সন্তান',\": 15711, \"(':',\": 15712, \"'sym'),\": 15713, '.mac_morpho': 15714, \".tagged_words()\\n[('jersei',\": 15715, \"'n'),\": 15716, \"('atinge',\": 15717, \"('m\\\\xe9dia',\": 15718, '.conll2002': 15719, \".tagged_words()\\n[('sao',\": 15720, \"'nc'),\": 15721, \"('paulo',\": 15722, \"'vmi'),\": 15723, \"('(',\": 15724, \"'fpa'),\": 15725, '.cess_cat': 15726, \".tagged_words()\\n[('el',\": 15727, \"'da0ms0'),\": 15728, \"('tribunal_suprem',\": 15729, \"'np0000o'),\": 15730, '.]\\n\\n\\n\\nif': 15731, 'correctly,': 15732, 'fonts,\\nyou': 15733, 'languages:': 15734, 'bangla,': 15735, 'telugu\\n\\n\\nif': 15736, 'have\\na': 15737, 'tagged_sents()': 15738, 'into\\nsentences': 15739, 'presenting': 15740, 'taggers,\\nas': 15741, 'trained': 15742, '.3\\xa0\\xa0\\xa0a': 15743, 'tagset\\ntagged': 15744, 'started,': 15745, 'simplified': 15746, 'tagset\\n(shown': 15747, '.\\n\\n\\n\\n\\n\\n\\n\\ntag\\nmeaning\\nenglish': 15748, 'examples\\n\\n\\n\\nadj\\nadjective\\nnew,': 15749, 'good,': 15750, 'high,': 15751, 'big,': 15752, 'local\\n\\nadp\\nadposition\\non,': 15753, 'of,': 15754, 'at,': 15755, 'with,': 15756, 'by,': 15757, 'into,': 15758, 'under\\n\\nadv\\nadverb\\nreally,': 15759, 'already,': 15760, 'early,': 15761, 'now\\n\\nconj\\nconjunction\\nand,': 15762, 'or,': 15763, 'but,': 15764, 'while,': 15765, 'although\\n\\ndet\\ndeterminer,': 15766, 'article\\nthe,': 15767, 'some,': 15768, 'most,': 15769, 'every,': 15770, 'which\\n\\nnoun\\nnoun\\nyear,': 15771, 'home,': 15772, 'costs,': 15773, 'africa\\n\\nnum\\nnumeral\\ntwenty-four,': 15774, 'fourth,': 15775, '1991,': 15776, '14:24\\n\\nprt\\nparticle\\nat,': 15777, 'out,': 15778, 'per,': 15779, 'with\\n\\npron\\npronoun\\nhe,': 15780, 'their,': 15781, 'its,': 15782, 'my,': 15783, 'i,': 15784, 'us\\n\\nverb\\nverb\\nis,': 15785, 'told,': 15786, 'given,': 15787, 'playing,': 15788, 'would\\n\\n': 15789, '.\\npunctuation': 15790, 'marks\\n': 15791, '!\\n\\nx\\nother\\nersatz,': 15792, 'esprit,': 15793, 'dunno,': 15794, 'gr8,': 15795, 'univeristy\\n\\n\\ntable': 15796, \"tagset\\n\\n\\nlet's\": 15797, 'news\\ncategory': 15798, 'brown_news_tagged': 15799, \".tagged_words(categories='news',\": 15800, \"tagset='universal')\\n>>>\": 15801, 'tag_fd': 15802, '.freqdist(tag': 15803, 'tag)': 15804, 'brown_news_tagged)\\n>>>': 15805, \".most_common()\\n[('noun',\": 15806, '30640),': 15807, \"('verb',\": 15808, '14399),': 15809, \"('adp',\": 15810, '12355),': 15811, '11928),': 15812, \"('det',\": 15813, '11389),\\n': 15814, \"('adj',\": 15815, '6706),': 15816, \"('adv',\": 15817, '3349),': 15818, \"('conj',\": 15819, '2717),': 15820, \"('pron',\": 15821, '2535),': 15822, \"('prt',\": 15823, '2264),\\n': 15824, \"('num',\": 15825, '2166),': 15826, \"('x',\": 15827, '106)]\\n\\n\\n\\n\\nnote\\nyour': 15828, 'turn:\\nplot': 15829, '.plot(cumulative=true)': 15830, 'list?\\n\\nwe': 15831, 'graphical\\npos-concordance': 15832, '.\\nn': 15833, 'hit/vd,': 15834, 'hit/vn,': 15835, 'adj': 15836, '.4\\xa0\\xa0\\xa0nouns\\nnouns': 15837, 'things,': 15838, '.:\\nwoman,': 15839, 'scotland,': 15840, 'after\\ndeterminers': 15841, 'adjectives,': 15842, 'the\\nverb,': 15843, '.\\n\\n\\n\\n\\n\\n\\n\\nword\\nafter': 15844, 'determiner\\nsubject': 15845, 'verb\\n\\n\\n\\nwoman\\nthe': 15846, 'yesterday': 15847, 'sat': 15848, 'down\\n\\nscotland\\nthe': 15849, 'scotland': 15850, '.\\nscotland': 15851, 'people\\n\\nbook\\nthe': 15852, 'recounts': 15853, 'colonization': 15854, 'australia\\n\\nintelligence\\nthe': 15855, \".\\nmary's\": 15856, 'impressed': 15857, 'teachers\\n\\n\\ntable': 15858, 'nouns\\n\\n\\nthe': 15859, 'book,\\nand': 15860, 'np': 15861, 'noun,\\nwith': 15862, 'list\\nof': 15863, 'themselves': 15864, 'word-tag': 15865, \"as\\n(('the',\": 15866, \"'np'))\": 15867, \"(('fulton',\": 15868, \"'np'),\": 15869, \"('county',\": 15870, 'word_tag_pairs': 15871, '.bigrams(brown_news_tagged)\\n>>>': 15872, 'noun_preceders': 15873, '[a[1]': 15874, '(a,': 15875, 'b[1]': 15876, '.freqdist(noun_preceders)\\n>>>': 15877, '[tag': 15878, '(tag,': 15879, \".most_common()]\\n['noun',\": 15880, \"'adj',\": 15881, \"'adp',\": 15882, \"'conj',\": 15883, \"'num',\": 15884, \"'adv',\": 15885, \"'prt',\": 15886, \"'pron',\": 15887, \"'x']\\n\\n\\n\\nthis\": 15888, 'confirms': 15889, 'assertion': 15890, 'and\\nadjectives,': 15891, 'numeral': 15892, '(tagged': 15893, 'num)': 15894, '.5\\xa0\\xa0\\xa0verbs\\nverbs': 15895, 'actions,': 15896, 'fall,\\neat': 15897, 'relation\\ninvolving': 15898, 'referents': 15899, '.\\n\\n\\n\\n\\n\\n\\n\\nword\\nsimple\\nwith': 15900, 'modifiers': 15901, 'adjuncts': 15902, '(italicized)\\n\\n\\n\\nfall\\nrome': 15903, 'fell\\ndot': 15904, 'com': 15905, 'stocks': 15906, 'suddenly': 15907, 'fell': 15908, 'stone\\n\\neat\\nmice': 15909, 'eat': 15910, 'cheese\\njohn': 15911, 'ate': 15912, 'pizza': 15913, 'gusto\\n\\n\\ntable': 15914, 'verbs\\n\\n\\nwhat': 15915, \".tagged_words(tagset='universal')\\n>>>\": 15916, 'word_tag_fd': 15917, '.freqdist(wsj)\\n>>>': 15918, '[wt[0]': 15919, '(wt,': 15920, '.most_common()': 15921, 'wt[1]': 15922, \"'verb']\\n['is',\": 15923, \"'are',\": 15924, \"'says',\": 15925, \"'would',\\n\": 15926, \"'say',\": 15927, \"'make',\": 15928, \"'may',\\n\": 15929, \"'did',\": 15930, \"'rose',\": 15931, \"'made',\": 15932, \"'expected',\": 15933, \"'buy',\": 15934, \"'get',\": 15935, \"'might',\\n\": 15936, \"'sell',\": 15937, \"'added',\": 15938, \"'sold',\": 15939, \"'help',\": 15940, \"'including',\": 15941, \"'reported',\": 15942, '.]\\n\\n\\n\\nnote': 15943, 'paired,': 15944, 'tag\\nas': 15945, 'event,': 15946, 'of\\ncondition-event': 15947, 'frequency-ordered': 15948, 'cfd1': 15949, '.conditionalfreqdist(wsj)\\n>>>': 15950, \"cfd1['yield']\": 15951, \".most_common()\\n[('verb',\": 15952, '28),': 15953, '20)]\\n>>>': 15954, \"cfd1['cut']\": 15955, '25),': 15956, '3)]\\n\\n\\n\\nwe': 15957, '.tagged_words()\\n>>>': 15958, 'cfd2': 15959, '.conditionalfreqdist((tag,': 15960, 'wsj)\\n>>>': 15961, \"list(cfd2['vbn'])\\n['been',\": 15962, \"'compared',\": 15963, \"'based',\": 15964, \"'priced',\": 15965, \"'used',\": 15966, \"'sold',\\n'named',\": 15967, \"'designed',\": 15968, \"'held',\": 15969, \"'fined',\": 15970, \"'taken',\": 15971, \"'paid',\": 15972, \"'traded',\": 15973, '.]\\n\\n\\n\\nto': 15974, 'clarify': 15975, 'vbd': 15976, '(past': 15977, 'tense)': 15978, 'vbn\\n(past': 15979, 'participle),': 15980, 'and\\nvbn,': 15981, 'surrounding': 15982, '.conditions()': 15983, \"'vbd'\": 15984, 'cfd1[w]': 15985, \"'vbn'\": 15986, \"cfd1[w]]\\n['asked',\": 15987, \"'accepted',\": 15988, \"'accused',\": 15989, \"'acquired',\": 15990, \"'adopted',\": 15991, 'idx1': 15992, \".index(('kicked',\": 15993, \"'vbd'))\\n>>>\": 15994, \"wsj[idx1-4:idx1+1]\\n[('while',\": 15995, \"('program',\": 15996, \"('trades',\": 15997, \"'nns'),\": 15998, \"('swiftly',\": 15999, \"'rb'),\\n\": 16000, \"('kicked',\": 16001, \"'vbd')]\\n>>>\": 16002, 'idx2': 16003, \"'vbn'))\\n>>>\": 16004, \"wsj[idx2-4:idx2+1]\\n[('head',\": 16005, \"('state',\": 16006, \"('has',\": 16007, \"'vbz'),\": 16008, \"'vbn')]\\n\\n\\n\\nin\": 16009, 'participle': 16010, 'kicked': 16011, 'preceded': 16012, 'auxiliary': 16013, 'true?\\n\\nnote\\nyour': 16014, 'turn:\\ngiven': 16015, 'participles': 16016, \"by\\nlist(cfd2['vn']),\": 16017, 'word-tag\\npairs': 16018, '.6\\xa0\\xa0\\xa0adjectives': 16019, 'adverbs\\ntwo': 16020, '.\\nadjectives': 16021, 'modifiers\\n(e': 16022, 'pizza),': 16023, 'predicates': 16024, 'the\\npizza': 16025, 'large)': 16026, 'structure\\n(e': 16027, 'fall+ing': 16028, 'falling\\nstocks)': 16029, 'manner,': 16030, 'or\\ndirection': 16031, 'quickly)': 16032, 'adjectives\\n(e': 16033, \"mary's\": 16034, 'teacher': 16035, 'nice)': 16036, '.\\nenglish': 16037, 'to\\nprepositions,': 16038, 'articles': 16039, 'determiners)\\n(e': 16040, 'a),': 16041, 'should,\\nmay),': 16042, 'personal': 16043, 'pronouns': 16044, 'she,': 16045, 'they)': 16046, 'classifies': 16047, 'turn:\\nif': 16048, 'uncertain': 16049, '.concordance(),': 16050, 'watch': 16051, 'schoolhouse': 16052, 'rock!\\ngrammar': 16053, 'videos': 16054, 'youtube,': 16055, 'reading\\nsection': 16056, '.7\\xa0\\xa0\\xa0unsimplified': 16057, \"tags\\nlet's\": 16058, 'nn,\\nand': 16059, 'that\\nthere': 16060, 'variants': 16061, 'nn;': 16062, '$\\nfor': 16063, 'possessive': 16064, '(since': 16065, 'nouns\\ntypically': 16066, 'addition,\\nmost': 16067, 'modifiers:': 16068, '-nc': 16069, 'citations,': 16070, '-hl\\nfor': 16071, 'headlines': 16072, '-tl': 16073, 'titles': 16074, 'tags)': 16075, 'findtags(tag_prefix,': 16076, 'tagged_text):\\n': 16077, 'tagged_text\\n': 16078, '.startswith(tag_prefix))\\n': 16079, 'dict((tag,': 16080, 'cfd[tag]': 16081, '.most_common(5))': 16082, '.conditions())\\n\\n>>>': 16083, 'tagdict': 16084, \"findtags('nn',\": 16085, \".tagged_words(categories='news'))\\n>>>\": 16086, 'sorted(tagdict):\\n': 16087, 'print(tag,': 16088, 'tagdict[tag])\\n': 16089, '.\\nnn': 16090, \"[('year',\": 16091, '137),': 16092, \"('time',\": 16093, '97),': 16094, '88),': 16095, \"('week',\": 16096, '85),': 16097, \"('man',\": 16098, '72)]\\nnn$': 16099, \"[(year's,\": 16100, '13),': 16101, \"(world's,\": 16102, '8),': 16103, \"(state's,\": 16104, '7),': 16105, \"(nation's,\": 16106, '6),': 16107, \"(company's,\": 16108, '6)]\\nnn$-hl': 16109, \"[(golf's,\": 16110, \"(navy's,\": 16111, '1)]\\nnn$-tl': 16112, \"[(president's,\": 16113, '11),': 16114, \"(army's,\": 16115, \"(gallery's,\": 16116, \"(university's,\": 16117, \"(league's,\": 16118, '3)]\\nnn-hl': 16119, \"[('sp\": 16120, '2),': 16121, \"('problem',\": 16122, \"('question',\": 16123, \"('business',\": 16124, \"('salary',\": 16125, '2)]\\nnn-nc': 16126, \"[('eva',\": 16127, \"('aya',\": 16128, \"('ova',\": 16129, '1)]\\nnn-tl': 16130, \"[('president',\": 16131, \"('house',\": 16132, '68),': 16133, '59),': 16134, \"('university',\": 16135, '42),': 16136, \"('city',\": 16137, '41)]\\nnn-tl-hl': 16138, \"[('fort',\": 16139, \"('dr\": 16140, \"('oak',\": 16141, \"('street',\": 16142, \"('basin',\": 16143, '1)]\\nnns': 16144, \"[('years',\": 16145, '101),': 16146, \"('members',\": 16147, '69),': 16148, \"('people',\": 16149, '52),': 16150, \"('sales',\": 16151, '51),': 16152, \"('men',\": 16153, '46)]\\nnns$': 16154, \"[(children's,\": 16155, \"(women's,\": 16156, '5),': 16157, \"(janitors',\": 16158, \"(men's,\": 16159, \"(taxpayers',\": 16160, '2)]\\nnns$-hl': 16161, \"[(dealers',\": 16162, \"(idols',\": 16163, '1)]\\nnns$-tl': 16164, \"[(women's,\": 16165, \"(states',\": 16166, \"(giants',\": 16167, '(bros': 16168, \"(writers',\": 16169, '1)]\\nnns-hl': 16170, \"[('comments',\": 16171, \"('offenses',\": 16172, \"('sacrifices',\": 16173, \"('funds',\": 16174, \"('results',\": 16175, '1)]\\nnns-tl': 16176, \"[('states',\": 16177, '38),': 16178, \"('nations',\": 16179, \"('masters',\": 16180, \"('rules',\": 16181, '9),': 16182, \"('communists',\": 16183, '9)]\\nnns-tl-hl': 16184, \"[('nations',\": 16185, '1)]\\n\\n\\nexample': 16186, '(code_findtags': 16187, 'tags\\n\\nwhen': 16188, 'constructing': 16189, 'taggers': 16190, 'chapter,\\nwe': 16191, 'unsimplified': 16192, '.8\\xa0\\xa0\\xa0exploring': 16193, \"corpora\\nlet's\": 16194, 'chapters,\\nthis': 16195, 'often\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 16196, 'brown_learned_text': 16197, \".words(categories='learned')\\n>>>\": 16198, 'sorted(set(b': 16199, '.bigrams(brown_learned_text)': 16200, \"'often'))\\n[',',\": 16201, \"'accomplished',\": 16202, \"'analytically',\": 16203, \"'appear',\": 16204, \"'apt',\": 16205, \"'associated',\": 16206, \"'assuming',\\n'became',\": 16207, \"'become',\": 16208, \"'began',\": 16209, \"'carefully',\": 16210, \"'chose',\": 16211, '.]\\n\\n\\n\\nhowever,': 16212, 'tagged_words()\\nmethod': 16213, 'brown_lrnd_tagged': 16214, \".tagged_words(categories='learned',\": 16215, '[b[1]': 16216, '.bigrams(brown_lrnd_tagged)': 16217, 'a[0]': 16218, \"'often']\\n>>>\": 16219, '.freqdist(tags)\\n>>>': 16220, 'prt': 16221, 'adv': 16222, 'adp': 16223, 'adj\\n': 16224, '37': 16225, '6\\n\\n\\n\\nnotice': 16226, 'high-frequency': 16227, '.\\nnouns': 16228, 'involving\\nparticular': 16229, '<verb>': 16230, '<verb>)': 16231, 'code-three-word-phrase': 16232, 'criterion': 16233, 'tags\\nmatch,': 16234, 'brown\\ndef': 16235, 'process(sentence):\\n': 16236, '(w1,t1),': 16237, '(w2,t2),': 16238, '(w3,t3)': 16239, '.trigrams(sentence):': 16240, '(t1': 16241, \".startswith('v')\": 16242, 't2': 16243, \"'to'\": 16244, 't3': 16245, \".startswith('v')):\": 16246, 'print(w1,': 16247, 'w2,': 16248, 'w3)': 16249, '\\n\\n>>>': 16250, 'tagged_sent': 16251, '.tagged_sents():\\n': 16252, 'process(tagged_sent)\\n': 16253, '.\\ncombined': 16254, 'achieve\\ncontinue': 16255, 'place\\nserve': 16256, 'protect\\nwanted': 16257, 'wait\\nallowed': 16258, 'place\\nexpected': 16259, 'become\\n': 16260, '(code_three_word_phrase': 16261, 'tags\\n\\nfinally,': 16262, '.\\nunderstanding': 16263, 'clarify\\nthe': 16264, '.conditionalfreqdist((word': 16265, '.lower(),': 16266, 'tag)\\n': 16267, 'sorted(data': 16268, 'len(data[word])': 16269, '3:\\n': 16270, 'data[word]': 16271, '.most_common()]\\n': 16272, '.join(tags))\\n': 16273, '.\\nbest': 16274, 'v\\nbetter': 16275, 'det\\nclose': 16276, 'n\\ncut': 16277, 'vn': 16278, 'vd\\neven': 16279, 'det': 16280, 'v\\ngrant': 16281, '-\\nhit': 16282, 'vd': 16283, 'n\\nlay': 16284, 'vd\\nleft': 16285, 'vn\\nlike': 16286, 'cnj': 16287, '-\\nnear': 16288, 'det\\nopen': 16289, 'adv\\npast': 16290, 'p\\npresent': 16291, 'n\\nread': 16292, 'np\\nright': 16293, 'adv\\nsecond': 16294, 'n\\nset': 16295, '-\\nthat': 16296, 'det\\n\\n\\n\\n\\nnote\\nyour': 16297, 'turn:\\nopen': 16298, 'complete\\nbrown': 16299, '(simplified': 16300, 'tagset)': 16301, 'tag\\nof': 16302, 'correlates': 16303, 'near': 16304, 'together,': 16305, 'near/adj': 16306, 'adjective,': 16307, 'follows,': 16308, 'having\\nthree': 16309, '.\\n\\n\\n\\n\\n3\\xa0\\xa0\\xa0mapping': 16310, 'dictionaries\\nas': 16311, 'creating\\nprograms': 16312, 'most\\nlikely': 16313, 'as\\nmapping': 16314, 'to\\nstore': 16315, 'mappings': 16316, 'type\\n(also': 16317, 'associative': 16318, 'hash': 16319, 'array\\nin': 16320, 'can\\nrepresent': 16321, 'including\\nparts': 16322, '.1\\xa0\\xa0\\xa0indexing': 16323, 'dictionaries\\na': 16324, 'particular\\nitem': 16325, 'text1[100]': 16326, 'specify\\na': 16327, 'simple\\nkind': 16328, 'table,': 16329, 'look-up:': 16330, '.\\n\\ncontrast': 16331, '(3),\\nwhere': 16332, \"fdist['monstrous'],\": 16333, 'which\\ntells': 16334, 'look-up': 16335, 'is\\nfamiliar': 16336, 'key\\nsuch': 16337, \"someone's\": 16338, 'domain,': 16339, 'word;\\nother': 16340, 'hashmap,': 16341, 'hash,': 16342, 'phonebook,': 16343, 'browser,\\nthe': 16344, 'ip': 16345, 'to\\nnumbers,': 16346, 'between\\narbitrary': 16347, 'variety\\nof': 16348, '.\\n\\n\\n\\n\\n\\n\\n\\nlinguistic': 16349, 'object\\nmaps': 16350, 'from\\nmaps': 16351, 'to\\n\\n\\n\\ndocument': 16352, 'index\\nword\\nlist': 16353, 'found)\\n\\nthesaurus\\nword': 16354, 'sense\\nlist': 16355, 'synonyms\\n\\ndictionary\\nheadword\\nentry': 16356, '(part-of-speech,': 16357, 'definitions,': 16358, 'etymology)\\n\\ncomparative': 16359, 'wordlist\\ngloss': 16360, 'term\\ncognates': 16361, '(list': 16362, 'language)\\n\\nmorph': 16363, 'analyzer\\nsurface': 16364, 'form\\nmorphological': 16365, 'morphemes)\\n\\n\\ntable': 16366, 'values\\n\\n\\nmost': 16367, 'represent\\nas': 16368, 'string),': 16369, 'integers)': 16370, '.2\\xa0\\xa0\\xa0dictionaries': 16371, 'python\\npython': 16372, 'dictionary,\\nin': 16373, 'however,\\nas': 16374, 'illustrate,': 16375, 'four\\nentries': 16376, 'add\\nentries': 16377, 'bracket': 16378, 'notation:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 16379, 'pos\\n{}\\n>>>': 16380, \"pos['colorless']\": 16381, \"'adj'\": 16382, \"pos\\n{'colorless':\": 16383, \"'adj'}\\n>>>\": 16384, \"pos['ideas']\": 16385, \"'n'\\n>>>\": 16386, \"pos['sleep']\": 16387, \"'v'\\n>>>\": 16388, \"pos['furiously']\": 16389, \"'adv'\\n>>>\": 16390, \"\\n{'furiously':\": 16391, \"'ideas':\": 16392, \"'colorless':\": 16393, \"'sleep':\": 16394, \"'v'}\\n\\n\\n\\nso,\": 16395, 'more\\nspecifically,': 16396, \"'colorless'\\nis\": 16397, 'see\\na': 16398, 'populated': 16399, 'dictionary\\nin': 16400, 'values:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 16401, \"pos['ideas']\\n'n'\\n>>>\": 16402, \"pos['colorless']\\n'adj'\\n\\n\\n\\nof\": 16403, \"hasn't\": 16404, \"pos['green']\\ntraceback\": 16405, '?\\nkeyerror:': 16406, \"'green'\\n\\n\\n\\nthis\": 16407, 'legal': 16408, 'indexes,\\nhow': 16409, 'dictionary?': 16410, 'dictionary\\nis': 16411, 'evaluating': 16412, '),': 16413, 'gives\\nus': 16414, 'they\\nwere': 16415, 'entered;': 16416, 'sequences\\nbut': 16417, '.2),': 16418, 'inherently\\nordered': 16419, 'keys,': 16420, 'the\\ndictionary': 16421, 'expected,\\nas': 16422, ',\\nor': 16423, 'list(pos)': 16424, \"\\n['ideas',\": 16425, \"'furiously',\": 16426, \"'colorless',\": 16427, \"'sleep']\\n>>>\": 16428, 'sorted(pos)': 16429, \"\\n['colorless',\": 16430, \"'ideas',\": 16431, \".endswith('s')]\": 16432, \"'ideas']\\n\\n\\n\\n\\nnote\\nwhen\": 16433, 'order\\nto': 16434, '.\\n\\nas': 16435, 'keys\\nin': 16436, 'loop\\nas': 16437, 'sorted(pos):\\n': 16438, 'print(word': 16439, ':,': 16440, 'pos[word])\\n': 16441, '.\\ncolorless:': 16442, 'adj\\nfuriously:': 16443, 'adv\\nsleep:': 16444, 'v\\nideas:': 16445, 'n\\n\\n\\n\\nfinally,': 16446, 'keys(),': 16447, 'values()': 16448, 'and\\nitems()': 16449, 'element\\n(and': 16450, 'same,': 16451, 'elements)': 16452, 'list(pos': 16453, \".keys())\\n['colorless',\": 16454, \"'sleep',\": 16455, \"'ideas']\\n>>>\": 16456, \".values())\\n['adj',\": 16457, \"'n']\\n>>>\": 16458, \".items())\\n[('colorless',\": 16459, \"'adj'),\": 16460, \"('furiously',\": 16461, \"'adv'),\": 16462, \"('sleep',\": 16463, \"('ideas',\": 16464, \"'n')]\\n>>>\": 16465, 'val': 16466, 'sorted(pos': 16467, '.items()):': 16468, 'val)\\n': 16469, 'adv\\nideas:': 16470, 'n\\nsleep:': 16471, 'v\\n\\n\\n\\nwe': 16472, 'we\\nonly': 16473, 'now\\nsuppose': 16474, 'noun:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 16475, \"pos['sleep']\\n'v'\\n>>>\": 16476, \"pos['sleep']\\n'n'\\n\\n\\n\\ninitially,\": 16477, 'is\\nimmediately': 16478, 'overwritten': 16479, \"'sleep'\": 16480, 'in\\nthat': 16481, 'entry:': 16482, 'value,\\ne': 16483, \"'v']\": 16484, 'we\\nsaw': 16485, 'dictionary,\\nwhich': 16486, '.3\\xa0\\xa0\\xa0defining': 16487, 'dictionaries\\nwe': 16488, \"there's\\na\": 16489, 'normally': 16490, 'first:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 16491, \"{'colorless':\": 16492, \"'furiously':\": 16493, \"'adv'}\\n>>>\": 16494, \"dict(colorless='adj',\": 16495, \"ideas='n',\": 16496, \"sleep='v',\": 16497, \"furiously='adv')\\n\\n\\n\\nnote\": 16498, 'typeerror:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 16499, \"{['ideas',\": 16500, \"'blogs',\": 16501, \"'adventures']:\": 16502, \"'n'}\\ntraceback\": 16503, 'unhashable\\n\\n\\n\\n\\n\\n3': 16504, '.4\\xa0\\xa0\\xa0default': 16505, 'dictionaries\\nif': 16506, 'create\\nan': 16507, 'or\\nthe': 16508, 'dictionary\\ncalled': 16509, 'defaultdict': 16510, 'to\\ncreate': 16511, 'int,': 16512, 'float,': 16513, 'dict,\\ntuple': 16514, 'defaultdict\\n>>>': 16515, 'defaultdict(int)\\n>>>': 16516, \"frequency['colorless']\": 16517, '4\\n>>>': 16518, \"frequency['ideas']\\n0\\n>>>\": 16519, 'defaultdict(list)\\n>>>': 16520, \"'verb']\\n>>>\": 16521, \"pos['ideas']\\n[]\\n\\n\\n\\n\\nnote\\nthese\": 16522, 'other\\nobjects': 16523, 'int(2),': 16524, 'list(2))': 16525, 'int(),': 16526, 'list()\\n—': 16527, 'specify\\nany': 16528, 'function\\nthat': 16529, 'dictionary\\nwhose': 16530, ',\\nit': 16531, 'defaultdict(lambda:': 16532, \"'noun')\": 16533, \"'adj'\\n>>>\": 16534, \"pos['blog']\": 16535, \"\\n'noun'\\n>>>\": 16536, \".items())\\n[('blog',\": 16537, \"('colorless',\": 16538, \"'adj')]\": 16539, '[_automatically-added]\\n\\n\\n\\n\\nnote\\nthe': 16540, 'no\\nparameters,': 16541, 'equivalent:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 16542, 'lambda:': 16543, \"'noun'\\n>>>\": 16544, \"f()\\n'noun'\\n>>>\": 16545, 'g():\\n': 16546, \"g()\\n'noun'\\n\\n\\n\\n\\nlet's\": 16547, 'substantial\\nlanguage': 16548, 'struggle': 16549, 'process\\nthe': 16550, 'a\\nguarantee': 16551, 'replace\\nlow-frequency': 16552, 'unk,': 16553, 'without\\nreading': 16554, 'on?)\\nwe': 16555, 'replacement': 16556, '.\\neverything': 16557, 'unk': 16558, \".words('carroll-alice\": 16559, '.freqdist(alice)\\n>>>': 16560, 'v1000': 16561, '.most_common(1000)]\\n>>>': 16562, \"'unk')\\n>>>\": 16563, 'v1000:\\n': 16564, 'mapping[v]': 16565, 'v\\n': 16566, 'alice2': 16567, '[mapping[v]': 16568, 'alice]\\n>>>': 16569, \"alice2[:100]\\n['unk',\": 16570, \"'alice',\": 16571, \"'unk',\": 16572, \"'unk',\\n'unk',\": 16573, \"'chapter',\": 16574, \"'rabbit',\": 16575, \"'alice',\\n'was',\": 16576, \"'tired',\": 16577, \"'sitting',\": 16578, \"'by',\\n'her',\": 16579, \"'sister',\": 16580, \"'nothing',\\n'to',\": 16581, \"'twice',\": 16582, \"'into',\": 16583, \"'the',\\n'book',\": 16584, \"'no',\\n'pictures',\": 16585, \"'the',\\n'use',\": 16586, \"'book',\": 16587, \",',\": 16588, \"'thought',\": 16589, \"'without',\\n'pictures',\": 16590, \"'conversation',\": 16591, \"?'\": 16592, 'len(set(alice2))\\n1001\\n\\n\\n\\n\\n\\n\\n3': 16593, '.5\\xa0\\xa0\\xa0incrementally': 16594, 'dictionary\\nwe': 16595, 'occurrences,': 16596, 'emulating': 16597, 'tallying': 16598, 'fig-tally': 16599, 'defaultdict,': 16600, 'each\\npart-of-speech': 16601, 'before,\\nit': 16602, 'tag,\\nwe': 16603, 'increment': 16604, \"tagset='universal'):\\n\": 16605, 'counts[tag]': 16606, \"counts['noun']\\n30640\\n>>>\": 16607, \"sorted(counts)\\n['adj',\": 16608, \"'noun',\": 16609, \"'det']\\n\\n>>>\": 16610, 'itemgetter\\n>>>': 16611, 'sorted(counts': 16612, '.items(),': 16613, 'key=itemgetter(1),': 16614, \"reverse=true)\\n[('noun',\": 16615, '[t': 16616, \"reverse=true)]\\n['noun',\": 16617, \"'x']\\n\\n\\nexample\": 16618, '(code_dictionary': 16619, 'incrementally': 16620, 'value\\n\\nthe': 16621, 'for\\nsorting': 16622, 'decreasing\\norder': 16623, 'items\\nto': 16624, 'sort,': 16625, 'itemgetter(n)': 16626, 'element,': 16627, \"('np',\": 16628, '8336)\\n>>>': 16629, 'pair[1]\\n8336\\n>>>': 16630, 'itemgetter(1)(pair)\\n8336\\n\\n\\n\\nthe': 16631, 'returned\\nin': 16632, 'of\\n3': 16633, '.3,': 16634, 'a\\nfor': 16635, 'schematic': 16636, 'version:\\n\\n>>>': 16637, 'my_dictionary': 16638, 'defaultdict(function': 16639, 'value)\\n>>>': 16640, 'sequence:\\n': 16641, 'my_dictionary[item_key]': 16642, \"item\\n\\nhere's\": 16643, 'last_letters': 16644, \".words('en')\\n>>>\": 16645, 'word[-2:]\\n': 16646, 'last_letters[key]': 16647, \"last_letters['ly']\\n['abactinally',\": 16648, \"'abandonedly',\": 16649, \"'abasedly',\": 16650, \"'abashedly',\": 16651, \"'abashlessly',\": 16652, \"'abbreviately',\\n'abdominally',\": 16653, \"'abhorrently',\": 16654, \"'abidingly',\": 16655, \"'abiogenetically',\": 16656, \"'abiologically',\": 16657, \"last_letters['zy']\\n['blazy',\": 16658, \"'bleezy',\": 16659, \"'blowzy',\": 16660, \"'boozy',\": 16661, \"'breezy',\": 16662, \"'bronzy',\": 16663, \"'buzzy',\": 16664, \"'chazy',\": 16665, 'anagram': 16666, '.\\n(you': 16667, 'anagrams': 16668, '.join(sorted(word))\\n': 16669, 'anagrams[key]': 16670, \"anagrams['aeilnrt']\\n['entrail',\": 16671, \"'latrine',\": 16672, \"'ratline',\": 16673, \"'reliant',\": 16674, \"'retinal',\": 16675, \"'trenail']\\n\\n\\n\\nsince\": 16676, 'task,\\nnltk': 16677, 'defaultdict(list),\\nin': 16678, \".index((''\": 16679, '.join(sorted(w)),': 16680, 'words)\\n>>>': 16681, \"'trenail']\\n\\n\\n\\n\\nnote\\nnltk\": 16682, '.index': 16683, 'defaultdict(list)': 16684, 'for\\ninitialization': 16685, 'similarly,\\nnltk': 16686, '.freqdist': 16687, 'essentially': 16688, 'defaultdict(int)': 16689, 'extra\\nsupport': 16690, 'initialization': 16691, '(along': 16692, 'methods)': 16693, '.6\\xa0\\xa0\\xa0complex': 16694, 'values\\nwe': 16695, 'itself,': 16696, 'see\\nhow': 16697, 'defaultdict(int))\\n>>>': 16698, '((w1,': 16699, 't1),': 16700, '(w2,': 16701, 't2))': 16702, '.bigrams(brown_news_tagged):': 16703, 'pos[(t1,': 16704, 'w2)][t2]': 16705, \"pos[('det',\": 16706, \"'right')]\": 16707, '\\ndefaultdict(<class': 16708, \"'int'>,\": 16709, \"{'adj':\": 16710, \"'noun':\": 16711, '5})\\n\\n\\n\\nthis': 16712, 'entry\\nis': 16713, '(whose': 16714, 'zero)': 16715, 'iterated': 16716, 'tagged\\ncorpus,': 16717, \"dictionary's\\nentry\": 16718, '(t1,': 16719, 'w2),': 16720, 'determiner,': 16721, '.7\\xa0\\xa0\\xa0inverting': 16722, 'dictionary\\ndictionaries': 16723, 'lookup,': 16724, 'd[k]': 16725, 'and\\nimmediately': 16726, 'more\\ncumbersome:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 16727, \".txt'):\\n\": 16728, 'counts[word]': 16729, '[key': 16730, '(key,': 16731, 'value)': 16732, '.items()': 16733, \"32]\\n['brought',\": 16734, \"'virtue',\": 16735, \"'thine',\": 16736, \"'king',\": 16737, \"'mortal',\\n'every',\": 16738, \"'been']\\n\\n\\n\\nif\": 16739, 'have\\nthe': 16740, 'key-value\\npairs': 16741, 'value-key\\npairs': 16742, 'pos2': 16743, 'dict((value,': 16744, '.items())\\n>>>': 16745, \"pos2['n']\\n'ideas'\\n\\n\\n\\nlet's\": 16746, 'realistic\\nand': 16747, 'update()': 16748, 'the\\ntechnique': 16749, '(why\\nnot?)': 16750, 'words\\nfor': 16751, 'part-of-speech,': 16752, \".update({'cats':\": 16753, \"'scratch':\": 16754, \"'peacefully':\": 16755, \"'old':\": 16756, \"'adj'})\\n>>>\": 16757, '.items():\\n': 16758, 'pos2[value]': 16759, '.append(key)\\n': 16760, \"pos2['adv']\\n['peacefully',\": 16761, \"'furiously']\\n\\n\\n\\nnow\": 16762, 'inverted': 16763, 'find\\nall': 16764, 'even\\nmore': 16765, '.index((value,': 16766, \"'furiously']\\n\\n\\n\\na\": 16767, '.\\n\\n\\n\\n\\n\\n\\nexample\\ndescription\\n\\n\\n\\nd': 16768, '{}\\ncreate': 16769, 'd\\n\\nd[key]': 16770, 'value\\nassign': 16771, 'key\\n\\nd': 16772, '.keys()\\nthe': 16773, 'dictionary\\n\\nlist(d)\\nthe': 16774, 'dictionary\\n\\nsorted(d)\\nthe': 16775, 'sorted\\n\\nkey': 16776, 'd\\ntest': 16777, 'dictionary\\n\\nfor': 16778, 'd\\niterate': 16779, 'dictionary\\n\\nd': 16780, '.values()\\nthe': 16781, 'dictionary\\n\\ndict([(k1,v1),': 16782, '(k2,v2),': 16783, '.])\\ncreate': 16784, 'pairs\\n\\nd1': 16785, '.update(d2)\\nadd': 16786, 'd2': 16787, 'd1\\n\\ndefaultdict(int)\\na': 16788, 'zero\\n\\n\\ntable': 16789, 'idioms\\ninvolving': 16790, '.\\n\\n\\n\\n\\n\\n4\\xa0\\xa0\\xa0automatic': 16791, 'tagging\\nin': 16792, 'automatically\\nadd': 16793, 'depends\\non': 16794, '(tagged)': 16795, 'brown_tagged_sents': 16796, \".tagged_sents(categories='news')\\n>>>\": 16797, 'brown_sents': 16798, \".sents(categories='news')\\n\\n\\n\\n\\n4\": 16799, '.1\\xa0\\xa0\\xa0the': 16800, 'tagger\\nthe': 16801, 'this\\nmay': 16802, 'banal': 16803, 'establishes': 16804, 'important\\nbaseline': 16805, 'we\\ntag': 16806, 'is\\nmost': 16807, '(now': 16808, 'tagset):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 16809, \".tagged_words(categories='news')]\\n>>>\": 16810, '.freqdist(tags)': 16811, \".max()\\n'nn'\\n\\n\\n\\nnow\": 16812, 'eggs': 16813, 'ham,': 16814, 'sam': 16815, \"am!'\\n>>>\": 16816, '.word_tokenize(raw)\\n>>>': 16817, 'default_tagger': 16818, \".defaulttagger('nn')\\n>>>\": 16819, \".tag(tokens)\\n[('i',\": 16820, \"('do',\": 16821, \"('green',\": 16822, \"'nn'),\\n('eggs',\": 16823, \"('ham',\": 16824, \"'nn'),\\n('do',\": 16825, \"('them',\": 16826, \"('sam',\": 16827, \"'nn'),\\n('i',\": 16828, \"('am',\": 16829, \"('!',\": 16830, \"'nn')]\\n\\n\\n\\nunsurprisingly,\": 16831, 'eighth': 16832, 'correctly,\\nas': 16833, '.evaluate(brown_tagged_sents)\\n0': 16834, '.13089484257215028\\n\\n\\n\\ndefault': 16835, 'that\\nhave': 16836, 'happens,': 16837, 'processed\\nseveral': 16838, 'thousand': 16839, 'the\\nrobustness': 16840, 'them\\nshortly': 16841, 'of\\nmatching': 16842, 'ending\\nin': 16843, \"with\\n's\": 16844, 'of\\nregular': 16845, 'expressions:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 16846, \"(r'\": 16847, \".*ing$',\": 16848, \"'vbg'),\": 16849, 'gerunds\\n': 16850, \".*ed$',\": 16851, 'past\\n': 16852, \".*es$',\": 16853, '3rd': 16854, 'present\\n': 16855, \".*ould$',\": 16856, \"'md'),\": 16857, 'modals\\n': 16858, \".*\\\\'s$',\": 16859, \"'nn$'),\": 16860, 'nouns\\n': 16861, \".*s$',\": 16862, \"(r'^-?[0-9]+(\\\\\": 16863, \".[0-9]+)?$',\": 16864, \"'cd'),\": 16865, 'cardinal': 16866, 'numbers\\n': 16867, \".*',\": 16868, \"'nn')\": 16869, '(default)\\n': 16870, ']\\n\\n\\n\\nnote': 16871, 'fifth\\nof': 16872, 'regexp_tagger': 16873, '.regexptagger(patterns)\\n>>>': 16874, \".tag(brown_sents[3])\\n[('``',\": 16875, \"('only',\": 16876, \"('relative',\": 16877, \"('handful',\": 16878, \"'nn'),\\n('of',\": 16879, \"('such',\": 16880, \"('reports',\": 16881, \"('received',\": 16882, \"'vbd'),\\n('',\": 16883, \"'nn'),\\n('``',\": 16884, \"('considering',\": 16885, \"('widespread',\": 16886, '.20326391789486245\\n\\n\\n\\nthe': 16887, '.*»': 16888, 'catch-all': 16889, '(only': 16890, 'efficient)': 16891, 're-specifying': 16892, 'tagger,\\nis': 16893, 'tagger?': 16894, 'turn:\\nsee': 16895, 'above\\nregular': 16896, '1\\ndescribes': 16897, 'partially': 16898, 'automate': 16899, '.)\\n\\n\\n\\n4': 16900, '.3\\xa0\\xa0\\xa0the': 16901, 'tagger\\n(an': 16902, 'unigramtagger):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 16903, '.freqdist(brown': 16904, \".words(categories='news'))\\n>>>\": 16905, '.conditionalfreqdist(brown': 16906, 'most_freq_words': 16907, '.most_common(100)\\n>>>': 16908, 'likely_tags': 16909, 'dict((word,': 16910, 'cfd[word]': 16911, '.max())': 16912, 'most_freq_words)\\n>>>': 16913, 'baseline_tagger': 16914, '.unigramtagger(model=likely_tags)\\n>>>': 16915, '.45578495136941344\\n\\n\\n\\nit': 16916, 'simply\\nknowing': 16917, 'of\\ntokens': 16918, '(nearly': 16919, 'fact)': 16920, 'untagged': 16921, \".sents(categories='news')[3]\\n>>>\": 16922, \".tag(sent)\\n[('``',\": 16923, \"'``'),\": 16924, 'none),': 16925, \"none),\\n('handful',\": 16926, \"none),\\n('was',\": 16927, \"'bedz'),\": 16928, \"','),\\n('the',\": 16929, \"','),\\n('``',\": 16930, \"none),\\n('interest',\": 16931, \"('election',\": 16932, \"none),\\n(',',\": 16933, \"'in'),\\n('voters',\": 16934, \"('size',\": 16935, \"none),\\n('of',\": 16936, \".')]\\n\\n\\n\\nmany\": 16937, 'none,\\nbecause': 16938, 'first,\\nand': 16939, 'tagger,\\na': 16940, 'backoff': 16941, '(5)': 16942, 'other,\\nas': 16943, 'pairs\\nfor': 16944, '.unigramtagger(model=likely_tags,\\n': 16945, 'backoff=nltk': 16946, \".defaulttagger('nn'))\\n\\n\\n\\nlet's\": 16947, 'and\\nevaluate': 16948, 'sizes,': 16949, 'performance(cfd,': 16950, 'wordlist):\\n': 16951, 'lt': 16952, 'wordlist)\\n': 16953, '.unigramtagger(model=lt,': 16954, \".defaulttagger('nn'))\\n\": 16955, '.evaluate(brown': 16956, \".tagged_sents(categories='news'))\\n\\ndef\": 16957, 'display():\\n': 16958, 'pylab\\n': 16959, 'word_freqs': 16960, \".words(categories='news'))\": 16961, '.most_common()\\n': 16962, 'words_by_freq': 16963, 'word_freqs]\\n': 16964, \".tagged_words(categories='news'))\\n\": 16965, '.arange(15)\\n': 16966, 'perfs': 16967, '[performance(cfd,': 16968, 'words_by_freq[:size])': 16969, 'sizes]\\n': 16970, '.plot(sizes,': 16971, 'perfs,': 16972, \"'-bo')\\n\": 16973, \".title('lookup\": 16974, 'varying': 16975, \"size')\\n\": 16976, \".xlabel('model\": 16977, \".ylabel('performance')\\n\": 16978, 'display()': 16979, '\\n\\n\\nexample': 16980, '(code_baseline_tagger': 16981, 'size\\n\\n\\n\\nfigure': 16982, 'tagger\\n\\nobserve': 16983, 'increases': 16984, 'rapidly': 16985, 'grows,': 16986, 'eventually\\nreaching': 16987, 'plateau,': 16988, 'improvement\\nin': 16989, 'discussed\\nin': 16990, '.)\\n\\n\\n4': 16991, '.4\\xa0\\xa0\\xa0evaluation\\nin': 16992, 'emphasis': 16993, 'on\\naccuracy': 16994, 'of\\nsuch': 16995, 'theme': 16996, 'recall': 16997, 'processing\\npipeline': 16998, 'fig-sds;': 16999, 'one\\nmodule': 17000, 'greatly': 17001, 'multiplied': 17002, 'tags\\na': 17003, 'expert': 17004, 'access\\nto': 17005, 'impartial': 17006, 'judge,': 17007, 'with\\ngold': 17008, 'manually\\nannotated': 17009, 'accepted': 17010, 'the\\nguesses': 17011, 'assessed': 17012, 'regarded': 17013, 'as\\nbeing': 17014, 'as\\nthe': 17015, 'gold': 17016, 'the\\noriginal': 17017, 'annotation': 17018, 'further\\nanalysis': 17019, 'mistakes': 17020, 'standard,': 17021, 'may\\neventually': 17022, 'lead': 17023, 'guidelines': 17024, '.\\nnevertheless,': 17025, 'correct\\nas': 17026, '.\\n\\nnote\\ndeveloping': 17027, 'undertaking': 17028, 'tools,\\ndocumentation,': 17029, 'practices': 17030, 'ensuring': 17031, 'high': 17032, 'quality\\nannotation': 17033, 'tagsets': 17034, 'schemes': 17035, 'inevitably\\ndepend': 17036, 'by\\nall,': 17037, 'however': 17038, 'creators': 17039, 'theory-neutral': 17040, 'to\\nmaximize': 17041, 'usefulness': 17042, 'discuss\\nthe': 17043, '.\\n\\n\\n\\n\\n5\\xa0\\xa0\\xa0n-gram': 17044, 'tagging\\n\\n5': 17045, '.1\\xa0\\xa0\\xa0unigram': 17046, 'tagging\\nunigram': 17047, 'algorithm:\\nfor': 17048, 'for\\nthat': 17049, 'jj': 17050, 'any\\noccurrence': 17051, 'frequent,': 17052, 'an\\nadjective': 17053, 'a\\nverb': 17054, 'cafe)': 17055, 'unigram': 17056, '(4),\\nexcept': 17057, 'up,\\ncalled': 17058, 'sample,\\nwe': 17059, 'evaluate:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 17060, \".sents(categories='news')\\n>>>\": 17061, 'unigram_tagger': 17062, '.unigramtagger(brown_tagged_sents)\\n>>>': 17063, \".tag(brown_sents[2007])\\n[('various',\": 17064, \"('apartments',\": 17065, \"'nns'),\\n('are',\": 17066, \"'ber'),\": 17067, \"('terrace',\": 17068, \"('type',\": 17069, \"'nn'),\\n(',',\": 17070, \"('being',\": 17071, \"'beg'),\": 17072, \"('ground',\": 17073, \"'nn'),\\n('floor',\": 17074, \"('so',\": 17075, \"'ql'),\": 17076, \"'cs'),\": 17077, \"('entrance',\": 17078, \"'bez'),\\n('direct',\": 17079, \".')]\\n>>>\": 17080, '.9349006503968017\\n\\n\\n\\nwe': 17081, 'unigramtagger': 17082, 'involves\\ninspecting': 17083, '.2\\xa0\\xa0\\xa0separating': 17084, 'data\\nnow': 17085, 'memorized': 17086, 'data\\nand': 17087, 'perfect': 17088, 'also\\nbe': 17089, 'useless': 17090, '10%:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 17091, 'int(len(brown_tagged_sents)': 17092, '.9)\\n>>>': 17093, 'size\\n4160\\n>>>': 17094, 'train_sents': 17095, 'brown_tagged_sents[:size]\\n>>>': 17096, 'test_sents': 17097, 'brown_tagged_sents[size:]\\n>>>': 17098, '.unigramtagger(train_sents)\\n>>>': 17099, '.evaluate(test_sents)\\n0': 17100, '.811721': 17101, '.\\n\\n\\n\\nalthough': 17102, 'worse,': 17103, 'of\\nthis': 17104, '.3\\xa0\\xa0\\xa0general': 17105, 'tagging\\nwhen': 17106, 'unigrams,': 17107, 'using\\none': 17108, 'current\\ntoken,': 17109, 'isolation': 17110, 'model,': 17111, 'best\\nwe': 17112, 'priori': 17113, 'wind': 17114, 'tag,\\nregardless': 17115, 'or\\nto': 17116, 'generalization': 17117, 'the\\nn-1': 17118, 'be\\nchosen,': 17119, 'tn,': 17120, 'circled,': 17121, 'shaded\\nin': 17122, 'grey': 17123, '.1,\\nwe': 17124, 'n=3;': 17125, 'addition\\nto': 17126, 'tagger\\npicks': 17127, 'context\\n\\n\\nnote\\na': 17128, '1-gram': 17129, 'tagger:': 17130, '.,\\nthe': 17131, '.\\n2-gram': 17132, '3-gram': 17133, 'taggers\\nare': 17134, 'trigram': 17135, 'ngramtagger': 17136, 'which\\npart-of-speech': 17137, 'bigram_tagger': 17138, '.bigramtagger(train_sents)\\n>>>': 17139, \"'nn'),\\n('type',\": 17140, \"'at'),\\n('ground',\": 17141, \"('floor',\": 17142, \"'cs'),\\n('entrance',\": 17143, \"'bez'),\": 17144, \"('direct',\": 17145, 'unseen_sent': 17146, 'brown_sents[4203]\\n>>>': 17147, \".tag(unseen_sent)\\n[('the',\": 17148, \"('population',\": 17149, \"('congo',\": 17150, \"'np'),\\n('is',\": 17151, \"('13\": 17152, \".5',\": 17153, \"('million',\": 17154, \"('divided',\": 17155, \"none),\\n('into',\": 17156, \"('least',\": 17157, \"('seven',\": 17158, \"('major',\": 17159, \"none),\\n('``',\": 17160, \"('culture',\": 17161, \"('clusters',\": 17162, \"none),\\n('innumerable',\": 17163, \"('tribes',\": 17164, \"('speaking',\": 17165, \"('400',\": 17166, \"none),\\n('separate',\": 17167, \"('dialects',\": 17168, 'none)]\\n\\n\\n\\nnotice': 17169, 'manages': 17170, 'during\\ntraining,': 17171, 'badly': 17172, 'word\\n(i': 17173, '.5),': 17174, 'million)': 17175, 'training,': 17176, 'never\\nsaw': 17177, 'the\\ntagger': 17178, 'overall': 17179, 'low:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 17180, '.102063': 17181, '.\\n\\n\\n\\n\\nas': 17182, 'larger,': 17183, 'specificity': 17184, 'increases,\\nas': 17185, 'wish': 17186, 'that\\nwere': 17187, 'sparse\\ndata': 17188, 'consequence,': 17189, 'a\\ntrade-off': 17190, '(and\\nthis': 17191, 'precision/recall': 17192, 'trade-off': 17193, 'information\\nretrieval)': 17194, '.\\n\\ncaution!\\nn-gram': 17195, 'crosses': 17196, 'a\\nsentence': 17197, 'work\\nwith': 17198, 'tn-1': 17199, 'preceding\\ntags': 17200, '.4\\xa0\\xa0\\xa0combining': 17201, 'taggers\\none': 17202, 'accurate': 17203, 'on\\nalgorithms': 17204, 'could\\ncombine': 17205, 'follows:\\n\\ntry': 17206, 'try\\nthe': 17207, '.\\n\\nmost': 17208, 'backoff-tagger': 17209, 'tagger:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 17210, 't0': 17211, 't1': 17212, '.unigramtagger(train_sents,': 17213, 'backoff=t0)\\n>>>': 17214, '.bigramtagger(train_sents,': 17215, 'backoff=t1)\\n>>>': 17216, '.844513': 17217, '.\\n\\n\\n\\n\\nnote\\nyour': 17218, 'turn:\\nextend': 17219, 'trigramtagger': 17220, 'called\\nt3,': 17221, 'backs': 17222, '.\\n\\nnote': 17223, 'is\\ninitialized': 17224, 'context,\\nthe': 17225, 'discards': 17226, 'can\\nfurther': 17227, 'a\\ncontext': 17228, 'retain': 17229, '.bigramtagger(sents,': 17230, 'cutoff=2,': 17231, 'backoff=t1)\\nwill': 17232, '.5\\xa0\\xa0\\xa0tagging': 17233, 'words\\nour': 17234, 'regular-expression': 17235, 'tagger\\nor': 17236, 'tagger\\nencountered': 17237, 'appeared': 17238, 'out-of-vocabulary': 17239, 'items?\\na': 17240, '.\\nduring': 17241, 'to),': 17242, 'unk\\nwill': 17243, '.6\\xa0\\xa0\\xa0storing': 17244, 'taggers\\ntraining': 17245, 'tagger\\nevery': 17246, '.pkl': 17247, 'pickle': 17248, 'dump\\n>>>': 17249, \"open('t2\": 17250, \".pkl',\": 17251, \"'wb')\\n>>>\": 17252, 'dump(t2,': 17253, '-1)\\n>>>': 17254, '.close()\\n\\n\\n\\nnow,': 17255, 'load\\n>>>': 17256, \"'rb')\\n>>>\": 17257, 'load(input)\\n>>>': 17258, '.close()\\n\\n\\n\\nnow': 17259, \"board's\": 17260, 'enterprise\\n': 17261, 'maze': 17262, 'regulatory': 17263, 'laws': 17264, '.split()\\n>>>': 17265, \".tag(tokens)\\n[('the',\": 17266, \"(board's,\": 17267, \"('action',\": 17268, \"('shows',\": 17269, \"'nns'),\\n('what',\": 17270, \"'wdt'),\": 17271, \"('free',\": 17272, \"('enterprise',\": 17273, \"'bez'),\\n('up',\": 17274, \"'rp'),\": 17275, \"('against',\": 17276, \"('our',\": 17277, \"'pp$'),\": 17278, \"('complex',\": 17279, \"'jj'),\\n('maze',\": 17280, \"('regulatory',\": 17281, \"('laws',\": 17282, \".')]\\n\\n\\n\\n\\n\\n5\": 17283, '.7\\xa0\\xa0\\xa0performance': 17284, 'limitations\\nwhat': 17285, 'tagger?\\nconsider': 17286, 'it\\nencounter?': 17287, 'empirically:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 17288, '((x[1],': 17289, 'y[1],': 17290, 'z[0]),': 17291, 'z[1])\\n': 17292, 'brown_tagged_sents\\n': 17293, 'z': 17294, '.trigrams(sent))\\n>>>': 17295, 'ambiguous_contexts': 17296, '[c': 17297, 'len(cfd[c])': 17298, 'sum(cfd[c]': 17299, 'ambiguous_contexts)': 17300, '.n()\\n0': 17301, '.049297702068029296\\n\\n\\n\\nthus,': 17302, 'trigrams': 17303, '[examples]': 17304, 'the\\ncurrent': 17305, '5%': 17306, 'tag\\nthat': 17307, 'legitimately': 17308, 'in\\nsuch': 17309, 'contexts,': 17310, 'lower': 17311, 'study\\nits': 17312, 'assign,': 17313, 'and\\nit': 17314, 'pre-': 17315, 'post-processing\\nthe': 17316, 'the\\nconfusion': 17317, 'matrix': 17318, 'standard)\\nagainst': 17319, 'test_tags': 17320, \".sents(categories='editorial')\\n\": 17321, '.tag(sent)]\\n>>>': 17322, 'gold_tags': 17323, \".tagged_words(categories='editorial')]\\n>>>\": 17324, '.confusionmatrix(gold_tags,': 17325, 'test_tags))': 17326, '\\n\\n\\n\\n\\nbased': 17327, 'perhaps\\na': 17328, 'dropped,\\nsince': 17329, '100%': 17330, 'annotators': 17331, '[more]\\nin': 17332, 'collapses': 17333, 'distinctions:\\ne': 17334, 'are\\ntagged': 17335, 'prp': 17336, 'introduces\\nnew': 17337, 'ambiguities:': 17338, 'vb': 17339, 'new\\ndistinctions': 17340, 'which\\nfacilitates': 17341, 'prediction': 17342, 'finer': 17343, 'tagset,': 17344, 'gets\\nmore': 17345, 'left-context': 17346, 'deciding\\nwhat': 17347, 'tagset),\\nthe': 17348, 'smaller\\nrange': 17349, 'leads': 17350, 'limit\\nin': 17351, 'resolve': 17352, 'the\\nambiguity': 17353, '(church,': 17354, 'young,': 17355, 'bloothooft,': 17356, '1996),': 17357, 'world\\nknowledge': 17358, 'despite': 17359, 'imperfections,': 17360, 'has\\nplayed': 17361, 'rise': 17362, '1990s,': 17363, 'of\\nstatistical': 17364, 'namely\\npart-of-speech': 17365, 'disambiguation,': 17366, 'pushed': 17367, 'further?': 17368, '.,\\nwe': 17369, '.\\n\\n\\n\\n6\\xa0\\xa0\\xa0transformation-based': 17370, 'tagging\\na': 17371, 'n-gram\\ntable': 17372, 'model)': 17373, 'employed': 17374, 'devices,': 17375, 'strike': 17376, 'tagger\\nperformance': 17377, 'and\\nbigram': 17378, 'tables,': 17379, 'sparse': 17380, 'millions\\nof': 17381, 'n-gram\\ntagger': 17382, 'considers': 17383, 'words\\nthemselves': 17384, 'simply\\nimpractical': 17385, 'conditioned': 17386, 'identities': 17387, 'brill': 17388, 'tagging,\\nan': 17389, 'inductive': 17390, 'models\\nthat': 17391, '.\\nbrill': 17392, 'transformation-based': 17393, 'learning,': 17394, 'named\\nafter': 17395, 'inventor': 17396, 'the\\ngeneral': 17397, 'back\\nand': 17398, 'successively\\ntransforms': 17399, 'bad': 17400, 'n-gram\\ntagging,': 17401, 'need\\nannotated': 17402, \"tagger's\": 17403, 'a\\nmistake': 17404, 'does\\nnot': 17405, 'compiles': 17406, 'transformational\\ncorrection': 17407, 'with\\npainting': 17408, 'painting': 17409, 'of\\nboughs,': 17410, 'branches,': 17411, 'twigs': 17412, 'leaves,': 17413, 'sky-blue\\nbackground': 17414, 'paint\\nblue': 17415, 'gaps,': 17416, 'canvas': 17417, 'then\\ncorrect': 17418, 'over-painting': 17419, 'blue': 17420, 'trunk': 17421, 'over-paint': 17422, 'brushes': 17423, 'brill\\ntagging': 17424, 'idea:': 17425, 'broad': 17426, 'strokes': 17427, 'up\\nthe': 17428, 'successively': 17429, 'an\\nexample': 17430, 'sentence:\\n\\n': 17431, '(1)the': 17432, 'congress': 17433, 'increase': 17434, 'grants': 17435, 'states\\nfor': 17436, 'vocational': 17437, 'rehabilitation\\nwe': 17438, 'rules:\\n(a)': 17439, 'to;\\n(b)': 17440, 'nns': 17441, '.\\n6': 17442, '.1\\nillustrates': 17443, 'then\\napplying': 17444, '.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nphrase\\nto\\nincrease\\ngrants\\nto\\nstates\\nfor\\nvocational\\nrehabilitation\\n\\nunigram\\nto\\nnn\\nnns\\nto\\nnns\\nin\\njj\\nnn\\n\\nrule': 17445, '1\\n\\xa0\\nvb\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\nrule': 17446, '2\\n\\xa0\\n\\xa0\\n\\xa0\\nin\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\noutput\\nto\\nvb\\nnns\\nin\\nnns\\nin\\njj\\nnn\\n\\ngold\\nto\\nvb\\nnns\\nin\\nnns\\nin\\njj\\nnn\\n\\n\\ntable': 17447, 'tagging\\n\\n\\nin': 17448, 'a\\ntemplate': 17449, 'with\\nt2': 17450, 'the\\nidentity': 17451, 'the\\nappearance': 17452, '2-3': 17453, 'during\\nits': 17454, 'phase,': 17455, 't1,\\nt2': 17456, 'scored': 17457, 'net': 17458, 'benefit:': 17459, 'corrects,': 17460, 'correct\\ntags': 17461, '.\\n\\nbrill': 17462, 'property:': 17463, 'are\\nlinguistically': 17464, 'interpretable': 17465, 'taggers,\\nwhich': 17466, 'potentially': 17467, 'n-grams': 17468, 'learn\\nmuch': 17469, 'direct': 17470, 'inspection': 17471, 'the\\nrules': 17472, 'learned': 17473, '.tbl': 17474, 'demo': 17475, 'brill_demo\\n>>>': 17476, 'brill_demo': 17477, '.demo()\\ntraining': 17478, '.\\nfinding': 17479, '6555': 17480, '|\\n': 17481, 'broken\\n': 17482, 'correct\\n': 17483, 'incorrect\\n': 17484, 'e\\n------------------+-------------------------------------------------------\\n': 17485, \"'to'\\n\": 17486, \"'dt'\\n\": 17487, \"'nns'\\n\": 17488, 'nnp': 17489, 'i-2': 17490, '.i-1': 17491, \"'-none-'\\n\": 17492, \"'nnp'\\n\": 17493, \"'like'\\n\": 17494, 'vbn': 17495, \"'*-1'\\n\": 17496, 'print(open(errors': 17497, '.out)': 17498, '.read())\\n': 17499, 'word/test->gold': 17500, 'context\\n--------------------------+------------------------+--------------------------\\n': 17501, 'then/nn->rb': 17502, 'in/in': 17503, 'the/dt': 17504, 'guests/n\\n,': 17505, 'guests/nns': 17506, \"'/vbd->pos\": 17507, 'honor/nn': 17508, \"speed\\n'/pos\": 17509, 'speedway/jj->nn': 17510, 'hauled/vbd': 17511, 'out/rp': 17512, 'four/cd\\nnn': 17513, 'speedway/nn': 17514, 'hauled/nn->vbd': 17515, 'four/cd': 17516, 'drivers/nn\\ndt': 17517, 'out/nnp->rp': 17518, 'drivers/nns': 17519, 'c\\ndway/nn': 17520, 'four/nnp->cd': 17521, 'crews/nns\\nhauled/vbd': 17522, 'drivers/nnp->nns': 17523, 'crews/nns': 17524, 'even\\np': 17525, 'crews/nn->nns': 17526, 'even/rb': 17527, 'off\\nnns': 17528, 'official/nnp->jj': 17529, 'indianapolis/nnp': 17530, '500/cd': 17531, 'after/vbd->in': 17532, 'race/nn': 17533, 'fortun\\nter/in': 17534, 'fortune/in->nnp': 17535, 'executives/nns': 17536, 'dro\\ns/nns': 17537, 'drooled/vbd': 17538, 'like/in': 17539, 'schoolboys/nnp->nns': 17540, 'over/in': 17541, 'cars/nns': 17542, 'a\\nolboys/nns': 17543, 'cars/nn->nns': 17544, '(code_brill_demo': 17545, 'demonstration:': 17546, 'of\\ntemplates': 17547, 'z;\\nthe': 17548, 'templates': 17549, 'instantiated': 17550, 'particular\\nwords': 17551, 'rules;': 17552, 'corrects': 17553, 'of\\ncorrect': 17554, 'breaks;': 17555, 'the\\ndemonstration': 17556, 'residual': 17557, '.\\n\\n\\n\\n\\n\\n7\\xa0\\xa0\\xa0how': 17558, 'word\\nnow': 17559, 'more\\nbasic': 17560, 'question:': 17561, 'belongs': 17562, 'place?': 17563, 'morphological,': 17564, 'syntactic,\\nand': 17565, 'clues': 17566, '.\\n\\n7': 17567, '.1\\xa0\\xa0\\xa0morphological': 17568, 'clues\\nthe': 17569, '-ness': 17570, 'suffix\\nthat': 17571, 'noun,': 17572, '.\\nhappy': 17573, 'happiness,': 17574, 'ill': 17575, 'illness': 17576, 'so\\nif': 17577, '-ness,': 17578, 'likely\\nto': 17579, '-ment': 17580, 'combines\\nwith': 17581, '.\\ngovern': 17582, 'establish': 17583, 'establishment': 17584, 'morphologically': 17585, 'the\\npresent': 17586, '-ing,': 17587, 'expresses\\nthe': 17588, 'ongoing,': 17589, 'incomplete': 17590, 'falling,': 17591, 'eating)': 17592, '-ing': 17593, 'the\\nfalling': 17594, 'gerund)': 17595, '.\\n\\n\\n7': 17596, '.2\\xa0\\xa0\\xa0syntactic': 17597, 'clues\\nanother': 17598, 'can\\noccur': 17599, 'determined': 17600, 'the\\ncategory': 17601, 'noun,\\nor': 17602, 'according\\nto': 17603, 'tests,': 17604, 'adjective:\\n\\n': 17605, 'window\\n\\n': 17606, '(very)': 17607, '.\\n\\n\\n\\n7': 17608, '.3\\xa0\\xa0\\xa0semantic': 17609, 'clues\\nfinally,': 17610, 'lexical\\ncategory': 17611, 'best-known': 17612, 'is\\nsemantic:': 17613, 'person,': 17614, 'linguistics,\\nsemantic': 17615, 'criteria': 17616, 'suspicion,': 17617, 'mainly\\nbecause': 17618, 'formalize': 17619, 'criteria\\nunderpin': 17620, 'intuitions': 17621, 'categorization': 17622, 'unfamiliar': 17623, 'word\\nverjaardag': 17624, 'word\\nbirthday,': 17625, 'verjaardag': 17626, 'in\\ndutch': 17627, 'needed:': 17628, 'zij\\nis': 17629, 'vandaag': 17630, 'jarig': 17631, 'birthday': 17632, 'today,': 17633, 'word\\njarig': 17634, 'dutch,': 17635, 'exact\\nequivalent': 17636, '.4\\xa0\\xa0\\xa0new': 17637, 'words\\nall': 17638, 'acquire': 17639, 'recently\\nadded': 17640, 'cyberslacker,\\nfatoush,': 17641, 'blamestorm,': 17642, 'sars,': 17643, 'cantopop,': 17644, 'bupkis,': 17645, 'noughties,': 17646, 'muggle,': 17647, 'and\\nrobata': 17648, 'is\\nreflected': 17649, 'prepositions\\nare': 17650, 'belonging': 17651, 'along,': 17652, 'below,': 17653, 'beside,\\nbetween,': 17654, 'during,': 17655, 'for,': 17656, 'from,': 17657, 'near,': 17658, 'outside,': 17659, 'past,\\nthrough,': 17660, 'towards,': 17661, 'under,': 17662, 'with),': 17663, 'only\\nchanges': 17664, 'gradually': 17665, '.5\\xa0\\xa0\\xa0morphology': 17666, 'tagsets\\n\\ncommon': 17667, 'morpho-syntactic': 17668, 'information;\\nthat': 17669, 'markings': 17670, 'receive': 17671, 'virtue': 17672, 'consider,': 17673, 'for\\nexample,': 17674, 'word\\ngo': 17675, 'sentences:\\n\\n': 17676, '.go': 17677, 'away!\\n\\n': 17678, '.he': 17679, 'cafe': 17680, '.all': 17681, 'cakes': 17682, '.we': 17683, 'went': 17684, 'excursion': 17685, '.\\n\\neach': 17686, 'go,': 17687, 'goes,': 17688, 'gone,': 17689, '—\\nis': 17690, 'form,\\ngoes': 17691, 'and\\nrequires': 17692, 'ungrammatical': 17693, '.*they': 17694, '.*i': 17695, 'form;': 17696, 'required\\nafter': 17697, 'by\\ngoes),': 17698, '.*all': 17699, '.*he': 17700, 'distinct\\ngrammatical': 17701, 'although\\nthis': 17702, 'fine-grained': 17703, 'tagset\\nprovides': 17704, 'help\\nother': 17705, 'processors': 17706, 'tag\\nsequences': 17707, 'distinctions,\\nas': 17708, '.\\n\\n\\n\\n\\n\\n\\n\\nform\\ncategory\\ntag\\n\\n\\n\\ngo\\nbase\\nvb\\n\\ngoes\\n3rd': 17709, 'present\\nvbz\\n\\ngone\\npast': 17710, 'participle\\nvbn\\n\\ngoing\\ngerund\\nvbg\\n\\nwent\\nsimple': 17711, 'past\\nvbd\\n\\n\\ntable': 17712, 'morphosyntactic': 17713, 'tagset\\n\\n\\nin': 17714, 'be\\nhave': 17715, 'tags:\\nbe/be,': 17716, 'being/beg,': 17717, 'am/bem,': 17718, 'are/ber,': 17719, 'is/bez,': 17720, 'been/ben,': 17721, 'were/bed': 17722, 'and\\nwas/bedz': 17723, '(plus': 17724, 'verb)': 17725, 'told,\\nthis': 17726, 'tagger\\nthat': 17727, 'carrying': 17728, 'amount\\nof': 17729, 'categories,\\nsuch': 17730, 'tagsets\\ndiffer': 17731, 'finely': 17732, 'in\\nhow': 17733, 'tagged\\nsimply': 17734, 'tagset;': 17735, 'be\\nin': 17736, 'variation': 17737, 'is\\nunavoidable,': 17738, 'for\\ndifferent': 17739, \"'right\": 17740, \"way'\": 17741, 'assign\\ntags,': 17742, \"one's\": 17743, '.\\n\\n\\n\\n\\n8\\xa0\\xa0\\xa0summary\\n\\nwords': 17744, '.\\nparts': 17745, 'labels,': 17746, 'vb,\\nthe': 17747, '.\\nautomatic': 17748, 'pipeline,\\nand': 17749, 'situations': 17750, 'including:\\npredicting': 17751, 'words,\\nanalyzing': 17752, 'possible,': 17753, '.\\ndefault': 17754, '.\\ntaggers': 17755, '.\\nbackoff': 17756, 'models:': 17757, 'specialized\\nmodel': 17758, 'tagger)': 17759, 'given\\ncontext,': 17760, '.\\npart-of-speech': 17761, 'important,': 17762, 'sequence\\nclassification': 17763, 'nlp:': 17764, 'point\\nin': 17765, 'information,\\nsuch': 17766, 'number:': 17767, \"freq['cat']\": 17768, 'create\\ndictionaries': 17769, 'brace': 17770, '{},\\npos': 17771, \"{'furiously':\": 17772, \"'adj'}\": 17773, '.\\nn-gram': 17774, 'once\\nn': 17775, 'problem;\\neven': 17776, 'tiny\\nfraction': 17777, '.\\ntransformation-based': 17778, 'series\\nof': 17779, 'repair': 17780, 'tag\\nt': 17781, 'rule\\nfixes': 17782, 'possibly': 17783, '(smaller)': 17784, '.\\n\\n\\n\\n9\\xa0\\xa0\\xa0further': 17785, 'the\\ntagging': 17786, '(petrov,': 17787, 'das,': 17788, 'mcdonald,': 17789, '2012)': 17790, '(chap-data-intensive)': 17791, 'a\\ncontiguous': 17792, 'see\\nnltk': 17793, '.upenn_tagset()': 17794, '.brown_tagset()': 17795, 'those\\nlisted': 17796, '.\\nwords': 17797, 'directives': 17798, 'synthesizer,\\nindicating': 17799, 'emphasized': 17800, 'sense\\nnumbers,': 17801, 'indicating': 17802, '.\\nexamples': 17803, 'reasons,': 17804, 'single\\nword': 17805, 'xml-style\\ntags,': 17806, 'is\\ntagged': 17807, '.\\n\\nspeech': 17808, 'synthesis': 17809, '(w3c': 17810, 'ssml):\\nthat': 17811, '<emphasis>big</emphasis>': 17812, 'car!\\nsemcor:': 17813, 'senses:\\nspace': 17814, '<wf': 17815, 'pos=nn': 17816, 'lemma=form': 17817, 'wnsn=4>form</wf>\\nis': 17818, 'measured': 17819, 'dimensions': 17820, '.\\n(wordnet': 17821, 'form/nn': 17822, 'shape,': 17823, 'configuration,\\ncontour,': 17824, 'conformation)\\nmorphological': 17825, 'turin': 17826, 'italian': 17827, \"treebank:\\ne'\": 17828, 'italiano': 17829, 'progetto': 17830, 'realizzazione': 17831, 'il\\nprimo': 17832, '(primo': 17833, 'ordin': 17834, 'sing)': 17835, 'porto': 17836, 'turistico': 17837, \"dell'\": 17838, 'albania': 17839, '(forsyth': 17840, 'martell,': 17841, 'with\\nnltk': 17842, 'communicative\\nfunction:\\n\\nstatement': 17843, 'user117': 17844, 'dude': 17845, 'that\\nynquestion': 17846, 'user120': 17847, 'something?\\nbye': 17848, 'gonna': 17849, 'food,': 17850, \"i'll\": 17851, '.\\nsystem': 17852, 'user122': 17853, 'join\\nsystem': 17854, 'user2': 17855, 'slaps': 17856, 'trout': 17857, '.\\nstatement': 17858, 'user121': 17859, '18/m': 17860, 'pm': 17861, 'me': 17862, 'tryin': 17863, 'chat\\n\\n\\n\\n10\\xa0\\xa0\\xa0exercises\\n\\n☼\\nsearch': 17864, 'spoof': 17865, 'newspaper': 17866, 'headlines,': 17867, 'gems': 17868, 'as:\\nbritish': 17869, 'waffles': 17870, 'falkland': 17871, 'islands,': 17872, 'and\\njuvenile': 17873, 'shooting': 17874, 'defendant': 17875, '.\\nmanually': 17876, 'part-of-speech\\ntags': 17877, '.\\n☼\\nworking': 17878, 'be\\neither': 17879, 'contest);': 17880, 'opponent': 17881, 'to\\npredict': 17882, \"the\\nopponent's\": 17883, 'prediction,': 17884, '.\\n☼\\ntokenize': 17885, 'sentence:\\nthey': 17886, 'clock,': 17887, 'chase': 17888, 'involved?\\n☼': 17889, 'other\\nexamples': 17890, 'map\\nfrom': 17891, 'to?\\n☼': 17892, 'mode,': 17893, 'd,': 17894, 'add\\nsome': 17895, 'non-existent\\nentry,': 17896, \"d['xyz']?\\n☼\": 17897, 'deleting': 17898, 'syntax\\ndel': 17899, \"d['abc']\": 17900, 'deleted': 17901, 'dictionaries,': 17902, 'd1': 17903, 'd2,': 17904, 'to\\neach': 17905, '.update(d2)': 17906, 'do?\\nwhat': 17907, 'for?\\n☼': 17908, 'e,': 17909, 'entry\\nfor': 17910, 'headword,': 17911, 'sense,': 17912, 'and\\nexample,': 17913, 'are\\nrestrictions': 17914, 'went,': 17915, 'the\\nsense': 17916, 'interchanged': 17917, 'contexts\\nillustrated': 17918, '(3d)': 17919, '.\\n☼\\ntrain': 17920, 'not?\\n☼\\nlearn': 17921, '.affixtagger))': 17922, '.\\ntrain': 17923, 'settings': 17924, 'length\\nand': 17925, 'training\\ndata': 17926, 'why?\\n☼': 17927, 'be\\nsubstituted': 17928, 'library\\ndocumentation': 17929, 'strings\\nhttp://docs': 17930, '.org/lib/typesseq-strings': 17931, '.html\\nand': 17932, 'two\\ndifferent': 17933, 'brown\\ncorpus,': 17934, 'removing': 17935, 'following\\nquestions:\\nwhich': 17936, 'singular\\nform?': 17937, 'plurals,': 17938, '.)\\nwhich': 17939, 'they,': 17940, 'and\\nwhat': 17941, 'represent?\\nlist': 17942, 'represent?\\nwhich': 17943, 'commonly': 17944, 'after?': 17945, 'represent?\\n\\n\\n◑': 17946, 'tagger:\\nwhat': 17947, 'various\\nmodel': 17948, 'omitted?\\nconsider': 17949, 'curve': 17950, 'a\\ngood': 17951, 'balances': 17952, 'scenarios': 17953, 'to\\nminimize': 17954, 'maximize': 17955, 'regard': 17956, 'usage?\\n\\n\\n◑': 17957, 'tagger,\\nassuming': 17958, 'table?': 17959, 'assigned\\nthe': 17960, '.)\\n◑': 17961, 'questions:\\nwhat': 17962, 'tag?\\nhow': 17963, 'tags?\\nwhat': 17964, 'involve\\nthese': 17965, 'words?\\n\\n\\n◑': 17966, 'evaluate()': 17967, 'accurately\\nthe': 17968, 'text\\nwas': 17969, \"[('the',\": 17970, \"'nn')]\": 17971, \"output\\n[('the',\": 17972, \"'nn')],\": 17973, 'works:\\na': 17974, 'words\\nas': 17975, '.evaluate()': 17976, 'tagging?\\nonce': 17977, 'newly': 17978, 'method\\ngo': 17979, 'score?\\nnow': 17980, 'inspect\\nnltk': 17981, '.api': 17982, '.__file__': 17983, '(be': 17984, 'and\\nnot': 17985, 'file)': 17986, 'phrases\\naccording': 17987, 'questions:\\nproduce': 17988, 'alphabetically': 17989, 'md': 17990, '.\\nidentify': 17991, 'verbs\\n(e': 17992, 'deals,': 17993, 'flies)': 17994, 'nn\\n(eg': 17995, 'lab)': 17996, 'masculine': 17997, 'feminine': 17998, 'pronouns?\\n\\n\\n◑': 17999, 'for\\nthe': 18000, 'and\\npreceding': 18001, 'qualifiers': 18002, '.\\ninvestigate': 18003, '.\\n◑\\nwe': 18004, 'fall-back': 18005, 'for\\ncardinal': 18006, 'strings,\\nit': 18007, 'example,\\nwe': 18008, 'regexptagger())\\nthat': 18009, '.\\n(use': 18010, '.)\\n◑\\nconsider': 18011, 'accuracy()': 18012, 'method,\\nand': 18013, 'process?\\n◑\\nhow': 18014, 'problem?': 18015, 'the\\nperformance': 18016, '.\\ntabulate': 18017, 'estimate': 18018, 'required\\nfor': 18019, 'of\\n105': 18020, '102': 18021, 'is\\nmorphologically': 18022, 'complex,': 18023, 'clues\\n(e': 18024, 'capitalization)': 18025, 'a\\nregular': 18026, '(ordered': 18027, 'unigram\\ntagger,': 18028, 'of\\nyour': 18029, 'tagger(s)': 18030, 'data?\\ndiscuss': 18031, 'showing\\nchange': 18032, '.\\nplot': 18033, 'varied': 18034, '.\\n◑\\ninspect': 18035, 'collapse': 18036, 'mapping,': 18037, '.\\n◑\\nexperiment': 18038, 'your\\nown': 18039, 'name)': 18040, '.\\nsuch': 18041, 'make,': 18042, 'less\\ninformation': 18043, '.\\n◑\\nrecall': 18044, \"hadn't\\nseen\": 18045, 'fail': 18046, 'sentence\\neven': 18047, '(even': 18048, 'during\\ntraining)': 18049, 'circumstance': 18050, 'happen?': 18051, 'this?\\n◑\\npreprocess': 18052, 'low': 18053, 'unk,\\nbut': 18054, 'untouched': 18055, 'tagger\\non': 18056, 'help?': 18057, 'contribution': 18058, 'unigram\\ntagger': 18059, 'now?\\n◑\\nmodify': 18060, 'scale': 18061, 'x-axis,': 18062, '.semilogx()': 18063, 'shape': 18064, 'plot?': 18065, 'gradient\\ntell': 18066, 'anything?\\n◑\\nconsult': 18067, 'function,\\nusing': 18068, '.brill': 18069, '.demo)': 18070, '.\\nis': 18071, '(corpus': 18072, 'size)': 18073, 'performance?\\n◑': 18074, 'wordi': 18075, 'tagi\\n→': 18076, 'tagi+1': 18077, '264': 18078, 'exactly\\nthree': 18079, 'column,': 18080, 'tags\\nin': 18081, 'print\\nout': 18082, 'each\\npossible': 18083, 'discriminate': 18084, 'the\\nepistemic': 18085, 'deontic': 18086, 'must?\\n★\\ncreate': 18087, 'taggers,\\nincorporating': 18088, '.\\ncreate': 18089, 'the\\naccuracy': 18090, 'best?\\ntry': 18091, 'affect\\nyour': 18092, 'results?\\n\\n\\n★\\nour': 18093, 'word\\n(using': 18094, 'regexptagger()),': 18095, 'altogether': 18096, 'tag\\nit': 18097, '.defaulttagger())': 18098, 'having\\nnew': 18099, \"kim's\": 18100, 'new\\nword,': 18101, 'np$)': 18102, '.\\ni': 18103, 'sensitive': 18104, 'word,\\nand': 18105, 'ignores': 18106, 'source\\ncode': 18107, 'unigramtagger(),': 18108, 'object-oriented\\nprogramming': 18109, '.)\\nadd': 18110, 'ordinary': 18111, 'trigram\\nand': 18112, '.\\nevaluate': 18113, '.\\n\\n\\n★\\nconsider': 18114, 'which\\ndetermines': 18115, \"abney's\": 18116, 'impossibility': 18117, 'of\\nexact': 18118, '1996)': 18119, 'than\\njust': 18120, 'problem?\\n★\\nuse': 18121, 'estimation': 18122, '.probability,\\nsuch': 18123, 'lidstone': 18124, 'laplace': 18125, 'estimation,': 18126, 'statistical\\ntagger': 18127, 'where\\ncontexts': 18128, '.\\n★\\ninspect': 18129, '.out': 18130, 'and\\nerrors': 18131, 'code\\n(at': 18132, '.nltk': 18133, '.org/code)\\nand': 18134, '.\\ndelete': 18135, 'templates,': 18136, 'to\\ncorrect': 18137, '.\\n★\\ndevelop': 18138, 'anti-n-grams': 18139, 'as\\n[the,': 18140, 'the]': 18141, 'initialized': 18142, 'anti-ngram': 18143, 'prevent\\nbackoff': 18144, 'avoid\\nestimating': 18145, 'p(the': 18146, 'p(the))': 18147, '.\\n★\\ninvestigate': 18148, 'corpus:\\ngenre': 18149, '(category),': 18150, '(fileid),': 18151, '.\\ncompare': 18152, 'method\\nis': 18153, 'n-fold': 18154, 'cross': 18155, 'validation,\\ndiscussed': 18156, 'evaluations': 18157, '.)\\n★\\ndevelop': 18158, 'inherits': 18159, 'class,\\nand': 18160, 'encapsulates': 18161, 'taggers\\nhave': 18162, 'acst6': 18163, 'text\\n\\n\\n\\n\\n6': 18164, 'text\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ndetecting': 18165, 'in\\n-ed': 18166, 'tend': 18167, '(5': 18168, 'of\\nwill': 18169, 'indicative': 18170, 'observable\\npatterns': 18171, 'happen': 18172, 'to\\ncorrelate': 18173, 'looking,': 18174, 'to\\nassociate': 18175, 'meaning?\\nthe': 18176, 'salient': 18177, 'it?\\nhow': 18178, 'automatically?\\nwhat': 18179, 'models?\\n\\nalong': 18180, 'learning\\ntechniques,': 18181, \"bayes'\": 18182, 'classifiers,\\nand': 18183, 'entropy': 18184, 'classifiers': 18185, 'and\\nstatistical': 18186, 'techniques,': 18187, 'focusing': 18188, 'how\\nand': 18189, 'more\\ntechnical': 18190, 'background)': 18191, '.\\n\\n1\\xa0\\xa0\\xa0supervised': 18192, 'classification\\nclassification': 18193, 'class\\nlabel': 18194, 'each\\ninput': 18195, 'of\\nlabels': 18196, 'tasks\\nare:\\n\\ndeciding': 18197, '.\\ndeciding': 18198, 'of\\ntopic': 18199, 'sports,': 18200, 'technology,': 18201, 'politics': 18202, 'bank': 18203, 'to\\nrefer': 18204, 'river': 18205, 'bank,': 18206, 'financial': 18207, 'institution,': 18208, 'tilting\\nto': 18209, 'side,': 18210, 'depositing': 18211, 'financial\\ninstitution': 18212, 'multi-class': 18213, 'be\\nassigned': 18214, 'labels;': 18215, 'open-class': 18216, 'advance;': 18217, 'a\\nlist': 18218, 'jointly': 18219, 'classifier': 18220, 'on\\ntraining': 18221, 'label': 18222, 'the\\nframework': 18223, '(a)': 18224, 'feature\\nextractor': 18225, 'about\\neach': 18226, '.\\npairs': 18227, 'fed': 18228, 'learning\\nalgorithm': 18229, '(b)': 18230, 'same\\nfeature': 18231, 'extractor': 18232, 'generates\\npredicted': 18233, 'be\\nemployed': 18234, 'intended\\nto': 18235, 'comprehensive,': 18236, 'that\\ncan': 18237, '.1\\xa0\\xa0\\xa0gender': 18238, 'identification\\nin': 18239, 'names\\nhave': 18240, 'distinctive': 18241, 'a,\\ne': 18242, 'female,': 18243, 'in\\nk,': 18244, 'precisely': 18245, 'deciding': 18246, 'what\\nfeatures': 18247, 'encode\\nthose': 18248, 'the\\nfinal': 18249, 'extractor\\nfunction': 18250, 'name:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18251, 'gender_features(word):\\n': 18252, \"{'last_letter':\": 18253, 'word[-1]}\\n>>>': 18254, \"gender_features('shrek')\\n{'last_letter':\": 18255, \"'k'}\\n\\n\\n\\nthe\": 18256, 'from\\nfeature': 18257, 'case-sensitive\\nstrings': 18258, 'feature,': 18259, \"'last_letter'\": 18260, 'as\\nbooleans,': 18261, '.\\n\\nnote\\nmost': 18262, 'using\\nsimple': 18263, 'booleans,': 18264, 'but\\nnote': 18265, 'not\\nnecessarily': 18266, \"feature's\": 18267, 'or\\ncompute': 18268, 'indeed,': 18269, 'and\\ninformative': 18270, 'supervised\\nclassifier,': 18271, 'extractor,': 18272, 'prepare\\na': 18273, 'corresponding\\nclass': 18274, 'names\\n>>>': 18275, 'labeled_names': 18276, '([(name,': 18277, \"'male')\": 18278, \".txt')]\": 18279, '[(name,': 18280, \"'female')\": 18281, \".txt')])\\n>>>\": 18282, '.shuffle(labeled_names)\\n\\n\\n\\n\\nnext,': 18283, 'and\\ndivide': 18284, 'set\\nand': 18285, 'new\\nnaive': 18286, 'bayes': 18287, 'featuresets': 18288, '[(gender_features(n),': 18289, 'gender)': 18290, '(n,': 18291, 'labeled_names]\\n>>>': 18292, 'train_set,': 18293, 'test_set': 18294, 'featuresets[500:],': 18295, 'featuresets[:500]\\n>>>': 18296, '.naivebayesclassifier': 18297, '.train(train_set)\\n\\n\\n\\nwe': 18298, 'the\\nchapter': 18299, 'not\\nappear': 18300, 'data:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18301, \".classify(gender_features('neo'))\\n'male'\\n>>>\": 18302, \".classify(gender_features('trinity'))\\n'female'\\n\\n\\n\\nobserve\": 18303, 'correctly\\nclassified': 18304, '2199,': 18305, 'it\\nstill': 18306, 'conforms': 18307, 'expectations': 18308, 'genders': 18309, 'can\\nsystematically': 18310, 'of\\nunseen': 18311, '.classify': 18312, '.accuracy(classifier,': 18313, 'test_set))\\n0': 18314, '.77\\n\\n\\n\\nfinally,': 18315, 'it\\nfound': 18316, 'distinguishing': 18317, \"names'\": 18318, 'genders:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18319, '.show_most_informative_features(5)\\nmost': 18320, 'features\\n': 18321, \"'a'\": 18322, '33': 18323, '.0\\n': 18324, \"'k'\": 18325, '32': 18326, '.0\\n\\n\\n\\nthis': 18327, 'a\\nare': 18328, 'end\\nin': 18329, 'these\\nratios': 18330, 'likelihood': 18331, 'ratios,': 18332, 'for\\ncomparing': 18333, 'feature-outcome': 18334, 'turn:\\nmodify': 18335, 'gender_features()': 18336, 'the\\nclassifier': 18337, 'first\\nletter,': 18338, 'be\\ninformative': 18339, 'retrain': 18340, 'and\\ntest': 18341, '.\\n\\nwhen': 18342, 'list\\nthat': 18343, 'large\\namount': 18344, 'function\\nnltk': 18345, '.apply_features,': 18346, 'acts\\nlike': 18347, 'memory:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18348, 'apply_features\\n>>>': 18349, 'train_set': 18350, 'apply_features(gender_features,': 18351, 'labeled_names[500:])\\n>>>': 18352, 'labeled_names[:500])\\n\\n\\n\\n\\n\\n1': 18353, '.2\\xa0\\xa0\\xa0choosing': 18354, 'features\\nselecting': 18355, 'encode': 18356, 'a\\nlearning': 18357, 'enormous': 18358, 'impact': 18359, \"method's\\nability\": 18360, 'decent\\nperformance': 18361, 'features,\\nthere': 18362, 'gains': 18363, 'carefully\\nconstructed': 18364, 'thorough': 18365, 'at\\nhand': 18366, '.\\ntypically,': 18367, 'extractors': 18368, 'of\\ntrial-and-error,': 18369, 'guided': 18370, 'is\\nrelevant': 18371, 'a\\nkitchen': 18372, 'sink': 18373, 'think\\nof,': 18374, 'are\\nhelpful': 18375, '.\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\ndef': 18376, 'gender_features2(name):\\n': 18377, 'features[first_letter]': 18378, '.lower()\\n': 18379, 'features[last_letter]': 18380, \"'abcdefghijklmnopqrstuvwxyz':\\n\": 18381, 'features[count({})': 18382, '.format(letter)]': 18383, '.count(letter)\\n': 18384, 'features[has({})': 18385, '(letter': 18386, '.lower())\\n': 18387, 'features\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18388, \"gender_features2('john')\": 18389, \"\\n{'count(j)':\": 18390, \"'has(d)':\": 18391, 'false,': 18392, \"'count(b)':\": 18393, '.}\\n\\n\\nexample': 18394, '(code_gender_features_overfitting': 18395, 'overfits': 18396, 'a\\nlarge': 18397, 'overfitting': 18398, 'relatively': 18399, '.\\n\\nhowever,': 18400, 'features\\nthat': 18401, 'provide\\ntoo': 18402, 'of\\nrelying': 18403, 'idiosyncrasies': 18404, 'generalize\\nwell': 18405, 'overfitting,': 18406, 'and\\ncan': 18407, 'problematic': 18408, 'overfit\\nthe': 18409, 'accuracy\\nis': 18410, '1%': 18411, 'only\\npays': 18412, '[(gender_features2(n),': 18413, '.train(train_set)\\n>>>': 18414, '.768\\n\\n\\n\\n\\n\\n\\n\\nonce': 18415, 'chosen,': 18416, 'productive\\nmethod': 18417, 'refining': 18418, 'first,\\nwe': 18419, 'subdivided\\ninto': 18420, 'dev-test': 18421, 'train_names': 18422, 'labeled_names[1500:]\\n>>>': 18423, 'devtest_names': 18424, 'labeled_names[500:1500]\\n>>>': 18425, 'test_names': 18426, 'labeled_names[:500]\\n\\n\\n\\nthe': 18427, 'is\\nused': 18428, 'final\\nevaluation': 18429, 'analysis,\\nrather': 18430, 'subsets': 18431, 'sets:': 18432, 'set,\\nand': 18433, '.\\n\\nhaving': 18434, 'datasets,': 18435, 'model\\nusing': 18436, 'the\\ndev-test': 18437, 'train_names]\\n>>>': 18438, 'devtest_set': 18439, 'devtest_names]\\n>>>': 18440, 'test_names]\\n>>>': 18441, '.train(train_set)': 18442, 'devtest_set))': 18443, '\\n0': 18444, '.75\\n\\n\\n\\nusing': 18445, '(name,': 18446, 'devtest_names:\\n': 18447, '.classify(gender_features(name))\\n': 18448, 'tag:\\n': 18449, '.append(': 18450, 'guess,': 18451, ')\\n\\n\\n\\nwe': 18452, 'predicted\\nthe': 18453, 'label,': 18454, 'of\\ninformation': 18455, 'which\\nexisting': 18456, 'tricking': 18457, 'wrong\\ndecision)': 18458, 'adjusted': 18459, 'the\\nnames': 18460, 'sorted(errors):\\n': 18461, \"print('correct={:<8}\": 18462, 'guess={:<8s}': 18463, \"name={:<30}'\": 18464, '.format(tag,': 18465, 'name))\\ncorrect=female': 18466, 'guess=male': 18467, 'name=abigail\\n': 18468, '.\\ncorrect=female': 18469, 'name=cindelyn\\n': 18470, 'name=katheryn\\ncorrect=female': 18471, 'name=kathryn\\n': 18472, '.\\ncorrect=male': 18473, 'guess=female': 18474, 'name=aldrich\\n': 18475, 'name=mitch\\n': 18476, 'name=rich\\n': 18477, '.\\n\\n\\n\\nlooking': 18478, 'suffixes\\nthat': 18479, 'yn': 18480, 'predominantly': 18481, 'female,\\ndespite': 18482, 'male;': 18483, 'h\\ntend': 18484, 'therefore\\nadjust': 18485, 'two-letter\\nsuffixes:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18486, \"{'suffix1':\": 18487, 'word[-1:],\\n': 18488, \"'suffix2':\": 18489, 'word[-2:]}\\n\\n\\n\\nrebuilding': 18490, 'dataset': 18491, '2\\npercentage': 18492, '76': 18493, '.5%': 18494, '.2%):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18495, 'devtest_set))\\n0': 18496, '.782\\n\\n\\n\\nthis': 18497, 'procedure': 18498, 'repeated,': 18499, 'for\\npatterns': 18500, 'a\\ndifferent': 18501, 'dev-test/training': 18502, 'classifier\\ndoes': 18503, 'the\\nmodel,': 18504, 'trust': 18505, 'of\\nhow': 18506, 'therefore\\nimportant': 18507, 'separate,': 18508, 'unused,': 18509, 'model\\ndevelopment': 18510, '.3\\xa0\\xa0\\xa0document': 18511, 'classification\\n\\nin': 18512, 'of\\ncorpora': 18513, 'labeled': 18514, 'using\\nthese': 18515, 'tag\\nnew': 18516, 'we\\nconstruct': 18517, 'appropriate\\ncategories': 18518, 'corpus,\\nwhich': 18519, 'categorizes': 18520, 'movie_reviews\\n>>>': 18521, '[(list(movie_reviews': 18522, '.words(fileid)),': 18523, 'category)\\n': 18524, 'movie_reviews': 18525, '.fileids(category)]\\n>>>': 18526, '.shuffle(documents)\\n\\n\\n\\nnext,': 18527, 'classifier\\nwill': 18528, 'to\\n(1': 18529, 'identification,': 18530, 'can\\ndefine': 18531, 'document\\ncontains': 18532, 'the\\n2000': 18533, 'corpus\\n': 18534, 'extractor\\n': 18535, '.\\n\\n\\n\\n\\n\\xa0\\n\\nall_words': 18536, '.words())\\nword_features': 18537, 'list(all_words)[:2000]': 18538, 'document_features(document):': 18539, 'document_words': 18540, 'set(document)': 18541, 'word_features:\\n': 18542, \"features['contains({})'\": 18543, '.format(word)]': 18544, 'document_words)\\n': 18545, 'print(document_features(movie_reviews': 18546, \".words('pos/cv957_8737\": 18547, \".txt')))\": 18548, \"\\n{'contains(waste)':\": 18549, \"'contains(lot)':\": 18550, '(code_document_classify_fd': 18551, 'whose\\nfeatures': 18552, 'present\\nin': 18553, '.\\n\\n\\nnote\\nthe': 18554, 'in\\n,': 18555, 'if\\nword': 18556, 'is\\nthat': 18557, 'than\\nchecking': 18558, 'a\\nclassifier': 18559, '(1': 18560, 'its\\naccuracy': 18561, 'again,\\nwe': 18562, 'show_most_informative_features()': 18563, 'which\\nfeatures': 18564, 'informative\\n': 18565, '.\\n\\n\\n\\n\\n\\xa0\\n\\nfeaturesets': 18566, '[(document_features(d),': 18567, '(d,c)': 18568, 'documents]\\ntrain_set,': 18569, 'featuresets[100:],': 18570, 'featuresets[:100]\\nclassifier': 18571, '.train(train_set)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18572, 'test_set))': 18573, '.81\\n>>>': 18574, '.show_most_informative_features(5)': 18575, '\\nmost': 18576, 'contains(outstanding)': 18577, 'neg': 18578, 'contains(seagal)': 18579, 'contains(wonderfully)': 18580, 'contains(damon)': 18581, 'contains(wasted)': 18582, '.0\\n\\n\\nexample': 18583, '(code_document_classify_use': 18584, '.\\n\\napparently': 18585, 'seagal': 18586, '8\\ntimes': 18587, 'positive,': 18588, 'that\\nmentions': 18589, 'damon': 18590, '.4\\xa0\\xa0\\xa0part-of-speech': 18591, 'tagging\\n\\nin': 18592, 'a\\npart-of-speech': 18593, 'make-up': 18594, 'be\\nhand-crafted': 18595, 'which\\nsuffixes': 18596, 'the\\nmost': 18597, 'are:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18598, 'suffix_fdist': 18599, '.words():\\n': 18600, 'suffix_fdist[word[-1:]]': 18601, 'suffix_fdist[word[-2:]]': 18602, 'suffix_fdist[word[-3:]]': 18603, '1\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18604, 'common_suffixes': 18605, '[suffix': 18606, '(suffix,': 18607, '.most_common(100)]\\n>>>': 18608, \"print(common_suffixes)\\n['e',\": 18609, \"'nd',\": 18610, \"'l',\\n\": 18611, \"'er',\": 18612, \"'ing',\": 18613, \"'or',\\n\": 18614, \"'``',\": 18615, \"'ion',\": 18616, '.]\\n\\n\\n\\n\\nnext,': 18617, 'given\\nword': 18618, 'suffixes:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18619, 'pos_features(word):\\n': 18620, 'common_suffixes:\\n': 18621, \"features['endswith({})'\": 18622, '.format(suffix)]': 18623, '.endswith(suffix)\\n': 18624, 'features\\n\\n\\n\\nfeature': 18625, 'tinted': 18626, 'glasses,': 18627, 'highlighting\\nsome': 18628, '(colors)': 18629, 'impossible\\nto': 18630, 'rely': 18631, 'exclusively': 18632, 'on\\nthese': 18633, 'highlighted': 18634, 'on\\ninformation': 18635, 'any)': 18636, 'word\\nhas': 18637, '.\\n\\n\\nnow': 18638, 'to\\ntrain': 18639, 'in\\n4):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18640, 'tagged_words': 18641, \".tagged_words(categories='news')\\n>>>\": 18642, '[(pos_features(n),': 18643, 'g)': 18644, '(n,g)': 18645, 'tagged_words]\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18646, 'int(len(featuresets)': 18647, '.1)\\n>>>': 18648, 'featuresets[size:],': 18649, 'featuresets[:size]\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18650, '.decisiontreeclassifier': 18651, 'test_set)\\n0': 18652, '.62705121829935351\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18653, \".classify(pos_features('cats'))\\n'nns'\\n\\n\\n\\n\\none\": 18654, 'fairly\\neasy': 18655, 'them\\nout': 18656, 'pseudocode:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18657, 'print(classifier': 18658, '.pseudocode(depth=4))\\nif': 18659, 'endswith(,)': 18660, 'true:': 18661, \"','\\nif\": 18662, 'false:\\n': 18663, 'endswith(the)': 18664, \"'at'\\n\": 18665, 'endswith(s)': 18666, 'true:\\n': 18667, 'endswith(is)': 18668, \"'bez'\\n\": 18669, 'false:': 18670, \"'vbz'\\n\": 18671, 'endswith(': 18672, \".'\\n\": 18673, \"'nn'\\n\\n\\n\\n\\nhere,\": 18674, 'word\\nends': 18675, 'tag\\n,': 18676, 'the,\\nin': 18677, 'certainly': 18678, 'gets\\nused': 18679, '.\\ncontinuing': 18680, 'so,\\nthen': 18681, 'vbz': 18682, '(unless': 18683, \"it's\\nthe\": 18684, 'bez),': 18685, 'not,\\nthen': 18686, 'if-then': 18687, 'below\\nthe': 18688, 'depth=4': 18689, 'the\\ntop': 18690, '.5\\xa0\\xa0\\xa0exploiting': 18691, 'context\\n\\nby': 18692, 'augmenting': 18693, 'this\\npart-of-speech': 18694, 'leverage': 18695, 'word-internal\\nfeatures,': 18696, 'it\\ncontains,': 18697, 'extractor\\njust': 18698, 'that\\ndepend': 18699, 'contextual\\nfeatures': 18700, 'knowing': 18701, 'word\\nis': 18702, 'functioning': 18703, 'not\\na': 18704, 'depend': 18705, \"word's\": 18706, 'revise': 18707, 'a\\ncomplete': 18708, '(untagged)': 18709, 'employs': 18710, 'a\\ncontext-dependent': 18711, 'tag\\nclassifier': 18712, 'pos_features(sentence,': 18713, 'i):': 18714, '{suffix(1):': 18715, 'sentence[i][-1:],\\n': 18716, 'suffix(2):': 18717, 'sentence[i][-2:],\\n': 18718, 'suffix(3):': 18719, 'sentence[i][-3:]}\\n': 18720, 'features[prev-word]': 18721, '<start>\\n': 18722, 'sentence[i-1]\\n': 18723, 'pos_features(brown': 18724, '.sents()[0],': 18725, \"8)\\n{'suffix(3)':\": 18726, \"'prev-word':\": 18727, \"'suffix(2)':\": 18728, \"'suffix(1)':\": 18729, \"'n'}\\n\\n>>>\": 18730, 'tagged_sents': 18731, 'tagged_sents:\\n': 18732, 'untagged_sent': 18733, '.untag(tagged_sent)\\n': 18734, 'enumerate(tagged_sent):\\n': 18735, '(pos_features(untagged_sent,': 18736, 'i),': 18737, ')\\n\\n>>>': 18738, 'featuresets[:size]\\n>>>': 18739, '.train(train_set)\\n\\n>>>': 18740, '.78915962207856782\\n\\n\\nexample': 18741, '(code_suffix_pos_tag': 18742, 'detector\\nexamines': 18743, 'to\\ndetermine': 18744, 'in\\nparticular,': 18745, 'a\\nfeature': 18746, '.\\n\\n\\n\\nit': 18747, 'performance\\nof': 18748, 'learns\\nthat': 18749, 'gubernatorial': 18750, 'follows\\nan': 18751, \"word's\\npart-of-speech\": 18752, 'independent': 18753, 'this\\nmakes': 18754, 'tend\\nto': 18755, 'case-by-case': 18756, 'however,\\nthere': 18757, 'are\\ninterested': 18758, 'related\\nto': 18759, '.6\\xa0\\xa0\\xa0sequence': 18760, 'classification\\nin': 18761, 'classification\\ntasks,': 18762, 'joint': 18763, 'an\\nappropriate': 18764, 'case\\nof': 18765, 'sequence\\nclassifier': 18766, '.\\n\\none': 18767, 'strategy,': 18768, 'consecutive\\nclassification': 18769, 'to\\nfind': 18770, 'input,\\nthen': 18771, 'next\\ninput': 18772, 'repeated': 18773, 'have\\nbeen': 18774, 'bigram\\ntagger': 18775, 'chose\\nthe': 18776, 'the\\npredicted': 18777, 'must\\naugment': 18778, 'history\\nargument,': 18779, 'already\\nclassified,': 18780, 'is\\npossible': 18781, 'right\\nof': 18782, 'those\\nwords': 18783, 'yet)': 18784, '.\\nhaving': 18785, 'proceed': 18786, 'our\\nsequence': 18787, 'to\\nprovide': 18788, 'when\\ntagging': 18789, 'the\\noutput': 18790, '.\\n\\n\\n\\n\\n\\xa0\\n\\n': 18791, 'history):': 18792, 'features[prev-tag]': 18793, 'history[i-1]\\n': 18794, 'features\\n\\nclass': 18795, 'consecutivepostagger(nltk': 18796, '.taggeri):': 18797, 'train_sents):\\n': 18798, 'train_sents:\\n': 18799, 'featureset': 18800, 'pos_features(untagged_sent,': 18801, 'history)\\n': 18802, '(featureset,': 18803, ')\\n': 18804, '.append(tag)\\n': 18805, '.classifier': 18806, '.train(train_set)\\n\\n': 18807, 'tag(self,': 18808, 'sentence):\\n': 18809, 'enumerate(sentence):\\n': 18810, '.classify(featureset)\\n': 18811, 'zip(sentence,': 18812, 'history)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18813, 'int(len(tagged_sents)': 18814, 'train_sents,': 18815, 'tagged_sents[size:],': 18816, 'tagged_sents[:size]\\n>>>': 18817, 'consecutivepostagger(train_sents)\\n>>>': 18818, 'print(tagger': 18819, '.evaluate(test_sents))\\n0': 18820, '.79796012981\\n\\n\\nexample': 18821, '(code_consecutive_pos_tagger': 18822, 'classifier\\n\\n\\n\\n1': 18823, '.7\\xa0\\xa0\\xa0other': 18824, 'classification\\none': 18825, 'shortcoming': 18826, 'commit': 18827, 'decision\\nthat': 18828, 'noun,\\nbut': 18829, 'no\\nway': 18830, 'mistake': 18831, 'transformational': 18832, 'joint\\nclassifiers': 18833, 'the\\ninputs,': 18834, 'iteratively': 18835, 'to\\nrepair': 18836, 'inconsistencies': 18837, 'tagger,\\ndescribed': 18838, 'possible\\nsequences': 18839, 'sequence\\nwhose': 18840, 'by\\nhidden': 18841, 'markov': 18842, '.\\nhidden': 18843, 'predicted\\ntags': 18844, 'tag\\nfor': 18845, 'over\\ntags': 18846, 'probabilities': 18847, 'calculate\\nprobability': 18848, 'unfortunately,': 18849, 'of\\npossible': 18850, '30\\ntags,': 18851, '600': 18852, 'trillion': 18853, '(3010)': 18854, 'label\\na': 18855, '10-word': 18856, 'separately,': 18857, 'the\\nfeature': 18858, 'most\\nrecent': 18859, 'small)': 18860, 'that\\nrestriction,': 18861, '.7)\\nto': 18862, 'efficiently': 18863, 'particular,\\nfor': 18864, 'i,\\na': 18865, 'by\\ntwo': 18866, 'and\\nlinear-chain': 18867, 'models;\\nbut': 18868, '.\\n\\n\\n\\n2\\xa0\\xa0\\xa0further': 18869, 'classification\\n\\n2': 18870, '.1\\xa0\\xa0\\xa0sentence': 18871, 'segmentation\\nsentence': 18872, 'viewed': 18873, 'for\\npunctuation:': 18874, 'a\\nsentence,': 18875, 'mark,': 18876, 'decide\\nwhether': 18877, 'terminates': 18878, 'segmented\\ninto': 18879, 'for\\nextracting': 18880, 'features:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18881, '.sents()\\n>>>': 18882, 'sents:\\n': 18883, '.extend(sent)\\n': 18884, 'len(sent)\\n': 18885, '.add(offset-1)\\n\\n\\n\\nhere,': 18886, 'merged': 18887, 'individual\\nsentences,': 18888, 'all\\nsentence-boundary': 18889, 'punctuation\\nindicates': 18890, 'sentence-boundary:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18891, 'punct_features(tokens,': 18892, 'i):\\n': 18893, \"{'next-word-capitalized':\": 18894, 'tokens[i+1][0]': 18895, '.isupper(),\\n': 18896, 'tokens[i-1]': 18897, '.lower(),\\n': 18898, \"'punct':\": 18899, 'tokens[i],\\n': 18900, \"'prev-word-is-one-char':\": 18901, 'len(tokens[i-1])': 18902, '1}\\n\\n\\n\\nbased': 18903, 'labeled\\nfeaturesets': 18904, 'tagging\\nwhether': 18905, 'not:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18906, '[(punct_features(tokens,': 18907, 'boundaries))\\n': 18908, 'range(1,': 18909, 'len(tokens)-1)\\n': 18910, \".?!']\\n\\n\\n\\nusing\": 18911, 'featuresets,': 18912, 'a\\npunctuation': 18913, 'classifier:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18914, '.936026936026936\\n\\n\\n\\nto': 18915, 'simply\\ncheck': 18916, 'boundary;\\nand': 18917, 'listing\\nin': 18918, 'segment_sentences(words):\\n': 18919, 'enumerate(words):\\n': 18920, \".?!'\": 18921, '.classify(punct_features(words,': 18922, 'i))': 18923, '.append(words[start:i+1])\\n': 18924, 'len(words):\\n': 18925, '.append(words[start:])\\n': 18926, 'sents\\n\\n\\nexample': 18927, '(code_classification_based_segmenter': 18928, 'segmenter\\n\\n\\n\\n2': 18929, '.2\\xa0\\xa0\\xa0identifying': 18930, 'types\\nwhen': 18931, 'dialogue,': 18932, 'of\\nutterances': 18933, 'speaker': 18934, 'this\\ninterpretation': 18935, 'straightforward': 18936, 'performative': 18937, 'statements\\nsuch': 18938, 'forgive': 18939, 'bet': 18940, 'climb': 18941, 'hill': 18942, 'but\\ngreetings,': 18943, 'assertions,': 18944, 'clarifications': 18945, 'all\\nbe': 18946, 'speech-based': 18947, 'the\\ndialogue': 18948, 'an\\nimportant': 18949, 'from\\ninstant': 18950, 'sessions': 18951, 'with\\none': 18952, 'emotion,\\nynquestion,': 18953, 'continuer': 18954, 'therefore': 18955, 'new\\ninstant': 18956, 'basic\\nmessaging': 18957, 'xml_posts()': 18958, 'structure\\nrepresenting': 18959, 'post:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 18960, '.xml_posts()[:10000]\\n\\n\\n\\nnext,': 18961, 'words\\nthe': 18962, 'dialogue_act_features(post):\\n': 18963, '.word_tokenize(post):\\n': 18964, '.format(word': 18965, '.lower())]': 18966, 'true\\n': 18967, 'features\\n\\n\\n\\nfinally,': 18968, \".get('class')\": 18969, 'get\\na': 18970, \"post's\": 18971, 'type),': 18972, '[(dialogue_act_features(post': 18973, '.text),': 18974, \".get('class'))\\n\": 18975, 'posts]\\n>>>': 18976, '.67\\n\\n\\n\\n\\n\\n\\n\\n2': 18977, '.3\\xa0\\xa0\\xa0recognizing': 18978, 'entailment\\nrecognizing': 18979, 'determining\\nwhether': 18980, 'the\\nhypothesis': 18981, 'rte': 18982, 'challenges,': 18983, 'where\\nshared': 18984, 'teams': 18985, 'text/hypothesis': 18986, 'the\\nentailment': 18987, 'holds,': 18988, '.\\n\\nchallenge': 18989, '(true)\\n\\nt:': 18990, 'parviz': 18991, 'davudi': 18992, 'iran': 18993, 'meeting': 18994, 'shanghai\\nco-operation': 18995, 'organisation': 18996, '(sco),': 18997, 'fledgling': 18998, 'that\\nbinds': 18999, 'russia,': 19000, 'soviet': 19001, 'republics': 19002, 'central\\nasia': 19003, 'terrorism': 19004, '.\\nh:': 19005, 'sco': 19006, '81': 19007, '(false)\\n\\nt:': 19008, 'nc': 19009, 'organization,': 19010, 'llc\\ncompany': 19011, 'nelson': 19012, 'beavers,': 19013, 'iii,': 19014, 'chester': 19015, 'beavers': 19016, 'jennie\\nbeavers': 19017, 'stewart': 19018, 'jennie': 19019, 'share-holder': 19020, 'carolina': 19021, 'analytical\\nlaboratory': 19022, 'and\\nhypothesis': 19023, 'entailment,': 19024, 'rather\\nwhether': 19025, 'reasonable\\nevidence': 19026, 'true/false': 19027, 'seems\\nlikely': 19028, 'successful': 19029, 'a\\ncombination': 19030, 'world\\nknowledge,': 19031, 'attempts': 19032, 'reasonably': 19033, 'results\\nwith': 19034, 'analysis,': 19035, 'ideal': 19036, 'if\\nthere': 19037, 'hypothesis\\nshould': 19038, 'information\\nfound': 19039, 'absent': 19040, 'there\\nwill': 19041, 'detector': 19042, '(2': 19043, 'words\\n(i': 19044, 'types)': 19045, 'and\\nour': 19046, 'degree': 19047, 'which\\nthere': 19048, '(captured': 19049, 'the\\nmethod': 19050, 'hyp_extra())': 19051, '—\\nnamed': 19052, 'and\\nplaces': 19053, 'motivates': 19054, 'to\\nextract': 19055, 'nes': 19056, '(named\\nentities)': 19057, 'are\\nfiltered': 19058, '.\\n\\n\\n\\n\\n[xx]give': 19059, 'intro': 19060, 'rtefeatureextractor??\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\ndef': 19061, 'rte_features(rtepair):\\n': 19062, '.rtefeatureextractor(rtepair)\\n': 19063, \"features['word_overlap']\": 19064, 'len(extractor': 19065, \".overlap('word'))\\n\": 19066, \"features['word_hyp_extra']\": 19067, \".hyp_extra('word'))\\n\": 19068, \"features['ne_overlap']\": 19069, \".overlap('ne'))\\n\": 19070, \"features['ne_hyp_extra']\": 19071, \".hyp_extra('ne'))\\n\": 19072, 'features\\n\\n\\nexample': 19073, '(code_rte_features': 19074, 'the\\nrtefeatureextractor': 19075, 'bag\\nof': 19076, 'throwing\\naway': 19077, 'calculates': 19078, 'some\\nattributes': 19079, 'earlier:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 19080, 'rtepair': 19081, '.rte': 19082, \".pairs(['rte3_dev\": 19083, \".xml'])[33]\\n>>>\": 19084, '.rtefeatureextractor(rtepair)\\n>>>': 19085, 'print(extractor': 19086, \".text_words)\\n{'russia',\": 19087, \"'organisation',\": 19088, \"'shanghai',\": 19089, \"'asia',\": 19090, \"'at',\\n'operation',\": 19091, \"'sco',\": 19092, '.}\\n>>>': 19093, \".hyp_words)\\n{'member',\": 19094, \"'china'}\\n>>>\": 19095, \".overlap('word'))\\nset()\\n>>>\": 19096, \".overlap('ne'))\\n{'sco',\": 19097, \".hyp_extra('word'))\\n{'member'}\\n\\n\\n\\nthese\": 19098, 'are\\ncontained': 19099, 'this\\nas': 19100, '.rte_classify': 19101, 'reaches': 19102, '58%\\naccuracy': 19103, 'impressive,': 19104, 'effort,': 19105, 'more\\nlinguistic': 19106, '.4\\xa0\\xa0\\xa0scaling': 19107, 'datasets\\npython': 19108, 'text\\nprocessing': 19109, 'perform\\nthe': 19110, 'numerically': 19111, 'intensive': 19112, 'learning\\nmethods': 19113, 'thus,\\nif': 19114, 'pure-python': 19115, 'implementations\\n(such': 19116, '.naivebayesclassifier)': 19117, 'may\\nfind': 19118, 'unreasonable': 19119, 'time\\nand': 19120, 'plan': 19121, 'amounts': 19122, 'data\\nor': 19123, 'recommend': 19124, \"explore\\nnltk's\": 19125, 'interfacing': 19126, 'learning\\npackages': 19127, 'can\\ntransparently': 19128, '(via': 19129, 'calls)': 19130, 'classifier\\nmodels': 19131, 'classifier\\nimplementations': 19132, 'recommended\\nmachine': 19133, '.\\n\\n\\n\\n\\n\\n\\n3\\xa0\\xa0\\xa0evaluation\\nin': 19134, 'accurately\\ncapturing': 19135, 'this\\nevaluation': 19136, 'trustworthy': 19137, 'and\\nfor': 19138, 'tool\\nfor': 19139, 'guiding': 19140, 'improvements': 19141, 'set\\nmost': 19142, 'comparing\\nthe': 19143, 'set\\n(or': 19144, 'set)\\nwith': 19145, 'set\\ntypically': 19146, 'training\\ncorpus:': 19147, 'test\\nset,': 19148, 'learning\\nhow': 19149, 'misleadingly': 19150, 'high\\nscores': 19151, 'available\\nfor': 19152, 'small\\nnumber': 19153, 'well-balanced': 19154, 'a\\nmeaningful': 19155, 'evaluation\\ninstances': 19156, 'labels,\\nor': 19157, 'test\\nset': 19158, 'at\\nleast': 19159, 'additionally,': 19160, 'many\\nclosely': 19161, 'single\\ndocument': 19162, 'lack': 19163, 'skew': 19164, 'evaluation\\nresults': 19165, 'is\\ncommon': 19166, 'err': 19167, 'safety': 19168, 'data\\nfor': 19169, 'consideration': 19170, 'degree\\nof': 19171, 'the\\ndevelopment': 19172, 'are,': 19173, 'less\\nconfident': 19174, 'other\\ndatasets': 19175, 'at\\none': 19176, 'by\\nrandomly': 19177, 'reflects': 19178, 'single\\ngenre': 19179, '(news):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 19180, 'list(brown': 19181, \".tagged_sents(categories='news'))\\n>>>\": 19182, '.shuffle(tagged_sents)\\n>>>': 19183, 'tagged_sents[:size]\\n\\n\\n\\nin': 19184, 'training\\nset': 19185, 'same\\ngenre,': 19186, 'would\\ngeneralize': 19187, \"what's\": 19188, 'to\\nrandom': 19189, '.shuffle(),': 19190, 'are\\ntaken': 19191, 'consistent': 19192, 'word\\nappears': 19193, 'then\\nthat': 19194, 'reflected': 19195, 'the\\ntest': 19196, 'somewhat': 19197, 'documents:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 19198, 'file_ids': 19199, \".fileids(categories='news')\\n>>>\": 19200, 'int(len(file_ids)': 19201, '.tagged_sents(file_ids[size:])\\n>>>': 19202, '.tagged_sents(file_ids[:size])\\n\\n\\n\\nif': 19203, 'stringent': 19204, 'evaluation,': 19205, 'those\\nin': 19206, \".tagged_sents(categories='fiction')\\n\\n\\n\\nif\": 19207, 'set,\\nthen': 19208, 'well\\nbeyond': 19209, '.2\\xa0\\xa0\\xa0accuracy\\nthe': 19210, 'metric': 19211, 'classifier,\\naccuracy,': 19212, 'gender\\nclassifier': 19213, 'predicts': 19214, '60': 19215, '60/80': 19216, '75%': 19217, '.accuracy()': 19218, \"print('accuracy:\": 19219, '{:4': 19220, \".2f}'\": 19221, '.format(nltk': 19222, 'test_set)))': 19223, '.75\\n\\n\\n\\nwhen': 19224, 'interpreting': 19225, 'classifier,': 19226, 'important\\nto': 19227, 'class\\nlabels': 19228, 'that\\ndetermines': 19229, 'word\\nbank': 19230, 'financial-institution': 19231, '19\\ntimes': 19232, '95%': 19233, 'hardly': 19234, 'be\\nimpressive,': 19235, 'that\\nalways': 19236, 'we\\ninstead': 19237, 'balanced': 19238, '40%,': 19239, 'accuracy\\nscore': 19240, 'arises\\nwhen': 19241, 'measuring': 19242, 'inter-annotator': 19243, 'in\\n2': 19244, '.)\\n\\n\\n3': 19245, '.3\\xa0\\xa0\\xa0precision': 19246, 'recall\\nanother': 19247, 'misleading': 19248, 'in\\nsearch': 19249, 'retrieval,': 19250, 'attempting\\nto': 19251, 'irrelevant': 19252, 'outweighs': 19253, 'relevant\\ndocuments,': 19254, 'document\\nas': 19255, 'positives': 19256, 'negatives\\n\\nit': 19257, 'for\\nsearch': 19258, 'four\\ncategories': 19259, '.1:\\n\\ntrue': 19260, 'identified\\nas': 19261, '.\\ntrue': 19262, 'negatives': 19263, '.\\nfalse': 19264, 'items\\nthat': 19265, '.\\n\\ngiven': 19266, 'metrics:\\n\\nprecision,': 19267, 'we\\nidentified': 19268, 'tp/(tp+fp)': 19269, '.\\nrecall,': 19270, 'we\\nidentified,': 19271, 'tp/(tp+fn)': 19272, 'f-measure': 19273, 'f-score),': 19274, 'precision\\nand': 19275, 'harmonic\\nmean': 19276, 'precision': 19277, 'recall:\\n(2': 19278, 'recall)': 19279, '(precision': 19280, '.4\\xa0\\xa0\\xa0confusion': 19281, 'matrices\\n\\nwhen': 19282, 'subdivide': 19283, 'on\\nwhich': 19284, 'table\\nwhere': 19285, '[i,j]': 19286, 'was\\npredicted': 19287, 'diagonal\\nentries': 19288, 'cells': 19289, '|ii|)': 19290, 'were\\ncorrectly': 19291, 'predicted,': 19292, 'off-diagonal': 19293, '4:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 19294, 'tag_list(tagged_sents):\\n': 19295, 'sent]\\n>>>': 19296, 'apply_tagger(tagger,': 19297, 'corpus):\\n': 19298, '[tagger': 19299, '.tag(nltk': 19300, '.untag(sent))': 19301, 'corpus]\\n>>>': 19302, 'tag_list(brown': 19303, \".tagged_sents(categories='editorial'))\\n>>>\": 19304, 'tag_list(apply_tagger(t2,': 19305, \".tagged_sents(categories='editorial')))\\n>>>\": 19306, 'cm': 19307, '.confusionmatrix(gold,': 19308, 'test)\\n>>>': 19309, 'print(cm': 19310, '.pretty_format(sort_by_count=true,': 19311, 'show_percents=true,': 19312, 'truncate=9))\\n': 19313, '|\\n----+----------------------------------------------------------------+\\n': 19314, '<11': 19315, '.8%>': 19316, '.0%': 19317, '.2%': 19318, '.3%': 19319, '<9': 19320, '.0%>': 19321, '<8': 19322, '.6%>': 19323, '.7%': 19324, '<3': 19325, '.9%>': 19326, '<4': 19327, '|\\nnns': 19328, '.2%>': 19329, '.4%>': 19330, '.9%': 19331, '<2': 19332, '<1': 19333, '.8%>|\\n----+----------------------------------------------------------------+\\n(row': 19334, 'reference;': 19335, 'col': 19336, 'test)\\n\\n\\n\\n\\nthe': 19337, 'a\\nsubstitution': 19338, '.6%': 19339, '(for\\n1': 19340, '(': 19341, 'cells\\nwhose': 19342, 'diagonal': 19343, 'classifications': 19344, 'xxx': 19345, 'legend': 19346, '.5\\xa0\\xa0\\xa0cross-validation\\nin': 19347, 'reserve': 19348, 'the\\nannotated': 19349, 'mentioned,\\nif': 19350, 'small,': 19351, 'then\\nour': 19352, 'set\\nlarger': 19353, 'smaller,': 19354, 'a\\nsignificant': 19355, 'annotated\\ndata': 19356, '.\\none': 19357, 'on\\ndifferent': 19358, 'those\\nevaluations,': 19359, 'cross-validation': 19360, 'n\\nsubsets': 19361, 'folds': 19362, 'folds,': 19363, 'all\\nof': 19364, 'fold,': 19365, 'that\\nmodel': 19366, 'fold': 19367, 'might\\nbe': 19368, 'the\\ncombined': 19369, 'is\\ntherefore': 19370, 'cross-validation\\nis': 19371, 'varies\\nacross': 19372, 'all\\nn': 19373, 'the\\nscore': 19374, 'hand,': 19375, 'vary': 19376, 'skeptical\\nabout': 19377, '.\\n\\n\\n\\n\\n\\n4\\xa0\\xa0\\xa0decision': 19378, 'trees\\nin': 19379, 'machine\\nlearning': 19380, 'build\\nclassification': 19381, 'classifiers,': 19382, 'and\\nmaximum': 19383, 'these\\nlearning': 19384, 'black': 19385, 'boxes,': 19386, 'them\\nfor': 19387, 'lot\\nto': 19388, 'methods\\nselect': 19389, 'an\\nunderstanding': 19390, 'of\\nappropriate': 19391, 'those\\nfeatures': 19392, 'generated\\nmodels': 19393, 'features\\nare': 19394, 'informative,': 19395, '.\\n\\n\\na': 19396, 'flowchart': 19397, 'selects\\nlabels': 19398, 'decision\\nnodes,': 19399, 'leaf': 19400, 'nodes,': 19401, 'which\\nassign': 19402, \"flowchart's\": 19403, \"value's\\nfeatures,\": 19404, 'selects': 19405, 'branch': 19406, '.\\nfollowing': 19407, 'arrive': 19408, 'a\\nnew': 19409, \"value's\": 19410, \"node's\": 19411, 'condition,\\nuntil': 19412, 'input\\nvalue': 19413, 'tree\\ndiagrams': 19414, 'conventionally': 19415, 'upside': 19416, 'the\\ntop,': 19417, '.\\n\\n\\n\\nonce': 19418, 'straightforward\\nis': 19419, 'given\\ntraining': 19420, 'for\\nbuilding': 19421, 'task:': 19422, 'picking': 19423, 'the\\nbest': 19424, 'stump': 19425, 'a\\ndecision': 19426, 'decides': 19427, 'inputs\\nbased': 19428, 'possible\\nfeature': 19429, 'to\\ninputs': 19430, 'decision\\nstump,': 19431, 'the\\nsimplest': 19432, 'possible\\nfeature,': 19433, 'achieves': 19434, 'the\\ntraining': 19435, 'alternatives': 19436, 'picked': 19437, 'a\\nlabel': 19438, 'selected\\nexamples': 19439, 'selected\\nfeature': 19440, '.\\n\\n\\ngiven': 19441, 'stumps,': 19442, 'for\\ngrowing': 19443, 'we\\nbegin': 19444, 'the\\nclassification': 19445, 'we\\nthen': 19446, '.\\nleaves': 19447, 'sufficient': 19448, 'then\\nreplaced': 19449, 'training\\ncorpus': 19450, 'grow': 19451, 'the\\nleftmost': 19452, 'stump,': 19453, 'vowel\\nor': 19454, '.1\\xa0\\xa0\\xa0entropy': 19455, 'gain\\nas': 19456, 'mentioned': 19457, 'one\\npopular': 19458, 'alternative,': 19459, 'gain,': 19460, 'how\\nmuch': 19461, 'up\\nusing': 19462, 'disorganized': 19463, 'set\\nof': 19464, 'many\\ninput': 19465, 'is\\ndefined': 19466, 'log\\nprobability': 19467, 'label:\\n\\n': 19468, '(1)h': 19469, '−σl': 19470, '|in|': 19471, 'labelsp(l)': 19472, 'log2p(l)': 19473, 'female\\nnames': 19474, 'if\\np(male)': 19475, 'particular,\\nlabels': 19476, 'contribute': 19477, 'entropy\\n(since': 19478, 'p(l)': 19479, 'small),': 19480, 'do\\nnot': 19481, 'medium\\nfrequency,': 19482, 'entropy\\nof': 19483, '.\\n\\n\\n\\n\\n\\n\\xa0\\n\\nimport': 19484, 'math\\ndef': 19485, 'entropy(labels):\\n': 19486, '.freqdist(labels)\\n': 19487, 'probs': 19488, '[freqdist': 19489, '.freq(l)': 19490, 'freqdist]\\n': 19491, '-sum(p': 19492, 'math': 19493, '.log(p,2)': 19494, 'probs)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 19495, \"print(entropy(['male',\": 19496, \"'male',\": 19497, \"'male']))\": 19498, \"'male']))\\n0\": 19499, '.811': 19500, \"print(entropy(['female',\": 19501, \"'male']))\\n1\": 19502, \"'female']))\\n0\": 19503, \"'female']))\": 19504, '(code_entropy': 19505, 'labels\\n\\n\\nonce': 19506, 'calculated': 19507, \"input\\nvalues'\": 19508, 'labels\\nbecome': 19509, 'the\\nentropy': 19510, \"stump's\": 19511, 'average\\nof': 19512, '(weighted': 19513, 'leaf)': 19514, 'original\\nentropy': 19515, 'information\\ngain,': 19516, 'input\\nvalues': 19517, 'coherent': 19518, 'groups,': 19519, 'by\\nselecting': 19520, 'stumps': 19521, 'simple\\nalgorithm': 19522, 'this\\nprocess': 19523, 'decision\\ntree': 19524, 'previously\\nevaluated': 19525, '.\\n\\n\\ndecision': 19526, \"with,\\nthey're\": 19527, 'understand,': 19528, 'is\\nespecially': 19529, 'usually\\npossible': 19530, '.\\ndecision': 19531, 'suited': 19532, 'many\\nhierarchical': 19533, 'categorical': 19534, 'example,\\ndecision': 19535, 'capturing': 19536, 'phylogeny': 19537, 'disadvantages': 19538, 'is\\nthat,': 19539, 'data,\\nthe': 19540, 'tree\\ncan': 19541, 'may\\n\\noverfit': 19542, 'reflect\\nidiosyncrasies': 19543, 'significant\\npatterns': 19544, 'stop\\ndividing': 19545, 'to\\nprune': 19546, 'a\\ndev-test': 19547, 'be\\nchecked': 19548, 'relatively\\nindependently': 19549, 'documents\\ninto': 19550, 'automotive,': 19551, 'murder': 19552, 'mystery),': 19553, 'features\\nsuch': 19554, 'hasword(football)': 19555, 'specific\\nlabel,': 19556, 'regardless': 19557, 'these\\nfeatures': 19558, 'branches': 19559, 'the\\ntree': 19560, 'exponentially': 19561, 'we\\ngo': 19562, 'repetition': 19563, 'of\\nfeatures': 19564, 'weak': 19565, 'predictors': 19566, 'incremental': 19567, 'improvements,': 19568, 'to\\noccur': 19569, 'descended': 19570, 'they\\nshould': 19571, 'features\\nacross': 19572, 'some\\nconclusions': 19573, 'affect': 19574, 'checked': 19575, 'a\\nspecific': 19576, 'exploit': 19577, 'are\\nrelatively': 19578, 'classification\\nmethod,': 19579, 'overcomes': 19580, 'limitation': 19581, 'by\\nallowing': 19582, 'parallel': 19583, '.\\n\\n\\n\\n5\\xa0\\xa0\\xa0naive': 19584, 'classifiers\\nin': 19585, 'in\\ndetermining': 19586, 'to\\nchoose': 19587, 'begins\\nby': 19588, 'is\\ndetermined': 19589, 'prior\\nprobability,': 19590, 'the\\nlabel': 19591, 'the\\ninput': 19592, '.\\n\\n\\n\\nfigure': 19593, 'bayes\\nclassifier': 19594, 'training\\ncorpus,': 19595, 'out\\nat': 19596, 'automotive': 19597, 'then\\nconsiders': 19598, 'input\\ndocument': 19599, 'dark,': 19600, 'indicator': 19601, 'for\\nmurder': 19602, 'mysteries,': 19603, 'football,': 19604, 'strong': 19605, 'feature\\nhas': 19606, 'contribution,': 19607, 'is\\nclosest': 19608, 'to,': 19609, '.\\n\\nindividual': 19610, 'by\\nvoting': 19611, 'by\\nmultiplying': 19612, 'label\\nwould': 19613, '12%\\nof': 19614, 'mystery': 19615, '2%\\nof': 19616, 'sports\\nlabel': 19617, '.12;': 19618, 'murder\\nmystery': 19619, 'effect\\nwill': 19620, 'more\\nthan': 19621, 'the\\nautomotive': 19622, 'and\\n5': 19623, 'likelihoods': 19624, 'how\\nfrequently': 19625, 'feature\\nthen': 19626, 'contributes': 19627, 'label\\nwill': 19628, 'be\\nthought': 19629, 'randomly\\nselected': 19630, 'given\\nlabel': 19631, 'feature\\nprobabilities': 19632, '.1\\xa0\\xa0\\xa0underlying': 19633, 'probabilistic': 19634, 'model\\nanother': 19635, 'it\\nchooses': 19636, 'that\\nevery': 19637, 'entirely\\nindependent': 19638, 'is\\nunrealistic;': 19639, 'dependent': 19640, \"we'll\\nreturn\": 19641, 'simplifying': 19642, 'assumption,': 19643, 'the\\nnaive': 19644, 'independence': 19645, 'assumption)\\nmakes': 19646, 'much\\neasier': 19647, 'contributions': 19648, 'since\\nwe': 19649, 'interact': 19650, 'one\\nanother': 19651, 'bayesian': 19652, 'generative': 19653, 'process\\nthat': 19654, 'a\\nlabeled': 19655, \"input's\": 19656, '.\\nevery': 19657, 'other\\nfeature,': 19658, '.\\n\\nbased': 19659, 'for\\np(label|features),': 19660, 'label\\nl': 19661, 'maximizes': 19662, 'p(l|features)': 19663, 'begin,': 19664, 'p(label|features)': 19665, 'the\\nprobability': 19666, 'specified\\nset': 19667, 'features:\\n\\n': 19668, '(2)p(label|features)': 19669, 'p(features,': 19670, 'label)/p(features)\\nnext,': 19671, 'p(features)': 19672, 'every\\nchoice': 19673, 'suffices': 19674, 'label),\\nwhich': 19675, 'each\\nlabel,': 19676, 'the\\neasiest': 19677, 'sum\\nover': 19678, 'label):\\n\\n': 19679, '(3)p(features)': 19680, '=\\nσl': 19681, 'in|': 19682, 'label)\\n\\nthe': 19683, '(4)p(features,': 19684, 'label)': 19685, 'p(label)': 19686, 'p(features|label)\\nfurthermore,': 19687, 'another\\n(given': 19688, 'label),': 19689, 'each\\nindividual': 19690, 'feature:\\n\\n': 19691, '(5)p(features,': 19692, 'prodf': 19693, 'featuresp(f|label)`\\nthis': 19694, 'equation': 19695, 'likelihood:': 19696, 'p(f|label)': 19697, 'single\\nfeature': 19698, '.2\\xa0\\xa0\\xa0zero': 19699, 'smoothing\\nthe': 19700, 'p(f|label),': 19701, 'toward': 19702, 'that\\nalso': 19703, '(6)p(f|label)': 19704, 'count(f,': 19705, 'count(label)\\nhowever,': 19706, 'feature\\nnever': 19707, 'this\\ncase,': 19708, 'will\\ncause': 19709, 'fit': 19710, 'feature/label': 19711, 'combination\\noccur': 19712, 'impossible': 19713, 'that\\ncombination': 19714, \"wouldn't\\nwant\": 19715, 'to\\nexist': 19716, 'count(f,label)/count(label)': 19717, 'for\\np(f|label)': 19718, 'this\\nestimate': 19719, 'count(f)': 19720, 'employ\\nmore': 19721, 'smoothing': 19722, 'techniques,\\nfor': 19723, 'basically': 19724, 'adds': 19725, 'each\\ncount(f,label)': 19726, 'heldout': 19727, 'heldout\\ncorpus': 19728, 'and\\nfeature': 19729, '.probability': 19730, 'support\\nfor': 19731, '.\\n\\n\\n\\n\\n5': 19732, '.3\\xa0\\xa0\\xa0non-binary': 19733, 'features\\nwe': 19734, 'binary,': 19735, 'label-valued\\nfeatures': 19736, 'blue,\\nwhite,': 19737, 'orange)': 19738, 'replacing\\nthem': 19739, 'color-is-red': 19740, 'be\\nconverted': 19741, 'binning,': 19742, 'replaces': 19743, 'with\\nfeatures': 19744, '4<x<6': 19745, 'the\\nprobabilities': 19746, 'the\\nheight': 19747, 'bell': 19748, 'estimate\\np(height|label)': 19749, 'variance': 19750, 'heights': 19751, 'the\\ninputs': 19752, 'p(f=v|label)': 19753, 'not\\nbe': 19754, '.4\\xa0\\xa0\\xa0the': 19755, 'naivete': 19756, 'independence\\nthe': 19757, '(given': 19758, 'real-world\\nproblems': 19759, 'degrees': 19760, 'dependence': 19761, 'one\\nanother,': 19762, 'sets\\nthat': 19763, 'independent?\\none': 19764, 'up\\ndouble-counting': 19765, 'correlated': 19766, 'pushing\\nthe': 19767, 'justified': 19768, 'occur,': 19769, 'f1': 19770, 'f2': 19771, 'exact': 19772, 'of\\nf1,': 19773, 'the\\ncontribution': 19774, 'when\\ndeciding': 19775, 'weight': 19776, 'deserves': 19777, 'contain\\ntwo': 19778, 'contain\\nfeatures': 19779, 'the\\nfeatures': 19780, 'ends-with(a)': 19781, 'ends-with(vowel)': 19782, 'on\\none': 19783, 'it\\nmust': 19784, 'the\\nduplicated': 19785, 'by\\nthe': 19786, '.5\\xa0\\xa0\\xa0the': 19787, 'double-counting\\nthe': 19788, 'that\\nduring': 19789, 'separately;\\nbut': 19790, 'those\\nfeature': 19791, 'to\\nconsider': 19792, 'interactions': 19793, 'contributions\\nduring': 19794, 'adjust': 19795, 'the\\ncontributions': 19796, 'precise,': 19797, 'separating': 19798, 'label):\\n\\n\\n': 19799, '(7)p(features,': 19800, 'w[label]': 19801, 'w[f,': 19802, 'label]\\nhere,': 19803, 'and\\nw[f,': 19804, 'label]': 19805, 'feature\\ntowards': 19806, \"label's\": 19807, 'w[label]\\nand': 19808, 'weights': 19809, 'the\\nmodel': 19810, 'these\\nparameters': 19811, 'independently:\\n\\n': 19812, '(8)w[label]': 19813, 'p(label)\\n\\n': 19814, '(9)w[f,': 19815, 'p(f|label)\\nhowever,': 19816, 'that\\nconsiders': 19817, 'when\\nchoosing': 19818, '.\\n\\n\\n\\n6\\xa0\\xa0\\xa0maximum': 19819, 'classifiers\\nthe': 19820, 'very\\nsimilar': 19821, \"model's\": 19822, 'search\\ntechniques': 19823, 'of\\nparameters': 19824, 'as:\\n\\n': 19825, '(10)p(features)': 19826, '=\\nσx': 19827, 'p(label(x)|features(x))\\nwhere': 19828, 'p(label|features),': 19829, '(11)p(label|features)': 19830, 'p(label,': 19831, 'features)': 19832, '/\\nσlabel': 19833, 'features)\\nbecause': 19834, 'effects': 19835, 'of\\nrelated': 19836, 'model\\nparameters': 19837, 'parameters\\nusing': 19838, 'optimization': 19839, \"the\\nmodel's\": 19840, 'refine': 19841, 'those\\nparameters': 19842, 'optimal': 19843, 'these\\niterative': 19844, 'guarantee': 19845, 'refinement': 19846, 'necessarily': 19847, 'optimal\\nvalues': 19848, 'reached': 19849, 'entropy\\nclassifiers': 19850, 'they\\ncan': 19851, 'size\\nof': 19852, '.\\n\\nnote\\nsome': 19853, 'than\\nothers': 19854, 'of\\ngeneralized': 19855, 'scaling': 19856, '(gis)': 19857, 'scaling\\n(iis),': 19858, 'considerably': 19859, 'conjugate\\ngradient': 19860, '(cg)': 19861, 'bfgs': 19862, '.\\n\\n\\n\\n\\n6': 19863, 'model\\nthe': 19864, 'model\\nused': 19865, 'the\\nmaximum': 19866, 'multiplying': 19867, 'are\\napplicable': 19868, 'classifier\\nmodel': 19869, '(feature,': 19870, 'pair,\\nspecifying': 19871, \"label's\\nlikelihood\": 19872, 'the\\nuser': 19873, 'receive\\ntheir': 19874, 'label;': 19875, 'will\\nsometimes': 19876, 'the\\ndifferences': 19877, 'receives': 19878, 'own\\nparameter': 19879, 'joint-feature': 19880, 'joint-features\\nare': 19881, '(simple)': 19882, 'are\\nproperties': 19883, 'unlabeled': 19884, '.\\n\\nnote\\nin': 19885, 'literature': 19886, 'entropy\\nmodels,': 19887, 'refers': 19888, 'to\\njoint-features;': 19889, '.\\n\\ntypically,': 19890, 'joint-features': 19891, 'maximum\\nentropy': 19892, 'mirror': 19893, 'bayes\\nmodel': 19894, 'label,\\ncorresponding': 19895, 'w[label],': 19896, 'of\\n(simple)': 19897, 'w[f,label]': 19898, 'score\\nassigned': 19899, 'product': 19900, 'the\\nparameters': 19901, 'input\\nand': 19902, 'label:\\n\\n\\n': 19903, '(12)p(input,': 19904, 'prodjoint-features(input,label)': 19905, 'w[joint-feature]\\n\\n\\n6': 19906, '.2\\xa0\\xa0\\xa0maximizing': 19907, 'entropy\\nthe': 19908, 'intuition': 19909, 'we\\nshould': 19910, 'individual\\njoint-features,': 19911, 'unwarranted': 19912, 'principle': 19913, '(labeled': 19914, 'a-j)': 19915, 'at\\nfirst,': 19916, 'the\\nten': 19917, 'as:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\na\\nb\\nc\\nd\\ne\\nf\\ng\\nh\\ni\\nj\\n\\n\\n\\n(i)\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n\\n(ii)\\n5%\\n15%\\n0%\\n30%\\n0%\\n8%\\n12%\\n0%\\n6%\\n24%\\n\\n(iii)\\n0%\\n100%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n\\n\\ntable': 19918, '.1\\n\\nalthough': 19919, 'correct,': 19920, '(i),': 19921, 'information,\\nthere': 19922, 'than\\nany': 19923, 'reflect\\nassumptions': 19924, 'fair\\nthan': 19925, 'the\\ndiscussion': 19926, 'how\\ndisorganized': 19927, 'label\\ndominates': 19928, 'low,': 19929, 'evenly\\ndistributed': 19930, 'chose\\ndistribution': 19931, 'in\\ngeneral,': 19932, 'the\\ndistributions': 19933, 'know,': 19934, 'choose\\nthe': 19935, '55%': 19936, 'this\\nnew': 19937, 'as:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\na\\nb\\nc\\nd\\ne\\nf\\ng\\nh\\ni\\nj\\n\\n\\n\\n(iv)\\n55%\\n45%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n\\n(v)\\n55%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n\\n(vi)\\n55%\\n3%\\n1%\\n2%\\n9%\\n5%\\n0%\\n25%\\n0%\\n0%\\n\\n\\ntable': 19938, '.2\\n\\nbut': 19939, 'the\\nfewest': 19940, '(v)': 19941, 'the\\nnearby': 19942, '80%': 19943, 'coming': 19944, 'appropriate\\ndistribution': 19945, 'hand;': 19946, 'following\\ndistribution': 19947, 'appropriate:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\na\\nb\\nc\\nd\\ne\\nf\\ng\\nh\\ni\\nj\\n\\n\\n\\n(vii)\\n+up\\n5': 19948, '.1%\\n0': 19949, '.25%\\n2': 19950, '.9%\\n0': 19951, '.25%\\n0': 19952, '.25%\\n\\n`': 19953, '`\\n-up\\n49': 19954, '.9%\\n4': 19955, '.46%\\n4': 19956, '.46%\\n\\n\\ntable': 19957, '.3\\n\\nin': 19958, 'know:': 19959, 'we\\nadd': 19960, '55%;': 19961, '10%;': 19962, 'boxes': 19963, 'for\\nsenses': 19964, '+up': 19965, '8%': 19966, 'cases)': 19967, '.\\nfurthermore,': 19968, '.\\nthroughout': 19969, 'distributions\\nthat': 19970, 'know;': 19971, 'the\\ndistribution': 19972, 'each\\njoint-feature,': 19973, 'empirical\\nfrequency': 19974, 'which\\nmaximizes': 19975, 'for\\neach': 19976, '.\\n\\n\\n6': 19977, '.3\\xa0\\xa0\\xa0generative': 19978, 'classifiers\\n\\nan': 19979, 'be\\nused': 19980, 'a\\ngenerative': 19981, 'predicts\\np(input,': 19982, '(input,\\nlabel)': 19983, 'input?\\nhow': 19984, 'input?\\nwhat': 19985, 'value?\\nhow': 19986, 'label?\\nwhat': 19987, 'one\\nof': 19988, 'which)?\\n\\n\\nthe': 19989, 'p(label|input)': 19990, 'label\\ngiven': 19991, '3-6': 19992, 'strictly': 19993, 'than\\nconditional': 19994, 'probability\\np(label|input)': 19995, 'p(input,\\nlabel),': 19996, 'price': 19997, 'powerful,': 19998, 'a\\nmore': 19999, \"parameter's\": 20000, 'best\\nparameter': 20001, 'good\\na': 20002, 'the\\nconditional': 20003, 'efforts': 20004, '3-6,': 20005, 'no\\nchoice': 20006, 'is\\nanalogous': 20007, 'topographical': 20008, 'skyline': 20009, 'wider\\nvariety': 20010, 'generate\\nan': 20011, '.\\n\\n\\n\\n\\n7\\xa0\\xa0\\xa0modeling': 20012, 'patterns\\nclassifiers': 20013, 'that\\noccur': 20014, 'explicit\\nmodels': 20015, 'typically,': 20016, 'analytically': 20017, 'purposes:': 20018, 'understand\\nlinguistic': 20019, 'predictions': 20020, 'about\\nnew': 20021, 'insights': 20022, 'into\\nlinguistic': 20023, 'largely': 20024, 'making\\ndecisions': 20025, 'other\\nmodels,': 20026, 'multi-level': 20027, 'neural': 20028, 'opaque': 20029, 'it\\ntypically': 20030, 'unseen\\nlanguage': 20031, 'assess': 20032, 'deemed': 20033, 'sufficiently': 20034, 'accurate,': 20035, 'then\\nbe': 20036, 'language\\ndata': 20037, 'that\\nperform': 20038, 'document\\nclassification,': 20039, '.1\\xa0\\xa0\\xa0what': 20040, \"us?\\nit's\": 20041, 'an\\nautomatically': 20042, 'when\\ndealing': 20043, 'descriptive\\nmodels': 20044, 'explanatory': 20045, 'data\\ncontains': 20046, '.1,\\nthe': 20047, 'not\\ninterchangeable:': 20048, 'adore': 20049, 'definitely\\nadore,': 20050, 'and\\nrelationships': 20051, 'we\\nmight': 20052, 'polar': 20053, 'that\\nhas': 20054, 'like\\nadore': 20055, 'detest': 20056, 'would\\ncontain': 20057, 'constraint': 20058, 'with\\npolar': 20059, 'non-polar\\nverbs': 20060, 'about\\ncorrelations': 20061, 'to\\npostulate': 20062, 'causal': 20063, 'are\\ndescriptive': 20064, 'models;': 20065, 'are\\nrelevant': 20066, 'construction,': 20067, \"can't\\nnecessarily\": 20068, 'then\\nwe': 20069, 'a\\nstarting': 20070, 'experiments': 20071, 'tease': 20072, 'the\\nrelationships': 20073, \"if\\nwe're\": 20074, 'as\\npart': 20075, 'system),': 20076, 'worrying': 20077, 'details\\nof': 20078, '.\\n\\n\\n\\n8\\xa0\\xa0\\xa0summary\\n\\nmodeling': 20079, 'predictions\\nabout': 20080, '.\\nsupervised': 20081, 'of\\nthat': 20082, 'of\\nnlp': 20083, 'part-of-speech\\ntagging,': 20084, 'identification,\\nand': 20085, 'corpus\\ninto': 20086, 'datasets:': 20087, 'model;': 20088, 'helping': 20089, 'select\\nand': 20090, 'tune': 20091, 'for\\nevaluating': 20092, 'you\\nuse': 20093, 'dev-test\\nset': 20094, 'otherwise,': 20095, 'unrealistically\\noptimistic': 20096, 'tree-structured\\nflowcharts': 20097, 'on\\ntheir': 20098, 'interpret,': 20099, 'not\\nvery': 20100, 'contributes\\nto': 20101, 'feature\\nvalues': 20102, 'interact,': 20103, '.\\nmaximum': 20104, 'bayes;': 20105, 'optimization\\nto': 20106, 'corpus\\nare': 20107, 'relevant\\nto': 20108, 'any\\ninformation': 20109, 'and\\npatterns': 20110, 'reading\\nplease': 20111, 'to\\ninstall': 20112, 'weka,': 20113, 'mallet,\\ntadm,': 20114, 'megam': 20115, 'nltk,\\nplease': 20116, 'recommend\\n(alpaydin,': 20117, 'mathematically': 20118, 'intense': 20119, '(hastie,': 20120, 'tibshirani,': 20121, 'friedman,': 20122, '2009)': 20123, 'on\\nusing': 20124, '(abney,': 20125, '2008),\\n(daelemans': 20126, 'bosch,': 20127, '(feldman': 20128, 'sanger,': 20129, '2007),': 20130, '(segaran,': 20131, '(weiss': 20132, 'see\\n(manning': 20133, 'schutze,': 20134, '1999)': 20135, 'modeling,': 20136, 'especially\\nhidden': 20137, '(manning': 20138, '.\\nchapter': 20139, '(manning,': 20140, 'raghavan,': 20141, 'for\\nclassifying': 20142, 'are\\nnumerically': 20143, 'intensive,': 20144, 'slowly': 20145, 'when\\ncoded': 20146, 'naively': 20147, 'increasing': 20148, 'efficiency\\nof': 20149, '(kiusalaas,': 20150, 'applied\\nto': 20151, '(agirre': 20152, 'edmonds,': 20153, 'uses\\nclassifiers': 20154, 'word-sense': 20155, 'disambiguation;': 20156, '(melamed,': 20157, '2001)\\nuses': 20158, 'that\\ncover': 20159, '(croft,': 20160, 'metzler,': 20161, 'strohman,': 20162, '.\\nmuch': 20163, 'learning\\ntechniques': 20164, 'driven': 20165, 'government-sponsored\\nchallenges,': 20166, 'provided\\nwith': 20167, 'the\\nresulting': 20168, 'compared': 20169, 'competitions': 20170, 'conll': 20171, 'ace\\ncompetitions,': 20172, 'competitions,\\nand': 20173, 'aquaint': 20174, 'webpages': 20175, '.\\n\\n\\n\\n10\\xa0\\xa0\\xa0exercises\\n\\n☼': 20176, 'as\\nword': 20177, 'answering,': 20178, 'translation,\\nnamed': 20179, '.\\nwhy': 20180, 'required?\\n\\n☼': 20181, 'this\\nchapter,': 20182, 'name\\ngender': 20183, 'subsets:': 20184, '6900': 20185, '.\\nthen,': 20186, 'make\\nincremental': 20187, 'your\\nprogress': 20188, 'its\\nfinal': 20189, 'set?\\nis': 20190, \"you'd\": 20191, 'expect?\\n\\n☼': 20192, 'senseval': 20193, 'train\\nword-sense': 20194, 'for\\nfour': 20195, 'hard,': 20196, 'these\\nfour': 20197, 'senseval\\n>>>': 20198, \".instances('hard\": 20199, \".pos')\\n>>>\": 20200, 'int(len(instances)': 20201, 'instances[size:],': 20202, 'instances[:size]\\n\\n\\n\\nusing': 20203, 'dataset,': 20204, 'correct\\nsense': 20205, 'at\\nhttp://nltk': 20206, 'objects\\nreturned': 20207, '.\\n\\n☼\\nusing': 20208, 'classifier\\nfinds': 20209, 'particular\\nfeatures': 20210, 'informative?': 20211, 'surprising?\\n\\n☼\\nselect': 20212, 'chapter,\\nsuch': 20213, 'detection,': 20214, 'classification,\\npart-of-speech': 20215, 'extractor,\\nbuild': 20216, 'naive\\nbayes': 20217, 'compare\\nthe': 20218, 'used\\na': 20219, 'extractor?\\n\\n☼\\nthe': 20220, 'pattern\\ndifferently': 20221, '(try': 20222, 'chip': 20223, 'sales)': 20224, 'distinction?\\nbuild': 20225, '.\\n\\n◑\\nthe': 20226, 'posts,\\nwithout': 20227, 'some\\nsequences': 20228, 'ynquestion': 20229, 'be\\nanswered': 20230, 'yanswer': 20231, 'greeting': 20232, 'this\\nfact': 20233, 'code\\nfor': 20234, '.\\n\\n◑\\nword': 20235, 'a\\nstrong': 20236, 'indication': 20237, 'however,\\nmany': 20238, 'infrequently,': 20239, 'our\\ntraining': 20240, 'lexicon,\\nwhich': 20241, 'using\\nwordnet': 20242, 'augment': 20243, 'classifier\\npresented': 20244, 'will\\nmatch': 20245, '.\\n\\n★\\nthe': 20246, 'pp': 20247, 'describing\\nprepositional': 20248, 'ppattachment': 20249, 'ppattach\\n>>>': 20250, 'ppattach': 20251, \".attachments('training')\\n[ppattachment(sent='0',\": 20252, \"verb='join',\": 20253, \"noun1='board',\\n\": 20254, \"prep='as',\": 20255, \"noun2='director',\": 20256, \"attachment='v'),\\n\": 20257, \"ppattachment(sent='1',\": 20258, \"verb='is',\": 20259, \"noun1='chairman',\\n\": 20260, \"prep='of',\": 20261, \"noun2='n\": 20262, \"attachment='n'),\\n\": 20263, 'inst': 20264, \".attachments('training')[1]\\n>>>\": 20265, '(inst': 20266, '.noun1,': 20267, '.prep,': 20268, \".noun2)\\n('chairman',\": 20269, \"'n\": 20270, \".')\\n\\n\\n\\nselect\": 20271, '.attachment': 20272, 'n:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 20273, 'nattach': 20274, '[inst': 20275, \".attachments('training')\\n\": 20276, \"'n']\\n\\n\\n\\nusing\": 20277, 'sub-corpus,': 20278, 'predict\\nwhich': 20279, 'connect': 20280, 'researchers,': 20281, 'corpus\\nhowto': 20282, 'pp\\nattachment': 20283, 'scene,\\nand': 20284, 'uniquely': 20285, 'jar,\\nand': 20286, 'relating\\nvarious': 20287, 'cupboard': 20288, 'shelf': 20289, 'data;': 20290, '.\\n\\n\\n\\n': 20291, '(13)\\n': 20292, 'train\\n\\n': 20293, 'campus\\n\\n': 20294, 'screen\\n\\n': 20295, 'macbeth': 20296, 'letterman\\n\\n\\n\\nabout': 20297, '(ch06': 20298, '1264);': 20299, 'backlink\\nundefined': 20300, 'referenced:': 20301, 'text\\n\\n\\n\\n\\n\\n7': 20302, 'text\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nfor': 20303, 'question,': 20304, 'the\\nanswer': 20305, 'truly': 20306, 'staggering,': 20307, 'increasing\\nevery': 20308, 'complexity': 20309, 'it\\nvery': 20310, 'being\\nable': 20311, 'general-purpose': 20312, 'or\\nentity': 20313, 'located,\\nor': 20314, 'company,': 20315, 'as\\ntables,': 20316, 'unstructured': 20317, 'text?\\nwhich': 20318, 'use\\nthem': 20319, 'recognition': 20320, '.\\n\\n1\\xa0\\xa0\\xa0information': 20321, 'extraction\\ninformation': 20322, 'shapes': 20323, 'is\\nstructured': 20324, 'predictable\\norganization': 20325, 'the\\nrelation': 20326, 'companies': 20327, 'company,\\nwe': 20328, 'does\\nbusiness;': 20329, 'discover\\nwhich': 20330, 'tabular\\nform,': 20331, 'then\\nanswering': 20332, 'queries': 20333, '.\\n\\n\\n\\n\\n\\n\\norgname\\nlocationname\\n\\n\\n\\nomnicom\\nnew': 20334, 'york\\n\\nddb': 20335, 'needham\\nnew': 20336, 'york\\n\\nkaplan': 20337, 'thaler': 20338, 'group\\nnew': 20339, 'york\\n\\nbbdo': 20340, 'south\\natlanta\\n\\ngeorgia-pacific\\natlanta\\n\\n\\ntable': 20341, 'data\\n\\n\\nif': 20342, 'tuples\\n(entity,': 20343, 'entity),': 20344, 'question\\nwhich': 20345, 'atlanta?': 20346, 'be\\ntranslated': 20347, 'locs': 20348, \"[('omnicom',\": 20349, \"'new\": 20350, \"york'),\\n\": 20351, \"('ddb\": 20352, \"needham',\": 20353, \"('kaplan\": 20354, \"group',\": 20355, \"('bbdo\": 20356, \"south',\": 20357, \"'atlanta'),\\n\": 20358, \"('georgia-pacific',\": 20359, \"'atlanta')]\\n>>>\": 20360, '[e1': 20361, '(e1,': 20362, 'rel,': 20363, 'e2)': 20364, \"e2=='atlanta']\\n>>>\": 20365, \"print(query)\\n['bbdo\": 20366, \"'georgia-pacific']\\n\\n\\n\\n\\n\\n\\n\\n\\norgname\\n\\n\\n\\nbbdo\": 20367, 'south\\n\\ngeorgia-pacific\\n\\n\\ntable': 20368, 'atlanta\\n\\n\\nthings': 20369, 'of\\ntext': 20370, '.ieer,': 20371, 'nyt19980315': 20372, '.0085)': 20373, 'wells': 20374, 'packaged\\npaper-products': 20375, 'georgia-pacific': 20376, 'corp': 20377, 'arrived': 20378, 'at\\nwells': 20379, 'hertz': 20380, 'channel,': 20381, 'is\\nalso': 20382, 'omnicom-owned': 20383, 'agency,': 20384, 'bbdo': 20385, 'south': 20386, 'of\\nbbdo': 20387, 'worldwide': 20388, 'atlanta,': 20389, 'corporate\\nadvertising': 20390, 'georgia-pacific,': 20391, 'duties': 20392, 'for\\nbrands': 20393, 'angel': 20394, 'soft': 20395, 'toilet': 20396, 'tissue': 20397, 'sparkle': 20398, 'towels,\\nsaid': 20399, 'ken': 20400, 'haldin,': 20401, 'spokesman': 20402, 'atlanta': 20403, 'glean': 20404, 'required\\nto': 20405, 'enough\\nabout': 20406, '.2?': 20407, 'is\\nobviously': 20408, '(1)\\ncontains': 20409, 'with\\nlocation': 20410, 'general\\nrepresentation': 20411, '(10': 20412, 'approach,\\ndeciding': 20413, 'and\\nlocations': 20414, 'directly,\\nwe': 20415, 'unstructured\\ndata': 20416, 'of\\n1': 20417, 'reap': 20418, 'benefits': 20419, 'query\\ntools': 20420, 'sql': 20421, '.\\ninformation': 20422, 'including\\nbusiness': 20423, 'resume': 20424, 'harvesting,': 20425, 'media': 20426, 'detection,\\npatent': 20427, 'search,': 20428, 'scanning': 20429, 'a\\nparticularly': 20430, 'attempt\\nto': 20431, 'electronically-available': 20432, 'scientific\\nliterature,': 20433, 'biology': 20434, 'medicine': 20435, '.1\\xa0\\xa0\\xa0information': 20436, 'architecture\\n1': 20437, 'information\\nextraction': 20438, 'several\\nof': 20439, 'first,\\nthe': 20440, 'sentence\\nsegmenter,': 20441, 'subdivided': 20442, 'a\\ntokenizer': 20443, 'tags,\\nwhich': 20444, 'prove': 20445, 'entity\\ndetection': 20446, 'potentially\\ninteresting': 20447, 'relation\\ndetection': 20448, 'different\\nentities': 20449, 'and\\ngenerates': 20450, '(entity,': 20451, 'entity)': 20452, 'its\\noutput': 20453, 'the\\ncompany': 20454, 'located': 20455, 'generate\\nthe': 20456, '([org:': 20457, \"'georgia-pacific']\": 20458, \"'in'\": 20459, '[loc:': 20460, \"'atlanta'])\": 20461, 'that\\nsimply': 20462, 'segmenter\\n,': 20463, 'tagger\\n:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 20464, 'ie_preprocess(document):\\n': 20465, '.sent_tokenize(document)': 20466, '.word_tokenize(sent)': 20467, 'sentences]': 20468, '.pos_tag(sent)': 20469, '\\n\\n\\n\\n\\nnote\\nremember': 20470, 'with:': 20471, 'pprint\\n\\nnext,': 20472, 'the\\nentities': 20473, 'participate': 20474, 'definite': 20475, 'the\\nknights': 20476, 'ni,': 20477, 'indefinite': 20478, 'noun\\nchunks,': 20479, 'cats,\\nand': 20480, 'to\\nentities': 20481, 'extraction,': 20482, 'patterns\\nbetween': 20483, 'and\\nuse': 20484, 'recording': 20485, 'relationships\\nbetween': 20486, '.\\n\\n\\n\\n2\\xa0\\xa0\\xa0chunking\\nthe': 20487, 'is\\nchunking,': 20488, 'segments': 20489, 'multi-token': 20490, 'as\\nillustrated': 20491, 'the\\nword-level': 20492, 'large\\nboxes': 20493, 'higher-level': 20494, 'chunk': 20495, 'omits': 20496, 'whitespace,\\nchunking': 20497, '.\\nalso': 20498, 'chunker': 20499, 'overlap\\nin': 20500, 'levels\\n\\nin': 20501, 'chunking,': 20502, 'chunkers': 20503, 'conll-2000': 20504, 'in\\n(5)': 20505, '6\\nto': 20506, '.1\\xa0\\xa0\\xa0noun': 20507, 'chunking\\nwe': 20508, 'chunking,\\nor': 20509, 'np-chunking,': 20510, 'np-chunks': 20511, 'brackets:\\n\\n': 20512, '(2)[': 20513, 'market/nn': 20514, 'for/in': 20515, 'system-management/nn': 20516, 'software/nn': 20517, ']\\nfor/in': 20518, 'digital/nnp': 20519, \"'s/pos\": 20520, 'hardware/nn': 20521, 'is/vbz': 20522, 'fragmented/jj\\nenough/rb': 20523, 'that/in': 20524, 'a/dt': 20525, 'giant/nn': 20526, 'such/jj': 20527, 'as/in': 20528, 'computer/nnp\\nassociates/nnps': 20529, 'should/md': 20530, 'do/vb': 20531, 'well/rb': 20532, 'there/rb': 20533, 'complete\\nnoun': 20534, 'system-management': 20535, 'software\\nfor': 20536, \"digital's\": 20537, 'hardware': 20538, 'two\\nnested': 20539, 'phrases),': 20540, 'captured': 20541, 'the\\nsimpler': 20542, 'motivations': 20543, 'this\\ndifference': 20544, 'contain\\nother': 20545, 'any\\nprepositional': 20546, 'nominal\\nwill': 20547, 'np-chunk,': 20548, 'they\\nalmost': 20549, 'np-chunking': 20550, 'is\\npart-of-speech': 20551, 'for\\nperforming': 20552, 'extraction\\nsystem': 20553, 'an\\nnp-chunker,': 20554, 'grammar,': 20555, 'rules\\nthat': 20556, 'chunked': 20557, 'will\\ndefine': 20558, 'rule\\n': 20559, 'formed\\nwhenever': 20560, '(dt)': 20561, 'any\\nnumber': 20562, '(jj)': 20563, 'grammar,\\nwe': 20564, 'parser': 20565, 'example\\nsentence': 20566, 'either\\nprint': 20567, 'graphically': 20568, '[(the,': 20569, 'dt),': 20570, '(little,': 20571, 'jj),': 20572, '(yellow,': 20573, '(dog,': 20574, 'nn),': 20575, '(barked,': 20576, 'vbd),': 20577, '(at,': 20578, 'in),': 20579, '(the,': 20580, '(cat,': 20581, 'nn)]\\n\\n>>>': 20582, 'np:': 20583, '{<dt>?<jj>*<nn>}': 20584, 'cp': 20585, '.regexpparser(grammar)': 20586, '.parse(sentence)': 20587, 'print(result)': 20588, '\\n(s\\n': 20589, '(np': 20590, 'little/jj': 20591, 'yellow/jj': 20592, 'dog/nn)\\n': 20593, 'barked/vbd\\n': 20594, 'at/in\\n': 20595, 'cat/nn))\\n>>>': 20596, '.draw()': 20597, '(code_chunkex': 20598, '.\\n\\n\\n\\n\\n2': 20599, '.2\\xa0\\xa0\\xa0tag': 20600, 'patterns\\nthe': 20601, 'to\\ndescribe': 20602, 'delimited\\nusing': 20603, '<dt>?<jj>*<nn>': 20604, 'are\\nsimilar': 20605, 'journal:\\n\\nanother/dt': 20606, 'sharp/jj': 20607, 'dive/nn\\ntrade/nn': 20608, 'figures/nns\\nany/dt': 20609, 'new/jj': 20610, 'policy/nn': 20611, 'measures/nns\\nearlier/jjr': 20612, 'stages/nns\\npanamanian/jj': 20613, 'dictator/nn': 20614, 'manuel/nnp': 20615, 'noriega/nnp\\n\\nwe': 20616, 'pattern\\nabove,': 20617, '<dt>?<jj': 20618, '.*>*<nn': 20619, '.*>+': 20620, 'by\\nzero': 20621, 'relative\\nadjectives': 20622, 'earlier/jjr),': 20623, 'any\\ntype': 20624, 'which\\nthis': 20625, 'cover:\\n\\nhis/prp$': 20626, 'mansion/nnp': 20627, 'house/nnp': 20628, 'speech/nn\\nthe/dt': 20629, 'price/nn': 20630, 'cutting/vbg\\n3/cd': 20631, '%/nn': 20632, 'to/to': 20633, '4/cd': 20634, '%/nn\\nmore/jjr': 20635, 'than/in': 20636, '10/cd': 20637, '%/nn\\nthe/dt': 20638, 'fastest/jjs': 20639, 'developing/vbg': 20640, \"trends/nns\\n's/pos\": 20641, 'skill/nn\\n\\n\\nnote\\nyour': 20642, '.\\ntest': 20643, 'interface\\nnltk': 20644, '.chunkparser()': 20645, 'your\\ntag': 20646, '.3\\xa0\\xa0\\xa0chunking': 20647, 'expressions\\nto': 20648, 'regexpparser\\nchunker': 20649, 'flat': 20650, 'are\\nchunked': 20651, 'turn,\\nsuccessively': 20652, 'the\\nchunk': 20653, 'invoked,': 20654, '.\\n2': 20655, 'a\\nsimple': 20656, 'rule\\nmatches': 20657, 'pronoun,\\nzero': 20658, '.\\n\\n\\n\\n\\n\\xa0\\n\\ngrammar': 20659, 'r\\n': 20660, '{<dt|pp\\\\$>?<jj>*<nn>}': 20661, 'determiner/possessive,': 20662, 'noun\\n': 20663, '{<nnp>+}': 20664, 'nouns\\n\\ncp': 20665, '.regexpparser(grammar)\\nsentence': 20666, '[(rapunzel,': 20667, 'nnp),': 20668, '(let,': 20669, '(down,': 20670, 'rp),': 20671, '(her,': 20672, 'pp$),': 20673, '(long,': 20674, '(golden,': 20675, '(hair,': 20676, 'nn)]\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 20677, 'print(cp': 20678, '.parse(sentence))': 20679, 'rapunzel/nnp)\\n': 20680, 'let/vbd\\n': 20681, 'down/rp\\n': 20682, 'her/pp$': 20683, 'long/jj': 20684, 'golden/jj': 20685, 'hair/nn))\\n\\n\\nexample': 20686, '(code_chunker1': 20687, 'chunker\\n\\n\\nnote\\nthe': 20688, 'regular\\nexpressions,': 20689, 'escaped\\nin': 20690, 'pp$': 20691, 'locations,': 20692, 'leftmost\\nmatch': 20693, 'matches\\ntwo': 20694, 'nouns,\\nthen': 20695, 'chunked:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 20696, '[(money,': 20697, '(market,': 20698, '(fund,': 20699, 'nn)]\\n>>>': 20700, '{<nn><nn>}': 20701, 'nouns\\n>>>': 20702, '.regexpparser(grammar)\\n>>>': 20703, '.parse(nouns))\\n(s': 20704, 'money/nn': 20705, 'market/nn)': 20706, 'fund/nn)\\n\\n\\n\\nonce': 20707, 'market,': 20708, 'have\\nremoved': 20709, 'fund': 20710, 'be\\nincluded': 20711, 'with\\na': 20712, 'permissive': 20713, 'rule,': 20714, '{<nn>+}': 20715, 'optional;': 20716, 'present,': 20717, 'chunker\\nprints': 20718, 'tracing': 20719, '.4\\xa0\\xa0\\xa0exploring': 20720, 'corpora\\nin': 20721, 'interrogate\\na': 20722, 'particular\\nsequence': 20723, 'work\\nmore': 20724, 'chunker,': 20725, \".regexpparser('chunk:\": 20726, '{<v': 20727, '<to>': 20728, '<v': 20729, \".*>}')\\n>>>\": 20730, '.brown\\n>>>': 20731, '.parse(sent)\\n': 20732, 'subtree': 20733, '.subtrees():\\n': 20734, '.label()': 20735, \"'chunk':\": 20736, 'print(subtree)\\n': 20737, '.\\n(chunk': 20738, 'combined/vbn': 20739, 'achieve/vb)\\n(chunk': 20740, 'continue/vb': 20741, 'place/vb)\\n(chunk': 20742, 'serve/vb': 20743, 'protect/vb)\\n(chunk': 20744, 'wanted/vbd': 20745, 'wait/vb)\\n(chunk': 20746, 'allowed/vbn': 20747, 'expected/vbn': 20748, 'become/vb)\\n': 20749, 'seems/vbz': 20750, 'overtake/vb)\\n(chunk': 20751, 'want/vb': 20752, 'buy/vb)\\n\\n\\n\\n\\nnote\\nyour': 20753, 'turn:\\nencapsulate': 20754, 'find_chunks()\\nthat': 20755, 'chunk:': 20756, '.*>}': 20757, 'four\\nor': 20758, 'nouns:': 20759, '{<n': 20760, '.*>{4,}}\\n\\n\\n\\n2': 20761, '.5\\xa0\\xa0\\xa0chinking\\nsometimes': 20762, 'chink': 20763, 'barked/vbd': 20764, 'at/in': 20765, 'chink:\\n\\n[': 20766, 'dog/nn': 20767, 'cat/nn': 20768, ']\\n\\nchinking': 20769, 'a\\nchunk': 20770, 'spans': 20771, 'chunk,': 20772, 'the\\nwhole': 20773, 'removed;': 20774, 'the\\nmiddle': 20775, 'chunks\\nwhere': 20776, 'periphery\\nof': 20777, 'remains': 20778, '.\\n\\n\\n\\n\\n\\n\\n\\n\\n`': 20779, '`\\nentire': 20780, 'chunk\\nmiddle': 20781, 'chunk\\nend': 20782, 'chunk\\n\\n\\n\\ninput\\n[a/dt': 20783, 'little/jj\\ndog/nn]\\n[a/dt': 20784, 'little/jj\\ndog/nn]\\n\\noperation\\nchink': 20785, 'dt': 20786, 'nn\\nchink': 20787, 'jj\\nchink': 20788, 'nn\\n\\npattern\\n}dt': 20789, 'nn{\\n}jj{\\n}nn{\\n\\noutput\\na/dt': 20790, 'little/jj\\ndog/nn\\n[a/dt]': 20791, 'little/jj\\n[dog/nn]\\n[a/dt': 20792, 'little/jj]\\ndog/nn\\n\\n\\ntable': 20793, 'chinking': 20794, 'chunk\\n\\n\\nin': 20795, '.4,': 20796, 'chunk,\\nthen': 20797, 'excise': 20798, 'chinks': 20799, 'np:\\n': 20800, '{<': 20801, '.*>+}': 20802, 'everything\\n': 20803, '}<vbd|in>+{': 20804, 'in\\n': 20805, '\\nsentence': 20806, 'jj),\\n': 20807, 'nn)]\\ncp': 20808, '.regexpparser(grammar)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 20809, '.parse(sentence))\\n': 20810, '(s\\n': 20811, 'cat/nn))\\n\\n\\nexample': 20812, '(code_chinker': 20813, 'chinker\\n\\n\\n\\n\\n\\n\\n2': 20814, '.6\\xa0\\xa0\\xa0representing': 20815, 'chunks:': 20816, 'trees\\nas': 20817, 'befits': 20818, 'status': 20819, '.),\\nchunk': 20820, 'most\\nwidespread': 20821, 'iob': 20822, 'this\\nscheme,': 20823, 'tags,\\ni': 20824, '(inside),': 20825, '(outside),': 20826, '(begin)': 20827, 'tagged\\nas': 20828, 'tokens\\nwithin': 20829, 'suffixed': 20830, 'type,\\ne': 20831, 'b-np,': 20832, 'i-np': 20833, 'just\\nlabeled': 20834, 'scheme': 20835, 'structures\\n\\niob': 20836, 'in\\nfiles,': 20837, 'is\\nhow': 20838, 'file:\\n\\nwe': 20839, 'b-np\\nsaw': 20840, 'o\\nthe': 20841, 'b-np\\nyellow': 20842, 'i-np\\ndog': 20843, 'i-np\\n\\nin': 20844, 'with\\nits': 20845, 'us\\nto': 20846, 'earlier,': 20847, 'using\\ntrees': 20848, 'constituent': 20849, 'manipulated': 20850, 'structures\\n\\n\\nnote\\nnltk': 20851, 'chunks,': 20852, 'but\\nprovides': 20853, '.\\n\\n\\n\\n\\n3\\xa0\\xa0\\xa0developing': 20854, 'chunkers\\nnow': 20855, \"haven't\\nexplained\": 20856, 'suitably': 20857, 'mechanics': 20858, 'an\\nnltk': 20859, 'a\\nchunked': 20860, 'corpus,\\nthen': 20861, 'data-driven': 20862, 'expanding': 20863, '.1\\xa0\\xa0\\xa0reading': 20864, 'corpus\\nusing': 20865, 'journal\\ntext': 20866, 'np,': 20867, 'vp': 20868, 'shown\\nbelow:\\n\\nhe': 20869, 'b-np\\naccepted': 20870, 'b-vp\\nthe': 20871, 'b-np\\nposition': 20872, 'i-np\\n': 20873, '.conllstr2tree()': 20874, 'tree\\nrepresentation': 20875, 'moreover,': 20876, 'it\\npermits': 20877, 'use,\\nhere': 20878, 'chunks:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 20879, 'b-np\\n': 20880, 'b-vp\\n': 20881, 'b-pp\\n': 20882, 'chairman': 20883, 'carlyle': 20884, 'o\\n': 20885, 'merchant': 20886, 'banking': 20887, 'concern': 20888, '.chunk': 20889, '.conllstr2tree(text,': 20890, \"chunk_types=['np'])\": 20891, '.draw()\\n\\n\\n\\n\\nwe': 20892, 'chunked\\ntext': 20893, '270k': 20894, 'portions,': 20895, 'with\\npart-of-speech': 20896, 'access\\nthe': 20897, '100th': 20898, 'conll2000\\n>>>': 20899, 'print(conll2000': 20900, \".chunked_sents('train\": 20901, \".txt')[99])\\n(s\\n\": 20902, '(pp': 20903, 'over/in)\\n': 20904, 'cup/nn)\\n': 20905, 'of/in)\\n': 20906, 'coffee/nn)\\n': 20907, ',/,\\n': 20908, './nnp': 20909, 'stone/nnp)\\n': 20910, '(vp': 20911, 'told/vbd)\\n': 20912, 'his/prp$': 20913, 'story/nn)\\n': 20914, '.)\\n\\n\\n\\nas': 20915, 'types:\\nnp': 20916, 'seen;': 20917, 'as\\nhas': 20918, 'delivered;': 20919, 'the\\nchunk_types': 20920, \"chunk_types=['np'])[99])\\n(s\\n\": 20921, 'over/in\\n': 20922, 'told/vbd\\n': 20923, '.)\\n\\n\\n\\n\\n\\n3': 20924, '.2\\xa0\\xa0\\xa0simple': 20925, 'baselines\\nnow': 20926, 'baseline': 20927, 'parser\\ncp': 20928, '.regexpparser()\\n>>>': 20929, 'conll2000': 20930, \".chunked_sents('test\": 20931, \"chunk_types=['np'])\\n>>>\": 20932, '.evaluate(test_sents))\\nchunkparse': 20933, 'score:\\n': 20934, 'accuracy:': 20935, '43': 20936, '.4%\\n': 20937, 'precision:': 20938, '.0%\\n': 20939, 'recall:': 20940, 'f-measure:': 20941, '.0%\\n\\n\\n\\nthe': 20942, 'our\\ntagger': 20943, 'precision,': 20944, 'f-measure\\nare': 20945, 'that\\nlooks': 20946, 'tags\\n(e': 20947, 'cd,': 20948, 'dt,': 20949, 'jj)': 20950, 'rnp:': 20951, '{<[cdjnp]': 20952, '.*>+}\\n>>>': 20953, '.7%\\n': 20954, '70': 20955, '.6%\\n': 20956, '67': 20957, '.8%\\n': 20958, '69': 20959, '.2%\\n\\n\\n\\nas': 20960, 'decent': 20961, 'adopting': 20962, 'we\\nuse': 20963, 'b)\\nthat': 20964, 'given\\neach': 20965, 'unigramchunker': 20966, 'class,': 20967, 'which\\nuses': 20968, 'chunkparseri\\ninterface,': 20969, 'constructor\\n': 20970, 'new\\nunigramchunker;': 20971, '\\nwhich': 20972, 'unigramchunker(nltk': 20973, '.chunkparseri):\\n': 20974, 'train_sents):': 20975, 'train_data': 20976, '[[(t,c)': 20977, 'w,t,c': 20978, '.tree2conlltags(sent)]\\n': 20979, 'train_sents]\\n': 20980, '.tagger': 20981, '.unigramtagger(train_data)': 20982, 'parse(self,': 20983, 'sentence):': 20984, 'pos_tags': 20985, '[pos': 20986, '(word,pos)': 20987, 'sentence]\\n': 20988, 'tagged_pos_tags': 20989, '.tag(pos_tags)\\n': 20990, 'chunktags': 20991, '[chunktag': 20992, '(pos,': 20993, 'chunktag)': 20994, 'tagged_pos_tags]\\n': 20995, 'conlltags': 20996, '[(word,': 20997, 'pos,': 20998, '((word,pos),chunktag)\\n': 20999, 'chunktags)]\\n': 21000, '.conlltags2tree(conlltags)\\n\\n\\nexample': 21001, '(code_unigram_chunker': 21002, 'tagger\\n\\nthe': 21003, 'constructor': 21004, 'of\\ntraining': 21005, 'it\\nfirst': 21006, 'the\\ntagger,': 21007, 'tree2conlltags': 21008, 'of\\nword,tag,chunk': 21009, 'triples': 21010, 'data\\nto': 21011, 'later\\nuse': 21012, 'sentence\\nas': 21013, 'from\\nthat': 21014, 'chunk\\ntags,': 21015, 'the\\nconstructor': 21016, 'uses\\nconlltags2tree': 21017, 'unigramchunker,': 21018, 'conll\\n2000': 21019, 'performance:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 21020, 'unigram_chunker': 21021, 'unigramchunker(train_sents)\\n>>>': 21022, 'print(unigram_chunker': 21023, '92': 21024, '.9%\\n': 21025, '.2%\\n\\n\\n\\nthis': 21026, 'achieving': 21027, 'f-measure\\nscore': 21028, '83%': 21029, 'learned,': 21030, 'its\\nunigram': 21031, 'that\\nappear': 21032, 'postags': 21033, 'sorted(set(pos': 21034, 'train_sents\\n': 21035, '.leaves()))\\n>>>': 21036, \".tag(postags))\\n[('#',\": 21037, \"'b-np'),\": 21038, \"('$',\": 21039, \"'o'),\": 21040, \"(')',\": 21041, \"'o'),\\n\": 21042, \"('cc',\": 21043, \"('cd',\": 21044, \"'i-np'),\\n\": 21045, \"('ex',\": 21046, \"('fw',\": 21047, \"'i-np'),\": 21048, \"('jj',\": 21049, \"('jjr',\": 21050, \"('jjs',\": 21051, \"('md',\": 21052, \"('nn',\": 21053, \"('nnp',\": 21054, \"('nnps',\": 21055, \"('nns',\": 21056, \"('pdt',\": 21057, \"('pos',\": 21058, \"('prp',\": 21059, \"('prp$',\": 21060, \"'b-np'),\\n\": 21061, \"('rb',\": 21062, \"('rbr',\": 21063, \"('rbs',\": 21064, \"('rp',\": 21065, \"('sym',\": 21066, \"('uh',\": 21067, \"('vb',\": 21068, \"('vbd',\": 21069, \"('vbg',\": 21070, \"('vbn',\": 21071, \"('vbp',\": 21072, \"('vbz',\": 21073, \"('wdt',\": 21074, \"('wp',\": 21075, \"('wp$',\": 21076, \"('wrb',\": 21077, \"'o')]\\n\\n\\n\\nit\": 21078, 'discovered': 21079, 'np\\nchunks,': 21080, '$,': 21081, 'markers': 21082, 'and\\npossessives': 21083, '(prp$': 21084, 'wp$)': 21085, 'beginnings': 21086, 'chunks,\\nwhile': 21087, '(nn,': 21088, 'nnp,': 21089, 'nnps,': 21090, 'nns)': 21091, 'occur\\ninside': 21092, 'bigram\\nchunker:': 21093, 'bigramchunker,': 21094, 'and\\nmodify': 21095, '.1\\nto': 21096, 'bigramtagger': 21097, 'chunker:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 21098, 'bigram_chunker': 21099, 'bigramchunker(train_sents)\\n>>>': 21100, 'print(bigram_chunker': 21101, '.3%\\n': 21102, '.5%\\n\\n\\n\\n\\n\\n3': 21103, '.3\\xa0\\xa0\\xa0training': 21104, 'classifier-based': 21105, 'chunkers\\nboth': 21106, 'chunkers\\ndecide': 21107, 'tags\\nare': 21108, 'insufficient': 21109, 'statements:\\n\\n': 21110, '.joey/nn': 21111, 'sold/vbd': 21112, 'farmer/nn': 21113, 'rice/nn': 21114, '.nick/nn': 21115, 'broke/vbd': 21116, 'my/dt': 21117, 'computer/nn': 21118, 'monitor/nn': 21119, '.\\n\\nthese': 21120, 'tags,\\nyet': 21121, 'sentence,\\nthe': 21122, 'farmer': 21123, 'rice': 21124, 'monitor,': 21125, 'make\\nuse': 21126, 'just\\ntheir': 21127, 'chunking\\nperformance': 21128, 'words\\nis': 21129, 'the\\nn-gram': 21130, 'this\\nclassifier-based': 21131, 'the\\nclassifier-based': 21132, 'we\\nused': 21133, 'first\\nclass': 21134, 'the\\nconsecutivepostagger': 21135, 'maxentclassifier': 21136, 'naivebayesclassifier': 21137, 'class\\n': 21138, 'wrapper': 21139, 'that\\nturns': 21140, 'sequences;': 21141, 'the\\nparse()': 21142, 'consecutivenpchunktagger(nltk': 21143, 'npchunk_features(untagged_sent,': 21144, 'history)': 21145, '.maxentclassifier': 21146, '.train(': 21147, \"algorithm='megam',\": 21148, 'trace=0)\\n\\n': 21149, 'npchunk_features(sentence,': 21150, 'history)\\n\\nclass': 21151, 'consecutivenpchunker(nltk': 21152, '.chunkparseri):': 21153, '[[((w,t),c)': 21154, '(w,t,c)': 21155, 'consecutivenpchunktagger(tagged_sents)\\n\\n': 21156, '.tag(sentence)\\n': 21157, '[(w,t,c)': 21158, '((w,t),c)': 21159, 'tagged_sents]\\n': 21160, '(code_classifier_chunker': 21161, 'classifier\\n\\n\\nthe': 21162, 'by\\ndefining': 21163, 'the\\npart-of-speech': 21164, 'our\\nclassifier-based': 21165, 'history):\\n': 21166, 'sentence[i]\\n': 21167, '{pos:': 21168, 'pos}\\n>>>': 21169, 'consecutivenpchunker(train_sents)\\n>>>': 21170, 'print(chunker': 21171, '.2%\\n\\n\\n\\nwe': 21172, 'this\\nfeature': 21173, 'adjacent\\ntags,': 21174, 'bigram\\nchunker': 21175, 'prevword,': 21176, 'prevpos': 21177, '<start>,': 21178, 'prevpos:': 21179, 'prevpos}\\n>>>': 21180, '.2%\\n': 21181, '.5%\\n\\n\\n\\nnext,': 21182, 'we\\nhypothesized': 21183, 'find\\nthat': 21184, 'indeed': 21185, \"chunker's\": 21186, 'performance,\\nby': 21187, '10%\\nreduction': 21188, 'rate)': 21189, '.5%\\n': 21190, '.7%\\n\\n\\n\\nfinally,': 21191, 'of\\nadditional': 21192, 'lookahead': 21193, ',\\npaired': 21194, 'tags-since-dt,': 21195, 'been\\nencountered': 21196, 'beginning\\nof': 21197, 'len(sentence)-1:\\n': 21198, 'nextword,': 21199, 'nextpos': 21200, '<end>,': 21201, '<end>\\n': 21202, 'sentence[i+1]\\n': 21203, 'pos,\\n': 21204, 'word,\\n': 21205, 'prevpos,\\n': 21206, 'nextpos:': 21207, 'nextpos,': 21208, 'prevpos+pos:': 21209, '%s+%s': 21210, '(prevpos,': 21211, 'pos),': 21212, 'pos+nextpos:': 21213, 'nextpos),\\n': 21214, 'tags-since-dt:': 21215, 'tags_since_dt(sentence,': 21216, 'i)}': 21217, '\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 21218, 'sentence[:i]:\\n': 21219, \"'dt':\\n\": 21220, '.add(pos)\\n': 21221, \"'+'\": 21222, '.join(sorted(tags))\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 21223, '96': 21224, '88': 21225, '91': 21226, '.8%\\n\\n\\n\\n\\nnote\\nyour': 21227, 'function\\nnpchunk_features,': 21228, '.\\n\\n\\n\\n\\n4\\xa0\\xa0\\xa0recursion': 21229, 'structure\\n\\n4': 21230, '.1\\xa0\\xa0\\xa0building': 21231, 'cascaded': 21232, 'chunkers\\nso': 21233, 'consist\\nof': 21234, 'as\\nnp': 21235, 'of\\narbitrary': 21236, 'multi-stage': 21237, 'grammar\\ncontaining': 21238, 'has\\npatterns': 21239, 'and\\nsentences': 21240, 'four-stage': 21241, 'create\\nstructures': 21242, '.\\n\\n\\n\\n\\n\\n\\xa0\\n\\ngrammar': 21243, '{<dt|jj|nn': 21244, 'nn\\n': 21245, 'pp:': 21246, '{<in><np>}': 21247, 'prepositions': 21248, 'np\\n': 21249, 'vp:': 21250, '{<vb': 21251, '.*><np|pp|clause>+$}': 21252, 'arguments\\n': 21253, 'clause:': 21254, '{<np><vp>}': 21255, 'vp\\n': 21256, '\\ncp': 21257, '[(mary,': 21258, '(saw,': 21259, 'nn),\\n': 21260, '(sit,': 21261, 'vb),': 21262, '(on,': 21263, '(mat,': 21264, '.parse(sentence))\\n(s\\n': 21265, 'mary/nn)\\n': 21266, 'saw/vbd\\n': 21267, '(clause\\n': 21268, 'cat/nn)\\n': 21269, 'sit/vb': 21270, 'mat/nn)))))\\n\\n\\nexample': 21271, '(code_cascaded_chunker': 21272, 'pp,': 21273, 's\\n\\nunfortunately': 21274, 'misses': 21275, 'headed': 21276, 'has\\nother': 21277, 'this\\nchunker': 21278, 'to\\nidentify': 21279, '[(john,': 21280, '(thinks,': 21281, 'vbz),': 21282, '(mary,': 21283, 'vb),\\n': 21284, 'john/nnp)\\n': 21285, 'thinks/vbz\\n': 21286, 'saw/vbd': 21287, '[_saw-vbd]\\n': 21288, 'mat/nn)))))\\n\\n\\n\\nthe': 21289, 'its\\npatterns:': 21290, 'run:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 21291, '.regexpparser(grammar,': 21292, 'loop=2)\\n>>>': 21293, '(vp\\n': 21294, 'mat/nn)))))))\\n\\n\\n\\n\\nnote\\nthis': 21295, 'cascading': 21296, 'deep': 21297, 'however,\\ncreating': 21298, 'cascade': 21299, 'comes\\na': 21300, '.\\nalso,': 21301, 'depth\\n(no': 21302, 'cascade),': 21303, '.2\\xa0\\xa0\\xa0trees\\na': 21304, 'connected': 21305, 'reachable\\nby': 21306, 'distinguished': 21307, 'standardly': 21308, 'upside-down):\\n\\n': 21309, '(4)\\nwe': 21310, \"'family'\": 21311, 'metaphor': 21312, 'talk': 21313, 'tree:': 21314, 'the\\nparent': 21315, 'vp;': 21316, 'conversely': 21317, 'child\\nof': 21318, 'also,': 21319, 'both\\nchildren': 21320, 'siblings': 21321, 'specifying\\ntrees:\\n\\n\\n\\n\\n\\xa0\\n\\n(s\\n': 21322, 'alice)\\n': 21323, '(v': 21324, 'chased)\\n': 21325, '(np\\n': 21326, '(det': 21327, 'the)\\n': 21328, '(n': 21329, 'rabbit))))\\n\\n\\n\\nalthough': 21330, 'encode\\nany': 21331, 'homogeneous': 21332, 'hierarchical': 21333, 'discourse': 21334, 'structure)': 21335, 'children:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 21336, 'tree1': 21337, \".tree('np',\": 21338, \"['alice'])\\n>>>\": 21339, 'print(tree1)\\n(np': 21340, 'alice)\\n>>>': 21341, 'tree2': 21342, \"'rabbit'])\\n>>>\": 21343, 'print(tree2)\\n(np': 21344, 'rabbit)\\n\\n\\n\\nwe': 21345, 'tree3': 21346, \".tree('vp',\": 21347, \"['chased',\": 21348, 'tree2])\\n>>>': 21349, 'tree4': 21350, \".tree('s',\": 21351, '[tree1,': 21352, 'tree3])\\n>>>': 21353, 'print(tree4)\\n(s': 21354, 'alice)': 21355, 'chased': 21356, 'rabbit)))\\n\\n\\n\\nhere': 21357, 'objects:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 21358, 'print(tree4[1])\\n(vp': 21359, 'rabbit))\\n>>>': 21360, 'tree4[1]': 21361, \".label()\\n'vp'\\n>>>\": 21362, \".leaves()\\n['alice',\": 21363, \"'chased',\": 21364, \"'rabbit']\\n>>>\": 21365, \"tree4[1][1][1]\\n'rabbit'\\n\\n\\n\\nthe\": 21366, 'representation\\nof': 21367, 'zoom': 21368, 'out,\\nto': 21369, 'subtrees,': 21370, 'graphical\\nrepresentation': 21371, 'postscript': 21372, 'inclusion': 21373, 'document)': 21374, '\\n\\n\\n\\n\\n\\n\\n4': 21375, '.3\\xa0\\xa0\\xa0tree': 21376, 'traversal\\nit': 21377, 'traverse': 21378, 'traverse(t):\\n': 21379, 'try:\\n': 21380, '.label()\\n': 21381, 'attributeerror:\\n': 21382, 'print(t,': 21383, '.node': 21384, 'defined\\n': 21385, \"print('(',\": 21386, '.label(),': 21387, 't:\\n': 21388, 'traverse(child)\\n': 21389, \"print(')',\": 21390, ')\\n\\n': 21391, \".tree('(s\": 21392, \"rabbit)))')\\n\": 21393, 'traverse(t)\\n': 21394, 'rabbit': 21395, ')\\n\\n\\nexample': 21396, '(code_traverse': 21397, 'tree\\n\\n\\nnote\\nwe': 21398, 'duck': 21399, 't\\nis': 21400, 'defined)': 21401, '.\\n\\n\\n\\n\\n5\\xa0\\xa0\\xa0named': 21402, 'recognition\\nat': 21403, 'entities\\n(nes)': 21404, 'that\\nrefer': 21405, 'individuals,': 21406, 'organizations,': 21407, 'persons,\\ndates,': 21408, 'used\\ntypes': 21409, 'self-explanatory,': 21410, 'facility:\\nhuman-made': 21411, 'artifacts': 21412, 'domains': 21413, 'civil\\nengineering;': 21414, 'gpe:': 21415, 'geo-political': 21416, 'city,': 21417, 'state/province,': 21418, '.\\n\\n\\n\\n\\n\\n\\nne': 21419, 'type\\nexamples\\n\\n\\n\\norganization\\ngeorgia-pacific': 21420, 'who\\n\\nperson\\neddy': 21421, 'bonte,': 21422, 'obama\\n\\nlocation\\nmurray': 21423, 'river,': 21424, 'mount': 21425, 'everest\\n\\ndate\\njune,': 21426, '2008-06-29\\n\\ntime\\ntwo': 21427, '1:30': 21428, '.m': 21429, '.\\n\\nmoney\\n175': 21430, 'dollars,': 21431, 'gbp': 21432, '.40\\n\\npercent\\ntwenty': 21433, 'pct,': 21434, '.75': 21435, '%\\n\\nfacility\\nwashington': 21436, 'monument,': 21437, 'stonehenge\\n\\ngpe\\nsouth': 21438, 'east': 21439, 'asia,': 21440, 'midlothian\\n\\n\\ntable': 21441, 'entity\\n\\n\\nthe': 21442, '(ner)': 21443, 'all\\ntextual': 21444, 'into\\ntwo': 21445, 'sub-tasks:': 21446, 'ne,': 21447, 'its\\ntype': 21448, '.\\nwhile': 21449, 'prelude': 21450, 'identifying\\nrelations': 21451, '(qa),': 21452, 'the\\nprecision': 21453, 'recovering': 21454, 'pages,': 21455, 'but\\njust': 21456, 'most\\nqa': 21457, 'information\\nretrieval,': 21458, 'isolate': 21459, 'minimal': 21460, 'the\\ndocument': 21461, 'was\\nthe': 21462, 'us?,': 21463, 'was\\nretrieved': 21464, 'passage:\\n\\n': 21465, '(5)the': 21466, 'washington': 21467, 'monument': 21468, 'prominent': 21469, 'in\\nwashington,': 21470, \"city's\": 21471, 'attractions': 21472, 'was\\nbuilt': 21473, 'honor': 21474, 'george': 21475, 'washington,': 21476, 'to\\nindependence': 21477, 'became': 21478, '.\\nanalysis': 21479, 'be\\nof': 21480, 'x\\nis': 21481, 'type\\nperson': 21482, 'the\\npassage': 21483, 'washington,\\nnamed': 21484, 'them\\nhas': 21485, 'entities?': 21486, 'to\\nlook': 21487, 'gazetteer,\\nor': 21488, 'alexandria': 21489, 'gazetteer': 21490, 'the\\ngetty': 21491, 'this\\nblindly': 21492, 'runs': 21493, 'story:': 21494, 'every\\nword': 21495, 'error-prone;': 21496, 'but\\nthese': 21497, '.\\n\\nobserve': 21498, 'countries,\\nand': 21499, 'sanchez': 21500, 'dominican': 21501, 'republic\\nand': 21502, 'vietnam': 21503, 'gazetteer,': 21504, \"won't\\nbe\": 21505, 'organizations\\ncome': 21506, 'existence': 21507, 'day,': 21508, 'deal\\nwith': 21509, 'contemporary': 21510, 'entries,': 21511, 'many\\nnamed': 21512, 'thus\\nmay': 21513, 'date\\nand': 21514, 'respectively,': 21515, 'person;\\nconversely': 21516, 'dior': 21517, 'more\\nlikely': 21518, 'yankee': 21519, 'be\\nordinary': 21520, 'modifier': 21521, 'of\\ntype': 21522, 'infielders': 21523, 'posed': 21524, 'multi-word': 21525, 'like\\nstanford': 21526, 'university,': 21527, 'names\\nsuch': 21528, 'cecil': 21529, 'escondido': 21530, 'village': 21531, 'conference\\nservice': 21532, 'recognition,': 21533, 'multi-token\\nsequences': 21534, '.\\nnamed': 21535, 'well-suited': 21536, 'of\\nclassifier-based': 21537, 'sentence\\nusing': 21538, '(conll2002)': 21539, 'data:\\n\\neddy': 21540, 'b-per\\nbonte': 21541, 'i-per\\nis': 21542, 'o\\nwoordvoerder': 21543, 'o\\nvan': 21544, 'prep': 21545, 'o\\ndiezelfde': 21546, 'o\\nhogeschool': 21547, 'b-org\\n': 21548, 'punc': 21549, 'o\\n\\nin': 21550, 'representation,': 21551, 'its\\npart-of-speech': 21552, 'new\\nsentences;': 21553, '.conlltags2tree()': 21554, 'entities,\\naccessed': 21555, '.ne_chunk()': 21556, 'the\\nparameter': 21557, 'binary=true': 21558, 'just\\ntagged': 21559, 'ne;': 21560, 'gpe': 21561, '.tagged_sents()[22]\\n>>>': 21562, '.ne_chunk(sent,': 21563, 'binary=true))': 21564, 'the/dt\\n': 21565, '(ne': 21566, './nnp)\\n': 21567, 'is/vbz\\n': 21568, 'one/cd\\n': 21569, 'according/vbg\\n': 21570, 'to/to\\n': 21571, 'brooke/nnp': 21572, 'mossman/nnp)\\n': 21573, '.)\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 21574, '.ne_chunk(sent))': 21575, '(gpe': 21576, '(person': 21577, '.)\\n\\n\\n\\n\\n\\n\\n\\n6\\xa0\\xa0\\xa0relation': 21578, 'extraction\\n\\nonce': 21579, 'extract\\nthe': 21580, 'indicated': 21581, 'will\\ntypically': 21582, 'of\\nnamed': 21583, 'approaching': 21584, 'all\\ntriples': 21585, '(x,': 21586, 'α,': 21587, 'y),': 21588, 'entities\\nof': 21589, 'α': 21590, 'that\\nintervenes': 21591, 'to\\npull': 21592, 'relation\\nthat': 21593, 'strings\\nthat': 21594, 'expression\\n(?!\\\\b': 21595, '.+ing\\\\b)': 21596, 'to\\ndisregard': 21597, 'supervising': 21598, 'transition\\nof,': 21599, 'gerund': 21600, \".compile(r'\": 21601, '.*\\\\bin\\\\b(?!\\\\b': 21602, \".+ing)')\\n>>>\": 21603, '.ieer': 21604, \".parsed_docs('nyt_19980315'):\\n\": 21605, 'rel': 21606, '.sem': 21607, \".extract_rels('org',\": 21608, \"'loc',\": 21609, 'doc,\\n': 21610, \"corpus='ieer',\": 21611, 'in):\\n': 21612, '.rtuple(rel))\\n[org:': 21613, \"'whyy']\": 21614, \"'philadelphia']\\n[org:\": 21615, \"'mcglashan\": 21616, '&amp;': 21617, \"sarrail']\": 21618, \"'firm\": 21619, \"in'\": 21620, \"'san\": 21621, \"mateo']\\n[org:\": 21622, \"'freedom\": 21623, \"forum']\": 21624, \"'arlington']\\n[org:\": 21625, \"'brookings\": 21626, \"institution']\": 21627, \"'washington']\\n[org:\": 21628, \"'idealab']\": 21629, 'self-described': 21630, 'incubator': 21631, \"'los\": 21632, \"angeles']\\n[org:\": 21633, \"'open\": 21634, \"text']\": 21635, \"'waterloo']\\n[org:\": 21636, \"'wgbh']\": 21637, \"'boston']\\n[org:\": 21638, \"'bastille\": 21639, \"opera']\": 21640, \"'paris']\\n[org:\": 21641, \"'omnicom']\": 21642, \"york']\\n[org:\": 21643, \"'ddb\": 21644, \"needham']\": 21645, \"'kaplan\": 21646, \"group']\": 21647, \"'bbdo\": 21648, \"south']\": 21649, \"'atlanta']\\n[org:\": 21650, \"'atlanta']\\n\\n\\n\\nsearching\": 21651, 'well,\\nthough': 21652, '[org:': 21653, 'house\\ntransportation': 21654, 'committee]': 21655, 'secured': 21656, 'new\\nyork];': 21657, 'string-based': 21658, 'of\\nexcluding': 21659, 'filler': 21660, 'conll2002': 21661, 'entity\\nannotation': 21662, 'devise\\npatterns': 21663, 'next\\nexample': 21664, 'clause()': 21665, 'a\\nclausal': 21666, 'relsym': 21667, 'conll2002\\n>>>': 21668, 'vnv': 21669, '(\\n': 21670, 'is/v|': 21671, 'sing': 21672, 'was/v|': 21673, 'zijn': 21674, \"('be')\\n\": 21675, 'werd/v|': 21676, 'wordt/v': 21677, 'worden': 21678, \"('become)\\n\": 21679, 'anything\\n': 21680, 'van/prep': 21681, 'van': 21682, \"('of')\\n\": 21683, '.compile(vnv,': 21684, '.verbose)\\n>>>': 21685, \".chunked_sents('ned\": 21686, \".train'):\\n\": 21687, \".extract_rels('per',\": 21688, \"'org',\": 21689, \"corpus='conll2002',\": 21690, 'pattern=van):\\n': 21691, '.clause(rel,': 21692, 'relsym=van))': 21693, \"\\nvan(cornet_d'elzius,\": 21694, \"'buitenlandse_handel')\\nvan('johan_rottiers',\": 21695, \"'kardinaal_van_roey_instituut')\\nvan('annie_lennox',\": 21696, \"'eurythmics')\\n\\n\\n\\n\\nnote\\nyour\": 21697, 'by\\nprint(nltk': 21698, '.rtuple(rel,': 21699, 'lcon=true,': 21700, 'rcon=true))': 21701, 'you\\nthe': 21702, 'intervene': 21703, 'and\\nalso': 21704, '10-word\\nwindow': 21705, 'to\\nfigure': 21706, \"van('annie_lennox',\": 21707, \"'eurythmics')\": 21708, 'hit': 21709, '.\\n\\n\\n\\n\\n\\n7\\xa0\\xa0\\xa0summary\\n\\ninformation': 21710, 'unrestricted\\ntext': 21711, 'to\\npopulate': 21712, 'well-organized': 21713, 'segmenting,': 21714, 'tokenizing,': 21715, 'determine\\nwhether': 21716, '.\\nentity': 21717, 'chunkers,': 21718, 'which\\nsegment': 21719, 'appropriate\\nentity': 21720, 'person,\\nlocation,': 21721, 'money,': 21722, '(geo-political': 21723, '.\\nchunkers': 21724, 'rule-based': 21725, 'the\\nregexpparser': 21726, 'nltk;': 21727, 'consecutivenpchunker': 21728, 'this\\nchapter': 21729, 'very\\nimportant': 21730, 'flat\\ndata': 21731, 'allowed': 21732, 'overlap,\\nthey': 21733, '.\\nrelation': 21734, 'rule-based\\nsystems': 21735, 'intervening': 21736, 'using\\nmachine-learning': 21737, 'learn\\nsuch': 21738, '.\\n\\n\\n\\n8\\xa0\\xa0\\xa0further': 21739, 'the\\nchunking': 21740, 'due': 21741, 'pioneering': 21742, 'by\\nabney': 21743, 'cass': 21744, 'in\\nhttp://www': 21745, '.vinartus': 21746, '.net/spa/97a': 21747, '.pdf': 21748, 'stopwords,\\naccording': 21749, '1975': 21750, 'ross': 21751, 'tukey': 21752, 'bio': 21753, 'format)': 21754, 'for\\nnp': 21755, '(ramshaw': 21756, 'marcus,': 21757, 'np\\nbracketing': 21758, 'conference': 21759, 'learning\\n(conll)': 21760, 'was\\nadopted': 21761, 'annotating': 21762, '.\\nsection': 21763, 'mining': 21764, 'medicine,': 21765, 'see\\n(ananiadou': 21766, 'mcnaught,': 21767, '.\\n\\n\\n\\n\\n\\n\\n9\\xa0\\xa0\\xa0exercises\\n\\n☼': 21768, 'i,\\no': 21769, 'necessary?': 21770, 'what\\nproblem': 21771, 'tags\\nexclusively?\\n☼': 21772, 'head': 21773, 'nouns,\\ne': 21774, 'many/jj': 21775, 'researchers/nns,': 21776, 'two/cd': 21777, 'weeks/nns,': 21778, 'both/dt': 21779, 'positions/nns': 21780, 'generalizing': 21781, 'singular\\nnoun': 21782, '.\\n☼\\npick': 21783, '.\\ninspect': 21784, 'sequences\\nthat': 21785, '.regexpparser': 21786, '.\\ndiscuss': 21787, '.\\n☼\\nan': 21788, '.\\ndevelop': 21789, 'single\\nchunk,': 21790, 'solely': 21791, '.\\ndetermine': 21792, 'sequences)': 21793, 'chinks\\nwith': 21794, 'and\\nsimplicity': 21795, 'on\\nchunk': 21796, 'gerunds,\\ne': 21797, 'receiving/vbg': 21798, 'end/nn,': 21799, 'assistant/nn': 21800, 'managing/vbg': 21801, 'editor/nn': 21802, 'using\\nsome': 21803, 'devising': 21804, 'coordinated': 21805, 'phrases,\\ne': 21806, 'july/nnp': 21807, 'august/nnp,\\nall/dt': 21808, 'your/prp$': 21809, 'managers/nns': 21810, 'supervisors/nns,\\ncompany/nn': 21811, 'courts/nns': 21812, 'adjudicators/nns': 21813, 'internal\\ninconsistencies,': 21814, 'approach\\nwill': 21815, '.)\\nevaluate': 21816, 'corpus,\\nand': 21817, 'chunkscore': 21818, '.missed()': 21819, '.incorrect()\\nmethods': 21820, 'chunker\\ndiscussed': 21821, '.\\n\\n\\n◑\\ndevelop': 21822, 'a\\nregular-expression': 21823, 'regexpchunk': 21824, 'any\\ncombination': 21825, 'chinking,': 21826, 'merging': 21827, 'in\\n12/cd': 21828, 'or/cc': 21829, 'so/rb': 21830, 'cases/vbz': 21831, 'correction': 21832, 'of\\ntagger': 21833, 'erroneous\\noutput': 21834, 'chunked\\nnoun': 21835, '.\\n◑\\nthe': 21836, '.\\nstudy': 21837, 'more?\\n★\\napply': 21838, 'tags\\nto': 21839, '(determiner)': 21840, 'occurs\\nat': 21841, '.\\n★\\nwe': 21842, 'establish\\nan': 21843, 'n-grams,\\nn-grams': 21844, '.\\napply': 21845, '.\\n★\\npick': 21846, 'functions\\nto': 21847, 'type:\\nlist': 21848, '.\\ncount': 21849, 'in\\norder': 21850, 'frequency;': 21851, 'frequency)\\nand': 21852, 'for\\ndeveloping': 21853, '.\\n\\n\\n★\\nthe': 21854, 'the\\nphrase:\\n[every/dt': 21855, 'time/nn]': 21856, '[she/prp]': 21857, 'sees/vbz': 21858, '[a/dt': 21859, 'newspaper/nn]\\ncontains': 21860, 'will\\nincorrectly': 21861, 'two:': 21862, '[every/dt': 21863, 'time/nn': 21864, 'she/prp]': 21865, 'chunk-internal': 21866, 'tags\\ntypically': 21867, 'then\\ndevise': 21868, 'and\\nre-evaluate': 21869, 'of\\ntuples,': 21870, 'of\\nnoun': 21871, 'prepositions,\\ne': 21872, 'cat': 21873, 'mat': 21874, \"('sat',\": 21875, \"'np')\": 21876, '.\\n★\\nthe': 21877, 'text\\nthat': 21878, 'brackets,\\nand': 21879, 'using:\\nfor': 21880, '.treebank_chunk': 21881, '.chunked_sents(fileid)': 21882, 'trees,\\njust': 21883, '.chunked_sents()': 21884, '.tree': 21885, '.pprint()': 21886, '.tree2conllstr()\\ncan': 21887, 'chunk2brackets()': 21888, 'chunk2iob()': 21889, 'single\\nchunk': 21890, 'command-line': 21891, 'utilities': 21892, 'bracket2iob': 21893, 'iob2bracket': 21894, '.py\\nthat': 21895, '(resp)': 21896, 'other\\nformat': 21897, '(obtain': 21898, '.)\\n\\n\\n★\\nan': 21899, 'current\\npart-of-speech': 21900, 'of\\nprevious': 21901, '.\\n★\\nconsider': 21902, 'example,\\nboth': 21903, 'adjectives\\n(in': 21904, 'english)': 21905, 'in\\ntwo': 21906, 'grows?\\nif': 21907, 'speculate': 21908, 'acst8': 21909, 'structure\\n\\n\\n\\n\\n\\n8': 21910, 'structure\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nearlier': 21911, 'them,\\nanalyze': 21912, 'categories,\\nand': 21913, 'scratch': 21914, 'surface': 21915, 'constraints\\nthat': 21916, 'govern': 21917, 'famous': 21918, 'cope': 21919, 'finite': 21920, 'their\\nstructures': 21921, 'sentences?\\nhow': 21922, 'trees?\\nhow': 21923, 'parsers': 21924, 'tree?\\n\\nalong': 21925, 'fundamentals': 21926, 'and\\nsee': 21927, 'easier\\nto': 21928, '.\\n\\n1\\xa0\\xa0\\xa0some': 21929, 'dilemmas\\n\\n1': 21930, '.1\\xa0\\xa0\\xa0linguistic': 21931, 'possibilities\\nprevious': 21932, 'analyse': 21933, 'text\\ncorpora,': 21934, 'stressed': 21935, 'growing\\ndaily': 21936, 'closely,': 21937, 'thought\\nexperiment': 21938, 'gigantic': 21939, 'everything\\nthat': 21940, 'uttered': 21941, 'last\\n50': 21942, 'language\\nof': 21943, 'english?': 21944, 'answer\\nno': 21945, 'search\\nthe': 21946, 'is\\neasy': 21947, 'as\\nnew': 21948, 'img\\n(http://www': 21949, '.telegraph': 21950, '.uk/sport/2387900/new-man-at-the-of-img': 21951, '.html),\\nspeakers': 21952, 'and\\ntherefore': 21953, '.\\naccordingly,': 21954, 'argue\\nthat': 21955, 'big\\nset': 21956, 'imaginary': 21957, 'speakers\\nof': 21958, 'judgements': 21959, 'reject\\nsome': 21960, '.\\nequally,': 21961, 'agree': 21962, 'perfectly\\ngood': 21963, 'property\\nthat': 21964, '.usain': 21965, 'bolt': 21966, '100m': 21967, 'record\\n\\n': 21968, 'jamaica': 21969, 'observer': 21970, 'reported': 21971, 'usain': 21972, '.andre': 21973, '.i': 21974, 'andre': 21975, 'record\\n\\nif': 21976, 'like\\nandre': 21977, 'sentence\\nand': 21978, 'like\\ns': 21979, 'ingenuity': 21980, 'can\\nconstruct': 21981, 'impressive': 21982, 'winnie': 21983, 'pooh': 21984, 'milne,\\nin': 21985, 'water:\\n\\n[you': 21986, \"piglet's\": 21987, 'joy': 21988, 'ship': 21989, 'of\\nhim': 21990, 'after-years': 21991, 'liked': 21992, 'very\\ngreat': 21993, 'terrible': 21994, 'flood,': 21995, 'had\\nreally': 21996, 'half-hour': 21997, 'imprisonment,': 21998, 'when\\nowl,': 21999, 'flown': 22000, 'comfort\\nhim,': 22001, 'aunt': 22002, 'laid\\na': 22003, \"seagull's\": 22004, 'egg': 22005, 'mistake,': 22006, 'rather\\nlike': 22007, 'listening': 22008, 'his\\nwindow': 22009, 'hope,': 22010, 'quietly': 22011, 'naturally,\\nslipping': 22012, 'was\\nonly': 22013, 'hanging': 22014, 'toes,': 22015, 'moment,': 22016, 'luckily,': 22017, 'sudden\\nloud': 22018, 'squawk': 22019, 'owl,': 22020, 'story,': 22021, 'being\\nwhat': 22022, 'said,': 22023, 'woke': 22024, 'to\\njerk': 22025, 'himself': 22026, 'interesting,': 22027, 'did\\nshe?': 22028, 'saw\\nthe': 22029, 'ship,': 22030, 'brain': 22031, '(captain,': 22032, 'robin;': 22033, '1st': 22034, 'mate,': 22035, 'bear)\\ncoming': 22036, 'rescue': 22037, 'begins\\ns': 22038, 'language\\nprovides': 22039, 'constructions': 22040, 'extend\\nsentences': 22041, 'indefinitely': 22042, 'length\\nthat': 22043, 'before:': 22044, 'concoct': 22045, 'an\\nentirely': 22046, 'before\\nin': 22047, 'language\\nwill': 22048, 'a\\nlanguage': 22049, 'closely\\nintertwined': 22050, 'observed': 22051, 'texts?': 22052, 'it\\nsomething': 22053, 'implicit': 22054, 'competent\\nspeakers': 22055, 'sentences?': 22056, 'combination\\nof': 22057, 'two?': 22058, 'stand': 22059, 'issue,': 22060, 'will\\nintroduce': 22061, 'framework\\nof': 22062, 'which\\na': 22063, 'an\\nenormous': 22064, 'a\\ngrammar': 22065, 'the\\nmembers': 22066, 'grammars': 22067, 'productions\\nof': 22068, 'this,\\nto': 22069, 'meanings\\nof': 22070, '.2\\xa0\\xa0\\xa0ubiquitous': 22071, 'ambiguity\\na': 22072, 'well-known': 22073, '(2),\\nfrom': 22074, 'groucho': 22075, 'marx': 22076, 'movie,': 22077, 'animal': 22078, 'crackers': 22079, '(1930):\\n\\n\\n': 22080, '(2)while': 22081, 'hunting': 22082, 'africa,': 22083, 'shot': 22084, 'elephant': 22085, 'pajamas': 22086, 'pajamas,': 22087, 'phrase:\\ni': 22088, 'we\\nneed': 22089, 'grammar:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 22090, 'groucho_grammar': 22091, '.cfg': 22092, '.fromstring(\\n': 22093, \"'i'\\n\": 22094, 'pp\\n': 22095, \"'an'\": 22096, \"'my'\\n\": 22097, \"'elephant'\": 22098, \"'pajamas'\\n\": 22099, \"'shot'\\n\": 22100, \"'in'\\n\": 22101, ')\\n\\n\\n\\nthis': 22102, 'ways,\\ndepending': 22103, 'pajamas\\ndescribes': 22104, \"'shot',\": 22105, \"'elephant',\": 22106, \"'pajamas']\\n>>>\": 22107, '.chartparser(groucho_grammar)\\n>>>': 22108, '.parse(sent):\\n': 22109, 'print(tree)\\n': 22110, '.\\n(s\\n': 22111, 'shot)': 22112, 'an)': 22113, 'elephant)))\\n': 22114, '(p': 22115, 'in)': 22116, 'my)': 22117, 'pajamas)))))\\n(s\\n': 22118, 'shot)\\n': 22119, 'elephant)': 22120, 'pajamas))))))\\n\\n\\n\\nthe': 22121, 'depict': 22122, 'as\\ntrees,': 22123, '(3b):\\n\\n': 22124, '.\\n\\nnotice': 22125, 'words;\\ne': 22126, 'gun': 22127, 'sentence,\\nand': 22128, 'camera': 22129, 'turn:\\nconsider': 22130, 'different\\ninterpretations:': 22131, 'fighting': 22132, '.\\nvisiting': 22133, 'relatives': 22134, 'tiresome': 22135, 'individual\\nwords': 22136, 'blame?': 22137, 'ambiguity?\\n\\nthis': 22138, 'and\\ncomputational': 22139, 'investigating': 22140, 'linguistic\\nphenomena': 22141, 'well-formedness': 22142, 'ill-formedness': 22143, 'the\\nphrase': 22144, 'formal\\nmodels': 22145, 'reliably\\nrecognize': 22146, 'contains?': 22147, 'simple\\nquestions': 22148, 'whom?': 22149, \".\\n\\n\\n\\n2\\xa0\\xa0\\xa0what's\": 22150, 'syntax?\\n\\n2': 22151, '.1\\xa0\\xa0\\xa0beyond': 22152, 'n-grams\\nwe': 22153, 'seems\\nperfectly': 22154, 'rapidly\\ndegenerates': 22155, 'by\\ncomputing': 22156, \"childrens'\": 22157, 'the\\nadventures': 22158, 'buster': 22159, '.org/files/22816/22816': 22160, '.txt):\\n\\n': 22161, 'roared': 22162, 'slip': 22163, 'back\\n\\n': 22164, 'worst': 22165, 'clumsy': 22166, 'whoever': 22167, 'light\\n\\nyou': 22168, 'intuitively': 22169, 'word-salad,': 22170, 'you\\nprobably': 22171, 'one\\nbenefit': 22172, 'framework\\nand': 22173, 'look\\nat': 22174, 'coordinate\\nstructure,': 22175, 'coordinating\\nconjunction': 22176, 'an\\ninformal': 22177, 'simplified)': 22178, 'coordination': 22179, 'works\\nsyntactically:\\ncoordinate': 22180, 'structure:\\n\\nif': 22181, 'grammatical\\ncategory': 22182, 'a\\nphrase': 22183, '(noun\\nphrases)': 22184, 'conjoined': 22185, 'second,\\ntwo': 22186, 'aps': 22187, '(adjective': 22188, 'phrases)': 22189, 'an\\nap': 22190, 'part)': 22191, '.on': 22192, '(ap': 22193, 'looking)': 22194, 'conjoin': 22195, 'ap,': 22196, 'is\\nwhy': 22197, 'ideas,': 22198, '.\\nconstituent': 22199, 'combine\\nwith': 22200, 'words\\nforms': 22201, 'substitutability': 22202, 'well-formed': 22203, 'a\\nshorter': 22204, 'rendering': 22205, 'ill-formed': 22206, 'clarify\\nthis': 22207, 'idea,': 22208, '(6)the': 22209, 'fine': 22210, 'fat': 22211, 'brook': 22212, 'substitute': 22213, 'bear\\nindicates': 22214, 'cannot\\nreplace': 22215, '.\\n\\n\\n': 22216, '.*the': 22217, 'sequences\\nby': 22218, 'preserves': 22219, 'grammaticality': 22220, 'sequence\\nthat': 22221, 'end\\nup': 22222, 'sequences:': 22223, 'replace\\nparticular': 22224, 'brook)': 22225, 'it);': 22226, 'grammatical\\ntwo-word': 22227, 'added\\ngrammatical': 22228, 'vp,': 22229, 'phrase,\\nverb': 22230, 'categories:\\nthis': 22231, 'reproduces': 22232, 'grammatical\\ncategories': 22233, '(np),': 22234, '(vp),\\nprepositional': 22235, '(pp),': 22236, 'nominals': 22237, '(nom)': 22238, 'topmost': 22239, 'an\\ns': 22240, 'flip': 22241, 'standard\\nphrase': 22242, '(8)': 22243, 'called\\na': 22244, 'constituents': 22245, 'of\\ns': 22246, '(8)\\nas': 22247, 'sentence\\ncan': 22248, 'constituents,': 22249, 'further\\nsubdivided': 22250, '.\\n\\nnote\\nas': 22251, '4\\ncan': 22252, 'bounded': 22253, \"methods\\naren't\": 22254, 'applicable': 22255, '.\\n\\n\\n\\n\\n3\\xa0\\xa0\\xa0context': 22256, 'grammar\\n\\n3': 22257, '.1\\xa0\\xa0\\xa0a': 22258, \"grammar\\n\\nlet's\": 22259, 'context-free': 22260, 'by\\nconvention,': 22261, 'left-hand-side': 22262, 'the\\nstart-symbol': 22263, 'all\\nwell-formed': 22264, 'in\\nnltk,': 22265, '.grammar\\nmodule': 22266, 'admitted': 22267, '.\\n\\n\\n\\n\\n\\xa0\\n\\ngrammar1': 22268, 'walked\\n': 22269, 'mary': 22270, 'bob': 22271, 'my\\n': 22272, 'telescope': 22273, 'park\\n': 22274, 'with\\n': 22275, ')\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 22276, 'rd_parser': 22277, '.recursivedescentparser(grammar1)\\n>>>': 22278, 'print(tree)\\n(s': 22279, 'mary)': 22280, 'saw)': 22281, 'bob)))\\n\\n\\nexample': 22282, '(code_cfg1': 22283, 'grammar\\n\\nthe': 22284, 'productions': 22285, 'categories,\\nas': 22286, '.\\n\\n\\n\\n\\n\\n\\n\\nsymbol\\nmeaning\\nexample\\n\\n\\n\\ns\\nsentence\\nthe': 22287, 'walked\\n\\nnp\\nnoun': 22288, 'phrase\\na': 22289, 'dog\\n\\nvp\\nverb': 22290, 'phrase\\nsaw': 22291, 'park\\n\\npp\\nprepositional': 22292, 'phrase\\nwith': 22293, 'telescope\\n\\ndet\\ndeterminer\\nthe\\n\\nn\\nnoun\\ndog\\n\\nv\\nverb\\nwalked\\n\\np\\npreposition\\nin\\n\\n\\ntable': 22294, 'categories\\n\\n\\na': 22295, 'the\\nrighthand': 22296, 'productions\\nvp': 22297, 'descent': 22298, 'demo:': 22299, 'against\\nthe': 22300, 'the\\nrecursive': 22301, '.rdparser(),\\nshown': 22302, 'can\\nedit': 22303, 'menu)': 22304, '.\\nchange': 22305, 'parsed,': 22306, 'and\\nrun': 22307, 'autostep': 22308, 'button': 22309, 'park': 22310, 'to\\nthose': 22311, '(9)\\n': 22312, 'is\\nsaid': 22313, 'structurally': 22314, 'the\\npp': 22315, 'attached': 22316, 'places\\nin': 22317, 'interpretation\\nis': 22318, 'happened\\nin': 22319, 'np,\\nthen': 22320, 'park,': 22321, 'agent': 22322, 'dog)\\nmight': 22323, 'sitting': 22324, 'balcony': 22325, 'apartment': 22326, 'overlooking': 22327, 'the\\npark': 22328, '.2\\xa0\\xa0\\xa0writing': 22329, 'grammars\\nif': 22330, 'cfgs,': 22331, 'will\\nfind': 22332, 'text\\nfile,': 22333, 'mygrammar': 22334, 'and\\nparse': 22335, 'grammar1': 22336, \".load('file:mygrammar\": 22337, \".cfg')\\n>>>\": 22338, 'print(tree)\\n\\n\\n\\nmake': 22339, \"'file:mygrammar\": 22340, \".cfg'\": 22341, 'print(tree)': 22342, 'because\\nyour': 22343, 'case,\\ncall': 22344, 'on:': 22345, '=\\nnltk': 22346, '.recursivedescentparser(grammar1,': 22347, 'trace=2)': 22348, 'check\\nwhat': 22349, 'currently': 22350, 'p\\nin': 22351, '.productions():': 22352, 'print(p)': 22353, 'cfgs': 22354, 'combine\\ngrammatical': 22355, 'righthand': 22356, \"'of'\": 22357, 'disallowed': 22358, 'in\\naddition,': 22359, \"'new\\nyork',\": 22360, \"'new_york'\\ninstead\": 22361, '.3\\xa0\\xa0\\xa0recursion': 22362, 'structure\\na': 22363, 'hand\\nside': 22364, 'production,': 22365, 'nom': 22366, 'nominals)': 22367, 'category\\nnom,': 22368, 'indirect': 22369, 'the\\ncombination': 22370, 'productions,': 22371, '.\\n\\n\\n\\n\\n\\xa0\\n\\ngrammar2': 22372, 'propn\\n': 22373, 'n\\n': 22374, 'propn': 22375, \"'buster'\": 22376, \"'chatterer'\": 22377, \"'joe'\\n\": 22378, \"'a'\\n\": 22379, \"'bear'\": 22380, \"'squirrel'\": 22381, \"'tree'\": 22382, \"'fish'\": 22383, \"'log'\\n\": 22384, \"'angry'\": 22385, \"'frightened'\": 22386, \"'little'\": 22387, \"'tall'\\n\": 22388, \"'chased'\": 22389, \"'saw'\": 22390, \"'said'\": 22391, \"'thought'\": 22392, \"'was'\": 22393, \"'put'\\n\": 22394, \"'on'\\n\": 22395, '(code_cfg2': 22396, 'grammar\\n\\nto': 22397, 'following\\ntrees': 22398, '(10a)': 22399, 'nominal': 22400, 'phrases,\\nwhile': 22401, '(10b)': 22402, '(10)\\n': 22403, \".\\n\\nwe've\": 22404, \"there's\\nno\": 22405, 'parsing\\nsentences': 22406, 'deeply': 22407, '.\\nbeware': 22408, 'recursivedescentparser': 22409, 'handle\\nleft-recursive': 22410, 'y;': 22411, 'will\\nreturn': 22412, '.\\n\\n\\n\\n4\\xa0\\xa0\\xa0parsing': 22413, 'grammar\\n\\na': 22414, 'the\\nproductions': 22415, 'more\\nconstituent': 22416, 'conform': 22417, 'specification': 22418, '—\\nit': 22419, 'licensed': 22420, 'grammar\\nto': 22421, 'fringe': 22422, 'against\\na': 22423, 'linguists\\nto': 22424, 'psycholinguistic': 22425, 'processing,\\nhelping': 22426, 'processing\\ncertain': 22427, 'point;\\nfor': 22428, 'questions\\nsubmitted': 22429, 'question-answering': 22430, 'undergo': 22431, 'algorithms,\\na': 22432, 'parsing,\\nand': 22433, 'shift-reduce': 22434, 'called\\nleft-corner': 22435, 'programming\\ntechnique': 22436, '.1\\xa0\\xa0\\xa0recursive': 22437, 'parsing\\nthe': 22438, 'specification\\nof': 22439, 'subgoals': 22440, 'top-level': 22441, 'vp\\nproduction': 22442, 'subgoals:\\nfind': 22443, 'sub-sub-goals,': 22444, 'np\\nand': 22445, 'left-hand': 22446, 'eventually,': 22447, 'expansion\\nprocess': 22448, 'such\\nsubgoals': 22449, 'and\\nsucceed': 22450, 'parser\\nmust': 22451, 'above\\nprocess': 22452, 's),': 22453, 'node\\nis': 22454, 'expands': 22455, 'downwards\\n(hence': 22456, 'descent)': 22457, '.rdparser()': 22458, '.\\nsix': 22459, 'parser:': 22460, 'a\\ntree': 22461, 's;': 22462, 'consults': 22463, 'enlarge': 22464, 'tree;': 22465, 'when\\na': 22466, 'encountered,': 22467, 'input;\\nafter': 22468, 'backtracks': 22469, 'parses': 22470, '.\\n\\nduring': 22471, 'several\\npossible': 22472, 'it\\ntries': 22473, 'work\\nit': 22474, 'backtracks,': 22475, 'it\\ngets': 22476, 'dog,': 22477, 'complete\\nparse': 22478, 'any\\ndangling': 22479, 'backtrack': 22480, 'other\\nchoices': 22481, 'parser:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 22482, \"'mary\": 22483, \"dog'\": 22484, 'dog))))\\n\\n\\n\\n\\nnote\\nrecursivedescentparser()': 22485, 'trace': 22486, 'steps\\nthat': 22487, '.\\n\\nrecursive': 22488, 'first,\\nleft-recursive': 22489, 'send': 22490, 'it\\ninto': 22491, 'infinite': 22492, 'wastes': 22493, 'time\\nconsidering': 22494, 'input\\nsentence': 22495, 'backtracking': 22496, 'parsed\\nconstituents': 22497, 'rebuilt': 22498, 'example,\\nbacktracking': 22499, 'subtree\\ncreated': 22500, 'proceeds': 22501, 'with\\nvp': 22502, 'all\\nover': 22503, '.\\nrecursive': 22504, '.\\ntop-down': 22505, 'be,\\nbefore': 22506, 'input!': 22507, 'called\\nbottom-up': 22508, '.2\\xa0\\xa0\\xa0shift-reduce': 22509, 'parsing\\na': 22510, 'shift-reduce\\nparser': 22511, 'them\\nwith': 22512, 'to\\nan': 22513, 'pushes': 22514, 'a\\nstack': 22515, '.1);': 22516, 'shift': 22517, 'match\\nthe': 22518, 'production,\\nthen': 22519, 'popped': 22520, 'left-hand\\nside': 22521, 'stack;\\nreducing': 22522, 'are\\npushed': 22523, 'is\\nconsumed': 22524, 'parse\\ntree': 22525, 'pops': 22526, 'partial': 22527, 'the\\ngraphical': 22528, '.srparser()': 22529, 'shifting': 22530, 'stack;': 22531, 'production;': 22532, 'succeeds': 22533, 'input\\nis': 22534, '.\\n\\nnltk': 22535, 'shiftreduceparser(),': 22536, 'simple\\nimplementation': 22537, 'not\\nimplement': 22538, 'backtracking,': 22539, 'guaranteed': 22540, 'parse\\nfor': 22541, 'exists': 22542, 'at\\nmost': 22543, 'parse,': 22544, 'an\\noptional': 22545, 'verbosely': 22546, 'the\\nparser': 22547, 'sr_parser': 22548, '.shiftreduceparser(grammar1)\\n>>>': 22549, '(s': 22550, 'dog))))\\n\\n\\n\\n\\nnote\\nyour': 22551, 'mode': 22552, 'reduce\\noperations,': 22553, 'sr_parse': 22554, '.shiftreduceparser(grammar1,': 22555, 'trace=2)\\n\\na': 22556, 'parse,\\neven': 22557, 'remains,': 22558, 'items\\nwhich': 22559, 'because\\nthere': 22560, 'undone': 22561, 'parser\\n(although': 22562, 'choices)': 22563, 'parser:\\n(a)': 22564, 'reduction': 22565, 'possible\\n(b)': 22566, 'policies': 22567, 'resolving': 22568, 'such\\nconflicts': 22569, 'conflicts': 22570, 'by\\nshifting': 22571, 'reductions': 22572, 'address\\nreduce-reduce': 22573, 'favoring': 22574, 'removes\\nthe': 22575, 'shift-reduce\\nparser,': 22576, 'lr': 22577, 'parser,': 22578, 'compilers': 22579, 'parsers\\nis': 22580, 'sub-structure': 22581, 'once,\\ne': 22582, 'np(det(the),': 22583, 'n(man))': 22584, 'stack\\na': 22585, 'the\\nvp': 22586, 'left-corner': 22587, 'parser\\none': 22588, 'it\\ngoes': 22589, 'left-recursive': 22590, 'grammar\\nproductions': 22591, 'blindly,': 22592, 'top-down\\napproaches': 22593, '.\\ngrammar': 22594, 'saw\\nmary:\\n\\n': 22595, '(11)\\nrecall': 22596, '(defined': 22597, 'np:\\n\\n': 22598, '(12)\\n': 22599, '.np': 22600, 'n\\n\\n': 22601, 'pp\\n\\n': 22602, 'bob\\n\\n\\nsuppose': 22603, '(11),': 22604, 'decide\\nwhich': 22605, 'to\\napply': 22606, 'obviously,': 22607, '(12c)': 22608, 'choice!': 22609, 'you\\nknow': 22610, 'pointless': 22611, '(12a)': 22612, '(12b)': 22613, 'instead?': 22614, 'because\\nneither': 22615, 'is\\njohn': 22616, 'successful\\nparse': 22617, 'mary,': 22618, 'more\\ngenerally,': 22619, '⇒*\\nb': 22620, '(13)\\na': 22621, 'trapped\\nin': 22622, 'preprocesses': 22623, 'the\\ncontext-free': 22624, 'two\\ncells,': 22625, 'non-terminal,': 22626, 'the\\ncollection': 22627, 'corners': 22628, 'non-terminal': 22629, 'grammar2': 22630, '.\\n\\n\\n\\n\\n\\n\\ncategory\\nleft-corners': 22631, '(pre-terminals)\\n\\n\\n\\ns\\nnp\\n\\nnp\\ndet,': 22632, 'propn\\n\\nvp\\nv\\n\\npp\\np\\n\\n\\ntable': 22633, 'left-corners': 22634, 'grammar2\\n\\n\\neach': 22635, 'pre-terminal\\ncategories': 22636, '.4\\xa0\\xa0\\xa0well-formed': 22637, 'tables\\nthe': 22638, 'suffer': 22639, 'in\\nboth': 22640, 'completeness': 22641, 'remedy': 22642, 'will\\napply': 22643, '.7,\\ndynamic': 22644, 're-uses': 22645, 'when\\nappropriate,': 22646, 'technique\\ncan': 22647, 'store\\npartial': 22648, 'as\\nnecessary': 22649, 'introduce\\nthe': 22650, 'section;': 22651, 'pajamas\\njust': 22652, 'it\\nup': 22653, 'subconstituent': 22654, 'a\\nwell-formed': 22655, 'wfst': 22656, 'contiguous': 22657, 'record\\nwhat': 22658, 'specified\\nspans': 22659, 'reminiscent': 22660, 'another\\nway': 22661, 'data\\nstructure': 22662, 'edge': 22663, 'wfst,': 22664, 'words\\nby': 22665, 'triangular': 22666, 'matrix:\\nthe': 22667, 'vertical': 22668, 'axis': 22669, 'denote': 22670, 'substring,\\nwhile': 22671, 'horizontal': 22672, 'position\\n(thus': 22673, 'coordinates': 22674, '2))': 22675, 'unique\\nlexical': 22676, 'category,': 22677, '.\\nmore': 22678, 'is\\na0a1': 22679, 'grammar\\ncontains': 22680, 'ai,': 22681, '`i`+1)': 22682, 'warning/2': 22683, '(ch08': 22684, '900);': 22685, 'backlink\\ninline': 22686, 'start-string': 22687, 'end-string': 22688, 'what\\ncategory': 22689, '.productions(rhs=text[1])\\n[v': 22690, \"'shot']\\n\\n\\n\\nfor\": 22691, '(n-1)': 22692, 'matrix\\nas': 22693, 'it\\nwith': 22694, 'init_wfst()\\nfunction': 22695, 'display()\\nto': 22696, 'pretty-print': 22697, 'init_wfst(tokens,': 22698, 'grammar):\\n': 22699, 'numtokens': 22700, 'len(tokens)\\n': 22701, '[[none': 22702, 'range(numtokens+1)]': 22703, 'range(numtokens+1)]\\n': 22704, 'range(numtokens):\\n': 22705, '.productions(rhs=tokens[i])\\n': 22706, 'wfst[i][i+1]': 22707, 'productions[0]': 22708, '.lhs()\\n': 22709, 'wfst\\n\\ndef': 22710, 'complete_wfst(wfst,': 22711, 'trace=false):\\n': 22712, 'dict((p': 22713, '.rhs(),': 22714, '.lhs())': 22715, '.productions())\\n': 22716, 'range(2,': 22717, 'numtokens+1):\\n': 22718, 'range(numtokens+1-span):\\n': 22719, 'span\\n': 22720, 'mid': 22721, 'range(start+1,': 22722, 'end):\\n': 22723, 'nt1,': 22724, 'nt2': 22725, 'wfst[start][mid],': 22726, 'wfst[mid][end]\\n': 22727, 'nt1': 22728, '(nt1,nt2)': 22729, 'index:\\n': 22730, 'wfst[start][end]': 22731, 'index[(nt1,nt2)]\\n': 22732, 'trace:\\n': 22733, 'print([%s]': 22734, '%3s': 22735, '[%s]': 22736, '==>': 22737, '(start,': 22738, 'mid,': 22739, 'nt2,': 22740, 'index[(nt1,nt2)],': 22741, 'end))\\n': 22742, 'display(wfst,': 22743, 'tokens):\\n': 22744, \"print('\\\\nwfst\": 22745, '.join((%-4d': 22746, 'len(wfst))))\\n': 22747, 'range(len(wfst)-1):\\n': 22748, 'print(%d': 22749, 'len(wfst)):\\n': 22750, 'print(%-4s': 22751, '(wfst[i][j]': 22752, 'wfst0': 22753, 'groucho_grammar)\\n>>>': 22754, 'display(wfst0,': 22755, 'tokens)\\nwfst': 22756, '7\\n0': 22757, 'n\\n>>>': 22758, 'wfst1': 22759, 'complete_wfst(wfst0,': 22760, 'display(wfst1,': 22761, 's\\n1': 22762, 'vp\\n2': 22763, 'pp\\n5': 22764, 'np\\n6': 22765, 'n\\n\\n\\nexample': 22766, '(code_wfst': 22767, 'acceptor': 22768, 'table\\n\\nreturning': 22769, 'det\\nin': 22770, 'elephant,': 22771, 'elephant?\\nwe': 22772, '.\\nconsulting': 22773, '.\\n\\nmore': 22774, 'j)': 22775, 'find\\nnonterminal': 22776, '(k,': 22777, 'complete_wfst(),\\nwe': 22778, 'constructed:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 22779, 'groucho_grammar,': 22780, 'trace=true)\\n[2]': 22781, '[3]': 22782, '[4]': 22783, '[2]': 22784, '[4]\\n[5]': 22785, '[6]': 22786, '[7]': 22787, '[5]': 22788, '[7]\\n[1]': 22789, '[1]': 22790, '[4]\\n[4]': 22791, '[7]\\n[0]': 22792, '[0]': 22793, '[4]\\n[1]': 22794, '[7]\\n\\n\\n\\nfor': 22795, 'at\\nwfst[2][3]': 22796, 'wfst[3][4],': 22797, 'to\\nwfst[2][4]': 22798, '.\\n\\nnote\\nto': 22799, 'hand\\nsides,': 22800, 'trade-off:': 22801, 'lookup\\non': 22802, 'of\\nproductions': 22803, 'non-terminals': 22804, 'once\\nwe': 22805, \"up!\\n\\nwfst's\": 22806, 'is\\nstrictly': 22807, 'speaking': 22808, 'a\\ngrammar,': 22809, 'non-lexical': 22810, 'form,\\nwe': 22811, 'requirement': 22812, 'wasteful,': 22813, 'propose': 22814, '.\\n\\nfinally,': 22815, 'structural': 22816, 'readings)': 22817, 'vp\\nin': 22818, '7)': 22819, 'twice,': 22820, 'np\\nreading,': 22821, 'different\\nhypotheses,': 22822, \"didn't\\nmatter\": 22823, '.)\\nchart': 22824, 'slighly': 22825, 'interesting\\nalgorithms': 22826, 'details)': 22827, '.chartparser()': 22828, '.\\n\\n\\n\\n\\n5\\xa0\\xa0\\xa0dependencies': 22829, 'grammar\\nphrase': 22830, 'complementary\\napproach,': 22831, 'focusses': 22832, 'words\\nrelate': 22833, 'asymmetric': 22834, 'that\\nholds': 22835, 'dependents': 22836, 'tensed': 22837, 'is\\neither': 22838, 'head,': 22839, 'of\\ndependencies': 22840, 'directed': 22841, 'graph,': 22842, 'the\\nnodes': 22843, 'arcs': 22844, 'dependency\\nrelations': 22845, 'heads': 22846, 'a\\ndependency': 22847, 'arrows': 22848, 'dependents;\\nlabels': 22849, 'as\\nsubject,': 22850, 'grammatical\\nfunction': 22851, 'example,\\ni': 22852, 'sbj': 22853, '(subject)': 22854, 'sentence),': 22855, 'nmod': 22856, '(noun': 22857, 'of\\nelephant)': 22858, 'therefore,\\ndependency': 22859, 'grammatical\\nfunctions': 22860, 'bare\\ndependency': 22861, 'dependency:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 22862, 'groucho_dep_grammar': 22863, '.dependencygrammar': 22864, \"'shot'\": 22865, \"'i'\": 22866, \"'pajamas'\": 22867, ')\\n>>>': 22868, 'print(groucho_dep_grammar)\\ndependency': 22869, 'productions\\n': 22870, \"'elephant'\\n\": 22871, \"'an'\\n\": 22872, \"'my'\\n\\n\\n\\na\": 22873, 'projective': 22874, 'are\\nwritten': 22875, 'words\\nwithout': 22876, 'crossing': 22877, 'its\\ndescendents': 22878, '(dependents': 22879, 'dependents,': 22880, 'is\\nprojective,': 22881, 'a\\nprojective': 22882, 'how\\ngroucho_dep_grammar': 22883, 'capturing\\nthe': 22884, 'phrase\\nstructure': 22885, 'pdp': 22886, '.projectivedependencyparser(groucho_dep_grammar)\\n>>>': 22887, \"pajamas'\": 22888, '.parse(sent)\\n>>>': 22889, 'trees:\\n': 22890, 'print(tree)\\n(shot': 22891, '(elephant': 22892, '(pajamas': 22893, 'my))))\\n(shot': 22894, 'my)))\\n\\n\\n\\nthese': 22895, 'trees,\\nwhere': 22896, '(14)\\nin': 22897, 'english,\\nnon-projective': 22898, '.\\n\\n\\nvarious': 22899, 'proposed': 22900, 'construction': 22901, 'important\\nare': 22902, 'following:\\n\\nh': 22903, 'c;': 22904, 'alternatively,': 22905, 'the\\nexternal': 22906, '.\\nh': 22907, 'or\\noptional': 22908, 'agreement\\nor': 22909, 'government)': 22910, 'immediate\\nconstituents': 22911, 'implicitly\\nappealing': 22912, 'phrase\\nis': 22913, 'preposition;': 22914, 'a\\ndependent': 22915, 'carries': 22916, 'other\\ntypes': 22917, 'from\\ndependency': 22918, 'grammars,': 22919, 'embody': 22920, 'of\\ndependency': 22921, 'capture\\ndependencies,': 22922, 'frameworks': 22923, 'increasingly\\nadopted': 22924, 'formalisms': 22925, '.1\\xa0\\xa0\\xa0valency': 22926, 'lexicon\\nlet': 22927, 'like\\n(15d)': 22928, '(15)\\n': 22929, 'squirrel': 22930, 'frightened': 22931, '.chatterer': 22932, 'angry': 22933, '.joe': 22934, 'fish': 22935, 'log': 22936, '.\\n\\n\\nthese': 22937, 'productions:\\n\\n\\n\\n\\n\\n\\nvp': 22938, 'adj\\nwas\\n\\nvp': 22939, 'np\\nsaw\\n\\nvp': 22940, 's\\nthought\\n\\nvp': 22941, 'pp\\nput\\n\\n\\ntable': 22942, 'heads\\n\\n\\n\\nthat': 22943, 'adj,': 22944, 'a\\nfollowing': 22945, 'and\\ns': 22946, 'complements': 22947, 'respective': 22948, 'strong\\nconstraints': 22949, 'with\\n(15d),': 22950, '(16d)': 22951, 'ill-formed:\\n\\n': 22952, '(16)\\n': 22953, '.*chatterer': 22954, '.*joe': 22955, '.\\n\\n\\nnote\\nwith': 22956, 'imagination,': 22957, 'to\\ninvent': 22958, 'and\\ncomplements': 22959, 'above\\nexamples': 22960, 'neutral': 22961, '.\\n\\n\\nin': 22962, 'tradition': 22963, 'said\\nto': 22964, 'valencies': 22965, 'valency': 22966, 'restrictions': 22967, 'just\\napplicable': 22968, '.\\n\\nwithin': 22969, 'various\\ntechniques': 22970, 'excluding': 22971, 'the\\nungrammatical': 22972, 'cfg,': 22973, 'constraining\\ngrammar': 22974, 'co-occur\\nwith': 22975, 'of\\nverbs': 22976, 'subcategories,': 22977, 'transitive': 22978, 'np\\nobject': 22979, 'complement;': 22980, 'subcategorized': 22981, 'np\\ndirect': 22982, 'namely\\ntv': 22983, 'verb),': 22984, 'productions:\\n\\nvp': 22985, 'tv': 22986, 'np\\ntv': 22987, \"'saw'\\n\\nnow\": 22988, '*joe': 22989, 'excluded': 22990, 'listed\\nthought': 22991, 'tv,': 22992, 'chatterer': 22993, 'subcategories': 22994, '.\\n\\n\\n\\n\\n\\n\\n\\nsymbol\\nmeaning\\nexample\\n\\n\\n\\niv\\nintransitive': 22995, 'verb\\nbarked\\n\\ntv\\ntransitive': 22996, 'verb\\nsaw': 22997, 'man\\n\\ndatv\\ndative': 22998, 'verb\\ngave': 22999, 'man\\n\\nsv\\nsentential': 23000, 'verb\\nsaid': 23001, 'barked\\n\\n\\ntable': 23002, 'subcategories\\n\\n\\nvalency': 23003, 'further\\nin': 23004, '.\\ncomplements': 23005, 'contrasted': 23006, 'adjuncts),\\nalthough': 23007, 'phrases,\\nadjectives': 23008, 'unlike\\ncomplements,': 23009, 'optional,': 23010, 'iterated,': 23011, 'are\\nnot': 23012, 'adverb': 23013, 'modifer': 23014, 'the\\nsentence': 23015, '(17d):\\n\\n': 23016, '(17)\\n': 23017, 'attachment,': 23018, 'have\\nillustrated': 23019, 'grammars,\\ncorresponds': 23020, '.2\\xa0\\xa0\\xa0scaling': 23021, 'up\\nso': 23022, 'toy': 23023, 'that\\nillustrate': 23024, 'obvious\\nquestion': 23025, 'scaled': 23026, 'cover\\nlarge': 23027, 'construct\\nsuch': 23028, 'hand?': 23029, 'is:': 23030, 'very\\nhard': 23031, 'devices': 23032, 'that\\ngive': 23033, 'succinct': 23034, 'extremely\\ndifficult': 23035, 'many\\nproductions': 23036, 'modularize': 23037, 'that\\none': 23038, 'in\\nturn': 23039, 'distribute': 23040, 'grammar\\nwriting': 23041, 'the\\ngrammar': 23042, 'constructions,\\nthere': 23043, 'analyses': 23044, 'are\\nadmitted': 23045, 'increases\\nwith': 23046, '.\\ndespite': 23047, 'collaborative\\nprojects': 23048, 'in\\ndeveloping': 23049, 'the\\nlexical': 23050, '(lfg)': 23051, 'pargram': 23052, 'project,\\nthe': 23053, 'head-driven': 23054, '(hpsg)': 23055, 'lingo': 23056, 'framework,\\nand': 23057, 'lexicalized': 23058, 'adjoining': 23059, 'xtag': 23060, '.\\n\\n\\n\\n6\\xa0\\xa0\\xa0grammar': 23061, 'development\\nparsing': 23062, 'above\\nonly': 23063, 'realistic': 23064, 'language?': 23065, 'will\\nsee': 23066, 'treebanks,': 23067, 'developing\\nbroad-coverage': 23068, '.\\n\\n6': 23069, '.1\\xa0\\xa0\\xa0treebanks': 23070, 'grammars\\nthe': 23071, 'reader,\\nwhich': 23072, 'treebank\\n>>>': 23073, \".parsed_sents('wsj_0001\": 23074, \".mrg')[0]\\n>>>\": 23075, 'print(t)\\n(s\\n': 23076, '(np-sbj\\n': 23077, '(nnp': 23078, 'pierre)': 23079, 'vinken))\\n': 23080, '(,': 23081, ',)\\n': 23082, '(adjp': 23083, '(cd': 23084, '61)': 23085, '(nns': 23086, 'years))': 23087, '(jj': 23088, 'old))\\n': 23089, ',))\\n': 23090, '(md': 23091, 'will)\\n': 23092, '(vb': 23093, 'join)\\n': 23094, '(dt': 23095, '(nn': 23096, 'board))\\n': 23097, '(pp-clr\\n': 23098, 'as)\\n': 23099, 'nonexecutive)': 23100, 'director)))\\n': 23101, '(np-tmp': 23102, 'nov': 23103, '29))))\\n': 23104, '.))\\n\\n\\n\\nwe': 23105, '.1\\nuses': 23106, 'sentential': 23107, 's,\\nthis': 23108, 'verbs\\nthat': 23109, 'expansion': 23110, 'filter(tree):\\n': 23111, 'child_nodes': 23112, '[child': 23113, 'tree\\n': 23114, 'isinstance(child,': 23115, '.tree)]\\n': 23116, '(tree': 23117, \"'vp')\": 23118, \"('s'\": 23119, 'child_nodes)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 23120, '[subtree': 23121, '.parsed_sents()\\n': 23122, '.subtrees(filter)]\\n': 23123, \"[tree('vp',\": 23124, \"[tree('vbn',\": 23125, \"['named']),\": 23126, \"tree('s',\": 23127, \"[tree('np-sbj',\": 23128, '.]),': 23129, '(code_sentential_complement': 23130, 'complements\\n\\nthe': 23131, '.ppattach\\nis': 23132, 'preposition\\nand': 23133, '.ppattach': 23134, \".attachments('training')\\n>>>\": 23135, 'defaultdict(set))\\n>>>': 23136, 'entries:\\n': 23137, '.noun1': 23138, '.prep': 23139, '.noun2\\n': 23140, 'table[key][entry': 23141, '.attachment]': 23142, '.add(entry': 23143, '.verb)\\n': 23144, 'sorted(table):\\n': 23145, 'len(table[key])': 23146, 'print(key,': 23147, \"'n:',\": 23148, \"sorted(table[key]['n']),\": 23149, \"'v:',\": 23150, \"sorted(table[key]['v']))\\n\\n\\n\\namongst\": 23151, 'find\\noffer-from-group': 23152, 'n:': 23153, \"['rejected']\": 23154, 'v:': 23155, \"['received'],\\nwhich\": 23156, 'received': 23157, 'separate\\npp': 23158, 'rejected': 23159, 'pe08\\ncross-framework': 23160, 'prepared': 23161, 'of\\ncomparing': 23162, 'downloading\\nthe': 23163, 'large_grammars': 23164, 'package\\n(e': 23165, '-m': 23166, '.downloader': 23167, 'large_grammars)': 23168, 'sinica': 23169, 'corpus,\\nconsisting': 23170, 'the\\nacademia': 23171, '.parsed_sents()[3450]': 23172, '\\n\\n\\n\\n\\n\\n\\n6': 23173, '.2\\xa0\\xa0\\xa0pernicious': 23174, 'ambiguity\\nunfortunately,': 23175, 'an\\nastronomical': 23176, 'rate': 23177, 'word\\nfish': 23178, 'sentence\\nfish': 23179, 'fish,': 23180, '.\\n(try': 23181, 'police': 23182, '.)\\nhere': 23183, 'sbar\\n': 23184, 'sbar': 23185, \"'fish'\\n\": 23186, ')\\n\\n\\n\\nnow': 23187, 'fish\\nfish,': 23188, 'amongst': 23189, \"'fish\": 23190, 'fish\\nfish': 23191, 'fishing': 23192, \"themselves'\": 23193, 'nltk\\nchart': 23194, '[fish]': 23195, '.chartparser(grammar)\\n>>>': 23196, '.parse(tokens):\\n': 23197, 'fish)': 23198, '(sbar': 23199, 'fish))))\\n(s': 23200, 'fish)))': 23201, 'fish))\\n\\n\\n\\nas': 23202, 'trees:\\n1;': 23203, '2;': 23204, '5;': 23205, '14;': 23206, '42;': 23207, '132;': 23208, '429;': 23209, '1,430;': 23210, '4,862;': 23211, '16,796;': 23212, '58,786;': 23213, '208,012;': 23214, '.\\n(these': 23215, 'exercise\\nin': 23216, '23,': 23217, 'length\\nof': 23218, 'sentence\\nof': 23219, '1012': 23220, 'parses,': 23221, 'sentence\\n(1),\\nwhich': 23222, 'effortlessly': 23223, 'either!\\nnote': 23224, '.\\n(church': 23225, 'patil,': 23226, '1982)': 23227, '(18)': 23228, 'catalan\\nnumbers': 23229, '(18)put': 23230, 'ambiguity;': 23231, 'ambiguity?\\nas': 23232, 'broad-coverage': 23233, 'we\\nare': 23234, 'of\\nspeech': 23235, 'is\\nonly': 23236, 'a\\nbroad-coverage': 23237, 'a),\\ndog': 23238, '(meaning': 23239, 'closely),': 23240, 'runs\\nis': 23241, 'runs)': 23242, 'be\\nreferred': 23243, \"'ate'\": 23244, 'spelled': 23245, 'three\\nletters;': 23246, 'overwhelmed': 23247, 'even\\ncomplete': 23248, 'gibberish': 23249, 'reading,': 23250, 'of\\ni': 23251, '(klavans': 23252, 'resnik,': 23253, 'salad': 23254, 'a\\ngrammatical': 23255, 'a\\nhundredth': 23256, 'hectare': 23257, 'sq': 23258, 'm),': 23259, 'are\\nnouns': 23260, 'designating': 23261, 'coordinates,': 23262, 'i:': 23263, 'drawing': 23264, 'paddocks,': 23265, 'being\\none': 23266, 'coordinates;\\nthe': 23267, '(after': 23268, 'abney)': 23269, '.\\n\\n\\neven': 23270, 'unlikely,': 23271, 'tree\\nfor': 23272, 'be\\nunambiguous,': 23273, 'other\\nreadings': 23274, 'anticipated': 23275, 'abney': 23276, 'explains)': 23277, 'this\\nambiguity': 23278, 'unavoidable,': 23279, 'horrendous': 23280, 'inefficiency': 23281, 'in\\nparsing': 23282, 'seemingly': 23283, 'innocuous': 23284, 'by\\nprobabilistic': 23285, 'rank\\nthe': 23286, '.3\\xa0\\xa0\\xa0weighted': 23287, 'grammar\\n\\nas': 23288, 'ambiguity\\nis': 23289, '.\\nchart': 23290, 'multiple\\nparses': 23291, 'sheer': 23292, 'weighted': 23293, 'and\\nprobabilistic': 23294, 'effective\\nsolution': 23295, 'notion': 23296, 'of\\ngrammaticality': 23297, 'gradient': 23298, 'given)\\nand': 23299, 'recipient)': 23300, '(19)': 23301, 'dative': 23302, 'in\\n(19a),': 23303, '(19)\\n': 23304, '.kim': 23305, 'bone': 23306, 'dog\\n\\n': 23307, 'bone\\n\\nin': 23308, '(19b),\\nthe': 23309, 'pronoun,': 23310, 'construction:\\n\\n': 23311, '(20)\\n': 23312, 'heebie-jeebies': 23313, '(*prepositional': 23314, 'dative)\\n\\n': 23315, '(double': 23316, 'object)\\n\\nusing': 23317, 'sample,': 23318, 'of\\nprepositional': 23319, 'involving\\ngive,': 23320, 'give(t):\\n': 23321, \"'vp'\": 23322, 't[1]': 23323, \"'np'\\\\\\n\": 23324, '(t[2]': 23325, \"'pp-dtv'\": 23326, 't[2]': 23327, \"'np')\\\\\\n\": 23328, \"('give'\": 23329, '.leaves()': 23330, \"'gave'\": 23331, '.leaves())\\ndef': 23332, 'sent(t):\\n': 23333, '.join(token': 23334, 'token[0]': 23335, \"'*-0')\\ndef\": 23336, 'print_node(t,': 23337, 'width):\\n': 23338, '%s:': 23339, '%\\\\\\n': 23340, '(sent(t[0]),': 23341, 'sent(t[1]),': 23342, 'sent(t[2]))\\n': 23343, 'len(output)': 23344, 'width:\\n': 23345, 'output[:width]': 23346, 'print(output)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 23347, '.parsed_sents():\\n': 23348, '.subtrees(give):\\n': 23349, '72)\\ngave': 23350, 'chefs': 23351, 'standing': 23352, 'ovation\\ngive': 23353, 'advertisers': 23354, 'discounts': 23355, 'ad': 23356, 'sp': 23357, 'pp-dtv:': 23358, 'politicians\\ngave': 23359, 'help\\ngive': 23360, 'np:\\ngive': 23361, 'french': 23362, 'federal': 23363, 'judges': 23364, 'raise\\ngive': 23365, 'consumers': 23366, 'straight': 23367, 'scoop': 23368, 'waste': 23369, 'crisis\\ngave': 23370, 'mitsui': 23371, 'high-tech': 23372, 'medical': 23373, 'product\\ngive': 23374, 'mitsubishi': 23375, 'glass': 23376, 'industry\\ngive': 23377, 'rates': 23378, 'receiving': 23379, 'foster': 23380, 'savings': 23381, 'institution': 23382, 'gift': 23383, 'suspend': 23384, 'trading': 23385, 'futu': 23386, '.\\ngave': 23387, 'quick': 23388, 'approval': 23389, '.18': 23390, 'billion': 23391, 'supplemental': 23392, 'appr': 23393, 'transportation': 23394, 'power\\ngive': 23395, 'heebie-jeebies\\ngive': 23396, 'holders': 23397, 'obligation': 23398, 'cal': 23399, 'thomas': 23400, '``': 23401, 'qualified': 23402, 'rating': 23403, 'line-item': 23404, 'veto': 23405, 'power\\n\\n\\nexample': 23406, '(code_give': 23407, 'sample\\n\\nwe': 23408, 'tendency': 23409, 'appear\\nfirst': 23410, 'like\\ngive': 23411, 'raise,': 23412, 'animacy': 23413, 'may\\nplay': 23414, 'contributing\\nfactors,': 23415, 'surveyed': 23416, '(bresnan': 23417, 'hay,': 23418, 'preferences': 23419, 'pcfg)': 23420, 'free\\ngrammar': 23421, 'associates': 23422, 'corresponding\\ncontext': 23423, 'pcfg': 23424, 'product\\nof': 23425, 'specially\\nformatted': 23426, 'productions,\\nwhere': 23427, '.pcfg': 23428, '[1': 23429, '.0]\\n': 23430, '[0': 23431, '.4]\\n': 23432, 'iv': 23433, '.3]\\n': 23434, 'datv': 23435, \"'telescopes'\": 23436, '.8]\\n': 23437, \"'jack'\": 23438, '.2]\\n': 23439, 'print(grammar)\\ngrammar': 23440, '(start': 23441, 's)\\n': 23442, '.2]\\n\\n\\nexample': 23443, '(code_pcfg1': 23444, '(pcfg)\\n\\nit': 23445, 'line,\\ne': 23446, '.4]': 23447, '.3]': 23448, 'a\\nprobability': 23449, 'impose': 23450, 'constraint\\nthat': 23451, 'have\\nprobabilities': 23452, 'obeys': 23453, 'constraint:': 23454, 's,\\nthere': 23455, '.0;': 23456, 'vp,\\n0': 23457, '.4+0': 23458, '.3+0': 23459, '.3=1': 23460, '.8+0': 23461, '.2=1': 23462, 'parse()': 23463, 'probabilities:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 23464, 'viterbi_parser': 23465, '.viterbiparser(grammar)\\n>>>': 23466, \".parse(['jack',\": 23467, \"'saw',\": 23468, \"'telescopes']):\\n\": 23469, 'jack)': 23470, '(tv': 23471, 'telescopes)))': 23472, '(p=0': 23473, '.064)\\n\\n\\n\\nnow': 23474, 'probabilities,': 23475, 'matters\\nthat': 23476, '.\\n\\n\\n\\n7\\xa0\\xa0\\xa0summary\\n\\nsentences': 23477, 'organization\\nthat': 23478, 'constituent\\nstructure': 23479, 'are:': 23480, 'heads,': 23481, 'characterization': 23482, 'sentences;\\nwe': 23483, 'grammar\\nuses': 23484, 'α1\\n': 23485, 'αn': 23486, 'dependents\\nare': 23487, '.\\nsyntactic': 23488, 'analysis\\n(e': 23489, 'ambiguity)': 23490, 'grammatically\\nwell-formed': 23491, 'recursively\\nexpands': 23492, 'grammar\\nproductions,': 23493, 'cannot\\nhandle': 23494, 'pp)': 23495, 'inefficient': 23496, 'expands\\ncategories': 23497, 'and\\nin': 23498, 'shifts': 23499, 'onto\\na': 23500, 'right\\nhand': 23501, 'exists,': 23502, 'substructure': 23503, 'without\\nchecking': 23504, 'globally': 23505, '.\\n\\n\\n\\n\\n8\\xa0\\xa0\\xa0further': 23506, 'the\\nparsing': 23507, 'a\\ngeneral': 23508, '(radford,': 23509, '1988)': 23510, 'a\\ngentle': 23511, 'be\\nrecommended': 23512, 'to\\nunbounded': 23513, 'used\\nterm': 23514, 'grammar,\\nthough': 23515, '(chomsky,': 23516, '1965)': 23517, 'x-bar': 23518, 'is\\ndue': 23519, '(jacobs': 23520, 'rosenbaum,': 23521, '1970),': 23522, 'explored': 23523, '(jackendoff,': 23524, '1977)\\n(the': 23525, 'primes': 23526, \"chomsky's\": 23527, 'typographically': 23528, 'demanding': 23529, '.)\\n(burton-roberts,': 23530, '1997)': 23531, 'practically': 23532, 'oriented': 23533, 'constituency': 23534, 'exemplification': 23535, '(huddleston': 23536, 'pullum,': 23537, 'up-to-date': 23538, 'of\\nsyntactic': 23539, 'phenomena': 23540, 'english;\\nsections': 23541, '.1-3': 23542, 'techniques\\nfor': 23543, 'ambiguity;\\nchapter': 23544, 'parsing;\\nchapter': 23545, 'chomsky': 23546, 'complexity\\nof': 23547, '.\\n(levin,': 23548, '1993)': 23549, 'classes,\\naccording': 23550, 'ongoing': 23551, 'large-scale': 23552, 'grammars,\\ne': 23553, 'lfg': 23554, 'http://www2': 23555, '.parc': 23556, '.com/istl/groups/nltt/pargram/,\\nthe': 23557, 'hpsg': 23558, '.delph-in': 23559, '.net/matrix/\\nand': 23560, '.edu/~xtag/': 23561, '.\\n\\n\\n9\\xa0\\xa0\\xa0exercises\\n\\n☼': 23562, 'never\\nbeen': 23563, 'before?': 23564, '(take': 23565, 'partner': 23566, 'you\\nabout': 23567, 'language?\\n\\n☼': 23568, 'prohibition': 23569, 'sentence-initial\\nhowever': 23570, '.\\ndo': 23571, 'construction?\\n\\n☼': 23572, 'kim': 23573, 'dana': 23574, 'everyone': 23575, 'cheered': 23576, 'and\\nand': 23577, 'interpretations': 23578, 'import\\nthe': 23579, 'help(tree)': 23580, 'phrase\\nold': 23581, 'women\\nencode': 23582, 'labeled\\nbracketing': 23583, '.tree()': 23584, 'draw()': 23585, 'the\\ndepth': 23586, 'have\\ndepth': 23587, 'depth\\nof': 23588, 'children,': 23589, 'milne': 23590, 'piglet,': 23591, 'underlining': 23592, 's\\n(e': 23593, 'when:lx`': 23594, '.\\ndraw': 23595, 'are\\nthe': 23596, 'long\\nsentence?\\n\\n☼': 23597, 'demo,': 23598, 'are\\nmore': 23599, 'length?\\n\\n☼': 23600, 'chart-parser': 23601, 'with\\ndifferent': 23602, 'invocation': 23603, 'strategies': 23604, 'strategy\\nthat': 23605, 'describe\\nthe': 23606, 'steps,': 23607, 'terms\\nof': 23608, 'chart)': 23609, 'grammar?': 23610, 'prospects': 23611, 'for\\nsignificant': 23612, 'boosts': 23613, 'cleverer': 23614, 'invocation\\nstrategies?\\n\\n☼': 23615, 'paper,': 23616, 'descent\\nparser': 23617, 'edges\\nfrom': 23618, 'why?\\n\\n☼': 23619, 'words:\\nbuffalo': 23620, 'buffalo': 23621, 'grammatically': 23622, 'at\\nhttp://en': 23623, '.org/wiki/buffalo_buffalo_buffalo_buffalo_buffalo_buffalo_buffalo_buffalo': 23624, 'page,': 23625, 'suitable\\ngrammar': 23626, 'listener': 23627, 'hearing\\nthis': 23628, 'sentence?\\nhow': 23629, 'longer?\\n(more': 23630, '.org/wiki/list_of_homophonous_phrases)': 23631, 'demo\\nby': 23632, 'change\\nthe': 23633, 'np\\npp': 23634, 'button,': 23635, 'happens?\\n\\n◑': 23636, 'as\\nintransitive,': 23637, 'pp\\ncomplement': 23638, 'the\\npreceding': 23639, 'lee': 23640, 'ran': 23641, 'home': 23642, 'tasks:\\n\\nwrite': 23643, 'verb\\nexhibits': 23644, 'attachments,': 23645, 'preposition,': 23646, 'stay': 23647, '.\\ndevise': 23648, 'parser\\ncompared': 23649, 'performance\\nusing': 23650, 'this)': 23651, 'top-down,': 23652, 'left-corner\\nparsers': 23653, 'test\\nsentences': 23654, 'each\\nparser': 23655, 'all\\nthree': 23656, '3-by-3': 23657, 'of\\ntimes,': 23658, 'totals': 23659, 'garden': 23660, 'computational\\nwork': 23661, 'with\\nprocessing': 23662, 'sentences?\\nhttp://en': 23663, '.org/wiki/garden_path_sentence\\n\\n◑': 23664, 'the\\ndraw_trees()': 23665, 'out:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 23666, '.draw': 23667, 'draw_trees\\n>>>': 23668, 'draw_trees(tree1,': 23669, 'tree2,': 23670, 'tree3)': 23671, '\\n\\n\\n\\n\\n◑': 23672, 'positions,': 23673, '100\\nsentences': 23674, 'treebank;': 23675, 'view,\\nlimit': 23676, 'subtrees': 23677, 'height': 23678, 'corpus\\nand': 23679, 'influence': 23680, 'claimed': 23681, 'regularities\\nthat': 23682, 'phrase\\nin': 23683, 'based\\non': 23684, 'n-grams?\\n\\nwhat': 23685, 'youngish': 23686, 'nikolay': 23687, 'parfenovich\\nalso': 23688, 'a\\nsincere': 23689, 'liking': 23690, 'discriminated-against': 23691, 'procurator': 23692, '.\\n(dostoevsky:': 23693, 'brothers': 23694, 'karamazov)\\n\\n\\n◑': 23695, 'bracketing': 23696, 'non-terminal\\nlabels': 23697, 'pierre\\nvinken': 23698, 'produce:\\n[[[nnp': 23699, 'nnp]np': 23700, '[adjp': 23701, '[cd': 23702, 'nns]np': 23703, 'jj]adjp': 23704, ',]np-sbj': 23705, '[vb': 23706, '[dt': 23707, 'nn]np': 23708, '[in': 23709, 'nn]np]pp-clr': 23710, '[nnp': 23711, 'cd]np-tmp]vp': 23712, '.]s\\nconsecutive': 23713, 'extremely': 23714, 'find?': 23715, 'construction(s)\\nare': 23716, 'sentences?\\n\\n◑': 23717, 'init_wfst()': 23718, 'complete_wfst()': 23719, 'of\\nnon-terminal': 23720, 'why\\nparsing': 23721, 'n3,': 23722, 'n\\nis': 23723, '.treebank\\nand': 23724, '.productions()': 23725, 'discard\\nthe': 23726, 'side,\\nand': 23727, 'collapsed,': 23728, 'but\\nmore': 23729, 'in\\nenglish': 23730, 'sibling': 23731, 'to\\nthis': 23732, 'subject?\\n\\n★': 23733, '.start()': 23734, 'grammar;\\ngrammar': 23735, '.productions(lhs)': 23736, 'grammar\\nthat': 23737, 'side;': 23738, '.rhs()': 23739, 'right-hand': 23740, 'backtracking,\\nso': 23741, 'ascent': 23742, 'backtracking\\nat': 23743, '.org/wiki/backtracking\\n\\n★\\nas': 23744, 'this\\nfor': 23745, 'gave,': 23746, 'patterns\\nsuch': 23747, 'following:\\n\\ngave': 23748, 'np\\ngave': 23749, 'up\\ngave': 23750, 'np\\n\\n\\nuse': 23751, 'complementation': 23752, 'verb\\nof': 23753, 'task\\nis': 23754, '.)\\nidentify': 23755, 'near-synonyms,': 23756, 'the\\ndumped/filled/loaded': 23757, 'verbs\\nbe': 23758, 'substituted': 23759, 'constraints?\\ndiscuss': 23760, 'inheriting': 23761, 'parsei': 23762, 'complete)': 23763, 'includes\\na': 23764, 'acst9': 23765, 'grammars\\n\\n\\n\\n\\n\\n9': 23766, 'grammars\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nnatural': 23767, 'gain\\nmore': 23768, 'flexibility,': 23769, 's,\\nnp': 23770, 'atomic': 23771, 'decompose': 23772, 'like\\ndictionaries,': 23773, 'to\\ngain': 23774, 'productions?\\nwhat': 23775, 'them\\ncomputationally?\\nwhat': 23776, 'capture\\nwith': 23777, 'grammars?\\n\\nalong': 23778, 'as\\nagreement,': 23779, 'subcategorization,': 23780, 'unbounded': 23781, '.\\n\\n1\\xa0\\xa0\\xa0grammatical': 23782, 'features\\nin': 23783, 'chap-data-intensive,': 23784, 'simple,': 23785, 'a\\nword,': 23786, 'predicted\\nby': 23787, 'extractors,': 23788, 'record\\nfeatures': 23789, 'detected,': 23790, 'to\\ndeclare': 23791, 'a\\nvery': 23792, \"{'cat':\": 23793, \"'np',\": 23794, \"'orth':\": 23795, \"'kim',\": 23796, \"'ref':\": 23797, \"'k'}\\n>>>\": 23798, \"'rel':\": 23799, \"'chase'}\\n\\n\\n\\nthe\": 23800, 'cat\\n(grammatical': 23801, 'category)': 23802, 'orth': 23803, '(orthography,': 23804, 'spelling)': 23805, 'each\\nhas': 23806, 'feature:': 23807, \"kim['ref']\": 23808, 'the\\nreferent': 23809, \"chase['rel']\": 23810, 'by\\nchase': 23811, 'pairings': 23812, 'and\\nvalues': 23813, 'alternative\\nnotations': 23814, '.\\nfeature': 23815, 'grammatical\\nentities': 23816, 'exhaustive,': 23817, 'further\\nproperties': 23818, 'what\\nsemantic': 23819, 'chase,\\nthe': 23820, 'plays': 23821, 'of\\npatient': 23822, \"'sbj'\": 23823, \"'obj'\": 23824, 'placeholders\\nwhich': 23825, 'filled': 23826, 'arguments:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 23827, \"chase['agt']\": 23828, \"'sbj'\\n>>>\": 23829, \"chase['pat']\": 23830, \"'obj'\\n\\n\\n\\nif\": 23831, 'lee,': 23832, 'bind': 23833, \"verb's\": 23834, 'patient': 23835, 'linking': 23836, 'the\\nref': 23837, 'the\\nsimple-minded': 23838, 'the\\nverb': 23839, 'feature\\nstructure': 23840, 'lee\\n>>>': 23841, \"'lee',\": 23842, \"'l'}\\n>>>\": 23843, 'lex2fs(word):\\n': 23844, 'fs': 23845, '[kim,': 23846, 'chase]:\\n': 23847, \"fs['orth']\": 23848, 'fs\\n>>>': 23849, 'subj,': 23850, 'obj': 23851, 'lex2fs(tokens[0]),': 23852, 'lex2fs(tokens[1]),': 23853, 'lex2fs(tokens[2])\\n>>>': 23854, \"verb['agt']\": 23855, \"subj['ref']\\n>>>\": 23856, \"verb['pat']\": 23857, \"obj['ref']\\n>>>\": 23858, \"['orth',\": 23859, \"'rel',\": 23860, \"'agt',\": 23861, \"'pat']:\\n\": 23862, 'print(%-5s': 23863, '=>': 23864, 'verb[k]))\\north': 23865, 'chased\\nrel': 23866, 'chase\\nagt': 23867, 'k\\npat': 23868, 'l\\n\\n\\n\\nthe': 23869, 'adopted': 23870, 'surprise,': 23871, '(src)': 23872, 'object,\\nthe': 23873, 'experiencer': 23874, '(exp):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 23875, \"'surprised',\": 23876, \"'surprise',\\n\": 23877, \"'src':\": 23878, \"'sbj',\": 23879, \"'exp':\": 23880, \"'obj'}\\n\\n\\n\\nfeature\": 23881, 'way\\nin': 23882, 'hoc': 23883, 'be\\nexpanded': 23884, 'this\\nin': 23885, 'generic': 23886, 'principled': 23887, 'the\\nphenomenon': 23888, 'agreement;': 23889, 'agreement\\nconstraints': 23890, 'elegantly': 23891, 'illustrate\\ntheir': 23892, 'kind,': 23893, 'briefly\\nlook': 23894, 'view,': 23895, 'feature\\nstructures': 23896, 'offered': 23897, 'expressiveness': 23898, 'opens\\nup': 23899, 'spectrum': 23900, 'sophisticated\\naspects': 23901, '.1\\xa0\\xa0\\xa0syntactic': 23902, 'agreement\\nthe': 23903, 'is\\ngrammatical': 23904, 'signal': 23905, '.)\\n\\n': 23906, '.this': 23907, '.*these': 23908, 'dog\\n\\n\\n': 23909, '.these': 23910, 'dogs\\n\\n': 23911, '.*this': 23912, 'dogs\\n\\nin': 23913, 'singular\\nor': 23914, 'demonstrative': 23915, 'varies:\\nthis': 23916, '(singular)': 23917, '(plural)': 23918, '(2b)': 23919, 'demonstratives': 23920, 'phrase:\\neither': 23921, 'similar\\nconstraint': 23922, 'predicates:\\n\\n': 23923, 'runs\\n\\n': 23924, 'run\\n\\n\\n': 23925, 'dogs': 23926, 'run\\n\\n': 23927, 'runs\\n\\n\\nhere': 23928, 'co-vary\\nwith': 23929, 'co-variance': 23930, 'that\\npresent': 23931, 'inflected': 23932, 'person\\nsingular,': 23933, 'number,\\nas': 23934, '.\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\nsingular\\nplural\\n\\n1st': 23935, 'per\\ni': 23936, 'run\\nwe': 23937, 'run\\n\\n2nd': 23938, 'per\\nyou': 23939, 'run\\nyou': 23940, 'run\\n\\n3rd': 23941, 'per\\nhe/she/it\\nruns\\nthey': 23942, 'run\\n\\n\\ntable': 23943, 'paradigm': 23944, 'verbs\\n\\n\\nwe': 23945, 'explicit\\nas': 23946, 'ex-runs': 23947, 'ex-run': 23948, 'agrees': 23949, 'sg': 23950, 'pl': 23951, '.)\\n\\nsystem': 23952, '(ch09': 23953, '252)\\nerror': 23954, 'directive:': 23955, '257)\\nerror': 23956, 'a\\ncontext-free': 23957, '(5)\\ns': 23958, 'vp\\nnp': 23959, 'n\\nvp': 23960, 'v\\n\\ndet': 23961, \"'this'\\nn\": 23962, \"'dog'\\nv\": 23963, \"'runs'\\n\\n\\ngrammar\": 23964, 'runs;\\nhowever,': 23965, 'dogs\\nrun': 23966, 'blocking': 23967, '*this': 23968, 'run\\nand': 23969, '*these': 23970, 'to\\nadd': 23971, 'grammar:\\n\\n': 23972, '(6)\\ns': 23973, 'np_sg': 23974, 'vp_sg\\ns': 23975, 'np_pl': 23976, 'vp_pl\\nnp_sg': 23977, 'det_sg': 23978, 'n_sg\\nnp_pl': 23979, 'det_pl': 23980, 'n_pl\\nvp_sg': 23981, 'v_sg\\nvp_pl': 23982, 'v_pl\\n\\ndet_sg': 23983, \"'this'\\ndet_pl\": 23984, \"'these'\\nn_sg\": 23985, \"'dog'\\nn_pl\": 23986, \"'dogs'\\nv_sg\": 23987, \"'runs'\\nv_pl\": 23988, \"'run'\\n\\n\\nin\": 23989, 'two\\nproductions,': 23990, 'subject\\nnps': 23991, 'vps,': 23992, 'plural\\nsubject': 23993, 'vps': 23994, 'counterparts': 23995, 'grammar,\\nthis': 23996, 'aesthetically\\nunappealing': 23997, 'reasonable\\nsubset': 23998, 'constructions,': 23999, 'doubling': 24000, 'grammar\\nsize': 24001, 'unattractive': 24002, 'same\\napproach': 24003, 'agreement,': 24004, 'for\\nboth': 24005, 'grammar\\nbeing': 24006, 'to\\navoid': 24007, 'show\\nthat': 24008, 'cost\\nof': 24009, 'blowing': 24010, '.2\\xa0\\xa0\\xa0using': 24011, 'constraints\\nwe': 24012, 'spoke': 24013, 'informally': 24014, 'properties;': 24015, \"let's\\nmake\": 24016, 'explicit:\\n\\n': 24017, '(7)\\nn[num=pl]\\n\\n\\nin': 24018, '(7),': 24019, '(grammatical)': 24020, '(short': 24021, \"for\\n'number')\": 24022, \"for\\n'plural')\": 24023, 'annotations': 24024, 'entries:\\n\\n': 24025, '(8)\\ndet[num=sg]': 24026, \"'this'\\ndet[num=pl]\": 24027, \"'these'\\n\\nn[num=sg]\": 24028, \"'dog'\\nn[num=pl]\": 24029, \"'dogs'\\nv[num=sg]\": 24030, \"'runs'\\nv[num=pl]\": 24031, \"'run'\\n\\n\\ndoes\": 24032, 'all?': 24033, 'more\\nverbose': 24034, 'become\\nmore': 24035, 'use\\nthese': 24036, 'constraints:\\n\\n': 24037, '(9)\\ns': 24038, 'np[num=?n]': 24039, 'vp[num=?n]\\nnp[num=?n]': 24040, 'det[num=?n]': 24041, 'n[num=?n]\\nvp[num=?n]': 24042, 'v[num=?n]\\n\\n\\nwe': 24043, '?n': 24044, 'num;': 24045, 'pl,': 24046, 'whatever\\nvalue': 24047, 'num,\\nvp': 24048, \"it's\\nhelpful\": 24049, 'lexical\\nproductions': 24050, 'admit': 24051, '(trees': 24052, 'of\\ndepth': 24053, 'one):\\n\\n': 24054, '(11)\\n': 24055, 'vp[num=?n]': 24056, 'num\\nvalues': 24057, 'n[num=?n]': 24058, 'will\\npermit': 24059, '(11a)': 24060, '(11b)': 24061, 'be\\ncombined,': 24062, '(13a)': 24063, '(13b)': 24064, 'are\\nprohibited': 24065, 'differ\\nin': 24066, 'feature;': 24067, 'incompatibility': 24068, 'is\\nindicated': 24069, '.\\n\\nproduction': 24070, 'v[num=?n]': 24071, 'says\\nthat': 24072, 'the\\nnum': 24073, 'for\\nexpanding': 24074, 'we\\nderive': 24075, 'head\\nnoun': 24076, \"vp's\\nhead\": 24077, '(14)\\ngrammar': 24078, 'this\\nand': 24079, 'noun\\nrespectively': 24080, 'choosy\\nabout': 24081, 'add\\ntwo': 24082, 'and\\nplural': 24083, 'the\\n\\ndet[num=sg]': 24084, \"'some'\": 24085, \"'any'\\ndet[num=pl]\": 24086, \"'any'\\n\\nhowever,\": 24087, 'to\\nleave': 24088, 'underspecified': 24089, 'letting': 24090, 'agree\\nin': 24091, 'variable\\nvalue': 24092, 'result:\\n\\ndet[num=?n]': 24093, \"'any'\\n\\nbut\": 24094, 'economical,': 24095, 'any\\nspecification': 24096, 'explicitly': 24097, 'another\\nvalue': 24098, '.\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 24099, \".show_cfg('grammars/book_grammars/feat0\": 24100, \".fcfg')\\n%\": 24101, 's\\n#': 24102, '###################\\n#': 24103, 'productions\\n#': 24104, 'productions\\ns': 24105, 'vp[num=?n]\\n#': 24106, 'productions\\nnp[num=?n]': 24107, 'n[num=?n]\\nnp[num=?n]': 24108, 'propn[num=?n]\\nnp[num=?n]': 24109, 'n[num=?n]\\nnp[num=pl]': 24110, 'n[num=pl]\\n#': 24111, 'productions\\nvp[tense=?t,': 24112, 'num=?n]': 24113, 'iv[tense=?t,': 24114, 'num=?n]\\nvp[tense=?t,': 24115, 'tv[tense=?t,': 24116, 'np\\n#': 24117, '###################\\ndet[num=sg]': 24118, \"'this'\": 24119, \"'every'\\ndet[num=pl]\": 24120, \"'these'\": 24121, \"'all'\\ndet\": 24122, \"'several'\\npropn[num=sg]->\": 24123, \"'kim'\": 24124, \"'jody'\\nn[num=sg]\": 24125, \"'girl'\": 24126, \"'car'\": 24127, \"'child'\\nn[num=pl]\": 24128, \"'dogs'\": 24129, \"'girls'\": 24130, \"'cars'\": 24131, \"'children'\\niv[tense=pres,\": 24132, 'num=sg]': 24133, \"'disappears'\": 24134, \"'walks'\\ntv[tense=pres,\": 24135, \"'sees'\": 24136, \"'likes'\\niv[tense=pres,\": 24137, 'num=pl]': 24138, \"'disappear'\": 24139, \"'walk'\\ntv[tense=pres,\": 24140, \"'see'\": 24141, \"'like'\\niv[tense=past]\": 24142, \"'disappeared'\": 24143, \"'walked'\\ntv[tense=past]\": 24144, \"'liked'\\n\\n\\nexample\": 24145, '(code_feat0cfg': 24146, 'grammar\\n\\nnotice': 24147, 'for\\nexample,\\nv[tense=pres,': 24148, '%start': 24149, 'directive': 24150, 'the\\nstart': 24151, 'grammar,\\nit': 24152, 'edited,\\ntested': 24153, \"named\\n'feat0\": 24154, \".fcfg'\": 24155, 'own\\ncopy': 24156, 'experimentation': 24157, '.load()': 24158, 'chart\\nparser': 24159, 'feature-based': 24160, 'load_parser': 24161, 'function\\n': 24162, 'a\\nchart': 24163, \"parser's\\nparse()\": 24164, 'trees;\\ntrees': 24165, 'and\\nwill': 24166, \"'kim\": 24167, 'likes': 24168, \"children'\": 24169, \"load_parser('grammars/book_grammars/feat0\": 24170, \".fcfg',\": 24171, '.\\n|': 24172, '.like': 24173, '.chil': 24174, '.|\\nleaf': 24175, 'init': 24176, 'rule:\\n|[----]': 24177, '.|': 24178, '[0:1]': 24179, \"'kim'\\n|\": 24180, '[----]': 24181, '[1:2]': 24182, \"'likes'\\n|\": 24183, '[----]|': 24184, '[2:3]': 24185, \"'children'\\nfeature\": 24186, \"propn[num='sg']\": 24187, '*\\nfeature': 24188, \"np[num='sg']\": 24189, 'rule:\\n|[---->': 24190, 's[]': 24191, '{?n:': 24192, \"'sg'}\\nfeature\": 24193, 'rule:\\n|': 24194, \"tv[num='sg',\": 24195, \"tense='pres']\": 24196, \"'likes'\": 24197, '[---->': 24198, 'vp[num=?n,': 24199, 'tense=?t]': 24200, 'tv[num=?n,': 24201, 'np[]': 24202, \"'sg',\": 24203, '?t:': 24204, \"'pres'}\\nfeature\": 24205, \"n[num='pl']\": 24206, \"'children'\": 24207, \"np[num='pl']\": 24208, '[---->|': 24209, \"'pl'}\\nfeature\": 24210, '[---------]|': 24211, '[1:3]': 24212, \"vp[num='sg',\": 24213, 'rule:\\n|[==============]|': 24214, '[0:3]': 24215, \"vp[num='sg']\": 24216, '*\\n(s[]\\n': 24217, \"(np[num='sg']\": 24218, \"(propn[num='sg']\": 24219, 'kim))\\n': 24220, \"(vp[num='sg',\": 24221, \"tense='pres']\\n\": 24222, \"(tv[num='sg',\": 24223, 'likes)\\n': 24224, \"(np[num='pl']\": 24225, \"(n[num='pl']\": 24226, 'children))))\\n\\n\\nexample': 24227, '(code_featurecharttrace': 24228, 'parser\\n\\nthe': 24229, 'for\\npresent': 24230, 'which\\nbears': 24231, 'approach\\nto': 24232, 'compile\\nout': 24233, 'admissible': 24234, 'by\\ncontrast,': 24235, 'the\\nunderspecified': 24236, 'flow\\nupwards': 24237, 'associated\\nwith': 24238, 'bindings': 24239, 'dictionaries)': 24240, \"{?n:\\n'sg',\": 24241, \"'pres'}\": 24242, 'assembles': 24243, 'building,': 24244, 'to\\ninstantiate': 24245, 'nodes;': 24246, 'underspecified\\nvp[num=?n,': 24247, 'becomes\\ninstantiated': 24248, \"tv[num='sg',\\ntense='pres']\": 24249, '?t': 24250, 'one)': 24251, 'trees:': 24252, 'print(tree)\\n(s[]\\n': 24253, 'children))))\\n\\n\\n\\n\\n\\n1': 24254, '.3\\xa0\\xa0\\xa0terminology\\nso': 24255, 'and\\npl': 24256, 'atomic\\n—': 24257, 'decomposed': 24258, 'subparts': 24259, 'special\\ncase': 24260, 'that\\njust': 24261, 'as\\ncan,': 24262, 'may,': 24263, 'feature\\naux': 24264, 'v[tense=pres,': 24265, 'aux=+]': 24266, \"'can'\\nmeans\": 24267, 'pres': 24268, 'and\\n+': 24269, 'aux': 24270, 'adopted\\nconvention': 24271, 'abbreviates': 24272, 'boolean\\nfeatures': 24273, 'f;': 24274, 'aux=+': 24275, 'aux=-,': 24276, '+aux': 24277, 'and\\n-aux': 24278, 'any\\nother': 24279, '(15)': 24280, 'productions:\\n\\n': 24281, '(15)\\nv[tense=pres,': 24282, '+aux]': 24283, \"'can'\\nv[tense=pres,\": 24284, \"'may'\\n\\nv[tense=pres,\": 24285, '-aux]': 24286, \"'walks'\\nv[tense=pres,\": 24287, \"'likes'\\n\\n\\nwe\": 24288, 'attaching': 24289, 'to\\nsyntactic': 24290, 'radical': 24291, 'category\\n—': 24292, '—\\nas': 24293, 'bundle': 24294, 'n[num=sg]': 24295, 'speech\\ninformation': 24296, 'as\\npos=n': 24297, 'therefore\\nis': 24298, '[pos=n,': 24299, 'atomic-valued': 24300, 'group\\ntogether': 24301, 'a\\ndistinguished': 24302, 'agr': 24303, 'case,\\nwe': 24304, '(16)': 24305, 'depicts': 24306, 'format\\nknown': 24307, '(avm)': 24308, '(16)\\n[pos': 24309, ']\\n[': 24310, ']\\n[agr': 24311, '[per': 24312, ']]\\n[': 24313, '[num': 24314, '[gnd': 24315, 'fem': 24316, ']]\\n\\n\\n\\n\\nfigure': 24317, 'matrix\\n\\nin': 24318, 'approaches\\nfor': 24319, 'avms;': 24320, '.\\nathough': 24321, 'rendered': 24322, 'less\\nvisually': 24323, 'pleasing,': 24324, 'stick': 24325, 'it\\ncorresponds': 24326, '.\\n\\non': 24327, 'no\\nparticular': 24328, 'significance': 24329, 'to:\\n\\n': 24330, '(17)\\n[agr': 24331, ']\\n[pos': 24332, ']\\n\\n\\nonce': 24333, 'possibility': 24334, 'agr,': 24335, 'are\\nbundled': 24336, '(18)\\ns': 24337, 'np[agr=?n]': 24338, 'vp[agr=?n]\\nnp[agr=?n]': 24339, 'propn[agr=?n]\\nvp[tense=?t,': 24340, 'agr=?n]': 24341, 'cop[tense=?t,': 24342, 'adj\\n\\ncop[tense=pres,': 24343, 'agr=[num=sg,': 24344, 'per=3]]': 24345, \"'is'\\npropn[agr=[num=sg,\": 24346, \"'kim'\\nadj\": 24347, \"'happy'\\n\\n\\n\\n\\n\\n2\\xa0\\xa0\\xa0processing\": 24348, 'structures\\nin': 24349, 'be\\nconstructed': 24350, 'the\\nfundamental': 24351, 'the\\ninformation': 24352, 'the\\nfeatstruct()': 24353, 'or\\nintegers': 24354, 'fs1': 24355, \".featstruct(tense='past',\": 24356, \"num='sg')\\n>>>\": 24357, 'print(fs1)\\n[': 24358, \"'sg'\": 24359, \"'past'\": 24360, ']\\n\\n\\n\\na': 24361, 'dictionary,\\nand': 24362, '.featstruct(per=3,': 24363, \"num='pl',\": 24364, \"gnd='fem')\\n>>>\": 24365, \"print(fs1['gnd'])\\nfem\\n>>>\": 24366, \"fs1['case']\": 24367, \"'acc'\\n\\n\\n\\nwe\": 24368, 'as\\ndiscussed': 24369, 'fs2': 24370, \".featstruct(pos='n',\": 24371, 'agr=fs1)\\n>>>': 24372, 'print(fs2)\\n[': 24373, \"'acc'\": 24374, 'gnd': 24375, \"'fem'\": 24376, \"'pl'\": 24377, ']\\n>>>': 24378, \"print(fs2['agr'])\\n[\": 24379, \"print(fs2['agr']['per'])\\n3\\n\\n\\n\\nan\": 24380, 'feature-value': 24381, 'format\\nfeature=value,': 24382, 'structures:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 24383, \".featstruct([pos='n',\": 24384, 'agr=[per=3,': 24385, \"gnd='fem']]))\\n[\": 24386, ']\\n\\n\\n\\nfeature': 24387, 'inherently': 24388, 'objects;': 24389, 'are\\ngeneral': 24390, 'structure:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 24391, \".featstruct(name='lee',\": 24392, \"telno='01\": 24393, '42': 24394, \"96',\": 24395, 'age=33))\\n[': 24396, \"'lee'\": 24397, 'telno': 24398, \"'01\": 24399, \"96'\": 24400, ']\\n\\n\\n\\nin': 24401, 'this\\nto': 24402, 'divert': 24403, 'language,\\nbut': 24404, 'lay': 24405, 'groundwork': 24406, 'tight!\\nit': 24407, 'graphs;': 24408, 'acyclic': 24409, '(dags)': 24410, '.\\n(19)': 24411, 'avm': 24412, '(19)\\nthe': 24413, 'arcs,': 24414, '.\\njust': 24415, 'complex:\\n\\n': 24416, '(20)\\nwhen': 24417, 'graphs,': 24418, 'of\\npaths': 24419, 'arcs\\nthat': 24420, 'as\\ntuples': 24421, \"('address',\": 24422, \"'street')\": 24423, 'value\\nin': 24424, '(20)': 24425, \"'rue\": 24426, \"pascal'\": 24427, 'spouse': 24428, \"and\\nkim's\": 24429, \"lee's\": 24430, '(21)': 24431, '(21)\\nhowever,': 24432, 'address\\ninformation': 24433, 'same\\nsub-graph': 24434, 'arcs:\\n\\n': 24435, '(22)\\nin': 24436, \"('address')\": 24437, '(22)': 24438, \"('spouse',\": 24439, \"'address')\": 24440, 'dags\\nsuch': 24441, 'sharing': 24442, 'or\\nreentrancy': 24443, 'reentrancy': 24444, 'matrix-style': 24445, 'representations,': 24446, 'will\\nprefix': 24447, 'structure\\nwith': 24448, 'notation\\n->(1),': 24449, \".featstruct([name='lee',\": 24450, 'address=(1)[number=74,': 24451, \"street='rue\": 24452, \"pascal'],\\n\": 24453, \"spouse=[name='kim',\": 24454, 'address->(1)]]))\\n[': 24455, ']\\n\\n\\n\\nthe': 24456, 'a\\ncoindex': 24457, \".featstruct([a='a',\": 24458, \"b=(1)[c='c'],\": 24459, 'd->(1),': 24460, 'e->(1)]))\\n[': 24461, \"'c'\": 24462, ']\\n\\n\\n\\n\\n\\n\\n2': 24463, '.1\\xa0\\xa0\\xa0subsumption': 24464, 'unification\\nit': 24465, 'partial\\ninformation': 24466, 'order\\nfeature': 24467, 'example,\\n(23a)': 24468, '(23b),': 24469, '(23c)': 24470, '(23)\\n': 24471, '.\\n[number': 24472, '74]\\n\\n\\n\\n': 24473, ']\\n[street': 24474, \"pascal']\\n\\n\\n\\n\": 24475, \"pascal']\\n[city\": 24476, \"'paris'\": 24477, ']\\n\\n\\n\\nthis': 24478, 'subsumption;': 24479, 'fs0': 24480, 'subsumes': 24481, '⊑': 24482, 'subsumption': 24483, 'reentrancy,': 24484, 'careful\\nabout': 24485, 'subsumption:': 24486, 'if\\nfs0': 24487, 'fs1,': 24488, 'the\\npaths': 24489, 'reentrancies': 24490, 'subsumes\\n(22),': 24491, 'on\\nfeature': 24492, 'are\\nincommensurable': 24493, '(24)': 24494, 'subsumed\\nby': 24495, '(23a)': 24496, '(24)\\n[telno': 24497, '01': 24498, '96]\\n\\n\\nso': 24499, 'structure?\\nfor': 24500, 'should\\nconsist': 24501, 'a\\ncity': 24502, '(25b)': 24503, '(25a)': 24504, 'to\\nyield': 24505, '(25c)': 24506, '(25)\\n': 24507, '.\\n\\nmerging': 24508, 'called\\nunification': 24509, 'unify()': 24510, '.featstruct(number=74,': 24511, \"pascal')\\n>>>\": 24512, \".featstruct(city='paris')\\n>>>\": 24513, 'print(fs1': 24514, '.unify(fs2))\\n[': 24515, ']\\n\\n\\n\\nunification': 24516, 'formally': 24517, '(partial)': 24518, 'operation:\\nfs0': 24519, '⊔\\nfs1': 24520, '.\\nunification': 24521, 'symmetric,': 24522, 'so\\nfs0': 24523, '⊔\\nfs0': 24524, 'print(fs2': 24525, '.unify(fs1))\\n[': 24526, ']\\n\\n\\n\\n\\n\\nif': 24527, 'unify': 24528, 'subsumption\\nrelationship,': 24529, 'unification': 24530, 'two:\\n\\n': 24531, '(26)if': 24532, 'fs0\\n⊔': 24533, 'fs1\\nfor': 24534, 'unifying': 24535, '(23b)': 24536, 'π,\\nbut': 24537, 'π': 24538, 'distinct\\natom': 24539, \".featstruct(a='a')\\n>>>\": 24540, \".featstruct(a='b')\\n>>>\": 24541, '.unify(fs1)\\n>>>': 24542, 'print(fs2)\\nnone\\n\\n\\n\\nnow,': 24543, 'interacts': 24544, 'structure-sharing,\\nthings': 24545, '.featstruct([name=lee,\\n': 24546, 'address=[number=74,\\n': 24547, 'spouse=': 24548, '[name=kim,\\n': 24549, \"pascal']]])\\n>>>\": 24550, 'print(fs0)\\n[': 24551, ']\\n\\n\\n\\nwhat': 24552, 'specification\\nfor': 24553, 'city?': 24554, '.featstruct([spouse': 24555, '[address': 24556, '[city': 24557, 'paris]]])\\n>>>': 24558, '.unify(fs0))\\n[': 24559, ']\\n\\n\\n\\nby': 24560, 'unified': 24561, 'structure-sharing': 24562, 'graph\\n(22)):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 24563, '.featstruct([name=lee,': 24564, 'spouse=[name=kim,': 24565, 'address->(1)]])\\n>>>': 24566, ']\\n\\n\\n\\nrather': 24567, 'address,\\nwe': 24568, 'some\\npath': 24569, 'π,': 24570, 'stated\\nusing': 24571, '?x': 24572, '.featstruct([address1=[number=74,': 24573, \"pascal']])\\n>>>\": 24574, '.featstruct([address1=?x,': 24575, 'address2=?x])\\n>>>': 24576, 'address1': 24577, 'address2': 24578, ']\\n\\n\\n\\n\\n\\n\\n3\\xa0\\xa0\\xa0extending': 24579, 'grammar\\nin': 24580, 'explore\\na': 24581, 'issues,': 24582, 'benefits\\nof': 24583, '.1\\xa0\\xa0\\xa0subcategorization\\nin': 24584, 'augmented': 24585, 'labels\\niv': 24586, 'intransitive': 24587, 'verbs\\nrespectively': 24588, 'the\\nfollowing:\\n\\n': 24589, '(27)\\nvp': 24590, 'iv\\nvp': 24591, 'np\\n\\n\\nalthough': 24592, 'v,\\nthey': 24593, 'nonterminal': 24594, 'distinct\\nfrom': 24595, \"doesn't\\nlet\": 24596, 'say\\nall': 24597, 'tense,\\nsince': 24598, 'walk,': 24599, 'iv,': 24600, 'iv\\nby': 24601, 'object\\nor': 24602, 'complement?\\na': 24603, 'framework\\ncalled': 24604, 'generalized': 24605, '(gpsg),': 24606, 'solve\\nthis': 24607, 'lexical\\ncategories': 24608, 'subcat': 24609, 'subcategorization\\nclass': 24610, 'gpsg': 24611, 'for\\nsubcat,': 24612, 'adopts': 24613, 'mnemonic': 24614, 'namely\\nintrans,': 24615, 'trans': 24616, 'clause:\\n\\n': 24617, '(28)\\nvp[tense=?t,': 24618, 'v[subcat=intrans,': 24619, 'tense=?t,': 24620, 'v[subcat=trans,': 24621, 'np\\nvp[tense=?t,': 24622, 'v[subcat=clause,': 24623, 'sbar\\n\\nv[subcat=intrans,': 24624, 'tense=pres,': 24625, \"'walks'\\nv[subcat=trans,\": 24626, \"'likes'\\nv[subcat=clause,\": 24627, \"'says'\": 24628, \"'claims'\\n\\nv[subcat=intrans,\": 24629, \"'walk'\\nv[subcat=trans,\": 24630, \"'like'\\nv[subcat=clause,\": 24631, \"'say'\": 24632, \"'claim'\\n\\nv[subcat=intrans,\": 24633, 'tense=past,': 24634, \"'walked'\\nv[subcat=trans,\": 24635, \"'liked'\\nv[subcat=clause,\": 24636, \"'claimed'\\n\\n\\nwhen\": 24637, 'v[subcat=trans],': 24638, 'can\\ninterpret': 24639, 'pointer': 24640, 'in\\nwhich': 24641, 'v[subcat=trans]': 24642, 'a\\nvp': 24643, 'correspondence': 24644, 'lexical\\nheads': 24645, 'lexical\\ncategories;': 24646, 'subcat\\nvalue': 24647, 'required,': 24648, 'walk': 24649, 'belong': 24650, 'in\\nvps': 24651, 'subcat=intrans\\non': 24652, 'a\\nsubcat=trans': 24653, 'category\\nsbar': 24654, 'the\\ncomplement': 24655, 'like\\nchildren': 24656, '(29)\\nsbar': 24657, 'comp': 24658, 's\\ncomp': 24659, \"'that'\\n\\n\\nthe\": 24660, '(30)\\nan': 24661, 'framework\\nknown': 24662, 'categorial': 24663, 'patr\\nand': 24664, 'using\\nsubcat': 24665, 'encodes': 24666, 'of\\narguments': 24667, 'with)': 24668, 'like\\nput': 24669, '(put': 24670, 'table)': 24671, '(31):\\n\\n\\n': 24672, '(31)\\nv[subcat=<np,': 24673, 'pp>]\\n\\n\\nthis': 24674, 'everything\\nelse': 24675, 'comprises': 24676, 'the\\nsubcategorized-for': 24677, 'combined\\nwith': 24678, 'complements,': 24679, 'requirements': 24680, 'discharged,': 24681, 'is\\nneeded': 24682, 'traditionally\\nthought': 24683, '(32)\\nv[subcat=<np>]\\n\\n\\nfinally,': 24684, 'verbal': 24685, 'no\\nrequirements': 24686, 'hence': 24687, 'subcat\\nwhose': 24688, '(33)': 24689, 'these\\ncategory': 24690, '(33)\\n\\n\\n3': 24691, '.2\\xa0\\xa0\\xa0heads': 24692, 'revisited\\n\\nwe': 24693, 'factoring': 24694, 'subcategorization\\ninformation': 24695, 'more\\ngeneralizations': 24696, 'this\\nkind': 24697, 'following:': 24698, 'of\\nphrases': 24699, 'similarly,\\nns': 24700, 'nps,\\nas': 24701, 'adjectives)': 24702, 'aps,': 24703, 'and\\nps': 24704, 'prepositions)': 24705, 'pps': 24706, '.\\nnot': 24707, 'coordinate\\nphrases': 24708, 'bell)': 24709, '—\\nnevertheless,': 24710, 'formalism': 24711, 'head-child': 24712, 'features\\n(as': 24713, 'tv)': 24714, '.\\nx-bar': 24715, 'addresses\\nthis': 24716, 'abstracting': 24717, 'phrasal': 24718, 'is\\nusual': 24719, \"n'\": 24720, 'up,\\ncorresponding': 24721, 'nom,': 24722, \"while\\nn''\": 24723, '(34a)': 24724, 'a\\nrepresentative': 24725, '(34b)': 24726, 'counterpart': 24727, '(34)\\n': 24728, \"n'\\nand\": 24729, \"n''\": 24730, '(phrasal)': 24731, 'projections': 24732, \"n''\\nis\": 24733, 'maximal': 24734, 'projection,': 24735, 'the\\nzero': 24736, 'projection': 24737, 'that\\ndirectly': 24738, 'always\\nplaced': 24739, 'are\\nplaced': 24740, \"x'\": 24741, 'the\\nconfiguration': 24742, \"p''\": 24743, '(35)': 24744, 'contrasts': 24745, 'that\\nof': 24746, '(35)\\nthe': 24747, '(36)': 24748, 'encoded\\nusing': 24749, 'is\\nachieved': 24750, 'n[bar=1]': 24751, '(36)\\ns': 24752, 'n[bar=2]': 24753, 'v[bar=2]\\nn[bar=2]': 24754, 'n[bar=1]\\nn[bar=1]': 24755, 'p[bar=2]\\nn[bar=1]': 24756, 'n[bar=0]': 24757, 'n[bar=0]xs\\n\\n\\n\\n\\n3': 24758, '.3\\xa0\\xa0\\xa0auxiliary': 24759, 'inversion\\ninverted': 24760, 'is\\nswitched': 24761, 'interrogatives': 24762, \"after\\n'negative'\": 24763, 'adverbs:\\n\\n': 24764, '(37)\\n': 24765, '.do': 24766, 'children?\\n\\n': 24767, '.can': 24768, 'jody': 24769, 'walk?\\n\\n\\n': 24770, '(38)\\n': 24771, '.rarely': 24772, '.never': 24773, 'pre-subject': 24774, 'position:\\n\\n': 24775, '(39)\\n': 24776, '.*like': 24777, '.*walks': 24778, 'jody?\\n\\n\\n': 24779, '(40)\\n': 24780, '.*rarely': 24781, '.*never': 24782, '.\\n\\nverbs': 24783, 'positioned': 24784, 'auxiliaries,': 24785, 'do,\\ncan': 24786, 'and\\nshall': 24787, 'production:\\n\\n': 24788, '(41)\\ns[+inv]': 24789, 'v[+aux]': 24790, 'vp\\n\\n\\nthat': 24791, '[+inv]': 24792, 'auxiliary\\nverb': 24793, 'depending\\non': 24794, '(42)': 24795, 'an\\ninverted': 24796, '(42)\\n\\n\\n3': 24797, '.4\\xa0\\xa0\\xa0unbounded': 24798, 'constructions\\nconsider': 24799, 'contrasts:\\n\\n': 24800, '(43)\\n': 24801, '.you': 24802, '.*you': 24803, '(44)\\n': 24804, 'slot': 24805, 'complement,': 24806, 'while\\nput': 24807, '.\\n(43)': 24808, '(44)': 24809, 'obligatory:\\nomitting': 24810, 'ungrammaticality': 24811, '(45)': 24812, '(46)\\nillustrate': 24813, '(45)\\n': 24814, 'music,': 24815, '(46)\\n': 24816, '.which': 24817, 'slot?\\n\\n': 24818, 'into?\\n\\nthat': 24819, 'word\\nwho': 24820, '(45a),': 24821, 'preposed': 24822, 'music': 24823, '(45b),': 24824, 'card/slot': 24825, '(46)': 24826, 'to\\nsay': 24827, '–': 24828, 'where\\nthe': 24829, 'are\\nsometimes': 24830, 'underscore:\\n\\n': 24831, '(47)\\n': 24832, '__': 24833, '__?\\n\\nso,': 24834, 'gap': 24835, 'conversely,\\nfillers': 24836, '(48)\\n': 24837, '.*kim': 24838, 'hip-hop': 24839, '(49)\\n': 24840, '.*which': 24841, 'one?\\n\\nthe': 24842, 'co-occurence': 24843, 'termed': 24844, 'considerable': 24845, 'theoretical\\nlinguistics': 24846, 'nature': 24847, 'intervene\\nbetween': 24848, 'licenses;': 24849, 'answer\\nis': 24850, 'no:': 24851, 'and\\ngap': 24852, 'involving\\nsentential': 24853, '(50)': 24854, '(50)\\n': 24855, '.who': 24856, '__?\\n\\n': 24857, '__?\\n\\nsince': 24858, 'sentential\\ncomplements,': 24859, 'whole\\nsentence': 24860, 'constellation': 24861, 'an\\nunbounded': 24862, 'construction;': 24863, 'filler-gap\\ndependency': 24864, 'between\\nfiller': 24865, 'mechanisms': 24866, 'unbounded\\ndependencies': 24867, 'grammars;': 24868, 'to\\ngeneralized': 24869, 'involves\\nslash': 24870, 'y/xp;\\nwe': 24871, 'that\\nis': 24872, 'sub-constituent': 24873, 'xp': 24874, 'example,\\ns/np': 24875, 'of\\nslash': 24876, '(51)': 24877, '(51)\\nthe': 24878, '(treated': 24879, 'np[+wh])': 24880, 'a\\ncorresponding': 24881, 'gap-containing': 24882, 's/np': 24883, 'is\\nthen': 24884, 'percolated': 24885, 'vp/np': 24886, 'it\\nreaches': 24887, 'np/np': 24888, 'dependency\\nis': 24889, 'discharged': 24890, 'string,\\nimmediately': 24891, 'dominated': 24892, 'of\\nobject?': 24893, 'fortunately,': 24894, 'framework,\\nby': 24895, 'right\\nas': 24896, 'value;': 24897, 'reducible': 24898, 's[slash=np]': 24899, 'practice,\\nthis': 24900, 'illustrates\\nthe': 24901, 'principles': 24902, 'for\\ninverted': 24903, \".show_cfg('grammars/book_grammars/feat1\": 24904, '###################\\ns[-inv]': 24905, 'vp\\ns[-inv]/?x': 24906, 'vp/?x\\ns[-inv]': 24907, 's/np\\ns[-inv]': 24908, 'adv[+neg]': 24909, 's[+inv]\\ns[+inv]': 24910, 'vp\\ns[+inv]/?x': 24911, 'vp/?x\\nsbar': 24912, 's[-inv]\\nsbar/?x': 24913, 's[-inv]/?x\\nvp': 24914, '-aux]\\nvp': 24915, 'np\\nvp/?x': 24916, 'np/?x\\nvp': 24917, 'sbar\\nvp/?x': 24918, 'sbar/?x\\nvp': 24919, 'vp\\nvp/?x': 24920, 'vp/?x\\n#': 24921, '###################\\nv[subcat=intrans,': 24922, \"'walk'\": 24923, \"'sing'\\nv[subcat=trans,\": 24924, \"'claim'\\nv[+aux]\": 24925, \"'do'\": 24926, \"'can'\\nnp[-wh]\": 24927, \"'you'\": 24928, \"'cats'\\nnp[+wh]\": 24929, \"'who'\\nadv[+neg]\": 24930, \"'rarely'\": 24931, \"'never'\\nnp/np\": 24932, '->\\ncomp': 24933, \"'that'\\n\\n\\nexample\": 24934, '(code_slashcfg': 24935, 'and\\nlong-distance': 24936, 'dependencies,': 24937, 'categories\\n\\nthe': 24938, 'gap-introduction\\nproduction,': 24939, 's[-inv]': 24940, 'percolate': 24941, 'add\\nslashes': 24942, 'productions\\nthat': 24943, 'vp/?x': 24944, 'sbar/?x': 24945, 'slashed': 24946, 'and\\nsays': 24947, 'a\\nconstituent': 24948, 'sbar\\nchild': 24949, 'you\\nlike\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 24950, \"like'\": 24951, 'load_parser\\n>>>': 24952, \"load_parser('grammars/book_grammars/feat1\": 24953, \".fcfg')\\n>>>\": 24954, 'print(tree)\\n(s[-inv]\\n': 24955, '(np[+wh]': 24956, 'who)\\n': 24957, '(s[+inv]/np[]\\n': 24958, '(v[+aux]': 24959, 'do)\\n': 24960, '(np[-wh]': 24961, 'you)\\n': 24962, '(vp[]/np[]\\n': 24963, '(v[-aux,': 24964, \"subcat='clause']\": 24965, 'claim)\\n': 24966, '(sbar[]/np[]\\n': 24967, '(comp[]': 24968, 'that)\\n': 24969, '(s[-inv]/np[]\\n': 24970, '(vp[]/np[]': 24971, \"subcat='trans']\": 24972, 'like)': 24973, '(np[]/np[]': 24974, ')))))))\\n\\n\\n\\n\\na': 24975, '(52)': 24976, '(52)\\nthe': 24977, 'sentences\\nwithout': 24978, 'gaps:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 24979, \"cats'\": 24980, '(vp[]\\n': 24981, '(sbar[]\\n': 24982, '(s[-inv]\\n': 24983, '(vp[]': 24984, 'cats))))))\\n\\n\\n\\nin': 24985, 'admits': 24986, 'involve\\nwh': 24987, 'constructions:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 24988, \"'rarely\": 24989, \"sing'\": 24990, '(adv[+neg]': 24991, 'rarely)\\n': 24992, '(s[+inv]\\n': 24993, \"subcat='intrans']\": 24994, 'sing))))\\n\\n\\n\\n\\n\\n3': 24995, '.5\\xa0\\xa0\\xa0case': 24996, 'german\\ncompared': 24997, 'for\\nagreement': 24998, 'varies': 24999, 'with\\ncase,': 25000, '.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ncase\\nmasc\\nfem\\nneut\\nplural\\n\\nnom\\nder\\ndie\\ndas\\ndie\\n\\ngen\\ndes\\nder\\ndes\\nder\\n\\ndat\\ndem\\nder\\ndem\\nden\\n\\nacc\\nden\\ndie\\ndas\\ndie\\n\\n\\ntable': 25001, 'article\\n\\n\\nsubjects': 25002, 'nominative': 25003, 'verbs\\ngovern': 25004, 'accusative': 25005, 'are\\nexceptions': 25006, 'helfen': 25007, 'case:\\n\\n': 25008, '(53)\\nsystem': 25009, '1693)\\nerror': 25010, '1698)\\nerror': 25011, '1702)\\nerror': 25012, '1707)\\nerror': 25013, 'interaction': 25014, 'agreement\\n(comprising': 25015, \".show_cfg('grammars/book_grammars/german\": 25016, 's\\n': 25017, 'np[case=nom,': 25018, 'agr=?a]': 25019, 'vp[agr=?a]\\n': 25020, 'np[case=?c,': 25021, 'pro[case=?c,': 25022, 'agr=?a]\\n': 25023, 'det[case=?c,': 25024, 'n[case=?c,': 25025, 'vp[agr=?a]': 25026, 'iv[agr=?a]\\n': 25027, 'tv[objcase=?c,': 25028, 'np[case=?c]\\n': 25029, 'determiners\\n': 25030, 'masc\\n': 25031, 'det[case=nom,': 25032, 'agr=[gnd=masc,per=3,num=sg]]': 25033, \"'der'\\n\": 25034, 'det[case=dat,': 25035, \"'dem'\\n\": 25036, 'det[case=acc,': 25037, \"'den'\\n\": 25038, 'fem\\n': 25039, 'agr=[gnd=fem,per=3,num=sg]]': 25040, \"'die'\\n\": 25041, 'agr=[per=3,num=pl]]': 25042, 'n[agr=[gnd=masc,per=3,num=sg]]': 25043, \"'hund'\\n\": 25044, 'n[case=nom,': 25045, 'agr=[gnd=masc,per=3,num=pl]]': 25046, \"'hunde'\\n\": 25047, 'n[case=dat,': 25048, \"'hunden'\\n\": 25049, 'n[case=acc,': 25050, 'n[agr=[gnd=fem,per=3,num=sg]]': 25051, \"'katze'\\n\": 25052, 'n[agr=[gnd=fem,per=3,num=pl]]': 25053, \"'katzen'\\n\": 25054, 'pronouns\\n': 25055, 'pro[case=nom,': 25056, 'agr=[per=1,num=sg]]': 25057, \"'ich'\\n\": 25058, 'pro[case=acc,': 25059, \"'mich'\\n\": 25060, 'pro[case=dat,': 25061, \"'mir'\\n\": 25062, 'agr=[per=2,num=sg]]': 25063, \"'du'\\n\": 25064, 'agr=[per=3,num=sg]]': 25065, \"'er'\": 25066, \"'sie'\": 25067, 'agr=[per=1,num=pl]]': 25068, \"'wir'\\n\": 25069, \"'uns'\\n\": 25070, 'agr=[per=2,num=pl]]': 25071, \"'ihr'\\n\": 25072, \"'sie'\\n\": 25073, 'verbs\\n': 25074, 'iv[agr=[num=sg,per=1]]': 25075, \"'komme'\\n\": 25076, 'iv[agr=[num=sg,per=2]]': 25077, \"'kommst'\\n\": 25078, 'iv[agr=[num=sg,per=3]]': 25079, \"'kommt'\\n\": 25080, 'iv[agr=[num=pl,': 25081, 'per=1]]': 25082, \"'kommen'\\n\": 25083, 'per=2]]': 25084, 'tv[objcase=acc,': 25085, 'agr=[num=sg,per=1]]': 25086, \"'sehe'\": 25087, \"'mag'\\n\": 25088, 'agr=[num=sg,per=2]]': 25089, \"'siehst'\": 25090, \"'magst'\\n\": 25091, 'agr=[num=sg,per=3]]': 25092, \"'sieht'\": 25093, 'tv[objcase=dat,': 25094, \"'folge'\": 25095, \"'helfe'\\n\": 25096, \"'folgst'\": 25097, \"'hilfst'\\n\": 25098, \"'folgt'\": 25099, \"'hilft'\\n\": 25100, 'agr=[num=pl,per=1]]': 25101, \"'sehen'\": 25102, \"'moegen'\\n\": 25103, 'agr=[num=pl,per=2]]': 25104, \"'moegt'\\n\": 25105, 'agr=[num=pl,per=3]]': 25106, \"'folgen'\": 25107, \"'helfen'\\n\": 25108, \"'helft'\\n\": 25109, \"'helfen'\\n\\n\\nexample\": 25110, '(code_germancfg': 25111, 'objcase': 25112, 'governs': 25113, \"'ich\": 25114, 'folge': 25115, 'den': 25116, \"katzen'\": 25117, \"load_parser('grammars/book_grammars/german\": 25118, \"(np[agr=[num='sg',\": 25119, 'per=1],': 25120, \"case='nom']\\n\": 25121, \"(pro[agr=[num='sg',\": 25122, \"case='nom']\": 25123, 'ich))\\n': 25124, \"(vp[agr=[num='sg',\": 25125, 'per=1]]\\n': 25126, \"(tv[agr=[num='sg',\": 25127, \"objcase='dat']\": 25128, 'folge)\\n': 25129, \"(np[agr=[gnd='fem',\": 25130, 'per=3],': 25131, \"case='dat']\\n\": 25132, \"(det[agr=[num='pl',\": 25133, \"case='dat']\": 25134, 'den)\\n': 25135, \"(n[agr=[gnd='fem',\": 25136, 'katzen))))\\n\\n\\n\\nin': 25137, 'as\\nchallenging': 25138, 'idea\\nwhere': 25139, 'trace\\nparameter': 25140, 'load_parser()': 25141, 'failure:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 25142, \"katze'\": 25143, 'trace=2)\\n>>>': 25144, 'print(tree)\\n|': 25145, '.ich': 25146, '.fol': 25147, '.den': 25148, '.kat': 25149, 'rule:\\n|[---]': 25150, \"'ich'\\n|\": 25151, '[---]': 25152, \"'folge'\\n|\": 25153, \"'den'\\n|\": 25154, '[---]|': 25155, '[3:4]': 25156, \"'katze'\\nfeature\": 25157, \"pro[agr=[num='sg',\": 25158, \"'ich'\": 25159, \"np[agr=[num='sg',\": 25160, 'rule:\\n|[--->': 25161, 'np[agr=?a,': 25162, '{?a:': 25163, \"[num='sg',\": 25164, 'per=1]}\\nfeature': 25165, \"tv[agr=[num='sg',\": 25166, '[--->': 25167, 'tv[agr=?a,': 25168, 'objcase=?c]': 25169, 'np[case=?c]': 25170, '?c:': 25171, \"'dat'}\\nfeature\": 25172, \"det[agr=[gnd='masc',\": 25173, \"num='sg',\": 25174, \"case='acc']\": 25175, \"'den'\": 25176, '*\\n|': 25177, \"det[agr=[num='pl',\": 25178, 'case=?c]': 25179, 'det[agr=?a,': 25180, 'n[agr=?a,': 25181, \"[num='pl',\": 25182, \"[gnd='masc',\": 25183, \"'acc'}\\nfeature\": 25184, \"n[agr=[gnd='fem',\": 25185, \"'katze'\": 25186, '*\\n\\n\\n\\nthe': 25187, 'scanner': 25188, 'recognized': 25189, 'as\\nadmitting': 25190, 'categories:': 25191, \"num='sg',\\nper=3],\": 25192, 'katze': 25193, 'category\\nn[agr=[gnd=fem,': 25194, 'num=sg,': 25195, 'binding': 25196, '?a': 25197, 'det[case=?c,\\nagr=?a]': 25198, 'constraints,': 25199, 'the\\nagr': 25200, 'agr\\nvalues': 25201, 'den,': 25202, \"num='sg',\\nper=3]\": 25203, 'per=3]': 25204, '.\\n\\n\\n\\n4\\xa0\\xa0\\xa0summary\\n\\nthe': 25205, 'atomic\\nsymbols': 25206, 'capture\\nfine-grained': 25207, 'otherwise': 25208, 'massive\\nmultiplication': 25209, 'constraints\\nin': 25210, 'realization': 25211, 'feature\\nspecifications': 25212, 'inter-dependent': 25213, '.\\ntypically': 25214, 'level\\nand': 25215, 'constrain': 25216, 'sub-case': 25217, 'of\\natomic': 25218, 'as\\n[+/-': 25219, 'f]': 25220, '(either': 25221, 'or\\ncomplex)': 25222, 'be\\nre-entrant': 25223, '(or\\ntags)': 25224, 'avms': 25225, 'features\\ncorresponding': 25226, 'graph\\nrepresentation': 25227, '.\\nfs0': 25228, 'when\\nall': 25229, 'in\\nfs0': 25230, 'in\\nfs1': 25231, 'and\\nfs1,': 25232, 'successful,': 25233, 'combined\\ninformation': 25234, 'fs,': 25235, 'also\\nadds': 25236, \"π'\": 25237, 'wide\\nvariety': 25238, 'phenomena,': 25239, 'subcategorization,\\ninversion': 25240, '.\\n\\n\\n\\n5\\xa0\\xa0\\xa0further': 25241, 'including\\nfeature': 25242, 'suites': 25243, 'syntax:': 25244, 'see\\n(corbett,': 25245, 'earliest': 25246, 'designed\\nto': 25247, 'phonological': 25248, 'sound\\nlike': 25249, '/b/': 25250, '[+labial,': 25251, '+voice]': 25252, 'capture\\ngeneralizations': 25253, 'segments;': 25254, '/n/': 25255, 'gets\\nrealized': 25256, '/m/': 25257, '+labial': 25258, 'chomskyan': 25259, 'for\\nphenomena': 25260, 'generalizations': 25261, 'across\\nsyntactic': 25262, 'was\\nadvocated': 25263, '(gpsg;\\n(gazdar,': 25264, '1985)),': 25265, '.\\ncoming': 25266, 'perspective': 25267, 'linguistics,\\n(dahl': 25268, 'saint-dizier,': 25269, '1985)': 25270, 'be\\ncaptured': 25271, 'similar\\napproach': 25272, 'elaborated': 25273, '(grosz': 25274, 'stickel,': 25275, '1983)': 25276, 'patr-ii\\nformalism': 25277, 'lexical-functional': 25278, '(lfg;\\n(bresnan,': 25279, '1982))': 25280, 'f-structure': 25281, 'that\\nwas': 25282, 'primarily': 25283, 'and\\npredicate-argument': 25284, 'structure\\nparse': 25285, '(shieber,': 25286, 'this\\nphase': 25287, 'algebraic': 25288, 'researchers': 25289, 'attempted': 25290, 'negation': 25291, 'an\\nalternative': 25292, 'pioneered': 25293, '(kasper': 25294, 'rounds,': 25295, 'and\\n(johnson,': 25296, '1988),': 25297, 'argues': 25298, 'descriptions': 25299, 'of\\nfeature': 25300, 'these\\ndescriptions': 25301, 'as\\nconjunction,': 25302, 'over\\nfeature': 25303, 'description-oriented': 25304, 'was\\nintegral': 25305, '(huang': 25306, 'chen,': 25307, '1989),': 25308, 'later\\nversions': 25309, '(hpsg;\\n(sag': 25310, 'wasow,': 25311, '1999))': 25312, 'bibliography': 25313, 'be\\nfound': 25314, '.cl': 25315, '.uni-bremen': 25316, '.de/hpsg-bib/': 25317, 'to\\ncapture': 25318, 'example,\\nthere': 25319, 'permissible': 25320, 'for\\nnum': 25321, '[num=masc]': 25322, 'anomalous': 25323, 'say\\nthat': 25324, 'contain\\nspecifications': 25325, 'and\\ngnd,': 25326, 'as\\n[subcat=trans]': 25327, 'to\\nremedy': 25328, 'deficiency': 25329, 'stipulate': 25330, 'just\\nare': 25331, 'hierarchically,': 25332, 'more\\ninformative': 25333, 'subtype\\nof': 25334, 'num,': 25335, 'are\\nthemselves': 25336, 'only\\nper,': 25337, 'typed\\nfeature': 25338, '(emele': 25339, 'zajac,': 25340, '1990)': 25341, 'examination': 25342, 'foundations': 25343, '(carpenter,': 25344, '1992),': 25345, 'while\\n(copestake,': 25346, 'focuses': 25347, 'implementing': 25348, 'hpsg-oriented': 25349, 'copious': 25350, 'within\\nfeature': 25351, '(nerbonne,': 25352, 'netter,': 25353, 'pollard,': 25354, '1994)': 25355, 'good\\nstarting': 25356, 'while\\n(m{\\\\u}ller,': 25357, 'of\\ngerman': 25358, 'structures,\\nthe': 25359, 'integration': 25360, 'into\\nparsing': 25361, '.\\n\\n\\n6\\xa0\\xa0\\xa0exercises\\n\\n☼': 25362, 'am\\nhappy': 25363, '*you': 25364, 'or\\n*they': 25365, 'happy?': 25366, 'tense\\nparadigm': 25367, 'grammar\\n(6)': 25368, '(18)\\nas': 25369, 'variant': 25370, 'below:\\n\\n': 25371, '(54)\\n': 25372, 'sings': 25373, '.*boy': 25374, '(55)\\n': 25375, 'boys': 25376, '.boys': 25377, '(56)\\n': 25378, '(57)\\n': 25379, 'precious': 25380, '.water': 25381, 'subsumes()': 25382, '(28)': 25383, 'to\\nincorporate': 25384, 'the\\ntreatment': 25385, 'subcategorization': 25386, 'following\\nspanish': 25387, 'phrases:\\n\\nsystem': 25388, '2028)\\nerror': 25389, '.\\n\\n\\nsystem': 25390, '2033)\\nerror': 25391, '2038)\\nerror': 25392, '2043)\\nerror': 25393, 'earleychartparser': 25394, 'only\\nprints': 25395, '.\\n\\n\\n\\n\\n\\n\\xa0\\n\\nfs1': 25396, '.featstruct([a': 25397, '?x,': 25398, 'b=': 25399, '?x]])\\nfs2': 25400, '.featstruct([b': 25401, '[d': 25402, 'd]])\\nfs3': 25403, 'd]])\\nfs4': 25404, '(1)[b': 25405, 'b],': 25406, 'c->(1)])\\nfs5': 25407, '(1)[d': 25408, '?x],': 25409, '[e': 25410, '?x]': 25411, '])\\nfs6': 25412, 'd]])\\nfs7': 25413, 'd],': 25414, '[f': 25415, 'd]]])\\nfs8': 25416, '[b': 25417, '(1)]': 25418, '])\\nfs9': 25419, '[g': 25420, 'e]]])\\nfs10': 25421, '(1)])\\n\\n\\nexample': 25422, '(code_featstructures': 25423, 'structures\\n\\nwork': 25424, 'following\\nunifications': 25425, '.)\\n\\nfs1': 25426, 'fs2\\nfs1': 25427, 'fs3\\nfs4': 25428, 'fs5\\nfs5': 25429, 'fs6\\nfs5': 25430, 'fs7\\nfs8': 25431, 'fs9\\nfs8': 25432, 'fs10\\n\\ncheck': 25433, 'subsume': 25434, '[a=?x,': 25435, 'b=?x]': 25436, 'sharing,': 25437, 'unifying\\ntwo': 25438, 'can\\nhandle': 25439, 'verb-second': 25440, 'following:\\n\\n': 25441, '(58)heute': 25442, 'sieht': 25443, 'hund': 25444, 'different\\nsyntactic': 25445, '(levin,': 25446, 'patterns\\nof': 25447, 'loaded,': 25448, 'filled,': 25449, 'dumped\\nbelow': 25450, 'data?\\n\\n': 25451, '(59)\\n': 25452, 'cart': 25453, 'sand\\n\\n\\n': 25454, 'sand': 25455, 'cart\\n\\n\\n': 25456, 'dumped': 25457, 'cart\\n\\n\\n\\n★': 25458, 'paradigms': 25459, 'rarely': 25460, 'regular,': 25461, 'different\\nrealization': 25462, 'conjugation': 25463, 'the\\nlexeme': 25464, 'walks': 25465, 'the\\n3rd': 25466, 'singular,': 25467, 'of\\nperson': 25468, 'require\\nredundantly': 25469, 'morphological\\ncombinations': 25470, 'a\\nmethod': 25471, 'parent\\nnode': 25472, 'feature\\nthat': 25473, 'v\\nchild': 25474, '(gazdar,': 25475, 'are\\nsubcat': 25476, 'head\\nfeatures': 25477, 'predictable,': 25478, 'stated': 25479, 'explicitly\\nin': 25480, 'automatically\\naccounts': 25481, 'into\\nlist-valued': 25482, 'hpsg-style': 25483, 'of\\nsubcategorization,': 25484, 'whereby': 25485, 'the\\nconcatenation': 25486, \"complements'\": 25487, 'its\\nimmediate': 25488, 'with\\nunderspecified': 25489, '-->': 25490, 's/?x': 25491, '2002),\\nand': 25492, '.\\n\\n\\n\\n\\nabout': 25493, 'acst10': 25494, 'sentences\\n\\n\\n\\n\\n\\n10': 25495, 'sentences\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nwe': 25496, 'harness': 25497, 'the\\nmachinery': 25498, 'anything\\nsimilarly': 25499, 'sentences?\\nthe': 25500, 'computer\\ncan': 25501, 'representations?\\nhow': 25502, 'an\\nunlimited': 25503, 'to\\nstores': 25504, 'knowledge?\\n\\nalong': 25505, 'interrogating': 25506, 'facts\\nabout': 25507, '.\\n\\n1\\xa0\\xa0\\xa0natural': 25508, 'understanding\\n\\n1': 25509, '.1\\xa0\\xa0\\xa0querying': 25510, 'database\\nsuppose': 25511, 'answer:\\n\\n': 25512, 'athens': 25513, 'in?\\n\\n': 25514, '.greece': 25515, '.\\n\\nhow': 25516, 'program?': 25517, \"that\\nwe've\": 25518, 'new?\\nin': 25519, 'pretty\\nstraightforward': 25520, 'the\\nrepresentation': 25521, 'and\\ncountries': 25522, 'concrete,': 25523, 'database\\ntable': 25524, 'chat-80': 25525, 'system\\n(warren': 25526, 'pereira,': 25527, 'population': 25528, 'thousands,\\nbut': 25529, 'least\\nto': 25530, '1980s,': 25531, 'point\\nwhen': 25532, '(warren': 25533, '.\\n\\n\\n\\n\\n\\n\\n\\n\\ncity\\ncountry\\npopulation\\n\\n\\n\\nathens\\ngreece\\n1368\\n\\nbangkok\\nthailand\\n1178\\n\\nbarcelona\\nspain\\n1280\\n\\nberlin\\neast_germany\\n3481\\n\\nbirmingham\\nunited_kingdom\\n1112\\n\\n\\ntable': 25534, 'city_table:': 25535, 'cities,': 25536, 'countries': 25537, 'populations\\n\\n\\nthe': 25538, 'involves\\nwriting': 25539, '.\\n\\nnote\\nsql': 25540, '(structured': 25541, 'language)': 25542, 'for\\nretrieving': 25543, 'sql,\\nhttp://www': 25544, '.w3schools': 25545, '.com/sql/': 25546, 'online\\nreference': 25547, \"'greece':\\n\\n\": 25548, '(2)select': 25549, 'city_table': 25550, \"'athens'\\nthis\": 25551, 'column\\ncountry': 25552, \"is\\n'athens'\": 25553, 'query\\nsystem?': 25554, 'in\\n9': 25555, 'from\\nenglish': 25556, 'sql0': 25557, '.fcfg': 25558, 'assemble\\na': 25559, 'tandem': 25560, 'supplemented': 25561, 'recipe': 25562, 'for\\nconstructing': 25563, 'sem': 25564, 'these\\nrecipes': 25565, 'simple;': 25566, 'splice\\nthe': 25567, \".show_cfg('grammars/book_grammars/sql0\": 25568, 's\\ns[sem=(?np': 25569, '?vp)]': 25570, 'np[sem=?np]': 25571, 'vp[sem=?vp]\\nvp[sem=(?v': 25572, '?pp)]': 25573, 'iv[sem=?v]': 25574, 'pp[sem=?pp]\\nvp[sem=(?v': 25575, '?ap)]': 25576, 'ap[sem=?ap]\\nnp[sem=(?det': 25577, '?n)]': 25578, 'det[sem=?det]': 25579, 'n[sem=?n]\\npp[sem=(?p': 25580, '?np)]': 25581, 'p[sem=?p]': 25582, 'np[sem=?np]\\nap[sem=?pp]': 25583, 'a[sem=?a]': 25584, \"pp[sem=?pp]\\nnp[sem='country=greece']\": 25585, \"'greece'\\nnp[sem='country=china']\": 25586, \"'china'\\ndet[sem='select']\": 25587, \"'which'\": 25588, \"'what'\\nn[sem='city\": 25589, \"city_table']\": 25590, \"'cities'\\niv[sem='']\": 25591, \"'are'\\na[sem='']\": 25592, \"'located'\\np[sem='']\": 25593, \"'in'\\n\\n\\n\\nthis\": 25594, \"load_parser('grammars/book_grammars/sql0\": 25595, \"'what\": 25596, \"china'\\n>>>\": 25597, 'list(cp': 25598, '.parse(query': 25599, '.split()))\\n>>>': 25600, 'trees[0]': 25601, \".label()['sem']\\n>>>\": 25602, 's]\\n>>>': 25603, 'q': 25604, '.join(answer)\\n>>>': 25605, 'print(q)\\nselect': 25606, 'country=china\\n\\n\\n\\n\\nnote\\nyour': 25607, '.,\\ncp': 25608, 'trace=3),': 25609, 'examine\\nhow': 25610, 'added\\nto': 25611, '.db': 25612, 'and\\nretrieve': 25613, 'chat80\\n>>>': 25614, 'chat80': 25615, \".sql_query('corpora/city_database/city\": 25616, \".db',\": 25617, 'q)\\n>>>': 25618, 'rows:': 25619, 'print(r[0],': 25620, '\\ncanton': 25621, 'chungking': 25622, 'dairen': 25623, 'harbin': 25624, 'kowloon': 25625, 'mukden': 25626, 'peking': 25627, 'shanghai': 25628, 'sian': 25629, 'tientsin\\n\\n\\n\\nsince': 25630, 'data\\nin': 25631, 'query,': 25632, 'this\\nby': 25633, 'that\\nour': 25634, 'understands': 25635, 'sql,': 25636, 'is\\nable': 25637, 'database,': 25638, 'extension': 25639, 'understands\\nqueries': 25640, 'parallels\\nbeing': 25641, 'native': 25642, 'means:\\n\\n': 25643, '(3)margrietje': 25644, 'houdt': 25645, 'brunoke': 25646, 'know\\nhow': 25647, 'whole\\nsentence,': 25648, 'margrietje': 25649, 'loves\\nbrunoke\\nan': 25650, 'olga': 25651, 'take\\nthis': 25652, 'this\\nwould': 25653, 'herself': 25654, \"doesn't,\\nthen\": 25655, 'convince\\nher': 25656, '.fcfg,': 25657, 'earley': 25658, 'is\\ninstrumental': 25659, 'this\\ngrammar?': 25660, 'justification': 25661, 'meaning\\nrepresentations': 25662, 'the\\nnoun': 25663, 'correspond\\nrespectively': 25664, 'fragments': 25665, 'from\\ncity_table': 25666, 'criticism': 25667, 'grammar:': 25668, 'have\\nhard-wired': 25669, 'embarrassing': 25670, 'into\\nit': 25671, '.,\\ncity_table)': 25672, 'could\\nhave': 25673, 'different\\ntable': 25674, 'queries\\nwould': 25675, 'equally,': 25676, 'xml,': 25677, 'retrieving': 25678, 'same\\nresults': 25679, 'xml\\nquery': 25680, 'considerations': 25681, 'and\\ngeneric': 25682, 'sharpen': 25683, 'query\\nand': 25684, 'translation:\\n\\n': 25685, '.what': 25686, 'populations': 25687, '1,000,000?\\n\\n': 25688, '.select': 25689, \"'china'\": 25690, 'and\\npopulation': 25691, '1000\\n\\n\\nnote\\nyour': 25692, 'translate\\n(4a)': 25693, '(4b),': 25694, 'to\\nhandle': 25695, 'above\\n1,000,000': 25696, 'go\\nat': 25697, 'grammars/book_grammars/sql1': 25698, '(4a)': 25699, 'translated\\ninto': 25700, 'counterpart,': 25701, '(4b)': 25702, 'true\\ntogether:': 25703, 'talks': 25704, 'about\\nwhat': 25705, 'situation,': 25706, 'that\\ncond1': 25707, 'cond2': 25708, 'that\\ncondition': 25709, 'cond1': 25710, 'in\\ns': 25711, 'of\\nmeanings': 25712, 'is\\nindependent': 25713, 'classical': 25714, 'following\\nsections,': 25715, 'query\\nlanguage': 25716, 'abstract\\nand': 25717, 'our\\ntranslation': 25718, 'other\\nspecial-purpose': 25719, 'query\\ndatabases': 25720, 'methodology': 25721, '.2\\xa0\\xa0\\xa0natural': 25722, 'logic\\nwe': 25723, 'a\\nquery': 25724, 'but\\nthis': 25725, 'begged': 25726, 'back\\nfrom': 25727, 'of\\ntranslating': 25728, 'is\\nabout': 25729, 'this\\nfurther': 25730, 'margrietje\\nand': 25731, 'favourite': 25732, 'doll,': 25733, 'part,': 25734, 'this\\nbecause': 25735, 'margrietje,': 25736, 'to\\nbrunoke,': 25737, 'two\\nfundamental': 25738, 'notions': 25739, 'things\\nin': 25740, '(3)\\nis': 25741, 'loves': 25742, 'doll': 25743, 'brunoke,\\nhere': 25744, 'depiction': 25745, 'truth': 25746, 'a\\npowerful': 25747, 'at\\nsets': 25748, 'some\\nsituation': 25749, 'true,\\nwhile': 25750, '(7)': 25751, 'consistent,': 25752, 'are\\ninconsistent': 25753, '.sylvania': 25754, 'freedonia': 25755, '.freedonia': 25756, 'republic': 25757, '.no': 25758, 'sylvania': 25759, 'fictional': 25760, '(featured': 25761, 'the\\nmarx': 25762, \"brothers'\": 25763, '1933': 25764, 'soup)': 25765, 'emphasize': 25766, 'ability\\nto': 25767, 'true\\nor': 25768, 'you\\nshould': 25769, 'are\\ninconsistent,': 25770, 'population\\nof': 25771, 'which\\nboth': 25772, 'relation\\nexpressed': 25773, 'asymmetric,': 25774, 'be\\nable': 25775, 'inconsistent': 25776, '.\\nbroadly': 25777, 'speaking,': 25778, 'logic-based': 25779, 'semantics\\nfocus': 25780, 'our\\njudgments': 25781, 'consistency': 25782, 'inconsistency': 25783, 'logical\\nlanguage': 25784, 'a\\nresult,': 25785, 'reduced\\nto': 25786, 'symbolic': 25787, 'manipulation,': 25788, 'to\\ndevelop': 25789, 'logicians': 25790, 'formal\\nrepresentation': 25791, 'w\\nare': 25792, 'domain\\nd': 25793, 'about)': 25794, 'of\\nindividuals,': 25795, 'as\\nsets': 25796, 'three\\nchildren,': 25797, 'stefan,': 25798, 'klaus': 25799, 'evi,': 25800, 's,\\nk': 25801, '{s,': 25802, 'e}': 25803, 'denotes': 25804, 'the\\nset': 25805, 'stefan': 25806, 'klaus,': 25807, 'evi': 25808, 'graphical\\nrendering': 25809, 'd\\ncorresponding': 25810, 'boy,': 25811, 'is\\nrunning': 25812, '.\\n\\nlater': 25813, 'falsity': 25814, 'of\\nenglish': 25815, 'representing\\nmeaning': 25816, 'a\\nbroader': 25817, 'raised': 25818, 'in\\n5': 25819, 'sentence?': 25820, 'did?': 25821, 'asking': 25822, 'a\\ncomputer': 25823, 'think?': 25824, 'alan': 25825, 'famously': 25826, 'the\\nability': 25827, 'conversations': 25828, 'human\\n(turing,': 25829, '1950)': 25830, 'computer,\\nbut': 25831, 'partners': 25832, 'has\\nsuccessfully': 25833, 'imitated': 25834, 'human\\nin': 25835, 'imitation': 25836, 'popularly': 25837, 'known),': 25838, 'turing,': 25839, 'be\\nsaid': 25840, 'side-stepped': 25841, 'somehow': 25842, 'the\\ninternal': 25843, 'of\\nintelligence': 25844, 'reasoning,': 25845, 'specifics': 25846, \"turing's\": 25847, 'game,': 25848, 'rather\\nthe': 25849, 'proposal': 25850, 'judge': 25851, 'capacity': 25852, 'of\\nobservable': 25853, '.\\n\\n\\n\\n2\\xa0\\xa0\\xa0propositional': 25854, 'logic\\na': 25855, 'language\\nwhich': 25856, 'φ': 25857, 'truth-conditions': 25858, 'start\\noff': 25859, 'example:\\n\\n': 25860, '(8)[klaus': 25861, 'evi]': 25862, '[evi': 25863, 'away]': 25864, 'sub-sentences': 25865, 'ψ\\nrespectively,': 25866, 'and:': 25867, 'ψ': 25868, 'the\\nlogical': 25869, '.\\npropositional': 25870, 'represent\\njust': 25871, 'certain\\nsentential': 25872, 'connectives': 25873, 'such\\nconnectives': 25874, '.,\\nthen': 25875, 'formalization': 25876, 'propositional': 25877, 'the\\ncounterparts': 25878, 'boolean\\noperators': 25879, 'are\\npropositional': 25880, 'p,\\nq,': 25881, 'for\\nrepresenting': 25882, 'of\\nexploring': 25883, 'ascii\\nversions': 25884, 'operators:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 25885, '.boolean_ops()\\nnegation': 25886, '-\\nconjunction': 25887, '&\\ndisjunction': 25888, '|\\nimplication': 25889, '->\\nequivalence': 25890, '<->\\n\\n\\n\\nfrom': 25891, 'build\\nan': 25892, 'formulas': 25893, 'formulas,': 25894, 'for\\nshort)': 25895, 'a\\nformula': 25896, 'formula,': 25897, '-φ': 25898, 'and\\nψ': 25899, 'are\\n(φ': 25900, 'ψ)\\n(φ': 25901, '<->': 25902, 'ψ)': 25903, 'for\\nformulas': 25904, 'if\\nas': 25905, 'iff': 25906, '.\\n\\n\\n\\n\\n\\n\\n\\n\\nboolean': 25907, 'operator\\ntruth': 25908, 'conditions\\n\\n\\n\\nnegation': 25909, '(it': 25910, '.)\\n-φ': 25911, 's\\niff\\nφ': 25912, 's\\n\\nconjunction': 25913, '(and)\\n(φ': 25914, 's\\n\\ndisjunction': 25915, '(or)\\n(φ': 25916, 's\\n\\nimplication': 25917, '.)\\n(φ': 25918, 's\\n\\nequivalence': 25919, 'if)\\n(φ': 25920, 'straightforward,': 25921, 'for\\nimplication': 25922, 'departs': 25923, 'q)': 25924, 'false\\nwhen': 25925, '(say': 25926, 'p\\ncorresponds': 25927, 'moon': 25928, 'cheese)': 25929, 'is\\ntrue': 25930, 'four)': 25931, 'p\\n->': 25932, '.\\nnltks': 25933, 'into\\nvarious': 25934, 'subclasses': 25935, 'read_expr': 25936, '.expression': 25937, '.fromstring\\n>>>': 25938, \"read_expr('-(p\": 25939, \"q)')\\n<negatedexpression\": 25940, '-(p': 25941, 'q)>\\n>>>': 25942, \"read_expr('p\": 25943, \"q')\\n<andexpression\": 25944, '(r': 25945, \"q)')\\n<orexpression\": 25946, 'q))>\\n>>>': 25947, \"p')\\n<iffexpression\": 25948, '--p)>\\n\\n\\n\\nfrom': 25949, 'logics': 25950, 'performing\\ninference': 25951, 'sylvania,': 25952, 'case,\\nyou': 25953, 'of\\nfreedonia': 25954, 'the\\nnorth': 25955, 'assumptions\\nto': 25956, 'informally,': 25957, '(9)\\nsylvania': 25958, 'sylvania\\n\\n\\nan': 25959, 'which\\nits': 25960, 'premises': 25961, 'validity': 25962, '(9)': 25963, 'crucially': 25964, 'asymmetric\\nrelation:\\n\\n': 25965, '(10)if': 25966, 'of\\nx': 25967, '.\\nunfortunately,': 25968, 'logic:': 25969, 'smallest\\nelements': 25970, 'inside\\nthese': 25971, 'individuals': 25972, 'asymmetry': 25973, 'the\\npropositional': 25974, 'snf': 25975, 'freedonia\\nand': 25976, 'fns': 25977, 'freedonia\\nis': 25978, '-fns': 25979, 'not\\nas': 25980, 'one-place': 25981, 'implication': 25982, 'in\\n(10)': 25983, 'as\\n\\n': 25984, '(11)snf': 25985, '-fns\\nhow': 25986, 'argument?': 25987, 'first\\nsentence': 25988, 'snf,': 25989, 'also\\nthe': 25990, '(rather': 25991, 'poorly)': 25992, 'background\\nknowledge': 25993, '[a1,': 25994, 'an]': 25995, 'c\\nto': 25996, '.,\\nan]': 25997, '(9):\\n\\n': 25998, '(12)[snf,': 25999, '-fns]': 26000, '-fns\\nthis': 26001, 'argument:': 26002, 'situation\\ns,': 26003, 'would\\nconflict': 26004, 'each\\nother': 26005, 'equivalently,': 26006, '[snf,': 26007, '-fns,': 26008, 'fns]\\nis': 26009, '.\\narguments': 26010, 'proof': 26011, 'will\\nsay': 26012, 'proofs': 26013, \"with\\nnltk's\": 26014, 'the\\nthird-party': 26015, 'prover9': 26016, 'mechanism': 26017, 'lp': 26018, \"read_expr('snf')\\n>>>\": 26019, 'notfns': 26020, \"read_expr('-fns')\\n>>>\": 26021, \"read_expr('snf\": 26022, \"-fns')\\n>>>\": 26023, '.prover9()\\n>>>': 26024, '.prove(notfns,': 26025, \"r])\\ntrue\\n\\n\\n\\nhere's\": 26026, 'semantically\\nequivalent': 26027, '-snf': 26028, 'the\\ntwo-place': 26029, 'is\\ntrue,': 26030, 'cannot\\nalso': 26031, 'true;': 26032, 'consequently,\\n-fns': 26033, 'a\\nmodel,': 26034, 'for\\npropositional': 26035, 'false\\nto': 26036, 'inductively:': 26037, 'every\\npropositional': 26038, 'consulting': 26039, '.e,': 26040, \"formula's\": 26041, 'valuation': 26042, 'basic\\nexpressions': 26043, \".valuation([('p',\": 26044, 'true),': 26045, \"('q',\": 26046, \"('r',\": 26047, 'false)])\\n\\n\\n\\nwe': 26048, 'which\\nconsists': 26049, 'resulting\\nobject': 26050, 'expressions\\n(treated': 26051, \"val['p']\\ntrue\\n\\n\\n\\nas\": 26052, 'complicated\\nin': 26053, 'dom': 26054, 'and\\ng': 26055, 'declarations': 26056, '.assignment(dom)\\n\\n\\n\\nnow': 26057, 'val:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 26058, '.model(dom,': 26059, 'val)\\n\\n\\n\\nevery': 26060, 'of\\npropositional': 26061, 'logic;': 26062, 'initial\\ntruth': 26063, 'p,\\nq': 26064, \".evaluate('(p\": 26065, \"q)',\": 26066, 'g))\\ntrue\\n>>>': 26067, \".evaluate('-(p\": 26068, 'g))\\nfalse\\n>>>': 26069, \"r)',\": 26070, 'g))\\ntrue\\n\\n\\n\\n\\nnote\\nyour': 26071, 'turn:\\nexperiment': 26072, 'expected?\\n\\nup': 26073, 'into\\npropositional': 26074, 'confined': 26075, 'atomic\\nsentences': 26076, 'q,': 26077, 'their\\ninternal': 26078, 'of\\nlogical': 26079, 'subjects,': 26080, 'objects\\nand': 26081, 'wrong:': 26082, 'formalize\\narguments': 26083, '(9),': 26084, 'inside\\nbasic': 26085, 'logic\\nto': 26086, 'expressive,': 26087, '.\\n\\n\\n3\\xa0\\xa0\\xa0first-order': 26088, 'logic\\nin': 26089, 'language\\nexpressions': 26090, 'good\\nchoice': 26091, 'expressive': 26092, 'deal,': 26093, 'shelf\\nfor': 26094, 'constructed,': 26095, 'how\\nsuch': 26096, '.1\\xa0\\xa0\\xa0syntax\\nfirst-order': 26097, 'it\\nadds': 26098, 'propositions': 26099, 'are\\nanalyzed': 26100, 'closer\\nto': 26101, 'rules\\nfor': 26102, 'and\\nindividual': 26103, 'constants,': 26104, 'differing\\nnumbers': 26105, 'angus': 26106, 'be\\nformalized': 26107, 'walk(angus)': 26108, 'sees': 26109, 'bertie': 26110, 'as\\nsee(angus,': 26111, 'bertie)': 26112, 'unary\\npredicate,': 26113, 'predicate': 26114, 'the\\nsymbols': 26115, 'intrinsic': 26116, 'examples,\\nthere': 26117, 'and\\n(13b)': 26118, '.love(margrietje,': 26119, 'brunoke)\\n\\n': 26120, '.houden_van(margrietje,': 26121, 'brunoke)\\n\\n\\nby': 26122, 'substantive': 26123, 'lexical\\nsemantics': 26124, 'although\\nsome': 26125, 'an\\natomic': 26126, 'predication': 26127, 'see(angus,': 26128, 'particular\\nvaluation': 26129, 'see,\\nangus': 26130, 'non-logical': 26131, 'logical\\nconstants': 26132, 'operators)': 26133, 'same\\ninterpretation': 26134, 'special\\nstatus,': 26135, 'equality,': 26136, '=\\naj': 26137, 'constant,': 26138, 'for\\nindividual': 26139, 't2,': 26140, 'to\\nexpressions': 26141, 'montague': 26142, 'types:': 26143, 'entities,\\nwhile': 26144, 'have\\ntruth': 26145, 'complex\\ntypes': 26146, 'σ': 26147, 'τ,': 26148, '〈σ,\\nτ〉': 26149, \"from\\n'σ\": 26150, \"things'\": 26151, \"'τ\": 26152, '〈e,\\nt〉': 26153, 'to\\ntruth': 26154, 'unary': 26155, 'be\\nprocessed': 26156, 'expr': 26157, \"read_expr('walk(angus)',\": 26158, 'type_check=true)\\n>>>': 26159, '.argument\\n<constantexpression': 26160, 'angus>\\n>>>': 26161, '.argument': 26162, '.type\\ne\\n>>>': 26163, '.function\\n<constantexpression': 26164, 'walk>\\n>>>': 26165, '.function': 26166, '.type\\n<e,?>\\n\\n\\n\\nwhy': 26167, '<e,?>': 26168, 'example?': 26169, 'the\\ntype-checker': 26170, 'infer': 26171, 'case\\nit': 26172, 'walk,\\nsince': 26173, 'intending': 26174, 'walk\\nto': 26175, '<e,': 26176, 't>,': 26177, 'type-checker': 26178, 'knows,': 26179, 'this\\ncontext': 26180, 'e>': 26181, 't>': 26182, 'help\\nthe': 26183, 'type-checker,': 26184, 'signature,': 26185, 'non-logical\\nconstants:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 26186, 'sig': 26187, \"{'walk':\": 26188, \"'<e,\": 26189, \"t>'}\\n>>>\": 26190, 'signature=sig)\\n>>>': 26191, '.type\\ne\\n\\n\\n\\na': 26192, '〈e,': 26193, 't〉〉': 26194, 'of\\nsomething': 26195, 'make\\na': 26196, 'predicate,': 26197, 'combining\\ndirectly': 26198, 'the\\ntranslation': 26199, 'cyril': 26200, 'will\\ncombine': 26201, 'cyril)': 26202, 'variables\\nsuch': 26203, 'that\\nvariables': 26204, '.\\nindividual': 26205, 'to\\npersonal': 26206, 'he,': 26207, 'their\\ndenotation': 26208, '(14)': 26209, 'by\\npointing': 26210, '(14)he': 26211, 'disappeared': 26212, 'pronoun\\nhe,': 26213, 'uttering': 26214, '(15a)': 26215, 'to\\n(14)': 26216, 'coreferential': 26217, '(15b)': 26218, '.cyril': 26219, \"angus's\": 26220, '.\\n\\nconsider': 26221, '(16a)': 26222, 'np\\na': 26223, 'dog,\\nand': 26224, 'than\\ncoreference': 26225, 'dog,\\nthe': 26226, '(16b)': 26227, '.angus': 26228, '.\\n\\ncorresponding': 26229, '(17a),': 26230, 'formula\\n(17b)': 26231, '(we\\nignore': 26232, 'exposition': 26233, '.dog(x)': 26234, '∧': 26235, 'disappear(x)\\n\\nby': 26236, 'existential': 26237, 'quantifier': 26238, '∃x': 26239, \"('for\\nsome\": 26240, \"x')\": 26241, 'front': 26242, '(17b),': 26243, 'these\\nvariables,': 26244, '(18a),': 26245, '(18b)': 26246, 'more\\nidiomatically,': 26247, '(18c)': 26248, '(18)\\n': 26249, '.∃x': 26250, '.(dog(x)': 26251, 'disappear(x))\\n\\n': 26252, '.at': 26253, '(18a):\\n\\n': 26254, '(19)exists': 26255, 'disappear(x))\\nin': 26256, 'quantifier,': 26257, '∀x': 26258, \"('for\": 26259, \"all\\nx'),\": 26260, '.∀x': 26261, '.everything': 26262, 'disappears': 26263, '.every': 26264, '(20a):\\n\\n': 26265, '(21)all': 26266, 'disappear(x))\\nalthough': 26267, '(20a)': 26268, '(20c),': 26269, 'truth\\nconditions': 26270, \"aren't\": 26271, 'then\\nx': 26272, 'so\\nin': 26273, 'dogs,': 26274, 'still\\ncome': 26275, '(remember': 26276, 'p\\nis': 26277, 'argue': 26278, 'presuppose': 26279, 'the\\nexistence': 26280, 'presupposition': 26281, 'expression\\nastring': 26282, \".replace('ate',\": 26283, \"'8')\": 26284, 'every\\noccurrence': 26285, 'astring': 26286, \"'8',\": 26287, 'there\\nmay': 26288, 'quantifiers': 26289, 'what\\nhappens': 26290, 'following?:\\n\\n((exists': 26291, 'dog(x))': 26292, 'bark(x))\\n\\nthe': 26293, 'dog(x),': 26294, 'x\\nin': 26295, 'bark(x)': 26296, 'unbound': 26297, 'consequently': 26298, 'quantifier,\\nfor': 26299, 'formula:\\n\\nall': 26300, '.((exists': 26301, 'bark(x))\\n\\nin': 26302, 'in\\nφ': 26303, 'φ,': 26304, '.φ': 26305, 'bound,': 26306, 'process\\nstrings,': 26307, 'each\\ninstance': 26308, 'free()': 26309, \"read_expr('dog(cyril)')\": 26310, '.free()\\nset()\\n>>>': 26311, \"read_expr('dog(x)')\": 26312, \".free()\\n{variable('x')}\\n>>>\": 26313, \"read_expr('own(angus,\": 26314, \"cyril)')\": 26315, \"read_expr('exists\": 26316, \".dog(x)')\": 26317, \"read_expr('((some\": 26318, 'walk(x))': 26319, \"sing(x))')\": 26320, '.own(y,': 26321, \"x)')\": 26322, \".free()\\n{variable('y')}\\n\\n\\n\\n\\n\\n3\": 26323, '.2\\xa0\\xa0\\xa0first': 26324, 'proving\\nrecall': 26325, '(10):\\n\\n': 26326, '(22)if': 26327, 'predicates,': 26328, 'we\\ndid': 26329, 'properly': 26330, 'doubt': 26331, 'is\\nideal': 26332, 'formalizing': 26333, 'rules:\\n\\nall': 26334, '.(north_of(x,': 26335, 'y)': 26336, '-north_of(y,': 26337, 'x))\\n\\neven': 26338, 'better,': 26339, 'to\\nshow': 26340, 'proving': 26341, 'goal)': 26342, 'derived\\nby': 26343, '⊢': 26344, '(possibly': 26345, 'empty)': 26346, 'assumptions,\\nand': 26347, 'the\\ntheorem': 26348, 'required\\nproof': 26349, 'prover9\\ninstance': 26350, 'prove()': 26351, 'goal,': 26352, 'of\\nassumptions': 26353, \"read_expr('-north_of(f,\": 26354, \"s)')\": 26355, \"read_expr('north_of(s,\": 26356, \"f)')\": 26357, \"read_expr('all\": 26358, '(north_of(x,': 26359, \"x))')\": 26360, '.prover9()': 26361, 'r])': 26362, '\\ntrue\\n\\n\\n\\nhappily,': 26363, 'contrast,\\nit': 26364, 'north_of(f,': 26365, 'our\\nassumptions:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 26366, \"read_expr('north_of(f,\": 26367, \"s)')\\n>>>\": 26368, '.prove(fns,': 26369, 'r])\\nfalse\\n\\n\\n\\n\\n\\n3': 26370, '.3\\xa0\\xa0\\xa0summarizing': 26371, \"logic\\nwe'll\": 26372, 'restate': 26373, 'quantifiers;\\ntogether,': 26374, 'expressions\\ninvolved': 26375, 'that\\n〈en,': 26376, 't〉\\nis': 26377, 'arguments\\nof': 26378, 'expression\\nof': 26379, 'the\\narity': 26380, '.\\n\\n\\nif': 26381, '〈en,\\nt〉,\\nand': 26382, 'α1,': 26383, 'αn\\nare': 26384, 'then\\np(α1,': 26385, 'αn)': 26386, 'is\\nof': 26387, 'β': 26388, '(α': 26389, 'β)': 26390, 'and\\n(α': 26391, 'ψ),\\n(φ': 26392, 'and\\n(φ': 26393, 'then\\nexists': 26394, 'logic\\nmodule,': 26395, '.\\n\\n\\n\\n\\n\\n\\nexample\\ndescription\\n\\n\\n\\n=\\nequality\\n\\n!=\\ninequality\\n\\nexists\\nexistential': 26396, 'quantifier\\n\\nall\\nuniversal': 26397, 'quantifier\\n\\ne': 26398, '.free()\\nshow': 26399, 'e\\n\\ne': 26400, '.simplify()\\ncarry': 26401, 'β-reduction': 26402, 'e\\n\\n\\ntable': 26403, 'first\\norder': 26404, 'the\\nexpression': 26405, '.4\\xa0\\xa0\\xa0truth': 26406, 'model\\nwe': 26407, 'argued': 26408, 'if\\nwe': 26409, 'truth-conditional': 26410, 'semantics,': 26411, 'obvious\\nlimits': 26412, 'push': 26413, 'talk\\nabout': 26414, 'the\\nmeans': 26415, 'symbolic\\nmanner': 26416, 'limitation,': 26417, 'a\\nclearer': 26418, 'in\\nnltk': 26419, 'a\\npair': 26420, '〈d,': 26421, 'val〉,': 26422, 'an\\nnonempty': 26423, 'values\\nfrom': 26424, 'follows:\\n\\n\\nfor': 26425, 'l,\\nval(c)': 26426, 'arity': 26427, '≥': 26428, '0,\\nval(p)': 26429, 'dn': 26430, 'to\\n{true,': 26431, 'false}': 26432, 'val(p)': 26433, 'the\\np': 26434, '.)\\n\\n\\naccording': 26435, '(ii),': 26436, 'val(p)\\nwill': 26437, 'which\\nval(p)': 26438, 'follows:\\n\\n': 26439, '(23)s': 26440, '{s': 26441, 'f(s)': 26442, 'true}\\nsuch': 26443, 's\\n(as': 26444, '.\\nrelations': 26445, 'standard\\nset-theoretic': 26446, 'way:': 26447, 'bertie,': 26448, 'olive': 26449, 'cyril,\\nwhere': 26450, 'mnemonic\\nreasons,': 26451, 'labels\\nin': 26452, \"{'b',\": 26453, \"'c'}\\n\\n\\n\\nwe\": 26454, '.fromstring()': 26455, 'value\\ninto': 26456, 'b\\n': 26457, 'c\\n': 26458, '{b}\\n': 26459, '{o}\\n': 26460, '{c}\\n': 26461, '{o,': 26462, 'c}\\n': 26463, '{(b,': 26464, 'o),': 26465, '(c,': 26466, 'b),': 26467, '(o,': 26468, 'c)}\\n': 26469, '.valuation': 26470, '.fromstring(v)\\n>>>': 26471, \"print(val)\\n{'bertie':\": 26472, \"'b',\\n\": 26473, \"'boy':\": 26474, \"{('b',)},\\n\": 26475, \"'cyril':\": 26476, \"'c',\\n\": 26477, \"'dog':\": 26478, \"{('c',)},\\n\": 26479, \"'girl':\": 26480, \"{('o',)},\\n\": 26481, \"'olive':\": 26482, \"'o',\\n\": 26483, \"'see':\": 26484, \"{('o',\": 26485, \"'c'),\": 26486, \"('c',\": 26487, \"'b'),\": 26488, \"('b',\": 26489, \"'o')},\\n\": 26490, \"'walk':\": 26491, \"{('c',),\": 26492, \"('o',)}}\\n\\n\\n\\nso\": 26493, 'valuation,': 26494, 'of\\ntuples': 26495, 'olive,': 26496, 'and\\nolive': 26497, 'turn:\\ndraw': 26498, 'girl,\\ndog)': 26499, 'singleton': 26500, 'tuples,': 26501, 'convenience': 26502, 'predication\\nof': 26503, 'p(τ1,': 26504, 'τn),': 26505, 'where\\np': 26506, 'the\\ntuple': 26507, '(τ1,': 26508, 'τn)': 26509, \"'c')\": 26510, \"val['see']\\ntrue\\n>>>\": 26511, \"('b',)\": 26512, \"val['boy']\\ntrue\\n\\n\\n\\n\\n\\n3\": 26513, '.5\\xa0\\xa0\\xa0individual': 26514, 'assignments\\nin': 26515, 'variable\\nassignment': 26516, 'constructor,': 26517, 'of\\ndiscourse': 26518, 'any\\nbindings,': 26519, '(variable,\\nvalue)': 26520, 'valuations': 26521, '.assignment(dom,': 26522, \"[('x',\": 26523, \"('y',\": 26524, \"'c')])\\n>>>\": 26525, \"g\\n{'y':\": 26526, \"'x':\": 26527, \"'o'}\\n\\n\\n\\nin\": 26528, 'textbooks:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 26529, \"print(g)\\ng[c/y][o/x]\\n\\n\\n\\nlet's\": 26530, 'of\\nfirst-order': 26531, 'method\\nto': 26532, 'val)\\n>>>': 26533, \".evaluate('see(olive,\": 26534, \"y)',\": 26535, \"g)\\ntrue\\n\\n\\n\\nwhat's\": 26536, 'here?': 26537, 'to\\nour': 26538, 'examplle,': 26539, 'see(olive,': 26540, 'y,\\nrather': 26541, 'val,': 26542, 'value:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 26543, \"g['y']\\n'c'\\n\\n\\n\\nsince\": 26544, 'see\\nrelation,': 26545, 'can\\nsay': 26546, 'satisfies': 26547, 'to\\ng': 26548, \".evaluate('see(y,\": 26549, \"x)',\": 26550, 'g)\\nfalse\\n\\n\\n\\nin': 26551, '(though': 26552, 'logic),': 26553, 'assignments\\nare': 26554, 'variables\\napart': 26555, 'purge()': 26556, 'clears': 26557, 'all\\nbindings': 26558, '.purge()\\n>>>': 26559, 'g\\n{}\\n\\n\\n\\nif': 26560, 'to\\ng,': 26561, 'when\\nwe': 26562, 'function\\nfails': 26563, \"g)\\n'undefined'\\n\\n\\n\\nsince\": 26564, 'boolean\\noperators,': 26565, 'arbitrarily': 26566, 'composed': 26567, \".evaluate('see(bertie,\": 26568, 'olive)': 26569, 'boy(bertie)': 26570, \"-walk(bertie)',\": 26571, 'g)\\ntrue\\n\\n\\n\\nthe': 26572, 'a\\nmodel': 26573, '.6\\xa0\\xa0\\xa0quantification\\none': 26574, 'modern\\nlogic': 26575, 'satisfaction': 26576, 'quantified': 26577, \"let's\\nuse\": 26578, '(24)exists': 26579, '.(girl(x)': 26580, 'walk(x))\\nwhen': 26581, 'true?': 26582, 'domain,\\ni': 26583, 'individuals\\nhave': 26584, 'g[u/x]\\nsatisfies': 26585, '(25)': 26586, '(25)girl(x)': 26587, 'walk(x)\\nconsider': 26588, \".evaluate('exists\": 26589, \"walk(x))',\": 26590, 'g)\\ntrue\\n\\n\\n\\nevaluate()': 26591, 'in\\ndom': 26592, 'binds\\nx': 26593, 'u:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 26594, \".evaluate('girl(x)\": 26595, \"walk(x)',\": 26596, \".add('x',\": 26597, \"'o'))\\ntrue\\n\\n\\n\\none\": 26598, 'satisfiers()': 26599, 'this\\nreturns': 26600, 'an\\nassignment': 26601, 'examples:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 26602, 'fmla1': 26603, \"read_expr('girl(x)\": 26604, \"boy(x)')\\n>>>\": 26605, '.satisfiers(fmla1,': 26606, \"g)\\n{'b',\": 26607, \"'o'}\\n>>>\": 26608, 'fmla2': 26609, \"walk(x)')\\n>>>\": 26610, '.satisfiers(fmla2,': 26611, \"g)\\n{'c',\": 26612, 'fmla3': 26613, \"read_expr('walk(x)\": 26614, \"girl(x)')\\n>>>\": 26615, '.satisfiers(fmla3,': 26616, \"'o'}\\n\\n\\n\\nit's\": 26617, 'the\\nvalues': 26618, 'that\\nfmla2': 26619, '-girl(x)': 26620, 'walk(x),': 26621, 'satisfied\\nby': 26622, 'girl\\nor': 26623, '(bertie)': 26624, '(cyril)\\nare': 26625, 'girls,': 26626, 'satisfy\\nthe': 26627, 'o\\nsatisfies': 26628, 'disjuncts': 26629, 'fmla2,': 26630, 'universally\\nquantified': 26631, \".evaluate('all\": 26632, 'g)\\ntrue\\n\\n\\n\\nin': 26633, 'universally': 26634, 'u,': 26635, 'g[u/x]': 26636, 'pencil': 26637, 'using\\nm': 26638, '.evaluate(),': 26639, '&\\nwalk(x))': 26640, '.(boy(x)': 26641, 'you\\nunderstand': 26642, '.7\\xa0\\xa0\\xa0quantifier': 26643, 'ambiguity\\nwhat': 26644, 'quantifiers,': 26645, 'following?\\n\\n': 26646, '(26)everybody': 26647, 'admires': 26648, 'least)': 26649, '(26)': 26650, 'logic:\\n\\n': 26651, '(27)\\n': 26652, '.(person(x)': 26653, '.(person(y)': 26654, 'admire(x,y)))\\n\\n': 26655, '.exists': 26656, 'admire(x,y)))\\n\\ncan': 26657, 'these?': 26658, 'yes,': 26659, 'different\\nmeanings': 26660, '(27b)': 26661, 'logically': 26662, 'stronger': 26663, '(27a):': 26664, 'bruce,': 26665, 'admired': 26666, '.\\n(27a),': 26667, 'person\\nu,': 26668, \"u'\": 26669, 'u\\nadmires;': 26670, 'we\\ndistinguish': 26671, '(27a)': 26672, 'scope\\nof': 26673, '∀': 26674, 'than\\n∃,': 26675, '(27b),': 26676, 'reversed': 26677, '(26),': 26678, 'are\\nboth': 26679, 'claiming': 26680, 'is\\nambiguous': 26681, 'in\\n(27)': 26682, 'associating': 26683, 'two\\ndistinct': 26684, 'detail\\nhow': 26685, 'our\\nvaluation': 26686, 'bruce': 26687, 'elspeth': 26688, 'e\\n': 26689, 'julia': 26690, 'j\\n': 26691, 'matthew': 26692, 'm\\n': 26693, '{b,': 26694, 'j,': 26695, 'm}\\n': 26696, 'admire': 26697, '{(j,': 26698, '(b,': 26699, '(m,': 26700, 'e),': 26701, '(e,': 26702, 'm)}\\n': 26703, 'val2': 26704, '.fromstring(v2)\\n\\n\\n\\nthe': 26705, 'visualized': 26706, 'the\\nmapping': 26707, '(28)\\nin': 26708, '(28),': 26709, 'and\\ny': 26710, 'admires\\ny': 26711, '(bruce': 26712, 'vain),': 26713, 'admires\\nm': 26714, 'above\\nis': 26715, 'by\\nusing': 26716, 'dom2': 26717, '.domain\\n>>>': 26718, 'm2': 26719, '.model(dom2,': 26720, 'val2)\\n>>>': 26721, 'g2': 26722, '.assignment(dom2)\\n>>>': 26723, 'fmla4': 26724, \"read_expr('(person(x)\": 26725, 'admire(x,': 26726, \"y)))')\\n>>>\": 26727, '.satisfiers(fmla4,': 26728, \"g2)\\n{'e',\": 26729, \"'j'}\\n\\n\\n\\nthis\": 26730, 'fmla5': 26731, 'below;': 26732, 'no\\nsatisfiers': 26733, \"read_expr('(person(y)\": 26734, '.satisfiers(fmla5,': 26735, 'g2)\\nset()\\n\\n\\n\\nthat': 26736, 'everybody': 26737, 'fmla6,': 26738, 'a\\nperson,': 26739, 'fmla6': 26740, '.((x': 26741, 'julia)': 26742, '.satisfiers(fmla6,': 26743, \"g2)\\n{'b'}\\n\\n\\n\\n\\nnote\\nyour\": 26744, 'turn:\\ndevise': 26745, '(27a)\\ncomes': 26746, 'devise': 26747, '(27b)\\ncomes': 26748, '.8\\xa0\\xa0\\xa0model': 26749, 'building\\nwe': 26750, 'check\\nthe': 26751, 'building\\ntries': 26752, 'it\\nsucceeds,': 26753, 'an\\nexistence': 26754, 'mace4': 26755, 'builder': 26756, 'of\\nmace()': 26757, 'build_model()': 26758, 'an\\nanalogous': 26759, 'to\\ntreat': 26760, 'assumptions,': 26761, 'the\\ngoal': 26762, 'unspecified': 26763, 'both\\n[a,': 26764, 'c1]': 26765, '[a,': 26766, 'c2]': 26767, 'mace': 26768, 'succeeds\\nin': 26769, '[c1,': 26770, 'is\\ninconsistent': 26771, 'a3': 26772, '.(man(x)': 26773, \"walks(x))')\\n>>>\": 26774, \"read_expr('mortal(socrates)')\\n>>>\": 26775, \"read_expr('-mortal(socrates)')\\n>>>\": 26776, 'mb': 26777, '.mace(5)\\n>>>': 26778, 'print(mb': 26779, '.build_model(none,': 26780, '[a3,': 26781, 'c1]))\\ntrue\\n>>>': 26782, 'c2]))\\ntrue\\n>>>': 26783, 'c2]))\\nfalse\\n\\n\\n\\nwe': 26784, 'adjunct': 26785, 'is\\nlogically': 26786, 'derivable': 26787, '[s1,': 26788, 's2,': 26789, 'sn]': 26790, 'mace4,': 26791, 'a\\ncounterexample,': 26792, 'from\\ns': 26793, \"s'\": 26794, '=\\n[s1,': 26795, 'sn,': 26796, '-g]': 26797, 'then\\nmace4': 26798, 'counterexample': 26799, 'prover9\\nconcludes': 26800, 'if\\ng': 26801, 'provable': 26802, 'time\\nunsuccessfully': 26803, 'countermodel,': 26804, 'eventually\\ngive': 26805, '[there': 26806, 'a\\nwoman': 26807, 'loves,': 26808, 'adam': 26809, 'man,': 26810, 'eve': 26811, 'a\\nwoman]': 26812, 'false?': 26813, 'macecommand()': 26814, 'inspect\\nthe': 26815, 'a4': 26816, '(woman(y)': 26817, '(man(x)': 26818, \"love(x,y)))')\\n>>>\": 26819, 'a5': 26820, \"read_expr('man(adam)')\\n>>>\": 26821, 'a6': 26822, \"read_expr('woman(eve)')\\n>>>\": 26823, \"read_expr('love(adam,eve)')\\n>>>\": 26824, 'mc': 26825, '.macecommand(g,': 26826, 'assumptions=[a4,': 26827, 'a5,': 26828, 'a6])\\n>>>': 26829, '.build_model()\\ntrue\\n\\n\\n\\nso': 26830, 'yes:': 26831, 'countermodel': 26832, 'than\\neve': 26833, \"mace4's\": 26834, 'model,\\nconverted': 26835, 'print(mc': 26836, \".valuation)\\n{'c1':\": 26837, \"'adam':\": 26838, \"'a',\\n\": 26839, \"'eve':\": 26840, \"'love':\": 26841, \"{('a',\": 26842, \"'b')},\\n\": 26843, \"'man':\": 26844, \"{('a',)},\\n\": 26845, \"'woman':\": 26846, \"{('a',),\": 26847, \"('b',)}}\\n\\n\\n\\nthe\": 26848, 'you:': 26849, 'appropriate\\nkind': 26850, 'puzzling': 26851, 'the\\nc1': 26852, 'skolem': 26853, 'model\\nbuilder': 26854, 'knew\\nthat': 26855, 'open\\nformula': 26856, 'know\\nwhether': 26857, 'denotation': 26858, 'constant\\nanywhere': 26859, 'fly,\\nnamely': 26860, 'about\\nthe': 26861, 'eve,': 26862, 'decided': 26863, 'no\\nreason': 26864, 'denoting': 26865, 'mapped\\nto': 26866, 'woman\\ndenote': 26867, 'disjoint': 26868, 'denotations': 26869, 'this\\nillustrates': 26870, 'dramatically': 26871, 'to\\nbear': 26872, 'knows\\nnothing': 26873, 'builder\\nstill': 26874, 'accord': 26875, 'intuitions\\nabout': 26876, 'situation:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 26877, 'a7': 26878, \"-woman(x))')\\n>>>\": 26879, 'a6,': 26880, 'a7])\\n>>>': 26881, '.build_model()\\ntrue\\n>>>': 26882, \"'c')},\\n\": 26883, \"('b',)}}\\n\\n\\n\\non\": 26884, 'reflection,': 26885, 'that\\neve': 26886, 'discourse,': 26887, 'fact\\nis': 26888, 'further\\nassumption': 26889, '(woman(x)': 26890, '(x': 26891, 'y))': 26892, '.\\n\\n\\n\\n4\\xa0\\xa0\\xa0the': 26893, 'sentences\\n\\n4': 26894, '.1\\xa0\\xa0\\xa0compositional': 26895, 'grammar\\nat': 26896, 'of\\nbuilding': 26897, 'parse,\\nusing': 26898, 'this\\ntime,': 26899, 'logical\\nform': 26900, 'the\\nprinciple': 26901, 'compositionality': 26902, \"frege's\\nprinciple;\": 26903, '(gleitman': 26904, 'liberman,': 26905, '1995)': 26906, 'formulation': 26907, '.)\\nprinciple': 26908, 'compositionality:\\nthe': 26909, 'parts\\nand': 26910, 'complex\\nexpression': 26911, 'granted': 26912, 'parsed\\nagainst': 26913, 'entailed': 26914, 'integrate': 26915, 'representation\\nin': 26916, 'smoothly': 26917, '(29)\\nillustrates': 26918, 'would\\nlike': 26919, '(29)\\nin': 26920, '(29),': 26921, 'semantic\\nrepresentation': 26922, 'at\\nlower': 26923, 'special\\nmanner,': 26924, 'being\\nenclosed': 26925, 'us\\nthis': 26926, 'result?': 26927, 'then\\ncompose': 26928, 'its\\nchild': 26929, 'function\\napplication': 26930, 'of\\ncomposition': 26931, 'specific,': 26932, 'and\\nvp': 26933, 'sem\\nnodes': 26934, 'like\\n(30)': 26935, '(observe': 26936, 'a\\nvariable,': 26937, '(30)s[sem=<?vp(?np)>]': 26938, 'vp[sem=?vp]\\n(30)': 26939, '?np': 26940, 'subject\\nnp': 26941, '?vp': 26942, 'sem\\nvalue': 26943, 'to\\ndenote': 26944, 'its\\ndomain': 26945, '(30)': 26946, 'straightforward;': 26947, '.\\n\\nvp[sem=?v]': 26948, 'iv[sem=?v]\\nnp[sem=<cyril>]': 26949, \"'cyril'\\niv[sem=<\\\\x\": 26950, '.bark(x)>]': 26951, \"'barks'\\n\\nthe\": 26952, \"parent's\": 26953, 'the\\nhead': 26954, \"child's\": 26955, 'non-logical\\nconstants': 26956, 'and\\nbarks': 26957, 'barks': 26958, 'launching': 26959, 'compositional': 26960, 'kit,': 26961, 'λ': 26962, 'calculus': 26963, 'an\\ninvaluable': 26964, 'assemble': 26965, 'meaning\\nrepresentation': 26966, 'λ-calculus\\nin': 26967, 'that\\nmathematical': 26968, 'specifying\\nproperties': 26969, 'a\\ndocument': 26970, '(31),': 26971, 'we\\nglossed': 26972, 'element\\nof': 26973, '(31){w': 26974, 'p(w)}\\nit': 26975, 'that\\nwill': 26976, 'operator\\n(pronounced': 26977, 'lambda)': 26978, 'to\\n(31)': 26979, '(32)': 26980, 'trying\\nto': 26981, '(32)λw': 26982, '(v(w)': 26983, 'p(w))\\n\\nnote\\nλ': 26984, 'alonzo': 26985, 'church': 26986, 'represent\\ncomputable': 26987, 'mathematics\\nand': 26988, 'λ-calculus': 26989, '.\\n\\nλ': 26990, '(33a),': 26991, 'bind\\nthe': 26992, 'in\\n(33b)': 26993, 'in\\n(33c)': 26994, '(33)\\n': 26995, '.(walk(x)': 26996, 'chew_gum(x))\\n\\n': 26997, '.λx': 26998, '.\\\\x': 26999, 'chew_gum(x))\\n\\nremember': 27000, '\\\\),': 27001, 'strings\\n(3': 27002, '.4):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 27003, \"read_expr(r'\\\\x\": 27004, \"chew_gum(x))')\\n>>>\": 27005, 'expr\\n<lambdaexpression': 27006, '\\\\x': 27007, 'chew_gum(x))>\\n>>>': 27008, \"print(read_expr(r'\\\\x\": 27009, \"chew_gum(y))'))\\n\\\\x\": 27010, 'chew_gum(y))\\n\\n\\n\\n\\nwe': 27011, 'an\\nexpression:': 27012, 'encounter\\nλ-abstracts,': 27013, 'their\\nmeaning': 27014, 'glosses': 27015, '(33b)': 27016, 'be\\nan': 27017, 'chews': 27018, 'gum': 27019, 'or\\nhave': 27020, 'chewing': 27021, 'suggested\\nthat': 27022, 'λ-abstracts': 27023, '(or\\nsubjectless': 27024, 'clauses),': 27025, 'in\\ntheir': 27026, 'its\\ntranslation': 27027, '.to': 27028, 'chew-gum': 27029, 'hard\\n\\n': 27030, '.hard(\\\\x': 27031, 'chew_gum(x)))\\n\\nso': 27032, 'with\\nfree': 27033, 'property\\nexpression': 27034, 'λx': 27035, 'being\\nan': 27036, 'official': 27037, 'abstracts': 27038, 'built:\\n\\n\\n': 27039, '(35)if': 27040, 'then\\n\\\\x': 27041, '.α': 27042, 'τ〉': 27043, '.\\n(34b)': 27044, 'property,': 27045, 'namely\\nthat': 27046, 'to\\nindividuals': 27047, '(36),\\n(33b)': 27048, 'predicated': 27049, 'gerald': 27050, '(36)\\\\x': 27051, 'chew_gum(x))': 27052, '(gerald)\\nnow': 27053, 'gum,\\nwhich': 27054, '(37)': 27055, '(37)(walk(gerald)': 27056, 'chew_gum(gerald))\\nwhat': 27057, '&\\nchew_gum(x))': 27058, '(walk(x)': 27059, 'α[β/x]': 27060, 'as\\nnotation': 27061, 'in\\nα': 27062, 'so:\\n\\n(walk(x)': 27063, 'chew_gum(x))[gerald/x]\\n\\nis': 27064, 'to\\n(37)': 27065, 'semantic\\nrepresentations,': 27066, 'operation\\nis': 27067, 'justified,': 27068, 'we\\nwant': 27069, 'α(β)': 27070, 'semantic\\nvalues': 27071, 'slight\\ncomplication': 27072, 'of\\nexpressions': 27073, 'simplify()': 27074, \"chew_gum(x))(gerald)')\\n>>>\": 27075, 'print(expr)\\n\\\\x': 27076, 'chew_gum(x))(gerald)\\n>>>': 27077, 'print(expr': 27078, '.simplify())': 27079, '\\n(walk(gerald)': 27080, 'chew_gum(gerald))\\n\\n\\n\\nalthough': 27081, 'the\\nλ': 27082, 'a\\nnecessary': 27083, 'restriction;': 27084, 'well-formed\\nexpression': 27085, 'λs': 27086, '(38)\\\\x': 27087, '.\\\\y': 27088, 'own(y,': 27089, 'x))\\njust': 27090, 'predicate,\\n(38)': 27091, 'predicate:': 27092, 'to\\ntwo': 27093, 'be\\nwritten': 27094, 'abbreviated': 27095, \"x))(cyril)')\": 27096, '.simplify())\\n\\\\y': 27097, '.(dog(cyril)': 27098, 'own(y,cyril))\\n>>>': 27099, 'x))(cyril,': 27100, \"angus)')\": 27101, '\\n(dog(cyril)': 27102, 'own(angus,cyril))\\n\\n\\n\\nall': 27103, 'variables:\\nx,': 27104, 'abstract,': 27105, 'say\\n\\\\x': 27106, '.walk(x)': 27107, 'the\\nargument': 27108, 'abstract?': 27109, 'this:\\n\\n\\\\y': 27110, '.y(angus)(\\\\x': 27111, '.walk(x))\\n\\nbut': 27112, 'stipulated': 27113, 'e,\\n\\\\y': 27114, '.y(angus)': 27115, 't〉!': 27116, 'allow\\nabstraction': 27117, 't〉,': 27118, 'as\\n\\\\p': 27119, '.p(angus)': 27120, '〈〈e,': 27121, 't〉': 27122, '\\\\p': 27123, '.p(angus)(\\\\x': 27124, '.walk(x))': 27125, 'legal,': 27126, '.walk(x)(angus)': 27127, 'walk(angus)\\nwhen': 27128, 'β-reduction,': 27129, 'with\\nvariables': 27130, '(39a)': 27131, 'and\\n(39b),': 27132, '.see(y,': 27133, 'x)\\n\\n': 27134, 'z)\\n\\nsuppose': 27135, 'λ-term': 27136, '.p(x)': 27137, 'terms:\\n\\n': 27138, '.\\\\p': 27139, '.p(x)(\\\\y': 27140, 'x))\\n\\n': 27141, 'z))\\n\\nwe': 27142, 'the\\nexistential': 27143, '(40a),': 27144, 'reduction,': 27145, 'results\\nwill': 27146, 'different:\\n\\n': 27147, '(41)\\n': 27148, '.see(x,': 27149, 'z)\\n\\n(41a)': 27150, 'him/herself,': 27151, 'whereas\\n(41b)': 27152, 'unspecified\\nindividual': 27153, 'forbid\\nthe': 27154, '(41a)': 27155, 'what\\nparticular': 27156, 'bound\\nby': 27157, '(40a)?': 27158, 'variable-binding': 27159, '(involving': 27160, '∀,\\n∃': 27161, 'λ),': 27162, 'bound\\nvariable': 27163, 'and\\nexists': 27164, '.p(y)': 27165, 'equivalent;': 27166, 'equivalents,\\nor': 27167, 'relabeling': 27168, 'bound\\nvariables': 27169, 'α-conversion': 27170, 'of\\nvariablebinderexpressions': 27171, 'using\\n==),': 27172, 'α-equivalence:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 27173, 'expr1': 27174, \".p(x)')\\n>>>\": 27175, 'print(expr1)\\nexists': 27176, '.p(x)\\n>>>': 27177, 'expr2': 27178, '.alpha_convert(nltk': 27179, \".variable('z'))\\n>>>\": 27180, 'print(expr2)\\nexists': 27181, '.p(z)\\n>>>': 27182, 'expr2\\ntrue\\n\\n\\n\\nwhen': 27183, 'application\\nf(a),': 27184, 'a\\nwhich': 27185, 'subterms': 27186, 'of\\nf': 27187, 'suppose,': 27188, 'that\\nx': 27189, 'the\\nsubterm': 27190, 'we\\nproduce': 27191, '.p(x),\\nsay,': 27192, 'z1': 27193, '.p(z1),': 27194, 'then\\ncarry': 27195, 'out\\nautomatically': 27196, 'the\\nresults': 27197, 'expr3': 27198, \"read_expr('\\\\p\": 27199, '.(exists': 27200, '.p(x))(\\\\y': 27201, \"x))')\\n>>>\": 27202, 'print(expr3)\\n(\\\\p': 27203, '.see(y,x))\\n>>>': 27204, 'print(expr3': 27205, '.simplify())\\nexists': 27206, '.see(z1,x)\\n\\n\\n\\n\\nnote\\nas': 27207, 'variable\\nnames;': 27208, 'z14': 27209, 'above\\nformula': 27210, 'an\\nillustration': 27211, '.\\n\\nafter': 27212, 'excursus,': 27213, 'english\\nsentences': 27214, '.3\\xa0\\xa0\\xa0quantified': 27215, 'nps\\nat': 27216, 'forgiven': 27217, 'thinking': 27218, 'all\\ntoo': 27219, 'surely': 27220, 'what\\nabout': 27221, 'instance?': 27222, 'want\\n(42a)': 27223, '(42b)': 27224, 'accomplished?\\n\\n': 27225, '(42)\\n': 27226, \"bark(x))\\n\\nlet's\": 27227, 'building\\ncomplex': 27228, 'bark': 27229, 'result\\nin': 27230, '(42b)?': 27231, \"subject's\": 27232, 'value\\nact': 27233, 'called\\ntype-raising': 27234, 'are\\nlooking': 27235, 'instantiating': 27236, '[sem=<?np(\\\\x': 27237, '.bark(x))>]\\nis': 27238, 'equivalent\\nto': 27239, '[sem=<exists': 27240, 'bark(x))>]': 27241, \".\\ndoesn't\": 27242, 'β-reduction\\nin': 27243, 'λ-calculus?': 27244, 'term\\nm': 27245, \"to\\n'bark'\": 27246, \"of\\n'bark'\": 27247, 'with\\nλ,': 27248, '(43)': 27249, '(43)\\\\p': 27250, 'p(x))\\nwe': 27251, 'in\\n(43)': 27252, \"'x'\": 27253, \"'y'\": 27254, 'signal\\nthat': 27255, 'an\\nindividual,': 27256, 'of\\n(43)': 27257, 'illustrate\\nfurther,': 27258, '(44)\\\\p': 27259, 'a\\nfurther': 27260, 'the\\nsemantics': 27261, '(43),': 27262, 'of\\ndog': 27263, '(45)\\\\q': 27264, '.(q(x)': 27265, 'p(x))\\napplying': 27266, 'applying\\nthat': 27267, 'us\\n\\\\p': 27268, 'p(x))(\\\\x': 27269, '.bark(x))': 27270, 'out\\nβ-reduction\\nyields': 27271, 'wanted,': 27272, '.4\\xa0\\xa0\\xa0transitive': 27273, 'verbs\\nour': 27274, 'transitive\\nverbs,': 27275, '(46)angus': 27276, 'chases': 27277, 'chase(angus,': 27278, 'x))': 27279, 'λ-abstraction': 27280, 'this\\nresult': 27281, 'require\\nthat': 27282, 'of\\nwhether': 27283, 'in\\nother': 27284, 'sticking': 27285, 'to\\n(43)': 27286, 'that\\nvps': 27287, 'regardless\\nof': 27288, 'transitive\\nverb': 27289, 'specifically,': 27290, 'vps\\nare': 27291, 'these\\nconstraints,': 27292, 'dog\\nwhich': 27293, 'trick': 27294, '(47)\\\\y': 27295, 'chase(y,': 27296, 'x))\\nthink': 27297, '(47)': 27298, 'that\\nfor': 27299, 'x;': 27300, 'more\\ncolloquially,': 27301, 'now\\nresolves': 27302, 'for\\nchases': 27303, 'allow\\n(47)': 27304, 'inverse': 27305, '(47),\\ngiving': 27306, '(48)': 27307, '(48)\\\\p': 27308, 'p(x))(\\\\z': 27309, '.chase(y,': 27310, 'z))\\n(48)': 27311, 'first;': 27312, 'from\\n(43)': 27313, '\\\\z': 27314, '.chase(y,z)': 27315, 'is\\nequivalent': 27316, 'np;': 27317, 'type\\n〈〈e,': 27318, '(49)x(\\\\z': 27319, 'z))\\nthe': 27320, 'of\\nvps,': 27321, 'ensure\\nthis': 27322, '(49)': 27323, 'by\\ngiving': 27324, '(50)\\\\x': 27325, '.x(\\\\x': 27326, 'x))\\nif': 27327, '(47),': 27328, 'along:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 27329, 'tvp': 27330, '.x(\\\\y': 27331, \".chase(x,y))')\\n>>>\": 27332, \"read_expr(r'(\\\\p\": 27333, \"p(x)))')\\n>>>\": 27334, '.applicationexpression(tvp,': 27335, 'np)\\n>>>': 27336, 'print(vp)\\n(\\\\x': 27337, '.chase(x,y)))(\\\\p': 27338, 'p(x)))\\n>>>': 27339, 'print(vp': 27340, '.simplify())\\n\\\\x': 27341, 'z2': 27342, '.(dog(z2)': 27343, 'chase(x,z2))\\n\\n\\n\\nin': 27344, 'also\\nneed': 27345, 'the\\nlatter': 27346, 'girl,': 27347, 'everything\\nproceeds': 27348, 'earlier\\non;': 27349, 'the\\nsemantic': 27350, 'have\\ncreated': 27351, 'these\\nhave': 27352, 'these\\ncannot': 27353, 'like\\n(47)': 27354, 'do\\nin': 27355, 're-interpret': 27356, 'are\\nfunction': 27357, 'required\\nλ': 27358, '(51)\\\\p': 27359, '.p(angus)\\n(51)': 27360, 'individual\\nconstant': 27361, 'of\\ntype-raising,': 27362, 'a\\nboolean-valued': 27363, 'an\\nequivalent': 27364, 'by\\nβ-reduction,': 27365, 'simple-sem': 27366, 'parsing\\nand': 27367, 'looking\\nat': 27368, \"load_parser('grammars/book_grammars/simple-sem\": 27369, 'trace=0)\\n>>>': 27370, \"'angus\": 27371, \"dog'\\n>>>\": 27372, 'print(tree': 27373, \".label()['sem'])\\nall\": 27374, '.(bone(z1)': 27375, 'give(angus,z1,z2)))\\n\\n\\n\\nnltk': 27376, 'inspect\\nsemantic': 27377, 'interpret_sents()': 27378, 'is\\nintended': 27379, 'it\\nbuilds': 27380, 'the\\ninput,': 27381, 'd[sent]': 27382, '(synrep,': 27383, 'semrep)': 27384, 'consisting\\nof': 27385, 'value\\nis': 27386, 'ambiguous;': 27387, 'sentence\\nin': 27388, \"['irene\": 27389, \"walks',\": 27390, \"'cyril\": 27391, 'bites': 27392, \"ankle']\\n>>>\": 27393, 'grammar_file': 27394, \"'grammars/book_grammars/simple-sem\": 27395, \".fcfg'\\n>>>\": 27396, '.interpret_sents(sents,': 27397, 'grammar_file):\\n': 27398, 'results:\\n': 27399, 'print(synrep)\\n(s[sem=<walk(irene)>]\\n': 27400, '(np[-loc,': 27401, 'sem=<\\\\p': 27402, '.p(irene)>]\\n': 27403, '(propn[-loc,': 27404, '.p(irene)>]': 27405, 'irene))\\n': 27406, 'sem=<\\\\x': 27407, '.walk(x)>]\\n': 27408, \"(iv[num='sg',\": 27409, '.walk(x)>,': 27410, \"tns='pres']\": 27411, 'walks)))\\n(s[sem=<exists': 27412, 'z3': 27413, '.(ankle(z3)': 27414, 'bite(cyril,z3))>]\\n': 27415, '.p(cyril)>]\\n': 27416, '.p(cyril)>]': 27417, 'cyril))\\n': 27418, 'bite(x,z3))>]\\n': 27419, '.bite(x,y))>,': 27420, 'bites)\\n': 27421, \"(np[num='sg',\": 27422, 'sem=<\\\\q': 27423, '.(ankle(x)': 27424, 'q(x))>]\\n': 27425, \"(det[num='sg',\": 27426, '.(p(x)': 27427, 'q(x))>]': 27428, 'an)\\n': 27429, \"(nom[num='sg',\": 27430, '.ankle(x)>]\\n': 27431, \"(n[num='sg',\": 27432, '.ankle(x)>]': 27433, 'ankle)))))\\n\\n\\n\\nwe': 27434, 'and\\nearlier': 27435, 'truth\\nvalue': 27436, 'as\\ndefined': 27437, 'evaluate_sents()': 27438, 'resembles\\ninterpret_sents()': 27439, 'triple': 27440, '(synrep,\\nsemrep,': 27441, 'synrep,\\nsemrep': 27442, 'simplicity,\\nthe': 27443, '.assignment(val': 27444, '.domain)\\n>>>': 27445, '.model(val': 27446, '.domain,': 27447, \"boy'\\n>>>\": 27448, '.evaluate_sents([sent],': 27449, 'grammar_file,': 27450, 'g)[0]\\n>>>': 27451, '(syntree,': 27452, 'semrep,': 27453, 'print(semrep)\\n': 27454, 'print(value)\\nall': 27455, 'z4': 27456, '.(boy(z4)': 27457, 'see(cyril,z4))\\ntrue\\n\\n\\n\\n\\n\\n\\n4': 27458, '.5\\xa0\\xa0\\xa0quantifier': 27459, 'revisited\\n\\none': 27460, 'is\\nsyntax-driven,': 27461, 'is\\nclosely': 27462, 'coupled': 27463, 'always\\nbe': 27464, '(53a),': 27465, '(53b)': 27466, '(52)every': 27467, '(53)\\n': 27468, '.(dog(y)': 27469, 'chase(x,y)))\\n\\n': 27470, 'chase(x,y)))\\n\\nthere': 27471, 'numerous': 27472, \"let's\\nbriefly\": 27473, 'scoped': 27474, \"scopings\\n\\nlet's\": 27475, 'top,': 27476, 'the\\nquantifier': 27477, 'thought\\nof': 27478, 'downwards,': 27479, 'plug': 27480, 'instantiation': 27481, 'of\\nφ': 27482, 'ψ,': 27483, \"'core'\": 27484, 'semantics,\\nnamely': 27485, 'chases\\ny': 27486, 'identical,': 27487, 'have\\nswapped': 27488, 'round': 27489, 'cooper': 27490, 'pair\\nconsisting': 27491, 'core': 27492, 'binding\\noperators': 27493, 'being\\nidentical': 27494, 'a\\ncooper-storage': 27495, '(52),': 27496, \"and\\nlet's\": 27497, 'chase(x,y)': 27498, 'in\\n(52),': 27499, '.\\n\\n\\\\p': 27500, 'p(y))(\\\\z2': 27501, '.chase(z1,z2))\\n\\nthen': 27502, 'p(x))(\\\\z1': 27503, 'chase(z1,x)))\\n\\nonce': 27504, 's-retrieval': 27505, 'possible\\norder': 27506, 'permutations\\nof': 27507, '.5),\\nthen': 27508, 'core+store\\nrepresentation': 27509, 'compositionally': 27510, 'lexical\\nrule': 27511, 'be\\nembedded': 27512, \"machinery,\\nlet's\": 27513, 'smiles': 27514, 'a\\nlexical': 27515, '(taken': 27516, 'grammar\\nstorage': 27517, '.fcfg)': 27518, '.\\n\\niv[sem=[core=<\\\\x': 27519, '.smile(x)>,': 27520, 'store=(/)]]': 27521, \"'smiles'\\n\\nthe\": 27522, '.\\n\\nnp[sem=[core=<@x>,': 27523, 'store=(<bo(\\\\p': 27524, '.p(cyril),@x)>)]]': 27525, \"'cyril'\\n\\nthe\": 27526, 'bo': 27527, 'subparts:': 27528, '(type-raised)\\nrepresentation': 27529, '@x,': 27530, 'the\\nneed': 27531, '@x': 27532, 'a\\nmetavariable,': 27533, 'individual\\nvariables': 27534, 'just\\npercolates': 27535, 'the\\ninteresting': 27536, '.\\n\\nvp[sem=?s]': 27537, 'iv[sem=?s]\\n\\ns[sem=[core=<?vp(?np)>,': 27538, 'store=(?b1+?b2)]]': 27539, '->\\n': 27540, 'np[sem=[core=?np,': 27541, 'store=?b1]]': 27542, 'vp[sem=[core=?vp,': 27543, 'store=?b2]]\\n\\nthe': 27544, \"the\\nvp's\": 27545, '.smile(x),': 27546, \"subject\\nnp's\": 27547, 'an\\ninstantiation': 27548, 'β-reduction,\\n<?vp(?np)>': 27549, '<smile(z3)>': 27550, 'when\\n@x': 27551, 'be\\ninstantiated': 27552, 'uniformly': 27553, \"np's\": 27554, 'z3,': 27555, 'yielding\\nthe': 27556, 'bo(\\\\p': 27557, '.p(cyril),z3)': 27558, '.\\n\\n(s[sem=[core=<smile(z3)>,': 27559, 'store=(bo(\\\\p': 27560, '.p(cyril),z3))]]\\n': 27561, '(np[sem=[core=<z3>,': 27562, '.p(cyril),z3))]]': 27563, 'cyril)\\n': 27564, '(vp[sem=[core=<\\\\x': 27565, 'store=()]]\\n': 27566, '(iv[sem=[core=<\\\\x': 27567, 'store=()]]': 27568, \"smiles)))\\n\\nlet's\": 27569, 'the\\nstorage': 27570, '.\\n\\ncore': 27571, '<chase(z1,z2)>\\nstore': 27572, '(bo(\\\\p': 27573, 'p(x)),z1),': 27574, 'p(x)),z2))\\n\\n\\nit': 27575, 'clearer': 27576, 'important\\npart': 27577, 's-retrieval,': 27578, 'them\\nsuccessively': 27579, '.(girl(x)\\n->': 27580, 'chase(z1,z2)': 27581, 'p(x)),\\nand': 27582, 'chase(z1,z2),': 27583, 'be\\nturned': 27584, 'λ-abstract': 27585, 'to\\nabstract': 27586, 'over?': 27587, 'us;': 27588, 'chaser': 27589, 'chasee': 27590, '.cooper_storage': 27591, 'turning\\nstorage-style': 27592, 'logical\\nforms': 27593, 'cooperstore': 27594, 'its\\nstore': 27595, 'cooper_storage': 27596, 'cs\\n>>>': 27597, \"'every\": 27598, 'cs': 27599, '.parse_with_bindops(sentence,': 27600, \"grammar='grammars/book_grammars/storage\": 27601, 'semrep': 27602, 'cs_semrep': 27603, '.cooperstore(semrep)\\n>>>': 27604, 'print(cs_semrep': 27605, '.core)\\nchase(z2,z4)\\n>>>': 27606, '.store:\\n': 27607, 'print(bo)\\nbo(\\\\p': 27608, 'p(x)),z2)\\nbo(\\\\p': 27609, 'p(x)),z4)\\n\\n\\n\\nfinally': 27610, 's_retrieve()': 27611, '.s_retrieve(trace=true)\\npermutation': 27612, '(\\\\p': 27613, 'p(x)))(\\\\z2': 27614, '.chase(z2,z4))\\n': 27615, 'p(x)))(\\\\z4': 27616, 'chase(x,z4)))\\npermutation': 27617, 'chase(z2,x)))\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 27618, '.readings:\\n': 27619, 'print(reading)\\nexists': 27620, '.(girl(z3)': 27621, 'chase(z3,x)))\\nall': 27622, '.(dog(z4)': 27623, 'chase(x,z4)))\\n\\n\\n\\n\\n\\n\\n5\\xa0\\xa0\\xa0discourse': 27624, 'semantics\\na': 27625, 'preceded\\nit': 27626, 'anaphoric': 27627, 'pronouns,': 27628, 'as\\nhe,': 27629, 'probably\\ninterpret': 27630, 'referring': 27631, 'angus\\nused': 27632, '.1\\xa0\\xa0\\xa0discourse': 27633, 'theory\\n\\nthe': 27634, 'quantification': 27635, 'single\\nsentences': 27636, 'a\\nquantifier': 27637, 'above,\\nand': 27638, 'owns': 27639, 'irene': 27640, 'own(angus,': 27641, 'x)': 27642, 'bite(x,': 27643, 'irene))\\n\\n\\nthat': 27644, 'binds\\nthe': 27645, 'theory\\n(drt)': 27646, 'of\\nproviding': 27647, 'which\\nseem': 27648, '(drs)': 27649, 'things\\nunder': 27650, 'drs': 27651, 'apply\\nto': 27652, 'referents,': 27653, 'open\\nformulas': 27654, '(54a)': 27655, 'drs;': 27656, 'processing\\nthe': 27657, 'of\\nprocessing': 27658, 'integrating': 27659, 'processed,': 27660, 'it\\ntriggers': 27661, 'referent,': 27662, 'u,\\nand': 27663, 'that\\nis,': 27664, 'drt,': 27665, 'task\\nof': 27666, 'involves\\nlinking': 27667, 'referent': 27668, 'drs,\\nand': 27669, 'about\\nanaphora': 27670, 'rise\\nto': 27671, 'and\\nthis': 27672, 'two-sentence': 27673, 'but\\nin': 27674, 'inquire': 27675, 'such\\nthat': 27676, 'named\\nangus,': 27677, 'named\\nirene': 27678, 'drss': 27679, 'computationally,': 27680, 'them\\ninto': 27681, 'conditions:\\n\\n([x,': 27682, 'y],': 27683, '[angus(x),': 27684, 'dog(y),': 27685, 'own(x,y)])\\n\\nthe': 27686, 'read_dexpr': 27687, '.drtexpression': 27688, 'drs1': 27689, \"read_dexpr('([x,\": 27690, 'own(x,': 27691, \"y)])')\": 27692, 'print(drs1)\\n([x,y],[angus(x),': 27693, 'own(x,y)])\\n\\n\\n\\n\\n\\nwe': 27694, '\\n\\n\\n\\n\\n\\nfigure': 27695, 'screenshot\\n\\nwhen': 27696, 'were\\ninterpreted': 27697, 'fol()': 27698, 'method\\nimplements': 27699, 'print(drs1': 27700, '.fol())\\nexists': 27701, '.(angus(x)': 27702, 'dog(y)': 27703, 'own(x,y))\\n\\n\\n\\nin': 27704, 'logic\\nexpressions,': 27705, 'drt': 27706, 'drs-concatenation': 27707, 'operator,\\nrepresented': 27708, 'drss\\nis': 27709, 'the\\nconditions': 27710, 'automatically\\nα-converts': 27711, 'name-clashes': 27712, 'drs2': 27713, \"read_dexpr('([x],\": 27714, '[walk(x)])': 27715, '([y],': 27716, \"[run(y)])')\\n>>>\": 27717, 'print(drs2)\\n(([x],[walk(x)])': 27718, '([y],[run(y)]))\\n>>>': 27719, 'print(drs2': 27720, '.simplify())\\n([x,y],[walk(x),': 27721, 'run(y)])\\n\\n\\n\\nwhile': 27722, 'atomic,': 27723, 'universal\\nquantification': 27724, 'drs3,': 27725, 'top-level\\ndiscourse': 27726, 'two\\nsub-drss,': 27727, 'use\\nfol()': 27728, 'drs3': 27729, \"read_dexpr('([],\": 27730, '[(([x],': 27731, '[dog(x)])': 27732, '([y],[ankle(y),': 27733, \"y)]))])')\\n>>>\": 27734, 'print(drs3': 27735, '.fol())\\nall': 27736, '.(ankle(y)': 27737, 'bite(x,y)))\\n\\n\\n\\nwe': 27738, 'be\\ninterpreted': 27739, 'sets\\nconstraints': 27740, 'possible\\nantecedents,': 27741, 'antecedent\\nis': 27742, 'candidates': 27743, '.drt_resolve_anaphora': 27744, 'similarly\\nconservative': 27745, 'strategy:': 27746, 'form\\npro(x),': 27747, 'resolve_anaphora()': 27748, 'a\\ncondition': 27749, 'antecedents': 27750, 'drs4': 27751, \"y)])')\\n>>>\": 27752, 'drs5': 27753, \"read_dexpr('([u,\": 27754, 'z],': 27755, '[pro(u),': 27756, 'irene(z),': 27757, 'bite(u,': 27758, \"z)])')\\n>>>\": 27759, 'drs6': 27760, 'drs5\\n>>>': 27761, 'print(drs6': 27762, '.simplify())\\n([u,x,y,z],[angus(x),': 27763, 'own(x,y),': 27764, 'pro(u),': 27765, 'bite(u,z)])\\n>>>': 27766, '.simplify()': 27767, '.resolve_anaphora())\\n([u,x,y,z],[angus(x),': 27768, '(u': 27769, '[x,y,z]),': 27770, 'bite(u,z)])\\n\\n\\n\\nsince': 27771, 'into\\nits': 27772, 'swapping': 27773, 'procedures\\nwhich': 27774, 'correct\\nantecedent': 27775, 'fully\\ncompatible': 27776, 'machinery': 27777, 'λ\\nabstraction,': 27778, 'semantic\\nrepresentations': 27779, 'this\\ntechnique': 27780, 'indefinites\\n(which': 27781, 'ease': 27782, 'comparison,\\nwe': 27783, 'indefinites': 27784, 'from\\nsimple-sem': 27785, '.\\n\\ndet[num=sg,sem=<\\\\p': 27786, '.(([x],[])': 27787, 'p(x)': 27788, \"'a'\\ndet[num=sg,sem=<\\\\p\": 27789, \"'a'\\n\\nto\": 27790, 'works,': 27791, 'subtree\\nfor': 27792, \".\\n\\n(np[num='sg',\": 27793, '.(([x],[dog(x)])': 27794, '.((([x],[])': 27795, 'p(x))': 27796, 'a)\\n': 27797, '.([],[dog(x)])>]\\n': 27798, '.([],[dog(x)])>]': 27799, 'dog)))))\\n\\nthe': 27800, 'to\\n\\\\x': 27801, '.([],[dog(x)])': 27802, '\\\\q': 27803, '([],[dog(x)])': 27804, '+\\nq(x));': 27805, 'simplification,': 27806, 'q(x))\\nas': 27807, 'to\\nload_parser()': 27808, 'be\\nparsed': 27809, 'drtparser': 27810, \"load_parser('grammars/book_grammars/drt\": 27811, 'logic_parser=nltk': 27812, '.drt': 27813, '.drtparser())\\n>>>': 27814, 'list(parser': 27815, \".parse('angus\": 27816, 'print(trees[0]': 27817, \".label()['sem']\": 27818, '.simplify())\\n([x,z2],[angus(x),': 27819, 'dog(z2),': 27820, 'own(x,z2)])\\n\\n\\n\\n\\n\\n5': 27821, '.2\\xa0\\xa0\\xa0discourse': 27822, 'processing\\nwhen': 27823, 'for\\ninterpretation,': 27824, 'in\\npart': 27825, 'the\\nmeaning': 27826, 'prior\\ndiscourse,': 27827, 'glaringly': 27828, 'the\\nprocessing': 27829, 'inference;': 27830, 'only\\nprocessed': 27831, 'omissions': 27832, 'redressed': 27833, 'the\\nmodule': 27834, '.inference': 27835, '.discourse': 27836, '.\\n\\nwhereas': 27837, 's1,': 27838, 'sn': 27839, 'of\\nsentences,': 27840, 'thread': 27841, 'sequence\\ns1-ri,': 27842, 'sn-rj\\nof': 27843, 'readings,': 27844, 'incrementally,': 27845, 'possible\\nthreads': 27846, 'example\\nignores': 27847, \".discoursetester(['a\": 27848, \"dances',\": 27849, \"person'])\\n>>>\": 27850, '.readings()\\n\\ns0': 27851, 'readings:\\n\\ns0-r0:': 27852, '.(student(x)': 27853, 'dance(x))\\n\\ns1': 27854, 'readings:\\n\\ns1-r0:': 27855, 'person(x))\\n\\n\\n\\nwhen': 27856, 'consistchk=true': 27857, 'checked\\nby': 27858, 'checker': 27859, 'thread,': 27860, 'of\\nadmissible': 27861, 'option\\nof': 27862, 'retracting': 27863, \".add_sentence('no\": 27864, 'consistchk=true)\\ninconsistent': 27865, 'discourse:': 27866, 'd0': 27867, \"['s0-r0',\": 27868, \"'s1-r0',\": 27869, \"'s2-r0']:\\n\": 27870, 's0-r0:': 27871, 'dance(x))\\n': 27872, 's1-r0:': 27873, 'person(x))\\n': 27874, 's2-r0:': 27875, '-exists': 27876, 'dance(x))\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 27877, \".retract_sentence('no\": 27878, 'verbose=true)\\ncurrent': 27879, 'are\\ns0:': 27880, 'dances\\ns1:': 27881, 'person\\n\\n\\n\\nin': 27882, 'informchk=true': 27883, 'φ\\nis': 27884, 'treats': 27885, 'existing\\nsentences': 27886, 'φ;': 27887, 'informative\\nif': 27888, \".add_sentence('a\": 27889, 'informchk=true)\\nsentence': 27890, \"dances'\": 27891, \"'exists\": 27892, \"dance(x))':\\nnot\": 27893, \"'d0'\\n\\n\\n\\nit\": 27894, 'as\\nbackground': 27895, 'inconsistent\\nreadings;': 27896, 'semantic\\nambiguity': 27897, 'invokes': 27898, 'glue\\nsemantics': 27899, 'configured': 27900, 'wide-coverage': 27901, 'malt': 27902, '(every': 27903, 'regexptagger\\n>>>': 27904, 'regexptagger(\\n': 27905, \"[('^(chases|runs)$',\": 27906, \"'vb'),\\n\": 27907, \"('^(a)$',\": 27908, \"'ex_quant'),\\n\": 27909, \"('^(every)$',\": 27910, \"'univ_quant'),\\n\": 27911, \"('^(dog|boy)$',\": 27912, \"'nn'),\\n\": 27913, \"('^(he)$',\": 27914, \"'prp')\\n\": 27915, 'rc': 27916, '.drtgluereadingcommand(depparser=nltk': 27917, '.maltparser(tagger=tagger))\\n>>>': 27918, \".discoursetester(['every\": 27919, \"boy',\": 27920, \"'he\": 27921, \"runs'],\": 27922, 'rc)\\n>>>': 27923, '([],[(([x],[dog(x)])': 27924, '([z3],[boy(z3),': 27925, 'chases(x,z3)]))])\\ns0-r1:': 27926, '([z4],[boy(z4),': 27927, '(([x],[dog(x)])': 27928, '([],[chases(x,z4)]))])\\n\\ns1': 27929, '([x],[pro(x),': 27930, 'runs(x)])\\n\\n\\n\\nthe': 27931, 'the\\nquantfier': 27932, 'scoping': 27933, 'pronoun\\nhe': 27934, 'pro(x)`': 27935, 'threads': 27936, 'that\\nresult:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 27937, '.readings(show_thread_readings=true)\\nd0:': 27938, \"'s1-r0']\": 27939, 'invalid:': 27940, 'anaphoraresolutionexception\\nd1:': 27941, \"['s0-r1',\": 27942, '([z6,z10],[boy(z6),': 27943, '->\\n([],[chases(x,z6)])),': 27944, '(z10': 27945, 'z6),': 27946, 'runs(z10)])\\n\\n\\n\\nwhen': 27947, 'd1,': 27948, 's0-r0,': 27949, 'out-scopes\\na': 27950, 'inadmissible': 27951, 'the\\npronoun': 27952, '(relettered': 27953, 'z24)': 27954, 'the\\nequation': 27955, '(z24': 27956, 'z20)': 27957, '.\\ninadmissible': 27958, 'filtered': 27959, 'parameter\\nfilter=true': 27960, '.readings(show_thread_readings=true,': 27961, 'filter=true)\\nd1:': 27962, '([z12,z15],[boy(z12),': 27963, '->\\n([],[chases(x,z12)])),': 27964, '(z17': 27965, 'z12),': 27966, 'runs(z15)])\\n\\n\\n\\nalthough': 27967, 'a\\nfeel': 27968, '.\\n\\n\\n\\n6\\xa0\\xa0\\xa0summary\\n\\nfirst': 27969, 'representing\\nnatural': 27970, 'is\\nflexible': 27971, 'and\\nthere': 27972, 'provers': 27973, 'order\\nlogic': 27974, '(equally,': 27975, 'semantics\\nwhich': 27976, 'believed': 27977, '.)\\nas': 27978, 'order\\nlogic,': 27979, 'by\\nexamining': 27980, 'compositionally,': 27981, 'we\\nsupplement': 27982, '.\\nβ-reduction': 27983, 'semantically\\nto': 27984, 'syntactically,': 27985, 'it\\ninvolves': 27986, 'valuation\\nwhich': 27987, 'are\\ninterpreted': 27988, 'n-ary': 27989, 'as\\nindividual': 27990, 'free\\nvariables': 27991, 'when\\ntheir': 27992, '.\\nquantifiers': 27993, 'constructing,': 27994, 'φ[x]': 27995, 'individuals\\nwhich': 27996, 'g\\nassigns': 27997, 'places\\nconstraints': 27998, 'variables;': 27999, 'the\\nvariables': 28000, 'or\\nfalse': 28001, 'by\\nbinding': 28002, 'quantifier)': 28003, 'equivalents': 28004, 'q1': 28005, 'q2,': 28006, 'outermost': 28007, 'wide\\nscope': 28008, 'q2)': 28009, 'frequently\\nambiguous': 28010, 'they\\ncontain': 28011, 'representation\\nby': 28012, 'the\\nsem': 28013, 'functional\\napplication': 28014, 'reading\\nconsult': 28015, 'the\\nprover9': 28016, 'two\\ninference': 28017, '(mccune,': 28018, 'and\\nlogic': 28019, 'other\\napproaches': 28020, 'in\\n(blackburn': 28021, 'bos,': 28022, '(dalrymple,': 28023, 'on\\nin': 28024, 'notably:\\n\\nevents,': 28025, 'aspect;\\nsemantic': 28026, 'roles;\\ngeneralized': 28027, 'most;\\nintensional': 28028, 'involving,': 28029, 'and\\nbelieve': 28030, '.\\n\\nwhile': 28031, 'dealt': 28032, 'language\\nfront-ends': 28033, '(androutsopoulos,': 28034, 'ritchie,': 28035, 'thanisch,': 28036, '(hodges,': 28037, '1977)': 28038, 'recommended': 28039, 'entertaining': 28040, 'insightful': 28041, 'text\\nwith': 28042, 'wide-ranging,': 28043, 'two-volume': 28044, 'contemporary\\nmaterial': 28045, 'and\\nintensional': 28046, '(gamut,': 28047, '1991)': 28048, '(kamp': 28049, 'reyle,': 28050, 'provides\\nthe': 28051, 'definitive': 28052, 'and\\ninteresting': 28053, 'tense,': 28054, 'and\\nmodality': 28055, 'language\\nconstructions': 28056, '(chierchia': 28057, 'mcconnell-ginet,': 28058, 'agnostic': 28059, 'while\\n(heim': 28060, 'kratzer,': 28061, '(larson': 28062, 'segal,': 28063, 'towards\\nintegrating': 28064, '.\\n(blackburn': 28065, 'computational\\nsemantics,': 28066, 'it\\nexpands': 28067, 'including\\nunderspecification': 28068, 'order\\ninference,': 28069, 'including\\ntreatments': 28070, '(lappin,': 28071, 'or\\n(benthem': 28072, 'meulen,': 28073, 'logic\\nand': 28074, '.\\nprovide': 28075, 'your\\ntranslation': 28076, 'sings,': 28077, 'sulks': 28078, '.\\ncyril': 28079, 'snow': 28080, 'rain': 28081, 'tofu': 28082, '.\\npat': 28083, 'cough': 28084, 'sneeze': 28085, 'into\\npredicate-argument': 28086, '.\\n\\nangus': 28087, 'hates': 28088, '.\\ntofu': 28089, 'taller': 28090, '.\\nbruce': 28091, 'fourlegged': 28092, 'friend': 28093, 'into\\nquantified': 28094, '.\\nangus': 28095, '.\\nnobody': 28096, '.\\nsomebody': 28097, 'coughs': 28098, 'sneezes': 28099, 'coughed': 28100, 'sneezed': 28101, 'somebody': 28102, '.\\nexactly': 28103, 'asleep': 28104, '.\\nquantified': 28105, '.\\n\\nfeed': 28106, 'capuccino': 28107, 'angus\\nbe': 28108, \"'war\": 28109, \"peace'\": 28110, 'pat\\nbe': 28111, 'loved': 28112, 'everyone\\nbe': 28113, 'detested': 28114, 'no-one\\n\\n\\n☼': 28115, 'statements:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 28116, 'e2': 28117, \"read_expr('pat')\\n>>>\": 28118, 'e3': 28119, '.applicationexpression(e1,': 28120, 'e2)\\n>>>': 28121, 'print(e3': 28122, '.love(pat,': 28123, 'y)\\n\\n\\n\\nclearly': 28124, 'e1': 28125, 'applicationexpression(e1,': 28126, 'e2)\\nto': 28127, 'β-convertible': 28128, 'e1\\nmust': 28129, 'to\\ne1,': 28130, 'all\\nsatisfied': 28131, '(up': 28132, 'variance)': 28133, 'of\\ne3': 28134, '.(love(pat,y)': 28135, 'love(y,pat))\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 28136, '.simplify())\\nwalk(fido)\\n\\n\\n\\n\\n☼': 28137, 'exercise,': 28138, 'yields\\nresults': 28139, \"read_expr('chase')\\n>>>\": 28140, 'chase(x,pat))\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 28141, 'chase(pat,x))\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 28142, \"read_expr('give')\\n>>>\": 28143, '.simplify())\\n\\\\x0': 28144, 'x1': 28145, '.(present(y)': 28146, 'give(x1,y,x0))\\n\\n\\n\\n\\n☼': 28147, \"read_expr('bark')\\n>>>\": 28148, 'bark(x))\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 28149, '.simplify())\\nbark(fido)\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 28150, \"read_expr('\\\\\\\\p\": 28151, '(dog(x)': 28152, \"p(x))')\\n>>>\": 28153, '.simplify())\\nall': 28154, 'bark(x))\\n\\n\\n\\n\\n◑': 28155, 'into\\nformulas': 28156, 'an\\napproach,': 28157, 'quantified\\nformula': 28158, 'q(a,': 28159, 'are\\nexpressions': 28160, 'example,\\nall(a,': 28161, 'the\\ntruth': 28162, 'be\\ncomputed': 28163, '.evaluate': 28164, 'will\\ngive': 28165, 'domain\\nof': 28166, 'possible\\nsource': 28167, '.gutenberg:': 28168, '.txt,\\nburgess-busterbrown': 28169, 'and\\nedgeworth-parents': 28170, 'your\\nsentences': 28171, 'truth\\nor': 28172, 'technique\\nfor': 28173, 'query\\nof': 28174, '(p(x)': 28175, 'q(x)),': 28176, '(q(x)': 28177, 'of\\np': 28178, 'acst11': 28179, 'data\\n\\n\\n\\n\\n\\n11': 28180, 'data\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nstructured': 28181, 'nlp,\\nhowever,': 28182, 'face': 28183, 'obstacles': 28184, 'its\\ncoverage,': 28185, 'balance,': 28186, 'uses?\\nwhen': 28187, 'tool,\\nhow': 28188, 'format?\\nwhat': 28189, 'it?\\n\\nalong': 28190, 'the\\ntypical': 28191, 'workflow': 28192, 'lifecycle': 28193, 'from\\npractical': 28194, 'including\\ndata': 28195, 'collected': 28196, 'fieldwork,\\nlaboratory': 28197, 'crawling': 28198, '.\\n\\n1\\xa0\\xa0\\xa0corpus': 28199, 'study\\nthe': 28200, 'timit': 28201, 'be\\nwidely': 28202, '.\\ntimit': 28203, 'texas': 28204, 'instruments': 28205, 'mit,': 28206, 'which\\nit': 28207, 'acoustic-phonetic': 28208, 'to\\nsupport': 28209, 'timit\\nlike': 28210, 'sources,\\ntimit': 28211, 'dialects,': 28212, 'speakers,': 28213, 'of\\neight': 28214, 'dialect': 28215, 'regions,': 28216, 'ages': 28217, 'educational\\nbackgrounds': 28218, 'all\\nspeakers,': 28219, 'variation:\\n\\n': 28220, '.she': 28221, 'dark': 28222, 'suit': 28223, 'greasy': 28224, 'wash': 28225, 'year\\n\\n': 28226, \".don't\": 28227, 'oily': 28228, 'rag': 28229, 'that\\n\\nthe': 28230, 'phonetically': 28231, 'rich,': 28232, '(sounds)': 28233, 'diphones': 28234, '(phone': 28235, 'bigrams)': 28236, 'strikes': 28237, 'balance\\nbetween': 28238, 'across\\nspeakers,': 28239, 'maximal\\ncoverage': 28240, 'other\\nspeakers': 28241, 'comparability)': 28242, 'coverage)': 28243, 'usual\\nway,': 28244, '.timit)': 28245, '.timit': 28246, '.fileids()': 28247, 'the\\n160': 28248, 'recorded': 28249, 'identifier:': 28250, 'made\\nup': 28251, \"speaker's\": 28252, 'region,': 28253, 'gender,': 28254, 'identifier,': 28255, 'type,\\nand': 28256, 'phonetic': 28257, 'transcription': 28258, 'phones()\\nmethod': 28259, 'customary': 28260, 'access\\nmethods': 28261, 'offset=true': 28262, 'offsets\\nof': 28263, 'audio': 28264, \".phones('dr1-fvmh0/sa1')\\n>>>\": 28265, \"phonetic\\n['h#',\": 28266, \"'sh',\": 28267, \"'iy',\": 28268, \"'hv',\": 28269, \"'ae',\": 28270, \"'dcl',\": 28271, \"'ix',\": 28272, \"'aa',\": 28273, \"'kcl',\\n's',\": 28274, \"'ux',\": 28275, \"'tcl',\": 28276, \"'gcl',\": 28277, \"'aa',\\n'sh',\": 28278, \"'epi',\": 28279, \"'dx',\": 28280, \"'ax',\": 28281, \"'ao',\": 28282, \"'ih',\": 28283, \"'h#']\\n>>>\": 28284, \".word_times('dr1-fvmh0/sa1')\\n[('she',\": 28285, '7812,': 28286, '10610),': 28287, '10610,': 28288, '14496),': 28289, \"('your',\": 28290, '14496,': 28291, \"15791),\\n('dark',\": 28292, '15791,': 28293, '20720),': 28294, \"('suit',\": 28295, '20720,': 28296, '25647),': 28297, '25647,': 28298, \"26906),\\n('greasy',\": 28299, '26906,': 28300, '32668),': 28301, \"('wash',\": 28302, '32668,': 28303, '37890),': 28304, \"('water',\": 28305, '38531,': 28306, \"42417),\\n('all',\": 28307, '43091,': 28308, '46052),': 28309, \"('year',\": 28310, '46052,': 28311, '50522)]\\n\\n\\n\\nin': 28312, 'canonical\\npronunciation': 28313, 'utterance:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 28314, 'timitdict': 28315, '.transcription_dict()\\n>>>': 28316, \"timitdict['greasy']\": 28317, \"timitdict['wash']\": 28318, \"timitdict['water']\\n['g',\": 28319, \"'iy1',\": 28320, \"'ao1',\": 28321, \"'axr']\\n>>>\": 28322, \"phonetic[17:30]\\n['g',\": 28323, \"'ax']\\n\\n\\n\\nthis\": 28324, 'system\\nwould': 28325, 'dialect\\n(new': 28326, 'england)': 28327, 'demographic': 28328, 'speakers,\\npermitting': 28329, 'vocal,': 28330, \".spkrinfo('dr1-fvmh0')\\nspeakerinfo(id='vmh0',\": 28331, \"sex='f',\": 28332, \"dr='1',\": 28333, \"use='trn',\": 28334, \"recdate='03/11/86',\\nbirthdate='01/08/60',\": 28335, \"ht='5\\\\'05',\": 28336, \"race='wht',\": 28337, \"edu='bs',\\ncomments='best\": 28338, 'england': 28339, 'accent': 28340, \"far')\\n\\n\\n\\n\\n\\n1\": 28341, '.2\\xa0\\xa0\\xa0notable': 28342, 'features\\ntimit': 28343, 'annotation,': 28344, 'orthographic\\nlevels': 28345, 'levels,\\nincluding': 28346, 'syntactic,': 28347, 'given\\nlevel': 28348, 'disagreement': 28349, 'annotators,\\nsuch': 28350, 'variation,\\nfor': 28351, 'speaker\\ndemographics': 28352, 'to\\naccount': 28353, 'envisaged': 28354, 'created,\\nsuch': 28355, 'sociolinguistics': 28356, 'sharp': 28357, 'original\\nlinguistic': 28358, 'recording,': 28359, 'usually\\nhas': 28360, 'artifact': 28361, 'transformations\\nof': 28362, 'judgment': 28363, 'as\\nsimple': 28364, 'revision,': 28365, 'to\\nretain': 28366, 'doc,': 28367, 'train,': 28368, 'directories\\nat': 28369, 'directories': 28370, 'sub-directories,': 28371, 'per\\ndialect': 28372, 'region;': 28373, 'subdirectories,': 28374, 'speaker;\\nthe': 28375, 'aks0': 28376, 'listed,': 28377, 'showing\\n10': 28378, 'wav': 28379, 'accompanied': 28380, 'transcription,': 28381, 'word-aligned': 28382, 'transcription,\\nand': 28383, '20,000': 28384, 'are\\norganized': 28385, 'schematically': 28386, 'gives\\naway': 28387, 'transcriptions': 28388, 'associated\\ndata': 28389, '.\\nmoreover,': 28390, 'into\\nthe': 28391, '.\\neven': 28392, 'demographics': 28393, 'the\\nprimary': 28394, 'subfields': 28395, 'management,\\nnamely': 28396, 'techniques\\nfrom': 28397, '.3\\xa0\\xa0\\xa0fundamental': 28398, 'types\\n\\n\\nfigure': 28399, 'texts:': 28400, 'amid': 28401, 'diversity,\\nlexicons': 28402, '.\\n\\ndespite': 28403, 'complexity,': 28404, 'types,\\nnamely': 28405, 'fields,': 28406, 'as\\nshown': 28407, 'conventional\\ndictionary': 28408, 'wordlist,': 28409, 'record-structured': 28410, 'entries\\nvia': 28411, 'non-key': 28412, 'tabulations': 28413, '(known': 28414, 'paradigms)\\nto': 28415, 'variation,': 28416, \"timit's\": 28417, 'kind\\nof': 28418, 'event,\\nand': 28419, 'time-course': 28420, 'small\\nunit,': 28421, 'narrative': 28422, 'with\\nannotations': 28423, '.),': 28424, 'higher-level\\nconstituents': 28425, 'complexities': 28426, 'are\\ncollections': 28427, 'biased': 28428, 'relate\\nthe': 28429, 'wordnet\\ncontains': 28430, 'records,': 28431, 'incorporates': 28432, '(mini-texts)\\nto': 28433, 'usages': 28434, 'mid-point': 28435, 'substantial\\nfree-standing': 28436, '.\\n\\n\\n\\n2\\xa0\\xa0\\xa0the': 28437, 'life-cycle': 28438, 'corpus\\ncorpora': 28439, 'fully-formed,': 28440, 'preparation\\nand': 28441, 'needs\\nto': 28442, 'collected,': 28443, 'cleaned': 28444, 'documented,': 28445, 'systematic\\nstructure': 28446, 'applied,': 28447, 'requiring\\nspecialized': 28448, '.\\nsuccess': 28449, 'workflow\\ninvolving': 28450, 'converters': 28451, '.\\nquality': 28452, 'inconsistencies\\nin': 28453, 'highest\\npossible': 28454, 'the\\nscale': 28455, 'to\\nprepare,': 28456, 'person-years': 28457, 'the\\nlife-cycle': 28458, '.1\\xa0\\xa0\\xa0three': 28459, 'creation': 28460, 'scenarios\\nin': 28461, 'unfolds': 28462, 'over\\nin': 28463, \"creator's\": 28464, 'explorations': 28465, 'pattern\\ntypical': 28466, 'from\\nelicitation': 28467, 'gathered,': 28468, \"tomorrow's\": 28469, 'elicitation': 28470, 'corpus\\nis': 28471, 'serve\\nas': 28472, 'archival': 28473, 'computerization': 28474, 'an\\nobvious': 28475, 'boon': 28476, 'exemplified': 28477, 'popular\\nprogram': 28478, 'shoebox,': 28479, 'decades': 28480, 're-released': 28481, 'toolbox\\n(see': 28482, 'tools,': 28483, 'spreadsheets,': 28484, 'routinely\\nused': 28485, 'extract\\ndata': 28486, 'experimental': 28487, 'research\\nwhere': 28488, 'carefully-designed': 28489, 'subjects,\\nthen': 28490, 're-used\\nwithin': 28491, 'laboratory': 28492, 'more\\nwidely': 28493, 'the\\ncommon': 28494, 'management,': 28495, 'the\\npast': 28496, 'norm': 28497, 'government-funded': 28498, 'research\\nprograms': 28499, 'chapters;\\nwe': 28500, 'of\\ncuration': 28501, 'gather': 28502, '(anc)\\nand': 28503, 'british': 28504, '(bnc)': 28505, 'goal\\nhas': 28506, 'the\\nmany': 28507, 'scale,': 28508, 'reliance\\non': 28509, 'post-editing': 28510, 'to\\nfix': 28511, 'locate\\nand': 28512, '.2\\xa0\\xa0\\xa0quality': 28513, 'control\\ngood': 28514, 'data\\nare': 28515, 'just\\nas': 28516, 'mundane': 28517, '.\\nannotation': 28518, 'markup\\nconventions': 28519, 'regularly': 28520, 'difficult\\ncases,': 28521, 'devised': 28522, 'more\\nconsistent': 28523, 'the\\nprocedures,': 28524, 'covered\\nin': 28525, 'established,': 28526, 'possibly\\nwith': 28527, 'initialized,\\nannotated,': 28528, 'validated,': 28529, 'checked,': 28530, 'of\\nannotation,': 28531, 'specialists': 28532, 'uncertainty\\nor': 28533, 'adjudication': 28534, '.\\nlarge': 28535, 'annotators,': 28536, 'which\\nraises': 28537, 'consistently': 28538, 'perform?\\nwe': 28539, 'reveal': 28540, 'or\\ndiffering': 28541, 'paramount,': 28542, 'adjudicated\\nby': 28543, 'inter-annotator\\nagreement': 28544, 'double-annotating\\n10%': 28545, 'trained\\non': 28546, '.\\n\\ncaution!\\ncare': 28547, 'exercised': 28548, 'their\\ndifficulty': 28549, 'terrible\\nscore': 28550, 'exceptional': 28551, 'score\\nfor': 28552, 'kappa': 28553, 'coefficient': 28554, 'between\\ntwo': 28555, 'judgments,': 28556, 'correcting': 28557, 'expected\\nchance': 28558, 'annotated,\\nand': 28559, 'options': 28560, 'better\\nlevels': 28561, '50%,': 28562, 'get\\nk': 28563, '.333,': 28564, 'exist;': 28565, '.agreement)': 28566, 'sequence:': 28567, 'rectangles': 28568, 'characters,\\nwords,': 28569, 'short,': 28570, 'units;': 28571, 's1': 28572, 's2': 28573, 'close\\nagreement,': 28574, 's3': 28575, 'segmentations\\nof': 28576, 'segmentation,\\nnamed-entity': 28577, 'three\\npossible': 28578, 'programs)': 28579, 'exactly,': 28580, 's2\\nare': 28581, '.\\nwindowdiff': 28582, 'of\\ntwo': 28583, 'the\\ndata': 28584, 'awarding': 28585, 'credit': 28586, 'ones,': 28587, 'to\\nrecord': 28588, 'the\\nsegmentations': 28589, 'windowdiff': 28590, 'scorer': 28591, '00000010000000001000000\\n>>>': 28592, '00000001000000010000000\\n>>>': 28593, '00010000000000000001000\\n>>>': 28594, '.windowdiff(s1,': 28595, '3)\\n0': 28596, '.190': 28597, '.windowdiff(s2,': 28598, 's3,': 28599, '.571': 28600, '.\\n\\n\\n\\nin': 28601, 'slides': 28602, 'this\\nwindow,': 28603, 'differences\\nare': 28604, 'summed': 28605, 'shrink': 28606, 'sensitivity': 28607, 'the\\nmeasure': 28608, '.3\\xa0\\xa0\\xa0curation': 28609, 'evolution\\nas': 28610, 'published,': 28611, 'increasingly\\nlikely': 28612, 'investigations': 28613, 'balanced,': 28614, 'focused\\nsubsets': 28615, 'entirely\\ndifferent': 28616, 'switchboard': 28617, 'database,\\noriginally': 28618, 'research,\\nhas': 28619, 'studies': 28620, 'recognition,\\nword': 28621, 'pronunciation,': 28622, 'disfluency,': 28623, 'intonation': 28624, 'recycling': 28625, 'the\\ndesire': 28626, 'desire': 28627, 'material\\navailable': 28628, 'replication,': 28629, 'study\\nmore': 28630, 'naturalistic': 28631, 'possible\\notherwise': 28632, 'may\\ncount': 28633, 'file\\n(e': 28634, 'xml),': 28635, 'renaming': 28636, 'retokenizing': 28637, 'text,\\nselecting': 28638, 'enrich,': 28639, '.\\nmultiple': 28640, 'groups': 28641, 'independently,': 28642, 'illustrated\\nin': 28643, 'sources\\nof': 28644, 'onerous': 28645, 'evolution': 28646, 'time:': 28647, 'research\\ngroups': 28648, 'enriching': 28649, 'pieces;\\nlater': 28650, 'confronts\\nthe': 28651, 'aligning': 28652, 'of\\nany': 28653, 'created,': 28654, 'most\\nup-to-date': 28655, 'chaotic': 28656, 'centrally': 28657, 'curated,\\nand': 28658, 'committees': 28659, 'periodic\\nintervals,': 28660, 'submissions': 28661, 'third-parties,': 28662, 'publishing': 28663, 'releases\\nfrom': 28664, 'curated': 28665, 'impractical': 28666, 'publication': 28667, 'identifying\\nany': 28668, 'sub-part': 28669, 'entry,': 28670, 'unique\\nidentifier,': 28671, '(respectively)': 28672, '.\\nannotations,': 28673, 'segmentations,': 28674, 'using\\nthis': 28675, 'standoff': 28676, 'annotation)': 28677, 'and\\nmultiple': 28678, 'be\\ncompared': 28679, 'version\\nnumber': 28680, 'correspondences': 28681, 'editions': 28682, 'corpus\\nwould': 28683, '.\\n\\ncaution!\\nsometimes': 28684, 'revisions': 28685, 'merged,': 28686, 'constituents\\nmay': 28687, 'rearranged': 28688, 'one-to-one': 28689, 'between\\nold': 28690, 'break\\non': 28691, 'silently': 28692, 'identifiers\\nto': 28693, '.\\n\\n\\n\\n\\n3\\xa0\\xa0\\xa0acquiring': 28694, 'data\\n\\n3': 28695, '.1\\xa0\\xa0\\xa0obtaining': 28696, 'web\\nthe': 28697, 'already\\ndiscussed': 28698, 'feeds,': 28699, 'engine\\nresults': 28700, 'acl\\nspecial': 28701, '(sigwac)': 28702, 'resources\\nat': 28703, '.sigwac': 28704, 'documented,\\nstable,': 28705, 'reproducible': 28706, 'desired': 28707, 'website,': 28708, 'many\\nutilities': 28709, 'site,': 28710, 'as\\ngnu': 28711, 'wget': 28712, '.gnu': 28713, '.org/software/wget/': 28714, 'flexibility': 28715, 'crawler': 28716, 'used,\\nsuch': 28717, 'heritrix': 28718, 'http://crawler': 28719, '.archive': 28720, '.\\ncrawlers': 28721, 'look,': 28722, 'to\\nfollow,': 28723, 'a\\nbilingual': 28724, 'language,\\nthe': 28725, 'the\\ncorrespondence': 28726, 'downloaded\\npages': 28727, 'tempting\\nto': 28728, 'web-crawler,': 28729, 'with\\ndetecting': 28730, 'mime': 28731, 'urls,': 28732, 'avoiding': 28733, 'getting\\ntrapped': 28734, 'cyclic': 28735, 'latencies,': 28736, 'avoiding\\noverloading': 28737, 'banned': 28738, '.2\\xa0\\xa0\\xa0obtaining': 28739, 'processor': 28740, 'files\\nword': 28741, 'preparation\\nof': 28742, 'computational\\ninfrastructure': 28743, 'data\\nentry,': 28744, 'may\\nbe': 28745, 'lexical\\nentry': 28746, 'larger\\nproportion': 28747, 'programs?\\nmoreover,': 28748, 'validate': 28749, 'to\\nhelp': 28750, 'well-structured': 28751, 'the\\nquality': 28752, 'maximized': 28753, 'authoring': 28754, 'process?\\nconsider': 28755, 'field,': 28756, '20\\npossibilities,': 28757, 'rendered\\nin': 28758, '11-point': 28759, 'macro\\nfunctions': 28760, 'capable': 28761, 'verifying': 28762, 'exhaustive\\nmanual': 28763, 'be\\nsaved': 28764, 'non-proprietary': 28765, 'html,': 28766, 'can\\nsometimes': 28767, 'entry:\\nsleep': 28768, '[sli:p]': 28769, 'page,\\nthen': 28770, 'file:\\n\\n<p': 28771, 'class=msonormal>sleep\\n': 28772, '<span': 28773, \"style='mso-spacerun:yes'>\": 28774, '</span>\\n': 28775, '[<span': 28776, 'class=spelle>sli:p</span>]\\n': 28777, '<b><span': 28778, \"style='font-size:11\": 28779, \".0pt'>v\": 28780, '.</span></b>\\n': 28781, '<i>a': 28782, '.<o:p></o:p></i>\\n</p>\\n\\nobserve': 28783, 'paragraph,': 28784, 'the\\n<p>': 28785, \"<span\\nstyle='font-size:11\": 28786, \".0pt'>\": 28787, 'defines\\nthe': 28788, 'parts-of-speech,': 28789, 'legal_pos': 28790, 'all\\n11-point': 28791, 'dict': 28792, '.htm': 28793, 'set\\nused_pos': 28794, 'a\\nparenthesized': 28795, 'sub-expression;': 28796, 'this\\nsub-expression': 28797, '.findall': 28798, 'program\\nconstructs': 28799, 'illegal': 28800, 'parts-of-speech': 28801, 'used_pos': 28802, '-\\nlegal_pos:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 28803, \"set(['n',\": 28804, \"'det'])\\n>>>\": 28805, \".compile(r'font-size:11\": 28806, \".0pt'>([a-z\": 28807, '.]+)<)\\n>>>': 28808, 'open(dict': 28809, '.htm,': 28810, 'encoding=windows-1252)': 28811, 'set(re': 28812, '.findall(pattern,': 28813, 'document))\\n>>>': 28814, 'illegal_pos': 28815, '.difference(legal_pos)\\n>>>': 28816, \"print(list(illegal_pos))\\n['v\": 28817, \"'intrans']\\n\\n\\n\\nthis\": 28818, 'tip': 28819, 'iceberg': 28820, 'develop\\nsophisticated': 28821, 'files,\\nand': 28822, 'maintainer': 28823, 'correct\\nthe': 28824, 'formatted,': 28825, 'beautifulsoup': 28826, 'library,\\nextracts': 28827, 'pronunciations,': 28828, 'output\\nin': 28829, '(csv)': 28830, 'lexical_data(html_file,': 28831, 'encoding=utf-8):\\n': 28832, \"'_entry'\\n\": 28833, 'open(html_file,': 28834, 'encoding=encoding)': 28835, \".sub(r'<p',\": 28836, \"'<p',\": 28837, 'html)\\n': 28838, '.join(text': 28839, '.split())\\n': 28840, '.split(sep):\\n': 28841, \".count('\": 28842, '2:\\n': 28843, '3)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 28844, 'writer': 28845, '.writer(open(dict1': 28846, 'encoding=utf-8))\\n>>>': 28847, '.writerows(lexical_data(dict': 28848, 'encoding=windows-1252))\\n\\n\\nexample': 28849, '(code_html2csv': 28850, 'microsoft': 28851, 'values\\n\\n\\nwith': 28852, 'gzip': 28853, '.open(fn+': 28854, '.gz,wb)': 28855, 'f_out:\\nf_out': 28856, '.write(bytes(s,': 28857, \"'utf-8'))\\n\\n\\n\\n3\": 28858, '.3\\xa0\\xa0\\xa0obtaining': 28859, 'spreadsheets': 28860, 'databases\\nspreadsheets': 28861, 'spreadsheet,\\nwith': 28862, 'language\\n(cf': 28863, '.swadesh,': 28864, 'www': 28865, '.rosettaproject': 28866, '.org)': 28867, 'csv\\ncomma-separated': 28868, 'for\\npython': 28869, 'full-fledged': 28870, 'normalized,': 28871, 'validity\\nof': 28872, 'can\\nrequire': 28873, 'by\\ndeclaring': 28874, 'enumerated': 28875, 'type\\nor': 28876, 'data\\n(the': 28877, 'schema)': 28878, 'advance,': 28879, 'dominant': 28880, 'structuring': 28881, 'is\\nhighly': 28882, 'exploratory': 28883, 'obligatory\\nand': 28884, 'repeatable': 28885, 'relational\\ndatabase': 28886, 'however\\nif': 28887, 'optional\\nor': 28888, 'unworkable': 28889, 'database,\\nit': 28890, 'dump': 28891, 'results)\\nin': 28892, 'perform\\na': 28893, '.\\nselect': 28894, 'record\\nfor': 28895, 'identified,': 28896, 'headwords': 28897, 'example\\nsentences': 28898, 'file\\ndict': 28899, '.csv:\\n\\nsleep,sli:p,v': 28900, '.i,a': 28901, '.\\nwalk,wo:k,v': 28902, '.intr,progress': 28903, '.\\nwake,weik,intrans,cease': 28904, 'sleep\\n\\nnow': 28905, \".reader(open('dict\": 28906, \".csv'))\\n>>>\": 28907, '[(lexeme,': 28908, 'defn)': 28909, '(lexeme,': 28910, '_,': 28911, 'lexicon]\\n>>>': 28912, 'lexemes,': 28913, 'defns': 28914, 'zip(*pairs)\\n>>>': 28915, 'defn_words': 28916, 'defn': 28917, '.split())\\n>>>': 28918, 'sorted(defn_words': 28919, \".difference(lexemes))\\n['\": 28920, \"'body',\": 28921, \"'cease',\": 28922, \"'condition',\": 28923, \"'down',\": 28924, \"'each',\\n'foot',\": 28925, \"'lifting',\": 28926, \"'mind',\": 28927, \"'progress',\": 28928, \"'setting',\": 28929, \"'to']\\n\\n\\n\\nthis\": 28930, 'enrich': 28931, 'lexicon,\\nwork': 28932, '.4\\xa0\\xa0\\xa0converting': 28933, 'formats\\nannotated': 28934, 'arrives': 28935, 'format,\\nand': 28936, '.\\nconverting': 28937, 'discussed\\n(see': 28938, 'isomorphic': 28939, 'toolbox\\nformat': 28940, 'transliterate': 28941, 'the\\nentries': 28942, 'structure\\nof': 28943, 'required\\nprogram:': 28944, 'care\\nof': 28945, 'digested': 28946, 'necessary\\nto': 28947, '.8),\\nthen': 28948, 'maps\\nthe': 28949, 'corresponding\\nlexeme': 28950, ',\\nhaving': 28951, 'discarded': 28952, 'over\\nthe': 28953, 'idx': 28954, '.index((defn_word,': 28955, 'lexeme)': 28956, 'defn_word': 28957, '.word_tokenize(defn)': 28958, 'len(defn_word)': 28959, '.idx,': 28960, 'idx_file:\\n': 28961, 'sorted(idx):\\n': 28962, 'idx_words': 28963, '.join(idx[word])\\n': 28964, 'idx_line': 28965, '{}:': 28966, 'idx_words)': 28967, 'print(idx_line,': 28968, 'file=idx_file)\\n\\n\\n\\nthe': 28969, '.idx': 28970, 'larger\\ndictionary': 28971, 'lexemes': 28972, '.)\\n\\nbody:': 28973, 'sleep\\ncease:': 28974, 'wake\\ncondition:': 28975, 'sleep\\ndown:': 28976, 'walk\\neach:': 28977, 'walk\\nfoot:': 28978, 'walk\\nlifting:': 28979, 'walk\\nmind:': 28980, 'sleep\\nprogress:': 28981, 'walk\\nsetting:': 28982, 'walk\\nsleep:': 28983, 'wake\\n\\nin': 28984, 'single\\ncolumn': 28985, 'two-dimensional\\ntable': 28986, 'populate\\nan': 28987, 'off\\nthe': 28988, 'vexing': 28989, 'different\\ncoverage': 28990, 'unavoidably': 28991, 'file\\ncontaining': 28992, 'loosing': 28993, '\\\\lx': 28994, 'labor-intensive': 28995, 'inject\\nthe': 28996, 'round-tripping\\nproblem': 28997, 'propagate\\nthe': 28998, '.5\\xa0\\xa0\\xa0deciding': 28999, 'include\\npublished': 29000, 'typically\\ncontain': 29001, 'morphology,': 29002, 'prosody,': 29003, 'and\\nsemantic': 29004, 'relations\\nor': 29005, 'annotation\\nmay': 29006, 'given\\nlinguistic': 29007, 'structures;\\nand': 29008, 'provided\\nannotation': 29009, 'layers:\\n\\nword': 29010, 'tokenization:': 29011, 'unambiguously\\nidentify': 29012, 'version,\\nin': 29013, 'version,\\nmay': 29014, 'segmentation:': 29015, 'sentence\\nsegmentation': 29016, 'corpora\\ntherefore': 29017, '.\\nparagraph': 29018, 'elements\\n(headings,': 29019, '.\\npart': 29020, 'speech:': 29021, '.\\nshallow': 29022, 'semantics:': 29023, 'coreference': 29024, '.\\ndialogue': 29025, 'rhetorical': 29026, 'structure\\n\\nunfortunately,': 29027, 'classes\\nof': 29028, 'inline\\nannotation': 29029, 'special\\nsymbols': 29030, 'string\\nfly': 29031, 'fly/nn,': 29032, 'indicate\\nthat': 29033, 'standoff\\nannotation': 29034, 'instead\\ncreates': 29035, 'pointers\\nthat': 29036, 'might\\ncontain': 29037, '<token': 29038, 'id=8': 29039, \"pos='nn'/>,\": 29040, 'tokenization\\nitself': 29041, 'references\\nto': 29042, '.6\\xa0\\xa0\\xa0standards': 29043, 'tools\\nfor': 29044, 'widely\\nsupported': 29045, 'creation,': 29046, 'must\\ndevelop': 29047, 'help\\nto': 29048, 'adequate,': 29049, 'generally-accepted': 29050, 'standards': 29051, 'for\\nexpressing': 29052, 'without\\nsuch': 29053, 'standards,': 29054, 'developed,': 29055, 'forge': 29056, 'developing\\na': 29057, 'of\\nannotation': 29058, 'examples)': 29059, 'generality\\nof': 29060, 'must\\nbe': 29061, 'validated': 29062, 'rootedness,': 29063, 'connectedness,\\nand': 29064, 'acyclicity': 29065, 'program\\nwould': 29066, 'loaded,\\nbut': 29067, 'invalidate': 29068, 'obliterate': 29069, 'data\\nwas': 29070, 'one-off': 29071, 'scripts\\nto': 29072, 'formats;': 29073, 'scripts': 29074, 'litter': 29075, 'filespaces': 29076, 'many\\nnlp': 29077, 'systematic\\napproach,': 29078, 'founded': 29079, 'premise': 29080, 'format\\nshould': 29081, '(per': 29082, 'interface\\n\\ninstead': 29083, 'focussing': 29084, '.corpus)': 29085, 'elements,\\nor': 29086, '(child-id,': 29087, 'parent-id)': 29088, 'line,\\nor': 29089, 'notation,': 29090, 'allows\\napplication': 29091, 'data\\nusing': 29092, 'children(),': 29093, 'leaves(),': 29094, 'depth(),': 29095, 'and\\nso': 29096, 'within\\ncomputer': 29097, 'science,': 29098, 'viz': 29099, 'design,\\nand': 29100, '—\\nallows': 29101, 'end-user': 29102, 'model)\\nand': 29103, '(sql),': 29104, 'idiosyncrasies\\nof': 29105, 'innovations': 29106, 'filesystem': 29107, 'technologies\\nto': 29108, 'disturbing': 29109, 'interface\\ninsulates': 29110, 'dissemination,': 29111, 'is\\nexpedient': 29112, 'widely-used': 29113, 'software\\n—': 29114, 'supports\\nexisting': 29115, '.7\\xa0\\xa0\\xa0special': 29116, 'languages\\nthe': 29117, 'in\\nsignificance': 29118, 'treasure': 29119, 'embodied': 29120, '~7,000': 29121, 'respects,\\nin': 29122, 'oral': 29123, 'histories': 29124, 'legends,': 29125, 'grammatical\\nconstructions': 29126, 'nuances': 29127, '.\\nthreatened': 29128, 'remnant': 29129, 'cultures': 29130, 'subspecies\\naccording': 29131, 'therapeutic': 29132, 'languages\\nevolve': 29133, 'each\\none': 29134, 'pre-history': 29135, 'world,': 29136, 'variations\\nfrom': 29137, 'drive': 29138, 'breathtaking': 29139, 'and\\ndiversity,': 29140, 'colorful': 29141, 'tapestry': 29142, 'stretching\\nthrough': 29143, 'extinction': 29144, 'constructing\\nrich': 29145, 'records': 29146, 'facet': 29147, 'heritage': 29148, 'offer': 29149, 'effort?': 29150, 'developing\\ntaggers,': 29151, 'recognizers,': 29152, 'etc,\\nis': 29153, 'priority,': 29154, 'voiced': 29155, 'curating': 29156, 'focus\\non': 29157, 'texts\\nin': 29158, 'vexed': 29159, 'who\\nowns': 29160, 'sensitivities': 29161, 'texts,\\nthere': 29162, 'orthography': 29163, 'language\\nhas': 29164, 'literary': 29165, 'tradition,': 29166, 'punctuation\\nare': 29167, 'well-established': 29168, 'practice\\nto': 29169, 'collection,': 29170, 'continually': 29171, 'updating\\nthe': 29172, 'done\\nusing': 29173, 'lexicon)': 29174, '.\\nbetter': 29175, \"sil's\": 29176, 'fieldworks\\nprovide': 29177, 'themselves,\\na': 29178, 'obstacle': 29179, 'overriding': 29180, 'methods\\nthat': 29181, 'arbitrary\\nword': 29182, 'acute': 29183, 'that\\nincludes': 29184, 'prefixes': 29185, 'with\\nsemantic': 29186, 'domains,': 29187, '.\\npermitting': 29188, 'confusible': 29189, 'sequences,\\nand': 29190, 'notice\\nthat': 29191, 'cluster': 29192, 'consonants\\nis': 29193, 'the\\norder': 29194, 'consonants': 29195, \"[('ph',\": 29196, \"'f'),\": 29197, \"('ght',\": 29198, \"'t'),\": 29199, \"('^kn',\": 29200, \"('qu',\": 29201, \"'kw'),\\n\": 29202, \"('[aeiou]+',\": 29203, \"(r'(\": 29204, \".)\\\\1',\": 29205, \"r'\\\\1')]\\n>>>\": 29206, 'signature(word):\\n': 29207, 'patt,': 29208, 'repl': 29209, 'mappings:\\n': 29210, '.sub(patt,': 29211, 'repl,': 29212, \".findall('[^aeiou]+',\": 29213, '.join(char': 29214, 'sorted(piece))[:8]\\n>>>': 29215, \"signature('illefent')\\n'lfnt'\\n>>>\": 29216, \"signature('ebsekwieous')\\n'bskws'\\n>>>\": 29217, \"signature('nuculerr')\\n'nclr'\\n\\n\\n\\nnext,\": 29218, 'signatures': 29219, 'corrections': 29220, 'signature)': 29221, '.index((signature(w),': 29222, \"signatures[signature('nuculerr')]\\n['anicular',\": 29223, \"'inocular',\": 29224, \"'nucellar',\": 29225, \"'nuclear',\": 29226, \"'unicolor',\": 29227, \"'uniocular',\": 29228, \"'unocular']\\n\\n\\n\\nfinally,\": 29229, 'rank()': 29230, 'rank(word,': 29231, 'sorted((nltk': 29232, '.edit_distance(word,': 29233, 'w),': 29234, 'ranked]\\n>>>': 29235, 'fuzzy_spell(word):\\n': 29236, 'signature(word)\\n': 29237, 'signatures:\\n': 29238, 'signatures[sig])\\n': 29239, \"fuzzy_spell('illefent')\\n['olefiant',\": 29240, \"'oliphant',\": 29241, \"'elephanta']\\n>>>\": 29242, \"fuzzy_spell('ebsekwieous')\\n['obsequious']\\n>>>\": 29243, \"fuzzy_spell('nucular')\\n['anicular',\": 29244, \"'unocular',\": 29245, \"'unicolor']\\n\\n\\n\\nthis\": 29246, 'lexical\\ndata': 29247, 'standardized,': 29248, 'or\\nwhere': 29249, 'spellings': 29250, 'simple\\napplications': 29251, 'include:': 29252, 'to\\ndata,': 29253, 'gleaning': 29254, 'texts,\\nlocating': 29255, 'prevalent': 29256, 'or\\nexceptional': 29257, 'validation\\non': 29258, 'last\\nof': 29259, '.\\n\\n\\n\\n4\\xa0\\xa0\\xa0working': 29260, 'xml\\nthe': 29261, '(xml)': 29262, 'designing\\ndomain-specific': 29263, 'representing\\nannotated': 29264, 'predefined\\ntags,': 29265, 'xml\\npermits': 29266, 'section\\nwe': 29267, 'representing\\nlinguistic': 29268, 'using\\npython': 29269, '.1\\xa0\\xa0\\xa0using': 29270, 'structures\\nthanks': 29271, 'extensibility,': 29272, 'natural\\nchoice': 29273, '(2)\\n<entry>\\n': 29274, '<headword>whale</headword>\\n': 29275, '<pos>noun</pos>\\n': 29276, '<gloss>any': 29277, 'cetacean': 29278, 'mammals': 29279, 'streamlined\\n': 29280, 'breathing': 29281, 'blowhole': 29282, 'head</gloss>\\n</entry>\\n\\n\\nit': 29283, 'enclosed': 29284, '<gloss>': 29285, 'closing': 29286, '</gloss>;\\ntogether': 29287, 'nicely': 29288, 'whitespace,': 29289, 'could\\nequally': 29290, 'processing\\nxml': 29291, 'be\\nwell': 29292, 'formed,': 29293, 'tree)': 29294, '.\\nxml': 29295, 'layout\\ndoes': 29296, '(3)\\n<entry><headword>whale</headword><pos>noun</pos><gloss>any': 29297, 'the\\nlarger': 29298, 'streamlined': 29299, 'breathing\\nthrough': 29300, 'head</gloss><gloss>a': 29301, 'person;\\nimpressive': 29302, 'qualities</gloss></entry>\\n\\n\\na': 29303, 'wordnet,\\nusing': 29304, 'identifier\\ninside': 29305, '(4)\\n<entry>\\n': 29306, '<sense>\\n': 29307, 'head</gloss>\\n': 29308, '<synset>whale': 29309, '.02</synset>\\n': 29310, '</sense>\\n': 29311, '<gloss>a': 29312, 'person;': 29313, 'qualities</gloss>\\n': 29314, '<synset>giant': 29315, '.04</synset>\\n': 29316, '</sense>\\n</entry>\\n\\n\\nalternatively,': 29317, 'attribute,\\nwithout': 29318, '(5)\\n<entry>\\n': 29319, '<gloss': 29320, 'synset=whale': 29321, '.02>any': 29322, 'having\\n': 29323, 'synset=giant': 29324, '.04>a': 29325, 'or\\n': 29326, 'qualities</gloss>\\n</entry>\\n\\n\\nthis': 29327, \"arbitrary\\nthat's\": 29328, 'is!': 29329, 'attribute\\nnames,': 29330, 'them\\nout,': 29331, 'whose\\npresence': 29332, 'speech\\nis': 29333, 'past_tense': 29334, 'past_tense\\nelement': 29335, 'all\\nthis': 29336, 'schema,\\nwhich': 29337, 'akin': 29338, 'schema': 29339, 'xml\\nwe': 29340, 'complication,': 29341, 'permitting\\nan': 29342, 'program\\nthat': 29343, 'the\\ndata,': 29344, 'interrogate': 29345, 'data\\nmodeling': 29346, 'data,\\nthen': 29347, 'schema,': 29348, 'then\\nwrite': 29349, '.\\nsimilarly,': 29350, 'concerning\\ndata': 29351, 'wise': 29352, 'when\\nonly': 29353, 'cross-reference': 29354, 'was\\nrepresented': 29355, '<xref>headword</xref>': 29356, 'storage\\nof': 29357, 'break\\nif': 29358, '.\\nexistential': 29359, 'be\\nmodeled,': 29360, 'independently\\nof': 29361, 'entry\\nelement': 29362, 'many-to-many': 29363, 'abstracted': 29364, 'of\\nhierarchical': 29365, 'corresponding\\nsenses,': 29366, 'then\\nboth': 29367, 'sense)': 29368, 'split\\nacross': 29369, 'format\\naccompanied': 29370, 'panacea': 29371, 'elementtree': 29372, \"interface\\npython's\": 29373, 'access\\ndata': 29374, 'first\\nat': 29375, 'some\\nxml': 29376, 'headers': 29377, '.dtd,\\nfollowed': 29378, '.\\n(some': 29379, 'merchant_file': 29380, \".find('corpora/shakespeare/merchant\": 29381, 'open(merchant_file)': 29382, 'print(raw[:163])': 29383, '\\n<?xml': 29384, 'version=1': 29385, '.0?>\\n<?xml-stylesheet': 29386, 'type=text/css': 29387, 'href=shakes': 29388, '.css?>\\n<!--': 29389, '<!doctype': 29390, '.dtd>': 29391, '-->\\n<play>\\n<title>the': 29392, 'venice</title>\\n>>>': 29393, 'print(raw[1789:2006])': 29394, '\\n<title>act': 29395, 'i</title>\\n<scene><title>scene': 29396, 'venice': 29397, '.</title>\\n<stagedir>enter': 29398, 'antonio,': 29399, 'salarino,': 29400, 'salanio</stagedir>\\n<speech>\\n<speaker>antonio</speaker>\\n<line>in': 29401, 'sooth,': 29402, 'sad:</line>\\n\\n\\n\\nwe': 29403, 'see,\\nthe': 29404, 'title,': 29405, 'scene,': 29406, 'directions,': 29407, 'data,\\nusing': 29408, 'string)\\nand': 29409, 'structure;': 29410, 'index\\nto': 29411, 'child,': 29412, 'the\\ngetchildren()': 29413, '.etree': 29414, '.elementtree': 29415, 'elementtree\\n>>>': 29416, 'elementtree()': 29417, '.parse(merchant_file)': 29418, 'merchant\\n<element': 29419, \"'play'\": 29420, '0x10ac43d18>': 29421, '[_element-play]\\n>>>': 29422, 'merchant[0]\\n<element': 29423, \"'title'\": 29424, '0x10ac43c28>': 29425, '[_element-title]\\n>>>': 29426, 'merchant[0]': 29427, \".text\\n'the\": 29428, \"venice'\": 29429, '[_element-text]\\n>>>': 29430, '.getchildren()': 29431, '\\n[<element': 29432, '0x10ac43c28>,': 29433, '<element': 29434, \"'personae'\": 29435, '0x10ac43bd8>,\\n<element': 29436, \"'scndescr'\": 29437, '0x10b067f98>,': 29438, \"'playsubt'\": 29439, '0x10af37048>,\\n<element': 29440, \"'act'\": 29441, '0x10af37098>,': 29442, '0x10b936368>,\\n<element': 29443, '0x10b934b88>,': 29444, '0x10cfd8188>,\\n<element': 29445, '0x10cfadb38>]\\n\\n\\n\\nthe': 29446, 'personae,': 29447, 'description,': 29448, 'subtitle,': 29449, 'scenes,': 29450, 'iv:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 29451, 'merchant[-2][0]': 29452, \".text\\n'act\": 29453, \"iv'\\n>>>\": 29454, 'merchant[-2][1]\\n<element': 29455, \"'scene'\": 29456, '0x10cfd8228>\\n>>>': 29457, 'merchant[-2][1][0]': 29458, \".text\\n'scene\": 29459, 'justice': 29460, 'merchant[-2][1][54]\\n<element': 29461, \"'speech'\": 29462, '0x10cfb02c8>\\n>>>': 29463, 'merchant[-2][1][54][0]\\n<element': 29464, \"'speaker'\": 29465, '0x10cfb0318>\\n>>>': 29466, 'merchant[-2][1][54][0]': 29467, \".text\\n'portia'\\n>>>\": 29468, 'merchant[-2][1][54][1]\\n<element': 29469, \"'line'\": 29470, '0x10cfb0368>\\n>>>': 29471, 'merchant[-2][1][54][1]': 29472, '.text\\nthe': 29473, 'mercy': 29474, \"strain'd,\\n\\n\\n\\n\\nnote\\nyour\": 29475, 'turn:\\nrepeat': 29476, 'plays\\nincluded': 29477, 'romeo': 29478, 'juliet': 29479, 'macbeth;\\nfor': 29480, '.shakespeare': 29481, '.\\n\\nalthough': 29482, 'for\\nsub-elements': 29483, 'have\\nseveral': 29484, 'acts),': 29485, \".findall('act')\": 29486, 'such\\ntag-specific': 29487, 'nesting:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 29488, 'enumerate(merchant': 29489, \".findall('act')):\\n\": 29490, 'enumerate(act': 29491, \".findall('scene')):\\n\": 29492, 'enumerate(scene': 29493, \".findall('speech')):\\n\": 29494, \".findall('line'):\\n\": 29495, \"'music'\": 29496, 'str(line': 29497, '.text):\\n': 29498, 'print(act': 29499, '%d:': 29500, '(i+1,': 29501, 'j+1,': 29502, 'k+1,': 29503, '.text))\\nact': 29504, 'doth': 29505, 'choice;\\nact': 29506, 'fading': 29507, 'music:': 29508, 'comparison\\nact': 29509, 'then?': 29510, 'is\\nact': 29511, '23:': 29512, 'air': 29513, '.\\nact': 29514, 'sit': 29515, 'music\\nact': 29516, '24:': 29517, 'merry': 29518, 'hear': 29519, 'sweet': 29520, '25:': 29521, 'ears,\\nact': 29522, 'poet\\nact': 29523, 'himself,\\nact': 29524, 'trusted': 29525, '29:': 29526, 'madam,': 29527, '32:': 29528, 'musician': 29529, 'wren': 29530, '.\\n\\n\\n\\ninstead': 29531, 'navigating': 29532, 'hierarchy,': 29533, 'for\\nparticular': 29534, 'say:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 29535, 'counter\\n>>>': 29536, 'speaker_seq': 29537, \".findall('act/scene/speech/speaker')]\\n>>>\": 29538, 'speaker_freq': 29539, 'counter(speaker_seq)\\n>>>': 29540, 'top5': 29541, '.most_common(5)\\n>>>': 29542, \"top5\\n[('portia',\": 29543, '117),': 29544, \"('shylock',\": 29545, '79),': 29546, \"('bassanio',\": 29547, \"73),\\n('gratiano',\": 29548, '48),': 29549, \"('lorenzo',\": 29550, '47)]\\n\\n\\n\\nwe': 29551, 'dialogues': 29552, 'vocabulary\\nto': 29553, 'manageable': 29554, \"'oth')\\n>>>\": 29555, 'speaker,': 29556, 'top5:\\n': 29557, 'abbreviate[speaker]': 29558, 'speaker[:4]\\n': 29559, 'speaker_seq2': 29560, '[abbreviate[speaker]': 29561, 'speaker_seq]\\n>>>': 29562, '.conditionalfreqdist(nltk': 29563, '.bigrams(speaker_seq2))\\n>>>': 29564, 'anto': 29565, 'bass': 29566, 'grat': 29567, 'oth': 29568, 'shyl\\nanto': 29569, '12\\nbass': 29570, '16\\ngrat': 29571, '153': 29572, '25\\nport': 29573, '21\\nshyl': 29574, '0\\n\\n\\n\\nignoring': 29575, 'exchanges': 29576, 'people\\nother': 29577, 'oth),': 29578, 'largest': 29579, 'portia': 29580, 'bassanio': 29581, '.4\\xa0\\xa0\\xa0using': 29582, 'interface\\nfor': 29583, 'format\\nused': 29584, 'of\\ntechniques': 29585, 'supported\\nby': 29586, 'be\\napplied': 29587, '.xml()': 29588, 'toolbox\\nfile': 29589, 'file\\ncontains': 29590, \".xml('rotokas\": 29591, \".dic')\\n\\n\\n\\nthere\": 29592, 'by\\nindexes': 29593, 'thus\\nlexicon[3]': 29594, 'fourth\\nentry': 29595, 'zero);': 29596, 'lexicon[3][0]': 29597, 'field:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 29598, 'lexicon[3][0]\\n<element': 29599, \"'lx'\": 29600, '0x10b2f6958>\\n>>>': 29601, \".tag\\n'lx'\\n>>>\": 29602, \".text\\n'kaa'\\n\\n\\n\\nthe\": 29603, 'uses\\npaths': 29604, 'lx': 29605, 'ps': 29606, 'can\\nconveniently': 29607, 'record/lx': 29608, 'record/lx,': 29609, 'element,\\nnormalizing': 29610, '[lexeme': 29611, \".findall('record/lx')]\\n['kaa',\": 29612, \"'kaa',\": 29613, \"'kaakaaro',\": 29614, \"'kaakaaviko',\": 29615, \"'kaakaavo',\": 29616, \"'kaakaoko',\\n'kaakasi',\": 29617, \"'kaakau',\": 29618, \"'kaakauko',\": 29619, \"'kaakito',\": 29620, \"'kaakuupato',\": 29621, \"'kuvuto']\\n\\n\\n\\nlet's\": 29622, 'write()': 29623, 'of\\nelementtree': 29624, 'these\\nusing': 29625, 'output\\ndisplayed': 29626, 'screen,': 29627, 'stdout': 29628, '(standard': 29629, 'output),': 29630, 'sys\\n>>>': 29631, '.util': 29632, 'elementtree_indent\\n>>>': 29633, 'elementtree_indent(lexicon)\\n>>>': 29634, 'elementtree(lexicon[3])\\n>>>': 29635, '.write(sys': 29636, '.stdout,': 29637, \"encoding='unicode')\": 29638, '\\n<record>\\n': 29639, '<lx>kaa</lx>\\n': 29640, '<ps>n</ps>\\n': 29641, '<pt>masc</pt>\\n': 29642, '<cl>isi</cl>\\n': 29643, '<ge>cooking': 29644, 'banana</ge>\\n': 29645, '<tkp>banana': 29646, 'kukim</tkp>\\n': 29647, '<pt>itoo</pt>\\n': 29648, '<sf>flora</sf>\\n': 29649, '<dt>12/aug/2005</dt>\\n': 29650, '<ex>taeavi': 29651, 'iria': 29652, 'isi': 29653, 'kovopaueva': 29654, 'kaparapasia': 29655, '.</ex>\\n': 29656, '<xp>taeavi': 29657, 'bin': 29658, 'planim': 29659, 'gaden': 29660, 'kukim': 29661, 'tasol': 29662, 'paia': 29663, '.</xp>\\n': 29664, '<xe>taeavi': 29665, 'planted': 29666, 'cook': 29667, '.</xe>\\n</record>\\n\\n\\n\\n\\n\\n4': 29668, '.5\\xa0\\xa0\\xa0formatting': 29669, 'entries\\nwe': 29670, '<table>,': 29671, '<tr>': 29672, '(table': 29673, 'row),': 29674, 'and\\n<td>': 29675, 'data)': 29676, '<table>\\\\n\\n>>>': 29677, 'lexicon[70:80]:\\n': 29678, \".findtext('lx')\\n\": 29679, \".findtext('ps')\\n\": 29680, 'ge': 29681, \".findtext('ge')\\n\": 29682, '<tr><td>%s</td><td>%s</td><td>%s</td></tr>\\\\n': 29683, '(lx,': 29684, 'ps,': 29685, 'ge)\\n>>>': 29686, '</table>\\n>>>': 29687, 'print(html)\\n<table>\\n': 29688, '<tr><td>kakae</td><td>???</td><td>small</td></tr>\\n': 29689, '<tr><td>kakae</td><td>class</td><td>child</td></tr>\\n': 29690, '<tr><td>kakaevira</td><td>adv</td><td>small-like</td></tr>\\n': 29691, '<tr><td>kakapikoa</td><td>???</td><td>small</td></tr>\\n': 29692, '<tr><td>kakapikoto</td><td>n</td><td>newborn': 29693, 'baby</td></tr>\\n': 29694, '<tr><td>kakapu</td><td>v</td><td>place': 29695, 'sling': 29696, 'carrying</td></tr>\\n': 29697, '<tr><td>kakapua</td><td>n</td><td>sling': 29698, 'lifting</td></tr>\\n': 29699, '<tr><td>kakara</td><td>n</td><td>arm': 29700, 'band</td></tr>\\n': 29701, '<tr><td>kakarapaia</td><td>n</td><td>village': 29702, 'name</td></tr>\\n': 29703, '<tr><td>kakarau</td><td>n</td><td>frog</td></tr>\\n</table>\\n\\n\\n\\n\\n\\n\\n5\\xa0\\xa0\\xa0working': 29704, 'data\\ngiven': 29705, 'linguists,': 29706, 'further\\nmethods': 29707, 'previous\\nchapters,': 29708, 'counting,': 29709, 'co-occurrences,\\ncan': 29710, 'trivially\\ncompute': 29711, 'sum(len(entry)': 29712, 'len(lexicon)\\n13': 29713, '.635': 29714, 'documentary\\nlinguistics,': 29715, '.1\\xa0\\xa0\\xa0adding': 29716, 'entry\\nit': 29717, 'from\\nexisting': 29718, 'cv()': 29719, 'which\\nmaps': 29720, 'sequence,\\ne': 29721, 'kakapua': 29722, 'cvcvcvv': 29723, 'lowercase,\\nthen': 29724, '[^a-z]': 29725, 'consonant,': 29726, 'entry;': 29727, 'note\\nthe': 29728, 'subelement\\n\\ndef': 29729, 'cv(s):\\n': 29730, \".sub(r'[^a-z]',\": 29731, \"r'_',\": 29732, \".sub(r'[aeiou]',\": 29733, \"r'v',\": 29734, \".sub(r'[^v_]',\": 29735, \"r'c',\": 29736, '(s)\\n\\ndef': 29737, 'add_cv_field(entry):\\n': 29738, 'entry:\\n': 29739, \"'lx':\\n\": 29740, 'cv_field': 29741, 'subelement(entry,': 29742, \"'cv')\\n\": 29743, 'cv(field': 29744, '.text)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 29745, 'add_cv_field(lexicon[53])\\n>>>': 29746, '.to_sfm_string(lexicon[53]))\\n\\\\lx': 29747, 'kaeviro\\n\\\\ps': 29748, 'v\\n\\\\pt': 29749, 'a\\n\\\\ge': 29750, 'lift': 29751, 'off\\n\\\\ge': 29752, 'off\\n\\\\tkp': 29753, 'antap\\n\\\\sc': 29754, 'motion\\n\\\\vx': 29755, '1\\n\\\\nt': 29756, 'plane\\n\\\\dt': 29757, '03/jun/2005\\n\\\\ex': 29758, 'pita': 29759, 'kaeviroroe': 29760, 'kepa': 29761, 'kekesia': 29762, 'oa': 29763, 'vuripierevo': 29764, 'kiuvu': 29765, '.\\n\\\\xp': 29766, 'antap': 29767, 'lukim': 29768, 'haus': 29769, 'win': 29770, 'bagarapim': 29771, '.\\n\\\\xe': 29772, 'peter': 29773, 'destroyed': 29774, '.\\n\\\\cv': 29775, 'cvvcvcv\\n\\n\\nexample': 29776, '(code_add_cv_field': 29777, 'entry\\n\\n\\nnote\\nif': 29778, 'updated,': 29779, 'in\\ncode-add-cv-field': 29780, 'would\\nbe': 29781, 'add_cv_field()': 29782, 'safer': 29783, 'such\\nprograms': 29784, 'analysis,\\nwithout': 29785, '.2\\xa0\\xa0\\xa0validating': 29786, 'lexicon\\nmany': 29787, 'fields\\nin': 29788, 'practicable': 29789, 'sequences,\\nwith': 29790, 'counter:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>': 29791, 'field_sequences': 29792, \"counter(':'\": 29793, '.join(field': 29794, 'entry)': 29795, 'lexicon)\\n>>>': 29796, \".most_common()\\n[('lx:ps:pt:ge:tkp:dt:ex:xp:xe',\": 29797, '41),': 29798, \"('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe',\": 29799, \"37),\\n('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe',\": 29800, '27),': 29801, \"('lx:ps:pt:ge:tkp:nt:dt:ex:xp:xe',\": 29802, '20),': 29803, '.]\\n\\n\\n\\nafter': 29804, 'context\\nfree': 29805, '.2\\nuses': 29806, 'implicit\\nnested': 29807, 'the\\nleaves': 29808, 'iterate\\nover': 29809, 'conformance': 29810, '.\\nthose': 29811, 'grammar\\nit': 29812, \".fromstring('''\\n\": 29813, 'sem_field': 29814, 'examples\\n': 29815, 'root\\n': 29816, 'lx\\n': 29817, 'rt': 29818, 'ps\\n': 29819, 'tkp': 29820, 'eng\\n': 29821, 'dt\\n': 29822, 'sf\\n': 29823, 'ex_pidgin': 29824, 'ex_english': 29825, 'ex\\n': 29826, 'xp\\n': 29827, 'xe\\n': 29828, 'cmt': 29829, 'nt': 29830, \"''')\\n\\ndef\": 29831, 'validate_lexicon(grammar,': 29832, 'ignored_tags):\\n': 29833, '.recursivedescentparser(grammar)\\n': 29834, 'lexicon:\\n': 29835, 'marker_list': 29836, '[field': 29837, 'ignored_tags]\\n': 29838, 'list(rd_parser': 29839, '.parse(marker_list)):\\n': 29840, 'print(+,': 29841, '.join(marker_list))': 29842, 'print(-,': 29843, \".dic')[10:20]\\n>>>\": 29844, 'ignored_tags': 29845, \"['arg',\": 29846, \"'dcsv',\": 29847, \"'vx']\": 29848, 'ignored_tags)\\n-': 29849, 'lx:ps:ge:tkp:sf:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe\\n-': 29850, 'lx:rt:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe\\n-': 29851, 'lx:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe\\n-': 29852, 'lx:ps:ge:tkp:nt:sf:dt\\n-': 29853, 'lx:ps:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe\\n-': 29854, 'lx:ps:ge:ge:ge:tkp:cmt:dt:ex:xp:xe\\n-': 29855, 'lx:rt:ps:ge:ge:tkp:dt\\n-': 29856, 'lx:rt:ps:ge:eng:eng:eng:ge:tkp:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe\\n-': 29857, 'lx:rt:ps:ge:tkp:dt:ex:xp:xe\\n-': 29858, 'lx:ps:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe\\n\\n\\nexample': 29859, '(code_toolbox_validation': 29860, 'validating': 29861, 'grammar\\n\\nanother': 29862, '.),\\nsince': 29863, 'partial\\nstructures,': 29864, 'up\\na': 29865, 'lexfunc:': 29866, '{<lf>(<lv><ln|le>*)*}\\n': 29867, '{<rf|xv><xn|xe>*}\\n': 29868, 'sense:': 29869, '{<sn><ps><pn|gv|dv|gn|gp|dn|rn|ge|de|re>*<example>*<lexfunc>*}\\n': 29870, 'record:': 29871, '{<lx><hm><sense>+<dt>}\\n': 29872, 'toolboxdata\\n>>>': 29873, 'db': 29874, 'toolboxdata()\\n>>>': 29875, '.open(nltk': 29876, \".find('corpora/toolbox/iu_mien_samp\": 29877, \".db'))\\n>>>\": 29878, '.parse(grammar,': 29879, \"encoding='utf8')\\n>>>\": 29880, 'elementtree(lexicon)\\n>>>': 29881, 'open(iu_mien_samp': 29882, '.xml,': 29883, 'wb)': 29884, 'output:\\n': 29885, '.write(output)\\n\\n\\nexample': 29886, '(code_chunk_toolbox': 29887, 'lexicon:': 29888, 'of\\nentries': 29889, 'iu': 29890, 'mien,': 29891, 'record\\n\\n\\n\\n\\n6\\xa0\\xa0\\xa0describing': 29892, 'metadata\\nmembers': 29893, 'community': 29894, 'libraries\\ncommunity': 29895, 'metadata': 29896, 'aggregation': 29897, 'metadata?\\nthe': 29898, '.\\nmetadata': 29899, 'whether\\nit': 29900, 'physical': 29901, 'new,\\nthe': 29902, 'as\\ncollections': 29903, '.\\nlibrary': 29904, 'catalogs': 29905, 'metadata;\\nthey': 29906, 'discovery\\ntools': 29907, 'either\\nby': 29908, 'dublin': 29909, 'initiative': 29910, '1995': 29911, 'develop\\nconventions': 29912, 'discovery': 29913, 'broad,\\ninterdisciplinary': 29914, 'consensus': 29915, 'elements\\nthat': 29916, 'each\\nelement': 29917, 'repeatable:': 29918, 'creator,': 29919, 'subject,\\ndescription,': 29920, 'contributor,': 29921, 'format,\\nidentifier,': 29922, 'coverage,': 29923, 'that\\nexist': 29924, 'archives': 29925, '(oai)': 29926, 'framework\\nacross': 29927, 'scholarly': 29928, 'type,\\nincluding': 29929, 'recordings,': 29930, 'artifacts,\\ndigital': 29931, 'surrogates,': 29932, 'repository': 29933, 'server': 29934, 'offering\\npublic': 29935, 'archived': 29936, 'identifier,\\nand': 29937, 'additional\\nrecords': 29938, 'formats)': 29939, 'oai': 29940, 'protocol': 29941, 'search\\nservices': 29942, 'harvest': 29943, '.2\\xa0\\xa0\\xa0olac:': 29944, 'community\\nthe': 29945, '(olac)': 29946, 'international\\npartnership': 29947, 'institutions': 29948, 'a\\nworldwide': 29949, 'virtual': 29950, 'by:\\n(i)': 29951, 'archiving': 29952, 'and\\n(ii)': 29953, 'interoperating': 29954, 'repositories\\nand': 29955, 'services': 29956, 'housing': 29957, \".\\nolac's\": 29958, '.\\nolac': 29959, '.\\nuniform': 29960, 'ensured': 29961, 'limiting\\nthe': 29962, 'terms\\nfrom': 29963, 'controlled': 29964, 'set,\\na': 29965, 'descriptors': 29966, 'fundamental\\nproperties': 29967, 'and\\nlinguistic': 29968, 'record:\\n\\n<?xml': 29969, 'encoding=utf-8?>\\n<olac:olac': 29970, 'xmlns:olac=http://www': 29971, '.org/olac/1': 29972, '.1/\\n': 29973, 'xmlns=http://purl': 29974, '.org/dc/elements/1': 29975, 'xmlns:dcterms=http://purl': 29976, '.org/dc/terms/\\n': 29977, 'xmlns:xsi=http://www': 29978, '.w3': 29979, '.org/2001/xmlschema-instance\\n': 29980, 'xsi:schemalocation=http://www': 29981, '.1/olac': 29982, '.xsd>\\n': 29983, '<title>a': 29984, 'kayardild': 29985, 'notes': 29986, 'tangkic': 29987, '.</title>\\n': 29988, '<creator>evans,': 29989, 'nicholas': 29990, '.</creator>\\n': 29991, '<subject>kayardild': 29992, 'grammar</subject>\\n': 29993, '<subject': 29994, 'xsi:type=olac:language': 29995, 'olac:code=gyd>kayardild</subject>\\n': 29996, '<language': 29997, 'olac:code=en>english</language>\\n': 29998, '<description>kayardild': 29999, '(isbn': 30000, '3110127954)</description>\\n': 30001, '<publisher>berlin': 30002, 'mouton': 30003, 'de': 30004, 'gruyter</publisher>\\n': 30005, '<contributor': 30006, 'xsi:type=olac:role': 30007, 'olac:code=author>nicholas': 30008, 'evans</contributor>\\n': 30009, '<format>hardcover,': 30010, '837': 30011, 'pages</format>\\n': 30012, '<relation>related': 30013, '0646119966</relation>\\n': 30014, '<coverage>australia</coverage>\\n': 30015, '<type': 30016, 'xsi:type=olac:linguistic-type': 30017, 'olac:code=language_description/>\\n': 30018, 'xsi:type=dcterms:dcmitype>text</type>\\n</olac:olac>\\n\\nparticipating': 30019, 'xml\\nformat,': 30020, 'harvested': 30021, 'services\\nusing': 30022, 'infrastructure,\\nolac': 30023, 'describing\\nlanguage': 30024, 'extended\\nconsultation': 30025, 'see\\nhttp://www': 30026, '.org/rec/bpr': 30027, '.\\nsearching': 30028, 'others:\\n\\ncallhome': 30029, 'lexicon\\nhttp://www': 30030, '.org/item/oai:www': 30031, '.edu:ldc97l18\\nmultilex': 30032, '.org/item/oai:elra': 30033, '.icp': 30034, '.inpg': 30035, '.fr:m0001\\nslelex': 30036, 'siemens': 30037, '.fr:s0048\\n\\nsearching': 30038, 'korean': 30039, 'treebank,': 30040, 'lexicon,\\na': 30041, 'child-language': 30042, 'interlinear': 30043, 'glossed': 30044, 'software\\nincluding': 30045, 'analyzer': 30046, 'urls': 30047, 'form:\\noai:www': 30048, '.edu:ldc97l18': 30049, 'identifier,\\nusing': 30050, 'uri': 30051, 'registered': 30052, 'icann\\n(the': 30053, 'corporation': 30054, 'numbers)': 30055, 'oai:archive:local_id,\\nwhere': 30056, 'scheme,\\narchive': 30057, 'archive': 30058, '.edu,\\nand': 30059, 'local_id': 30060, 'ldc97l18': 30061, 'retrieve\\nthe': 30062, 'form:\\n\\nhttp://www': 30063, '.org/static-records/oai:archive:local_id\\n\\n\\n\\n6': 30064, '.3\\xa0\\xa0\\xa0disseminating': 30065, 'resources\\nthe': 30066, 'repository,\\nan': 30067, 'open-access': 30068, 'upload': 30069, 'and\\nsaved': 30070, \"nltk's\\ndownloader\": 30071, '.\\n\\n\\n\\n7\\xa0\\xa0\\xa0summary\\n\\nfundamental': 30072, 'texts\\nand': 30073, 'a\\nrecord': 30074, 'quality\\ncontrol,': 30075, '.\\ncorpus': 30076, 'representative\\nsample': 30077, 'one\\nsource': 30078, 'useful;': 30079, 'of\\nvariability': 30080, 'interchange': 30081, 'linguistic\\ndata,': 30082, 'shortcuts': 30083, 'projects;': 30084, 'can\\nwrite': 30085, 'curation': 30086, 'convert\\nthem': 30087, 'for\\ndocumenting': 30088, 'available:\\namerican': 30089, '(reppen,': 30090, 'ide,': 30091, 'suderman,': 30092, '2005),\\nbritish': 30093, '({bnc},': 30094, '1999),\\nthesaurus': 30095, 'linguae': 30096, 'graecae': 30097, '({tlg},': 30098, '1999),\\nchild': 30099, 'exchange': 30100, '(childes)': 30101, '(macwhinney,': 30102, '1995),\\ntimit': 30103, 'lamel,': 30104, 'william,': 30105, 'linguistics\\nthat': 30106, 'workshops': 30107, 'proceedings': 30108, 'are\\nsigwac,': 30109, 'promotes': 30110, 'has\\nsponsored': 30111, 'cleaneval': 30112, 'markup,\\nand': 30113, 'sigann,': 30114, 'encouraging': 30115, 'interoperability\\nof': 30116, '.\\nfull': 30117, '(buseman,': 30118, 'buseman,': 30119, '1996),\\nand': 30120, '.org/computing/ddp/': 30121, 'toolbox\\nare': 30122, '(tamanji,': 30123, 'hirotani,': 30124, '1999),': 30125, '(robinson,': 30126, 'aumann,': 30127, '.\\ndozens': 30128, 'some\\nsurveyed': 30129, '(bird': 30130, 'simons,': 30131, 'latech': 30132, 'on\\nlanguage': 30133, 'http://zvon': 30134, '.org/)\\nand': 30135, 'modes': 30136, 'include\\nolif': 30137, '.olif': 30138, '.net/\\nand': 30139, '.com/p/lift-standard/': 30140, 'survey': 30141, '.edu/annotation/': 30142, '(thompson': 30143, 'mckelvie,': 30144, 'called\\nannotation': 30145, '2001)': 30146, '(gold)': 30147, 'is\\ndocumented': 30148, '.linguistics-ontology': 30149, '(farghaly,': 30150, '2003)\\nmore': 30151, 'available\\nin': 30152, '(artstein': 30153, 'poesio,': 30154, '(pevzner': 30155, 'hearst,': 30156, 'robinson,': 30157, 'mien': 30158, 'greg': 30159, 'aumann': 30160, 'visit\\nhttp://www': 30161, '(simons': 30162, '.\\n\\n\\n9\\xa0\\xa0\\xa0exercises\\n\\n◑': 30163, 'the\\nentry': 30164, 'inserts': 30165, 'subelement': 30166, \"element('cv'),\\nassign\": 30167, 'insert()': 30168, '.)\\n\\n◑': 30169, 'deletes': 30170, 'sanitize': 30171, 'others,\\ne': 30172, 'entries\\nhaving': 30173, '(ps': 30174, 'field)': 30175, 'that\\noccurred': 30176, 'mistakes?\\n\\n◑': 30177, 'whole-word': 30178, 'reduplication': 30179, 'partial\\nreduplication': 30180, '.search()': 30181, 'following\\nregular': 30182, '.+)\\\\1\\n\\n◑': 30183, 'interesting\\nissue': 30184, 'this\\nprogram': 30185, 'syl': 30186, 'similarly': 30187, 'of\\nconsecutive': 30188, 'pt)': 30189, 'per\\nrow,': 30190, 'the\\nspreadsheet': 30191, 'in\\ntoolbox': 30192, 'gl': 30193, \"shakespeare's\": 30194, 'plays,': 30195, 'music,\\nreturning': 30196, 'acts,': 30197, 'speeches,': 30198, 'form\\n[(3,': 30199, '23),': 30200, 'indicates\\nact': 30201, 'length\\nfor': 30202, 'venice,': 30203, 'character,\\ne': 30204, \"cfd['portia'][12]\": 30205, 'portia\\nconsisting': 30206, 'cognates': 30207, 'edit-distance': 30208, 'three\\nfrom': 30209, 'xrf': 30210, 'referencing\\nthe': 30211, 'containing\\nw': 30212, 'toolbox-format': 30213, 'a\\ntree,': 30214, 'represented\\nas': 30215, '.:\\n\\n<s>\\n': 30216, '<np': 30217, 'type=sbj>\\n': 30218, '<np>\\n': 30219, '<nnp>pierre</nnp>\\n': 30220, '<nnp>vinken</nnp>\\n': 30221, '</np>\\n': 30222, '<comma>,</comma>\\n\\n\\n\\n\\n\\nabout': 30223, 'acstafterword:': 30224, 'challenge\\n\\n\\n\\n\\nafterword:': 30225, 'challenge\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nnatural': 30226, 'throws': 30227, 'including\\ntokenization,': 30228, 'extraction,\\nand': 30229, 'equipped': 30230, 'create\\nrobust': 30231, 'into\\ncomponents': 30232, 'exciting\\nendeavor': 30233, 'broader\\naudience': 30234, 'spite': 30235, 'with\\nfar': 30236, 'following\\nsentences': 30237, 'attest': 30238, 'riches': 30239, 'language:\\n\\n': 30240, '.overhead': 30241, 'drives': 30242, 'grey,': 30243, 'hiding': 30244, 'sun': 30245, '(william': 30246, 'faulkner,': 30247, 'dying,': 30248, '1935)\\n\\n': 30249, '.when': 30250, 'toaster': 30251, 'exhaust': 30252, 'fan': 30253, '(sign': 30254, 'dormitory': 30255, 'kitchen)\\n\\n': 30256, '.amiodarone': 30257, 'weakly': 30258, 'inhibited': 30259, 'cyp2c9,': 30260, 'cyp2d6,': 30261, 'cyp3a4-mediated': 30262, 'activities': 30263, 'ki': 30264, '.1-271': 30265, 'μm': 30266, '(medline,': 30267, 'pmid:': 30268, '10718780)\\n\\n': 30269, '.iraqi': 30270, 'arms': 30271, '(spoof': 30272, 'headline)\\n\\n': 30273, 'earnest': 30274, 'prayer': 30275, 'righteous': 30276, 'wonderful': 30277, '(james': 30278, '5:16b)\\n\\n': 30279, '.twas': 30280, 'brillig,': 30281, 'slithy': 30282, 'toves': 30283, 'gyre': 30284, 'gimble': 30285, 'wabe': 30286, '(lewis': 30287, 'carroll,': 30288, 'jabberwocky,': 30289, '1872)\\n\\n': 30290, '.there': 30291, 'afaik': 30292, ':smile:': 30293, '(internet': 30294, 'archive)\\n\\nother': 30295, 'disciplines\\nwhose': 30296, 'centers': 30297, 'disciplines': 30298, 'include\\ntranslation,': 30299, 'criticism,': 30300, 'philosophy,': 30301, 'anthropology': 30302, 'psychology': 30303, 'including\\nlaw,': 30304, 'hermeneutics,': 30305, 'forensics,': 30306, 'telephony,': 30307, 'pedagogy,': 30308, 'archaeology,': 30309, 'cryptanalysis': 30310, 'speech\\npathology': 30311, 'methodologies': 30312, 'gather\\nobservations,': 30313, 'hypotheses': 30314, 'to\\ndeepen': 30315, 'intellect': 30316, 'is\\nmanifested': 30317, 'interest\\nin': 30318, 'angles,': 30319, 'barely\\nscratched': 30320, 'itself,\\nthere': 30321, \"haven't\\nmentioned\": 30322, 'remarks': 30323, 'nlp,\\nincluding': 30324, 'directions': 30325, 'might\\nwant': 30326, 'well-supported': 30327, 'nltk,\\nand': 30328, 'rectify': 30329, 'contributing': 30330, 'software\\nand': 30331, 'a\\ncomputational': 30332, 'grew': 30333, 'dating': 30334, '1900s,': 30335, 'logic,\\nmost': 30336, 'manifested': 30337, 'frege,': 30338, 'russell,': 30339, 'wittgenstein,\\ntarski,': 30340, 'lambek': 30341, 'carnap': 30342, 'amenable': 30343, 'later\\ndevelopments': 30344, 'automata,': 30345, 'context-free\\nlanguages': 30346, 'pushdown': 30347, 'a\\nformal': 30348, 'in\\nsymbolic': 30349, 'rules\\nof': 30350, 'possibly,': 30351, 'set-theoretic\\nmodel;': 30352, 'given\\nsuch': 30353, 'becomes\\npossible': 30354, 'by\\ntranslating': 30355, 'example,\\nif': 30356, 'saw(j,m),': 30357, 'we\\n(implicitly': 30358, 'explicitly)': 30359, 'intepret': 30360, 'a\\nbinary': 30361, 'denoting\\nindividuals': 30362, 'birds': 30363, 'require\\nquantifiers,': 30364, '∀,': 30365, 'all:': 30366, '(bird(x)': 30367, 'fly(x))': 30368, 'provided\\nthe': 30369, 'inferences': 30370, 'of\\ncompositionality,': 30371, 'expression\\nis': 30372, 'of\\ncombination': 30373, 'between\\nsyntax': 30374, 'expression\\ncould': 30375, 'true\\nthat': 30376, 'proposition': 30377, 'not(p)': 30378, 'saw(j,': 30379, 'm)': 30380, 'mary\\nrecursively,': 30381, 'get\\nnot(saw(j,m))': 30382, 'outlined': 30383, 'with\\nnatural': 30384, 'relies': 30385, 'symbolic\\nrepresentations': 30386, 'nlp,\\nparticularly': 30387, 'starting\\npoint': 30388, 'practitioners': 30389, 'family\\nof': 30390, 'unification-based': 30391, 'feature-based)\\ngrammar': 30392, 'prolog\\nprogramming': 30393, 'grammar-based': 30394, 'eclipsed': 30395, '15–20': 30396, 'one\\nsignificant': 30397, 'although\\nearly': 30398, 'emulated': 30399, 'the\\nkind': 30400, 'typified\\nby': 30401, '(chomsky': 30402, 'halle,': 30403, '1968),\\nthis': 30404, 'hopelessly': 30405, 'dealing\\nwith': 30406, 'like\\nreal': 30407, 'from\\nlarge': 30408, 'accurate,\\nefficient': 30409, 'that\\nprogress': 30410, 'hugely': 30411, 'assisted': 30412, 'the\\nconstruction': 30413, 'quantitatively': 30414, 'measuring\\nperformance': 30415, 'nlp\\ncommunity': 30416, 'embraced': 30417, 'machine-learning': 30418, 'techniques\\nand': 30419, 'evaluation-led': 30420, '.\\n\\n\\ncontemporary': 30421, 'divides\\nthe': 30422, 'contrasting': 30423, 'section\\nrelate': 30424, 'metaphysical': 30425, 'debates': 30426, 'rationalism\\nversus': 30427, 'empiricism': 30428, 'realism': 30429, 'idealism': 30430, 'enlightenment': 30431, 'western': 30432, 'philosophy': 30433, 'these\\ndebates': 30434, 'backdrop': 30435, 'orthodox': 30436, 'divine': 30437, 'revelation': 30438, 'seventeenth': 30439, 'eighteenth': 30440, 'centuries,\\nphilosophers': 30441, 'sensory': 30442, 'has\\npriority': 30443, 'descartes': 30444, 'leibniz,': 30445, 'took\\nthe': 30446, 'rationalist': 30447, 'position,': 30448, 'asserting': 30449, 'origins': 30450, 'in\\nhuman': 30451, 'thought,': 30452, 'innate': 30453, 'implanted': 30454, 'our\\nminds': 30455, 'birth': 30456, 'of\\neuclidean': 30457, 'geometry': 30458, 'supernatural': 30459, 'contrast,\\nlocke': 30460, 'empiricist': 30461, 'of\\nknowledge': 30462, 'faculties,': 30463, 'reason\\nplays': 30464, 'reflecting': 30465, 'often-cited\\nevidence': 30466, \"galileo's\": 30467, 'on\\ncareful': 30468, 'motion': 30469, 'planets': 30470, 'the\\nsolar': 30471, 'heliocentric': 30472, 'geocentric': 30473, 'of\\nlinguistics,': 30474, 'debate': 30475, 'what\\nextent': 30476, 'experience,': 30477, 'language\\nfaculty,': 30478, 'nlp\\nthis': 30479, 'surfaces': 30480, 'priority': 30481, 'data\\nversus': 30482, 'introspection': 30483, 'concern,': 30484, 'enshrined': 30485, 'and\\nidealism,': 30486, '.\\nkant': 30487, 'manifestations': 30488, 'be\\nknown': 30489, 'realist': 30490, 'theoretical\\nconstruct': 30491, 'exists\\nindependently': 30492, 'perception': 30493, 'actually\\ncauses': 30494, 'idealist,': 30495, 'more\\nabstract': 30496, 'intrinsically\\nunobservable,': 30497, 'fictions': 30498, 'way\\nlinguists': 30499, 'betrays': 30500, 'while\\nnlp': 30501, 'occupy': 30502, 'territory': 30503, 'lean': 30504, 'the\\nidealist': 30505, 'theoretical\\nabstraction': 30506, 'sheds': 30507, 'alive': 30508, 'distinctions\\nbetween': 30509, 'processing,\\nbinary': 30510, 'classifications,': 30511, 'engineering\\ngoals': 30512, 'nuanced,': 30513, 'debate\\nis': 30514, 'polarized': 30515, 'the\\ndiscussions': 30516, 'a\\nbalancing': 30517, 'assume\\nthat': 30518, 'innately': 30519, 'analogical': 30520, 'memory-based\\nlearning': 30521, '(weak': 30522, 'rationalism),': 30523, 'identify\\nmeaningful': 30524, '(empiricism)': 30525, '.\\nstatistical': 30526, 'statistics\\nguide': 30527, 'grammar,\\ni': 30528, '.\\nsymbolic': 30529, 'features\\nfor': 30530, 'model,\\ni': 30531, 'circle': 30532, '.\\n\\n\\nnltk': 30533, 'roadmap\\nthe': 30534, 'work-in-progress,': 30535, 'being\\ncontinually': 30536, '(yet)': 30537, '.\\ncheck': 30538, 'developments': 30539, 'publication\\ndate': 30540, '.\\n\\n\\n\\n\\nphonology': 30541, 'morphology:\\n\\xa0computational': 30542, 'structures\\ntypically': 30543, 'suppletion': 30544, 'and\\nnon-concatenative': 30545, 'string\\nprocessing': 30546, 'high-performance': 30547, 'but\\nto': 30548, 'duplication': 30549, 'morphosyntactic\\nfeatures': 30550, 'morph': 30551, 'analyzers': 30552, '.\\n\\nhigh-performance': 30553, 'components:\\n\\xa0some': 30554, 'computationally': 30555, 'pure': 30556, 'python\\nimplementations': 30557, 'expense\\nonly': 30558, 'distribute\\ntrained': 30559, 'be\\nfreely': 30560, 'interfaces\\nto': 30561, 'the\\nreach': 30562, 'mapreduce': 30563, 'semantics:\\n\\xa0this': 30564, 'vibrant': 30565, 'encompassing\\ninheritance': 30566, 'ontologies,': 30567, 'expressions,\\netc,': 30568, 'conservative': 30569, 'information\\nfrom': 30570, 'in\\nword': 30571, '.\\n\\nnatural': 30572, 'generation:\\n\\xa0producing': 30573, 'meaning\\nis': 30574, 'nlp;': 30575, 'to\\nnlg': 30576, 'more\\ncontributions': 30577, '.\\n\\nlinguistic': 30578, 'fieldwork:\\n\\xa0a': 30579, 'of\\nendangered': 30580, 'heterogeneous': 30581, 'and\\nrapidly': 30582, 'evolving': 30583, 'fieldwork\\ndata': 30584, 'lexicon\\ninterchange': 30585, 'nltk,\\nhelping': 30586, 'curate': 30587, 'data,\\nwhile': 30588, 'liberating': 30589, 'spend': 30590, 'possible\\non': 30591, '.\\n\\nother': 30592, 'languages:\\n\\xa0improved': 30593, 'could\\ninvolve': 30594, 'areas:': 30595, 'obtaining': 30596, 'distribute\\nmore': 30597, 'collection;\\nwriting': 30598, 'language-specific': 30599, '.org/howto,\\nillustrating': 30600, 'language-specific\\nproblems': 30601, 'encodings,': 30602, 'segmentation,\\nand': 30603, 'language\\ncould': 30604, 'arrange': 30605, 'host': 30606, 'the\\nnltk': 30607, 'website;': 30608, 'discussions\\nto': 30609, 'the\\ntarget': 30610, '.\\n\\nnltk-contrib:many': 30611, 'the\\nnlp': 30612, 'housed': 30613, 'contrib\\npackage,': 30614, 'nltk_contrib': 30615, 'python,\\nrelevant': 30616, 'the\\nrest': 30617, 'imperfect': 30618, 'welcome,': 30619, 'probably\\nbe': 30620, '.\\n\\nteaching': 30621, 'materials:\\n\\xa0since': 30622, 'have\\naccompanied': 30623, 'to\\nfill': 30624, 'slides,\\nproblem': 30625, 'treatments': 30626, 'the\\ntopics': 30627, 'covered,': 30628, 'notify\\nthe': 30629, 'mainstream': 30630, 'departments,\\nor': 30631, 'is\\nsignificant': 30632, 'literature,\\ncomputer': 30633, 'curricula': 30634, '.\\n\\nonly': 30635, 'toolkit:as': 30636, 'preface,': 30637, 'tackled': 30638, 'nltk,\\npython,': 30639, 'external\\nnlp': 30640, '.\\n\\n\\n\\n\\n\\nenvoi': 30641, '.\\nlinguists': 30642, 'speak,\\nand': 30643, 'the\\nstudy': 30644, 'languages,\\na': 30645, 'profound': 30646, 'elusive': 30647, 'speak\\nas': 30648, 'scientists': 30649, 'many\\nprogramming': 30650, 'science\\nactually': 30651, 'be\\nimplemented': 30652, 'language,\\na': 30653, 'striving': 30654, 'for\\nfluency': 30655, 'unfortunate': 30656, 'concluded': 30657, 'that\\nnlp': 30658, 'text,\\nor': 30659, 'broadly,': 30660, 'language)\\nto': 30661, 'expedient,': 30662, 'end:\\nas': 30663, 'algorithms\\nfor': 30664, 'text,\\nas': 30665, 'serve\\nthe': 30666, 'society,\\nand': 30667, 'pathway': 30668, 'riches\\nof': 30669, 'present:': 30670, 'hacking!\\n\\n\\nabout': 30671, 'acst': 30672}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'preface\\n\\n\\n\\n\\npreface\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nthis', 1: 'is', 2: 'a', 3: 'book', 4: 'about', 5: 'natural', 6: 'language', 7: 'processing', 8: '.', 9: 'by', 10: 'natural\\nlanguage', 11: 'we', 12: 'mean', 13: 'that', 14: 'used', 15: 'for', 16: 'everyday\\ncommunication', 17: 'humans;', 18: 'languages', 19: 'like', 20: 'english,', 21: 'hindi', 22: 'or\\nportuguese', 23: 'in', 24: 'contrast', 25: 'to', 26: 'artificial', 27: 'such', 28: 'as\\nprogramming', 29: 'and', 30: 'mathematical', 31: 'notations,', 32: 'have\\nevolved', 33: 'as', 34: 'they', 35: 'pass', 36: 'from', 37: 'generation', 38: 'generation,', 39: 'are', 40: 'hard', 41: 'to\\npin', 42: 'down', 43: 'with', 44: 'explicit', 45: 'rules', 46: 'will', 47: 'take', 48: '—', 49: 'or', 50: 'nlp\\nfor', 51: 'short', 52: 'wide', 53: 'sense', 54: 'cover', 55: 'any', 56: 'kind', 57: 'of', 58: 'computer', 59: 'manipulation\\nof', 60: 'at', 61: 'one', 62: 'extreme,', 63: 'it', 64: 'could', 65: 'be', 66: 'simple', 67: 'counting\\nword', 68: 'frequencies', 69: 'compare', 70: 'different', 71: 'writing', 72: 'styles', 73: '.\\nat', 74: 'the', 75: 'other', 76: 'nlp', 77: 'involves', 78: 'understanding', 79: 'complete', 80: 'human\\nutterances,', 81: 'least', 82: 'extent', 83: 'being', 84: 'able', 85: 'give', 86: 'useful\\nresponses', 87: 'them', 88: '.\\ntechnologies', 89: 'based', 90: 'on', 91: 'nlp\\nare', 92: 'becoming', 93: 'increasingly', 94: 'widespread', 95: 'example,', 96: 'phones', 97: 'handheld', 98: 'computers\\nsupport', 99: 'predictive', 100: 'text', 101: 'handwriting', 102: 'recognition;', 103: '', 104: 'web\\nsearch', 105: 'engines', 106: 'access', 107: 'information', 108: 'locked', 109: 'up', 110: 'unstructured\\ntext;', 111: 'machine', 112: 'translation', 113: 'allows', 114: 'us', 115: 'retrieve', 116: 'texts', 117: 'written', 118: 'in\\nchinese', 119: 'read', 120: 'spanish;', 121: 'analysis', 122: 'enables', 123: 'to\\ndetect', 124: 'sentiment', 125: 'tweets', 126: 'blogs', 127: 'providing', 128: 'more', 129: 'human-machine', 130: 'interfaces,', 131: 'more\\nsophisticated', 132: 'stored', 133: 'information,', 134: 'has\\ncome', 135: 'play', 136: 'central', 137: 'role', 138: 'multilingual', 139: 'society', 140: '.\\nthis', 141: 'provides', 142: 'highly', 143: 'accessible', 144: 'introduction', 145: 'field', 146: '.\\nit', 147: 'can', 148: 'individual', 149: 'study', 150: 'textbook', 151: 'course\\non', 152: 'computational', 153: 'linguistics,\\nor', 154: 'supplement', 155: 'courses', 156: 'intelligence,\\ntext', 157: 'mining,', 158: 'corpus', 159: 'linguistics', 160: '.\\nthe', 161: 'intensely', 162: 'practical,', 163: 'containing\\nhundreds', 164: 'fully-worked', 165: 'examples', 166: 'graded', 167: 'exercises', 168: 'python', 169: 'programming', 170: 'together', 171: 'an', 172: 'open', 173: 'source\\nlibrary', 174: 'called', 175: 'toolkit', 176: '(nltk)', 177: '.\\nnltk', 178: 'includes', 179: 'extensive', 180: 'software,', 181: 'data,', 182: 'documentation,', 183: 'all', 184: 'freely', 185: 'downloadable', 186: 'http://nltk', 187: '.org/', 188: '.\\ndistributions', 189: 'provided', 190: 'windows,', 191: 'macintosh', 192: 'unix', 193: 'platforms', 194: '.\\nwe', 195: 'strongly', 196: 'encourage', 197: 'you', 198: 'download', 199: 'nltk,', 200: 'try', 201: 'out', 202: 'the\\nexamples', 203: 'along', 204: 'way', 205: '.\\n\\naudience\\nnlp', 206: 'important', 207: 'scientific,', 208: 'economic,', 209: 'social,', 210: 'cultural', 211: 'reasons', 212: '.\\nnlp', 213: 'experiencing', 214: 'rapid', 215: 'growth', 216: 'its', 217: 'theories', 218: 'methods', 219: 'deployed', 220: 'in\\na', 221: 'variety', 222: 'new', 223: 'technologies', 224: 'this', 225: 'reason', 226: 'is\\nimportant', 227: 'range', 228: 'people', 229: 'have', 230: 'working', 231: 'knowledge', 232: '.\\nwithin', 233: 'industry,', 234: 'in\\nhuman-computer', 235: 'interaction,', 236: 'business', 237: 'analysis,\\nand', 238: 'web', 239: 'software', 240: 'development', 241: 'academia,', 242: 'areas', 243: 'from\\nhumanities', 244: 'computing', 245: 'linguistics\\nthrough', 246: 'science', 247: 'intelligence', 248: '.\\n(to', 249: 'many', 250: 'known', 251: 'name', 252: 'of\\ncomputational', 253: '.)\\nthis', 254: 'intended', 255: 'diverse', 256: 'who', 257: 'want', 258: 'to\\nlearn', 259: 'how', 260: 'write', 261: 'programs', 262: 'analyze', 263: 'language,\\nregardless', 264: 'previous', 265: 'experience:\\n\\n\\n\\n\\nnew', 266: 'programming?:\\n\\xa0the', 267: 'early', 268: 'chapters', 269: 'suitable\\nfor', 270: 'readers', 271: 'no', 272: 'prior', 273: 'programming,', 274: 'so', 275: 'long', 276: \"you\\naren't\", 277: 'afraid', 278: 'tackle', 279: 'concepts', 280: 'develop', 281: 'skills', 282: 'full', 283: 'copy', 284: 'and\\ntry', 285: 'yourself,', 286: 'hundreds', 287: '.\\nif', 288: 'need', 289: 'general', 290: 'python,', 291: 'see', 292: 'the\\nlist', 293: 'resources', 294: 'http://docs', 295: '.python', 296: '.\\n\\nnew', 297: 'python?:experienced', 298: 'programmers', 299: 'quickly', 300: 'learn', 301: 'enough\\npython', 302: 'using', 303: 'get', 304: 'immersed', 305: '.\\nall', 306: 'relevant', 307: 'features', 308: 'carefully', 309: 'explained', 310: 'exemplified,\\nand', 311: 'come', 312: 'appreciate', 313: \"python's\", 314: 'suitability', 315: 'this\\napplication', 316: 'area', 317: 'index', 318: 'help', 319: 'locate', 320: 'relevant\\ndiscussions', 321: '.\\n\\nalready', 322: 'dreaming', 323: 'python?:\\n\\xa0skim', 324: 'examples\\nand', 325: 'dig', 326: 'into', 327: 'interesting', 328: 'analysis\\nmaterial', 329: 'starts', 330: '1', 331: \".\\nyou'll\", 332: 'soon', 333: 'applying', 334: 'your', 335: 'fascinating', 336: 'domain', 337: '.\\n\\n\\n\\n\\n\\nemphasis\\nthis', 338: 'practical', 339: 'by\\nexample,', 340: 'real', 341: 'programs,', 342: 'grasp', 343: 'value', 344: 'to\\ntest', 345: 'idea', 346: 'through', 347: 'implementation', 348: 'if', 349: \"haven't\", 350: 'learnt', 351: 'already,\\nthis', 352: 'teach', 353: 'unlike', 354: 'programming\\nbooks,', 355: 'provide', 356: 'illustrations', 357: 'the\\napproach', 358: 'taken', 359: 'also', 360: 'principled,', 361: 'the\\ntheoretical', 362: 'underpinnings', 363: \"don't\", 364: 'shy', 365: 'away', 366: 'careful', 367: 'linguistic\\nand', 368: 'tried', 369: 'pragmatic', 370: 'in\\nstriking', 371: 'balance', 372: 'between', 373: 'theory', 374: 'application,', 375: 'identifying', 376: 'the\\nconnections', 377: 'tensions', 378: 'finally,', 379: 'recognize', 380: \"you\\nwon't\", 381: 'unless', 382: 'pleasurable,', 383: 'have\\ntried', 384: 'include', 385: 'applications', 386: 'interesting\\nand', 387: 'entertaining,', 388: 'sometimes', 389: 'whimsical', 390: '.\\nnote', 391: 'not', 392: 'reference', 393: 'work', 394: 'coverage', 395: 'python\\nand', 396: 'selective,', 397: 'presented', 398: 'tutorial', 399: 'style', 400: 'for\\nreference', 401: 'material,', 402: 'please', 403: 'consult', 404: 'substantial', 405: 'quantity', 406: 'of\\nsearchable', 407: 'available', 408: 'http://python', 409: 'advanced', 410: 'content', 411: 'ranges', 412: 'introductory', 413: 'intermediate,', 414: 'is\\ndirected', 415: 'analyze\\ntext', 416: '.\\nto', 417: 'algorithms', 418: 'implemented', 419: 'nltk,\\nyou', 420: 'examine', 421: 'code', 422: 'linked', 423: '.org/,\\nand', 424: 'materials', 425: 'cited', 426: '.\\n\\n\\nwhat', 427: 'learn\\nby', 428: 'digging', 429: 'material', 430: 'here,', 431: 'learn:\\n\\nhow', 432: 'manipulate', 433: 'analyze\\nlanguage', 434: 'these', 435: 'programs\\nhow', 436: 'key', 437: 'describe', 438: 'and\\nanalyse', 439: 'language\\nhow', 440: 'data', 441: 'structures', 442: 'nlp\\nhow', 443: 'standard', 444: 'formats,', 445: 'can\\nbe', 446: 'evaluate', 447: 'performance', 448: 'techniques\\n\\ndepending', 449: 'background,', 450: 'motivation', 451: 'interested', 452: 'nlp,\\nyou', 453: 'gain', 454: 'kinds', 455: 'book,', 456: 'set', 457: 'out\\nin', 458: 'iii', 459: '.1', 460: '.\\n\\n\\n\\n\\n\\n\\n\\ngoals\\nbackground', 461: 'arts', 462: 'humanities\\nbackground', 463: 'engineering\\n\\n\\n\\nlanguage', 464: 'analysis\\nmanipulating', 465: 'large', 466: 'corpora,\\nexploring', 467: 'linguistic', 468: 'models,\\nand', 469: 'testing', 470: 'empirical', 471: 'claims', 472: '.\\nusing', 473: 'techniques', 474: 'modeling,\\ndata', 475: 'discovery\\nto', 476: '.\\n\\nlanguage', 477: 'technology\\nbuilding', 478: 'robust', 479: 'systems', 480: 'to\\nperform', 481: 'tasks\\nwith', 482: 'technological', 483: 'and\\ndata', 484: 'robust\\nlanguage', 485: '.\\n\\n\\ntable', 486: '.1:', 487: 'gained', 488: 'reading', 489: 'depending', 490: \"readers'\", 491: 'goals', 492: 'background\\n\\n\\n\\n\\norganization\\nthe', 493: 'organized', 494: 'order', 495: 'conceptual', 496: 'difficulty,\\nstarting', 497: 'processing\\nthat', 498: 'shows', 499: 'explore', 500: 'bodies', 501: 'using\\ntiny', 502: '(chapters', 503: '1-3)', 504: 'followed', 505: 'by\\na', 506: 'chapter', 507: 'structured', 508: '(chapter', 509: '4)', 510: 'consolidates', 511: 'the\\nprogramming', 512: 'topics', 513: 'scattered', 514: 'across', 515: 'preceding', 516: '.\\nafter', 517: 'this,', 518: 'pace', 519: 'picks', 520: 'up,', 521: 'move\\non', 522: 'series', 523: 'covering', 524: 'fundamental', 525: 'in\\nlanguage', 526: 'processing:\\ntagging,', 527: 'classification,', 528: 'extraction', 529: '5-7)', 530: 'next', 531: 'three', 532: 'look', 533: 'ways', 534: 'parse', 535: 'sentence,', 536: 'recognize\\nits', 537: 'syntactic', 538: 'structure,', 539: 'construct', 540: 'representations', 541: 'meaning', 542: '8-10)', 543: 'final', 544: 'devoted', 545: 'managed', 546: 'effectively', 547: '11)', 548: 'concludes', 549: 'afterword,', 550: 'briefly', 551: 'discussing', 552: 'past', 553: 'future', 554: 'each', 555: 'chapter,', 556: 'switch', 557: 'presentation', 558: '.\\nin', 559: 'style,', 560: 'driver', 561: 'language,\\nexplore', 562: 'concepts,', 563: 'use', 564: 'support\\nthe', 565: 'discussion', 566: 'often', 567: 'employ', 568: 'constructs', 569: 'have\\nnot', 570: 'been', 571: 'introduced', 572: 'systematically,', 573: 'their', 574: 'purpose\\nbefore', 575: 'delving', 576: 'details', 577: 'why', 578: 'just', 579: 'learning', 580: 'idiomatic', 581: 'expressions', 582: 'foreign', 583: \"language:\\nyou're\", 584: 'buy', 585: 'nice', 586: 'pastry', 587: 'without', 588: 'first', 589: 'having', 590: 'learnt\\nthe', 591: 'intricacies', 592: 'question', 593: 'formation', 594: 'presentation,', 595: \".\\nwe'll\", 596: 'algorithms,', 597: 'examples\\nwill', 598: 'supporting', 599: '.\\neach', 600: 'ends', 601: 'exercises,\\nwhich', 602: 'useful', 603: 'consolidating', 604: 'according', 605: 'following', 606: 'scheme:\\n☼', 607: 'easy', 608: 'involve', 609: 'minor', 610: 'modifications\\nto', 611: 'supplied', 612: 'samples', 613: 'activities;\\n◑', 614: 'intermediate', 615: 'aspect\\nof', 616: 'depth,', 617: 'requiring', 618: 'design;\\n★', 619: 'difficult,', 620: 'open-ended', 621: 'tasks', 622: 'challenge', 623: 'your\\nunderstanding', 624: 'force', 625: 'think', 626: 'independently\\n(readers', 627: 'should', 628: 'skip', 629: 'these)', 630: 'has', 631: 'further', 632: 'section', 633: 'online', 634: 'extras\\nsection', 635: '.org/,', 636: 'pointers', 637: 'and\\nonline', 638: 'versions', 639: 'also\\navailable', 640: 'there', 641: '.\\n\\n\\nwhy', 642: 'python?\\npython', 643: 'yet', 644: 'powerful', 645: 'excellent\\nfunctionality', 646: 'be\\ndownloaded', 647: 'free', 648: '.\\ninstallers', 649: '.\\nhere', 650: 'five-line', 651: 'program', 652: 'processes', 653: 'file', 654: '.txt\\nand', 655: 'prints', 656: 'words', 657: 'ending', 658: 'ing:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 659: 'line', 660: 'open(file', 661: '.txt):\\n', 662: 'word', 663: '.split():\\n', 664: \".endswith('ing'):\\n\", 665: 'print(word)\\n\\n\\n\\nthis', 666: 'illustrates', 667: 'some', 668: 'main', 669: 'first,\\nwhitespace', 670: 'nest', 671: 'lines', 672: 'code,', 673: 'thus', 674: 'starting\\nwith', 675: 'falls', 676: 'inside', 677: 'scope', 678: 'starting', 679: 'with\\nfor;', 680: 'ensures', 681: 'ing', 682: 'test', 683: 'performed', 684: 'each\\nword', 685: 'second,', 686: 'object-oriented;', 687: 'variable', 688: 'entity\\nthat', 689: 'certain', 690: 'defined', 691: 'attributes', 692: 'the\\nvalue', 693: 'than', 694: 'sequence', 695: 'characters', 696: 'string', 697: 'object', 698: 'method', 699: '(or', 700: 'operation)', 701: 'called\\nsplit()', 702: 'break', 703: 'apply\\na', 704: 'object,', 705: 'name,', 706: 'period,\\nfollowed', 707: 'i', 708: '.e', 709: '.split()', 710: 'third,', 711: 'methods\\nhave', 712: 'arguments', 713: 'expressed', 714: 'parentheses', 715: 'instance,', 716: 'the\\nexample,', 717: \".endswith('ing')\", 718: 'had', 719: 'argument', 720: \"'ing'\", 721: 'to\\nindicate', 722: 'wanted', 723: 'something', 724: 'else', 725: '.\\nfinally', 726: 'most', 727: 'importantly', 728: '—\\npython', 729: 'readable,', 730: 'much', 731: 'fairly', 732: 'guess\\nwhat', 733: 'does', 734: 'even', 735: 'never', 736: 'program\\nbefore', 737: 'chose', 738: 'because', 739: 'shallow', 740: 'curve,\\nits', 741: 'syntax', 742: 'semantics', 743: 'transparent,\\nand', 744: 'good', 745: 'string-handling', 746: 'functionality', 747: 'interpreted\\nlanguage,', 748: 'facilitates', 749: 'interactive', 750: 'exploration', 751: 'an\\nobject-oriented', 752: 'language,', 753: 'permits', 754: 'be\\nencapsulated', 755: 're-used', 756: 'easily', 757: 'dynamic', 758: 'python\\npermits', 759: 'added', 760: 'objects', 761: 'fly,', 762: 'permits\\nvariables', 763: 'typed', 764: 'dynamically,', 765: 'facilitating', 766: '.\\npython', 767: 'comes', 768: 'extensive\\nstandard', 769: 'library,', 770: 'including', 771: 'components', 772: 'graphical', 773: 'programming,\\nnumerical', 774: 'processing,', 775: 'connectivity', 776: 'heavily', 777: 'scientific', 778: 'research,', 779: 'education\\naround', 780: 'world', 781: 'praised', 782: 'facilitates\\nproductivity,', 783: 'quality,', 784: 'maintainability', 785: 'collection', 786: 'of\\npython', 787: 'success', 788: 'stories', 789: 'posted', 790: '.org/about/success/', 791: 'defines', 792: 'infrastructure', 793: 'build', 794: 'nlp\\nprograms', 795: 'provides\\nbasic', 796: 'classes', 797: 'representing', 798: 'processing;\\nstandard', 799: 'interfaces', 800: 'performing', 801: 'as\\npart-of-speech', 802: 'tagging,', 803: 'parsing,', 804: 'classification;\\nand', 805: 'implementations', 806: 'task', 807: 'which', 808: 'combined', 809: 'solve', 810: 'complex', 811: 'problems', 812: 'documentation', 813: 'addition', 814: 'this\\nbook,', 815: 'website', 816: 'api', 817: 'documentation\\nthat', 818: 'covers', 819: 'every', 820: 'module,', 821: 'class', 822: 'function', 823: 'toolkit,\\nspecifying', 824: 'parameters', 825: 'giving', 826: 'usage', 827: '.\\n\\n\\npython', 828: '3', 829: 'nltk', 830: '3\\nthis', 831: 'version', 832: 'updated', 833: 'support', 834: 'nltk\\n3', 835: 'significant', 836: 'changes:\\n\\nthe', 837: 'print', 838: 'statement', 839: 'now', 840: 'parentheses;\\nmany', 841: 'functions', 842: 'return', 843: 'iterators', 844: 'instead', 845: 'lists', 846: '(to', 847: 'save', 848: 'memory', 849: 'usage);\\ninteger', 850: 'division', 851: 'returns', 852: 'floating', 853: 'point', 854: 'number\\nall', 855: 'unicode\\nstrings', 856: 'formatted', 857: 'format', 858: 'method\\n\\nfor', 859: 'detailed', 860: 'list', 861: 'changes,', 862: 'see\\nhttps://docs', 863: '.org/dev/whatsnew/3', 864: '.0', 865: '.html', 866: '.\\nthere', 867: 'utility', 868: '2to3', 869: '.py', 870: 'convert', 871: '2\\ncode', 872: '3;', 873: '.org/2/library/2to3', 874: 'pervasive', 875: 'changes:\\n\\nmany', 876: 'types', 877: 'initialised', 878: 'strings', 879: 'fromstring()', 880: 'method\\nmany', 881: 'lists\\ncontextfreegrammar', 882: 'cfg', 883: 'weightedgrammar', 884: 'pcfg\\nbatch_tokenize()', 885: 'tokenize_sents();', 886: 'corresponding', 887: 'changes', 888: 'batch', 889: 'taggers,', 890: 'parsers,', 891: 'classifiers\\nsome', 892: 'removed', 893: 'favour', 894: 'external', 895: 'packages,', 896: 'maintained', 897: 'adequately\\n\\nfor', 898: 'see\\nhttps://github', 899: '.com/nltk/nltk/wiki/porting-your-code-to-nltk-3', 900: '.\\n\\n\\nsoftware', 901: 'requirements\\nto', 902: 'install', 903: 'several', 904: 'packages', 905: '.\\ncurrent', 906: 'instructions', 907: '.\\n\\n\\n\\n\\npython:the', 908: 'assumes', 909: '.2', 910: 'later', 911: '.\\n(note', 912: 'works', 913: '2', 914: '.6', 915: '.7', 916: '.)\\n\\nnltk:the', 917: 'subsequent', 918: 'releases', 919: 'nltk\\nwill', 920: 'backward-compatible', 921: '.\\n\\nnltk-data:this', 922: 'contains', 923: 'corpora', 924: 'analyzed', 925: 'processed', 926: '.\\n\\nnumpy:(recommended)\\nthis', 927: 'library', 928: 'multidimensional', 929: 'arrays', 930: 'and\\nlinear', 931: 'algebra,', 932: 'required', 933: 'probability,', 934: 'clustering,', 935: 'classification\\ntasks', 936: '.\\n\\nmatplotlib:(recommended)\\nthis', 937: '2d', 938: 'plotting', 939: 'visualization,\\nand', 940: \"book's\", 941: 'produce', 942: 'graphs', 943: 'bar', 944: 'charts', 945: '.\\n\\nstanford', 946: 'tools:\\n\\xa0(recommended)\\nnltk', 947: 'stanford', 948: 'tools', 949: 'scale\\nlanguage', 950: '(see', 951: 'http://nlp', 952: '.stanford', 953: '.edu/software/)', 954: '.\\n\\nnetworkx:(optional)\\nthis', 955: 'storing', 956: 'manipulating', 957: 'network', 958: 'consisting', 959: 'of\\nnodes', 960: 'edges', 961: 'visualizing', 962: 'semantic', 963: 'networks,', 964: 'graphviz', 965: '.\\n\\nprover9:(optional)\\nthis', 966: 'automated', 967: 'theorem', 968: 'prover', 969: 'first-order', 970: 'equational', 971: 'logic,', 972: 'used\\nto', 973: 'inference', 974: '.\\n\\n\\n\\n\\n\\nnatural', 975: '(nltk)\\nnltk', 976: 'was', 977: 'originally', 978: 'created', 979: '2001', 980: 'part', 981: 'linguistics\\ncourse', 982: 'department', 983: 'the\\nuniversity', 984: 'pennsylvania', 985: 'since', 986: 'then', 987: 'developed\\nand', 988: 'expanded', 989: 'dozens', 990: 'contributors', 991: 'been\\nadopted', 992: 'universities,', 993: 'serves', 994: 'basis\\nof', 995: 'research', 996: 'projects', 997: 'viii', 998: 'most\\nimportant', 999: 'modules', 1000: '.\\n\\n\\n\\n\\n\\n\\n\\nlanguage', 1001: 'task\\nnltk', 1002: 'modules\\nfunctionality\\n\\n\\n\\naccessing', 1003: 'corpora\\ncorpus\\nstandardized', 1004: 'lexicons\\n\\nstring', 1005: 'processing\\ntokenize,', 1006: 'stem\\ntokenizers,', 1007: 'sentence', 1008: 'tokenizers,', 1009: 'stemmers\\n\\ncollocation', 1010: 'discovery\\ncollocations\\nt-test,', 1011: 'chi-squared,', 1012: 'point-wise', 1013: 'mutual', 1014: 'information\\n\\npart-of-speech', 1015: 'tagging\\ntag\\nn-gram,', 1016: 'backoff,', 1017: 'brill,', 1018: 'hmm,', 1019: 'tnt\\n\\nmachine', 1020: 'learning\\nclassify,', 1021: 'cluster,', 1022: 'tbl\\ndecision', 1023: 'tree,', 1024: 'maximum', 1025: 'entropy,', 1026: 'naive', 1027: 'bayes,', 1028: 'em,', 1029: 'k-means\\n\\nchunking\\nchunk\\nregular', 1030: 'expression,', 1031: 'n-gram,', 1032: 'named-entity\\n\\nparsing\\nparse,', 1033: 'ccg\\nchart,', 1034: 'feature-based,', 1035: 'unification,', 1036: 'probabilistic,', 1037: 'dependency\\n\\nsemantic', 1038: 'interpretation\\nsem,', 1039: 'inference\\nlambda', 1040: 'calculus,', 1041: 'model', 1042: 'checking\\n\\nevaluation', 1043: 'metrics\\nmetrics\\nprecision,', 1044: 'recall,', 1045: 'agreement', 1046: 'coefficients\\n\\nprobability', 1047: 'estimation\\nprobability\\nfrequency', 1048: 'distributions,', 1049: 'smoothed', 1050: 'probability', 1051: 'distributions\\n\\napplications\\napp,', 1052: 'chat\\ngraphical', 1053: 'concordancer,', 1054: 'wordnet', 1055: 'browser,', 1056: 'chatbots\\n\\nlinguistic', 1057: 'fieldwork\\ntoolbox\\nmanipulate', 1058: 'sil', 1059: 'toolbox', 1060: 'format\\n\\n\\ntable', 1061: 'functionality\\n\\n\\nnltk', 1062: 'designed', 1063: 'four', 1064: 'primary', 1065: 'mind:\\n\\n\\n\\n\\nsimplicity:to', 1066: 'intuitive', 1067: 'framework', 1068: 'with\\nsubstantial', 1069: 'building', 1070: 'blocks,', 1071: 'users', 1072: 'practical\\nknowledge', 1073: 'getting', 1074: 'bogged', 1075: 'tedious\\nhouse-keeping', 1076: 'usually', 1077: 'associated', 1078: 'annotated\\nlanguage', 1079: 'data\\n\\nconsistency:to', 1080: 'uniform', 1081: 'consistent\\ninterfaces', 1082: 'structures,', 1083: 'easily-guessable', 1084: 'names\\n\\nextensibility:to', 1085: 'structure', 1086: 'software\\nmodules', 1087: 'accommodated,', 1088: 'alternative\\nimplementations', 1089: 'competing', 1090: 'approaches', 1091: 'same', 1092: 'task\\n\\nmodularity:to', 1093: 'used\\nindependently', 1094: 'needing', 1095: 'understand', 1096: 'rest', 1097: 'of\\nthe', 1098: 'toolkit\\n\\n\\n\\ncontrasting', 1099: 'non-requirements', 1100: '—\\npotentially', 1101: 'qualities', 1102: 'deliberately', 1103: 'avoided', 1104: 'first,\\nwhile', 1105: 'functions,', 1106: 'not\\nencyclopedic;', 1107: 'toolkit,', 1108: 'system,', 1109: 'will\\ncontinue', 1110: 'evolve', 1111: '.\\nsecond,', 1112: 'while', 1113: 'efficient', 1114: 'enough', 1115: 'support\\nmeaningful', 1116: 'tasks,', 1117: 'optimized', 1118: 'runtime', 1119: 'performance;\\nsuch', 1120: 'optimizations', 1121: 'algorithms,\\nor', 1122: 'lower-level', 1123: 'c', 1124: 'c++', 1125: 'would', 1126: 'make', 1127: 'less', 1128: 'readable', 1129: 'difficult', 1130: '.\\nthird,', 1131: 'avoid', 1132: 'clever', 1133: 'tricks,\\nsince', 1134: 'believe', 1135: 'clear', 1136: 'preferable\\nto', 1137: 'ingenious', 1138: 'indecipherable', 1139: 'ones', 1140: '.\\n\\n\\nfor', 1141: 'instructors\\nnatural', 1142: 'taught', 1143: 'within', 1144: 'the\\nconfines', 1145: 'single-semester', 1146: 'course', 1147: 'undergraduate', 1148: 'level\\nor', 1149: 'postgraduate', 1150: 'level', 1151: 'instructors', 1152: 'found', 1153: 'is\\ndifficult', 1154: 'both', 1155: 'theoretical', 1156: 'sides', 1157: 'the\\nsubject', 1158: 'span', 1159: 'time', 1160: 'focus', 1161: 'to\\nthe', 1162: 'exclusion', 1163: 'exercises,', 1164: 'deprive', 1165: 'students', 1166: 'the\\nchallenge', 1167: 'excitement', 1168: 'automatically', 1169: 'process\\nlanguage', 1170: 'simply', 1171: 'for\\nlinguists,', 1172: 'do', 1173: 'manage', 1174: 'developed', 1175: 'address', 1176: 'problem,\\nmaking', 1177: 'feasible', 1178: 'amount', 1179: 'and\\npractice', 1180: 'course,', 1181: 'no\\nprior', 1182: 'experience', 1183: '.\\na', 1184: 'fraction', 1185: 'syllabus', 1186: 'deals', 1187: 'with\\nalgorithms', 1188: 'own', 1189: 'rather\\ndry,', 1190: 'but', 1191: 'brings', 1192: 'life', 1193: 'of\\ninteractive', 1194: 'user', 1195: 'possible\\nto', 1196: 'view', 1197: 'step-by-step', 1198: 'include\\na', 1199: 'demonstration', 1200: 'performs', 1201: 'without\\nrequiring', 1202: 'special', 1203: 'input', 1204: '.\\nan', 1205: 'effective', 1206: 'deliver', 1207: 'interactive\\npresentation', 1208: 'entering', 1209: 'session,\\nobserving', 1210: 'what', 1211: 'do,', 1212: 'modifying', 1213: 'empirical\\nor', 1214: 'issue', 1215: 'used\\nas', 1216: 'basis', 1217: 'student', 1218: 'assignments', 1219: 'simplest', 1220: 'involve\\nmodifying', 1221: 'fragment', 1222: 'specified', 1223: 'to\\nanswer', 1224: 'concrete', 1225: 'end', 1226: 'spectrum,', 1227: 'nltk\\nprovides', 1228: 'flexible', 1229: 'graduate-level', 1230: 'projects,\\nwith', 1231: 'basic', 1232: 'structures\\nand', 1233: 'widely', 1234: 'datasets', 1235: '(corpora),\\nand', 1236: 'extensible', 1237: 'architecture', 1238: 'additional', 1239: 'for\\nteaching', 1240: '.\\n\\nwe', 1241: 'unique', 1242: 'comprehensive\\nframework', 1243: 'context', 1244: 'learning\\nto', 1245: 'sets', 1246: 'these\\nmaterials', 1247: 'apart', 1248: 'tight', 1249: 'coupling', 1250: 'chapters\\nand', 1251: 'those', 1252: 'with\\nno', 1253: 'to\\nnlp', 1254: 'after', 1255: 'completing', 1256: 'materials,', 1257: 'ready', 1258: 'to\\nattempt', 1259: 'textbooks,', 1260: 'speech', 1261: 'and\\nlanguage', 1262: 'jurafsky', 1263: 'martin', 1264: '(prentice', 1265: 'hall,', 1266: '2008)', 1267: 'presents', 1268: 'unusual', 1269: 'order,', 1270: 'beginning\\nwith', 1271: 'non-trivial', 1272: 'type', 1273: 'introducing\\nnon-trivial', 1274: 'control', 1275: 'comprehensions', 1276: 'conditionals', 1277: '.\\nthese', 1278: 'idioms', 1279: 'permit', 1280: 'start', 1281: '.\\nonce', 1282: 'place,', 1283: 'systematic', 1284: 'presentation\\nof', 1285: 'strings,', 1286: 'loops,', 1287: 'files,', 1288: 'forth', 1289: 'way,', 1290: 'ground', 1291: 'conventional', 1292: 'approaches,\\nwithout', 1293: 'expecting', 1294: 'programming\\nlanguage', 1295: 'sake', 1296: '.\\ntwo', 1297: 'possible', 1298: 'plans', 1299: 'illustrated', 1300: 'ix', 1301: 'first\\none', 1302: 'presumes', 1303: 'arts/humanities', 1304: 'audience,', 1305: 'whereas', 1306: 'second', 1307: 'presumes\\na', 1308: 'science/engineering', 1309: 'audience', 1310: 'first\\nfive', 1311: 'chapters,', 1312: 'devote', 1313: 'remaining', 1314: 'single', 1315: 'area,\\nsuch', 1316: 'classification', 1317: '6-7),', 1318: '8-9),\\nsemantics', 1319: '10),', 1320: 'management', 1321: '.\\n\\n\\n\\n\\n\\n\\n\\nchapter\\narts', 1322: 'humanities\\nscience', 1323: 'engineering\\n\\n\\n\\n1', 1324: 'python\\n2-4\\n2\\n\\n2', 1325: 'accessing', 1326: 'lexical', 1327: 'resources\\n2-4\\n2\\n\\n3', 1328: 'raw', 1329: 'text\\n2-4\\n2\\n\\n4', 1330: 'programs\\n2-4\\n1-2\\n\\n5', 1331: 'categorizing', 1332: 'tagging', 1333: 'words\\n2-4\\n2-4\\n\\n6', 1334: 'classify', 1335: 'text\\n0-2\\n2-4\\n\\n7', 1336: 'extracting', 1337: 'text\\n2\\n2-4\\n\\n8', 1338: 'analyzing', 1339: 'structure\\n2-4\\n2-4\\n\\n9', 1340: 'feature', 1341: 'grammars\\n2-4\\n1-4\\n\\n10', 1342: 'sentences\\n1-2\\n1-4\\n\\n11', 1343: 'managing', 1344: 'data\\n1-2\\n1-4\\n\\ntotal\\n18-36\\n18-36\\n\\n\\ntable', 1345: 'suggested', 1346: 'plans;', 1347: 'approximate', 1348: 'number', 1349: 'lectures', 1350: 'per', 1351: 'chapter\\n\\n\\n\\n\\nconventions', 1352: 'book\\nthe', 1353: 'typographical', 1354: 'conventions', 1355: 'book:\\nbold', 1356: '--', 1357: 'indicates', 1358: 'terms', 1359: '.\\nitalic', 1360: 'paragraphs', 1361: 'refer', 1362: 'examples,\\nthe', 1363: 'names', 1364: 'texts,', 1365: 'urls;', 1366: 'filenames', 1367: 'extensions', 1368: '.\\nconstant', 1369: 'width', 1370: 'listings,\\nas', 1371: 'well', 1372: 'elements\\nsuch', 1373: 'names,', 1374: 'statements,', 1375: 'keywords;\\nalso', 1376: 'bold', 1377: 'commands', 1378: 'should\\nbe', 1379: 'literally', 1380: 'italic', 1381: 'be\\nreplaced', 1382: 'user-supplied', 1383: 'values', 1384: 'values\\ndetermined', 1385: 'context;', 1386: 'metavariables', 1387: '.\\n\\nnote\\nthis', 1388: 'icon', 1389: 'signifies', 1390: 'tip,', 1391: 'suggestion,', 1392: 'note', 1393: '.\\n\\n\\ncaution!\\nthis', 1394: 'warning', 1395: 'caution', 1396: '.\\n\\n\\n\\nusing', 1397: 'examples\\nthis', 1398: 'here', 1399: 'job', 1400: 'done', 1401: 'general,', 1402: 'may', 1403: 'in\\nthis', 1404: 'contact', 1405: 'for\\npermission', 1406: 'youõre', 1407: 'reproducing', 1408: 'portion', 1409: 'example,\\nwriting', 1410: 'uses', 1411: 'chunks', 1412: 'require\\npermission', 1413: 'selling', 1414: 'distributing', 1415: 'cd-rom', 1416: 'oõreilly', 1417: 'books', 1418: 'does\\nrequire', 1419: 'permission', 1420: 'answering', 1421: 'citing', 1422: 'quoting', 1423: 'example\\ncode', 1424: 'require', 1425: 'incorporating', 1426: 'example', 1427: 'code\\nfrom', 1428: 'productõs', 1429: 'appreciate,', 1430: 'require,', 1431: 'attribution', 1432: 'title,\\nauthor,', 1433: 'publisher,', 1434: 'isbn', 1435: 'example:', 1436: 'ònatural', 1437: 'python,\\nby', 1438: 'steven', 1439: 'bird,', 1440: 'ewan', 1441: 'klein,', 1442: 'edward', 1443: 'loper', 1444: \"o'reilly\", 1445: 'media,', 1446: '978-0-596-51649-9', 1447: '.ó\\nif', 1448: 'feel', 1449: 'outside', 1450: 'fair', 1451: 'given', 1452: 'above,\\nfeel', 1453: 'permissions@oreilly', 1454: '.com', 1455: '.\\n\\n\\nacknowledgments\\nthe', 1456: 'authors', 1457: 'indebted', 1458: 'feedback', 1459: 'on\\nearlier', 1460: 'drafts', 1461: 'book:\\ndoug', 1462: 'arnold,\\nmichaela', 1463: 'atterer,\\ngreg', 1464: 'aumann,\\nkenneth', 1465: 'beesley,\\nsteven', 1466: 'bethard,\\nondrej', 1467: 'bojar,\\nchris', 1468: 'cieri,\\nrobin', 1469: 'cooper,\\ngrev', 1470: 'corbett,\\njames', 1471: 'curran,\\ndan', 1472: 'garrette,\\njean', 1473: 'mark', 1474: 'gawron,\\ndoug', 1475: 'hellmann,\\nnitin', 1476: 'indurkhya,\\nmark', 1477: 'liberman,\\npeter', 1478: 'ljunglöf,\\nstefan', 1479: 'müller,\\nrobin', 1480: 'munn,\\njoel', 1481: 'nothman,\\nadam', 1482: 'przepiorkowski,\\nbrandon', 1483: 'rhodes,\\nstuart', 1484: 'robinson,\\njussi', 1485: 'salmela,\\nkyle', 1486: 'schlansker,\\nrob', 1487: 'speer,\\nand\\nrichard', 1488: 'sproat', 1489: 'thankful', 1490: 'colleagues', 1491: 'comments', 1492: 'on\\nthe', 1493: 'evolved', 1494: 'chapters,\\nincluding', 1495: 'participants', 1496: 'summer', 1497: 'schools\\nin', 1498: 'brazil,', 1499: 'india,', 1500: 'usa', 1501: 'exist', 1502: 'members', 1503: 'nltk-dev\\ndeveloper', 1504: 'community,', 1505: 'named', 1506: 'website,\\nwho', 1507: 'expertise', 1508: 'extending', 1509: 'grateful', 1510: 'u', 1511: '.s', 1512: 'national', 1513: 'foundation,', 1514: 'consortium,\\nan', 1515: 'clarence', 1516: 'dyason', 1517: 'fellowship,\\nand', 1518: 'universities', 1519: 'pennsylvania,', 1520: 'edinburgh,', 1521: 'melbourne', 1522: 'our', 1523: 'work\\non', 1524: 'thank', 1525: 'julie', 1526: 'steele,', 1527: 'abby', 1528: 'fox,', 1529: 'loranah', 1530: 'dimant,', 1531: 'team,', 1532: 'for\\norganizing', 1533: 'comprehensive', 1534: 'reviews', 1535: 'nlp\\nand', 1536: 'communities,', 1537: 'cheerfully', 1538: 'customizing', 1539: \"o'reilly's\", 1540: 'production', 1541: 'tools,\\nand', 1542: 'meticulous', 1543: 'copy-editing', 1544: 'preparing', 1545: 'revised', 1546: 'edition', 1547: '3,', 1548: 'to\\nmichael', 1549: 'korobov', 1550: 'leading', 1551: 'effort', 1552: 'port', 1553: 'to\\nantoine', 1554: 'trux', 1555: 'his', 1556: '.\\nfinally,', 1557: 'owe', 1558: 'huge', 1559: 'debt', 1560: 'gratitude', 1561: 'mimo', 1562: 'jee\\nfor', 1563: 'love,', 1564: 'patience,', 1565: 'over', 1566: 'years', 1567: 'worked', 1568: 'hope', 1569: 'children', 1570: 'andrew,', 1571: 'alison,', 1572: 'kirsten,', 1573: 'leonie,', 1574: 'maaike', 1575: '—\\ncatch', 1576: 'enthusiasm', 1577: 'computation', 1578: 'pages', 1579: '.\\n\\n\\n\\nabout', 1580: 'authors\\nsteven', 1581: 'bird', 1582: 'associate', 1583: 'professor', 1584: 'the\\ndepartment', 1585: 'engineering\\nat', 1586: 'university', 1587: 'melbourne,', 1588: 'senior', 1589: 'the\\nlinguistic', 1590: 'consortium', 1591: '.\\nhe', 1592: 'completed', 1593: 'phd', 1594: 'phonology', 1595: 'of\\nedinburgh', 1596: '1990,', 1597: 'supervised', 1598: 'klein', 1599: 'moved', 1600: 'cameroon', 1601: 'conduct', 1602: 'fieldwork', 1603: 'the\\ngrassfields', 1604: 'bantu', 1605: 'under', 1606: 'auspices', 1607: 'institute\\nof', 1608: 'recently,', 1609: 'he', 1610: 'spent\\nseveral', 1611: 'director', 1612: 'consortium\\nwhere', 1613: 'led', 1614: 'r&d', 1615: 'team', 1616: 'create', 1617: 'models', 1618: 'large\\ndatabases', 1619: 'annotated', 1620: 'university,\\nhe', 1621: 'established', 1622: 'technology', 1623: 'group', 1624: 'has\\ntaught', 1625: 'levels', 1626: 'curriculum', 1627: '2009,', 1628: 'president', 1629: 'association', 1630: '.\\newan', 1631: 'school', 1632: 'of\\ninformatics', 1633: 'edinburgh', 1634: 'on\\nformal', 1635: 'cambridge', 1636: '1978', 1637: 'some\\nyears', 1638: 'sussex', 1639: 'newcastle', 1640: 'upon', 1641: 'tyne,\\newan', 1642: 'took', 1643: 'teaching', 1644: 'position', 1645: 'involved', 1646: 'the\\nestablishment', 1647: \"edinburgh's\", 1648: '1993,', 1649: 'has\\nbeen', 1650: 'closely', 1651: 'ever', 1652: '2000–2002,\\nhe', 1653: 'leave', 1654: 'act', 1655: 'manager', 1656: 'the\\nedinburgh-based', 1657: 'edify', 1658: 'corporation,\\nsanta', 1659: 'clara,', 1660: 'responsible', 1661: 'spoken', 1662: 'dialogue', 1663: 'ewan\\nis', 1664: 'european', 1665: 'for\\ncomputational', 1666: 'founding', 1667: 'member', 1668: 'coordinator', 1669: 'excellence', 1670: 'human', 1671: 'technologies\\n(elsnet)', 1672: '.\\nedward', 1673: 'recently', 1674: 'phd\\non', 1675: 'processing\\nat', 1676: \"steven's\", 1677: 'graduate', 1678: 'on\\ncomputational', 1679: 'fall', 1680: '2000,', 1681: 'and\\nwent', 1682: 'ta', 1683: 'share', 1684: 'of\\nnltk', 1685: 'has\\nhelped', 1686: 'two', 1687: 'documenting', 1688: 'and\\ntesting', 1689: 'epydoc', 1690: 'doctest', 1691: '.\\n\\n\\nroyalties\\nroyalties', 1692: 'sale', 1693: '.\\n\\n\\nfigure', 1694: 'xiv', 1695: 'loper,', 1696: 'stanford,', 1697: 'july', 1698: '2007\\n\\n\\n\\n\\nabout', 1699: 'document', 1700: '.\\nupdated', 1701: 'loper,\\ncopyright', 1702: '©', 1703: '2019', 1704: 'distributed', 1705: '[http://nltk', 1706: '.org/],\\nversion', 1707: '.0,', 1708: 'the\\ncreative', 1709: 'commons', 1710: 'attribution-noncommercial-no', 1711: 'derivative', 1712: 'united', 1713: 'states', 1714: 'license\\n[http://creativecommons', 1715: '.org/licenses/by-nc-nd/3', 1716: '.0/us/]', 1717: 'built', 1718: 'on\\nwed', 1719: '4', 1720: 'sep', 1721: '11:40:48', 1722: 'acst1', 1723: 'python\\n\\n\\n\\n\\n\\n1', 1724: 'python\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nit', 1725: 'hands', 1726: 'millions', 1727: '.\\nwhat', 1728: 'it,', 1729: 'assuming', 1730: 'programs?\\nin', 1731: \"we'll\", 1732: 'questions:\\n\\nwhat', 1733: 'achieve', 1734: 'combining', 1735: 'quantities', 1736: 'text?\\nhow', 1737: 'extract', 1738: 'phrases', 1739: 'sum', 1740: 'text?\\nwhat', 1741: 'work?\\nwhat', 1742: 'challenges', 1743: 'processing?\\n\\nthis', 1744: 'divided', 1745: 'sections', 1746: 'quite\\ndifferent', 1747: 'will\\ntake', 1748: 'linguistically', 1749: 'motivated', 1750: 'necessarily\\nexplaining', 1751: 'closer', 1752: 'we\\nwill', 1753: 'systematically', 1754: 'review', 1755: 'flag', 1756: 'titles,\\nbut', 1757: 'mix', 1758: 'up-front', 1759: 'gives', 1760: 'an\\nauthentic', 1761: 'taste', 1762: 'later,', 1763: 'of\\nelementary', 1764: 'familiarity', 1765: 'areas,', 1766: 'to\\n5;\\nwe', 1767: 'repeat', 1768: 'points', 1769: 'miss', 1770: 'anything\\nyou', 1771: 'completely', 1772: 'you,', 1773: 'raise\\nmore', 1774: 'questions', 1775: 'answers,', 1776: 'addressed', 1777: 'in\\nthe', 1778: '.\\n\\n1\\xa0\\xa0\\xa0computing', 1779: 'language:', 1780: \"words\\nwe're\", 1781: 'very', 1782: 'familiar', 1783: 'text,', 1784: 'day', 1785: 'treat', 1786: 'write,\\nprograms', 1787: '.\\nbut', 1788: 'before', 1789: 'started', 1790: 'interpreter', 1791: '.\\n\\n1', 1792: '.1\\xa0\\xa0\\xa0getting', 1793: 'python\\none', 1794: 'friendly', 1795: 'things', 1796: 'you\\nto', 1797: 'directly', 1798: '—\\nthe', 1799: 'running', 1800: '.\\nyou', 1801: 'interface\\ncalled', 1802: 'environment', 1803: '(idle)', 1804: '.\\non', 1805: 'mac', 1806: 'find', 1807: 'applications→macpython,\\nand', 1808: 'windows', 1809: 'programs→python', 1810: '.\\nunder', 1811: 'run', 1812: 'shell', 1813: 'typing', 1814: 'idle\\n(if', 1815: 'installed,', 1816: 'python)', 1817: 'blurb', 1818: 'version;\\nsimply', 1819: 'check', 1820: 'later\\n(here', 1821: '.4', 1822: '.2):\\n\\n\\n\\n\\n\\xa0\\n\\npython', 1823: '(default,', 1824: 'oct', 1825: '15', 1826: '2014,', 1827: '22:01:37)\\n[gcc', 1828: 'compatible', 1829: 'apple', 1830: 'llvm', 1831: '5', 1832: '(clang-503', 1833: '.40)]', 1834: 'darwin\\ntype', 1835: 'help,', 1836: 'copyright,', 1837: 'credits', 1838: 'license', 1839: '.\\n>>>\\n\\n\\n\\n\\nnote\\nif', 1840: 'unable', 1841: 'interpreter,', 1842: 'probably', 1843: \"don't\\nhave\", 1844: 'installed', 1845: 'correctly', 1846: 'visit', 1847: 'for\\ndetailed', 1848: 'and\\n2', 1849: 'older', 1850: 'versions,', 1851: 'that\\nthe', 1852: '/', 1853: 'operator', 1854: 'rounds\\nfractional', 1855: 'results', 1856: 'downwards', 1857: '(so', 1858: '1/3', 1859: '0)', 1860: 'expected', 1861: 'behavior', 1862: 'division\\nyou', 1863: 'type:', 1864: '__future__', 1865: 'import', 1866: 'division\\n\\nthe', 1867: '>>>', 1868: 'prompt', 1869: 'waiting\\nfor', 1870: 'when', 1871: 'copying', 1872: 'type\\nthe', 1873: 'yourself', 1874: 'now,', 1875: \"let's\", 1876: 'begin', 1877: 'calculator:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 1878: '+', 1879: '*', 1880: '-', 1881: '3\\n8\\n>>>\\n\\n\\n\\nonce', 1882: 'finished', 1883: 'calculating', 1884: 'answer', 1885: 'displaying', 1886: 'the\\nprompt', 1887: 'reappears', 1888: 'means', 1889: 'waiting', 1890: 'another', 1891: 'instruction', 1892: '.\\n\\nnote\\nyour', 1893: 'turn:\\nenter', 1894: 'few', 1895: 'asterisk', 1896: '(*)\\nfor', 1897: 'multiplication', 1898: 'slash', 1899: '(/)', 1900: 'division,', 1901: 'for\\nbracketing', 1902: '.\\n\\n\\nthe', 1903: 'demonstrate', 1904: 'interactively', 1905: 'the\\npython', 1906: 'experimenting', 1907: 'various', 1908: 'language\\nto', 1909: '.\\nnow', 1910: 'nonsensical', 1911: 'expression', 1912: 'handles', 1913: 'it:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 1914: '+\\n', 1915: '<stdin>,', 1916: '1\\n', 1917: '^\\nsyntaxerror:', 1918: 'invalid', 1919: 'syntax\\n>>>\\n\\n\\n\\nthis', 1920: 'produced', 1921: 'error', 1922: \"doesn't\", 1923: 'sense\\nto', 1924: 'plus', 1925: 'sign', 1926: 'interpreter\\nindicates', 1927: 'where', 1928: 'problem', 1929: 'occurred', 1930: '(line', 1931: '<stdin>,\\nwhich', 1932: 'stands', 1933: 'input)', 1934: \"we're\", 1935: 'working\\nwith', 1936: '.\\n\\n\\n1', 1937: '.2\\xa0\\xa0\\xa0getting', 1938: 'nltk\\nbefore', 1939: 'going', 1940: '.\\nfollow', 1941: 'platform', 1942: \"you've\", 1943: 'as\\nbefore,', 1944: 'by\\ntyping', 1945: 'prompt,', 1946: 'selecting\\nthe', 1947: 'shown', 1948: '.\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 1949: 'nltk\\n>>>', 1950: '.download()\\n\\n\\n\\n\\n\\nfigure', 1951: 'downloading', 1952: 'collection:', 1953: 'browse', 1954: 'packages\\nusing', 1955: '.download()', 1956: 'collections', 1957: 'tab', 1958: 'downloader\\nshows', 1959: 'grouped', 1960: 'sets,', 1961: 'select', 1962: 'labeled\\nbook', 1963: 'obtain', 1964: 'all\\ndata', 1965: 'consists\\nof', 1966: '30', 1967: 'compressed', 1968: 'files', 1969: '100mb', 1970: 'disk', 1971: 'space', 1972: '(i', 1973: '.,', 1974: 'downloader)', 1975: 'is\\nnearly', 1976: 'ten', 1977: 'times', 1978: 'size', 1979: '(at', 1980: 'writing)', 1981: 'continues', 1982: 'expand', 1983: '.\\n\\nonce', 1984: 'downloaded', 1985: 'machine,', 1986: 'load', 1987: 'it\\nusing', 1988: 'step', 1989: 'command', 1990: 'tells', 1991: 'to\\nexplore:', 1992: '.book', 1993: 'says', 1994: \"nltk's\", 1995: 'load\\nall', 1996: 'items', 1997: 'module', 1998: 'need\\nas', 1999: 'printing', 2000: 'welcome', 2001: 'message,', 2002: 'loads\\nthe', 2003: '(this', 2004: 'seconds)', 2005: \"here's\", 2006: 'the\\ncommand', 2007: 'again,', 2008: 'output', 2009: 'that\\nyou', 2010: 'care', 2011: 'spelling', 2012: 'punctuation', 2013: 'right,', 2014: 'and\\nremember', 2015: '*\\n***', 2016: '***\\nloading', 2017: 'text1,', 2018: 'text9', 2019: 'sent1,', 2020: 'sent9\\ntype', 2021: '.\\ntype:', 2022: \"'texts()'\", 2023: \"'sents()'\", 2024: '.\\ntext1:', 2025: 'moby', 2026: 'dick', 2027: 'herman', 2028: 'melville', 2029: '1851\\ntext2:', 2030: 'sensibility', 2031: 'jane', 2032: 'austen', 2033: '1811\\ntext3:', 2034: 'genesis\\ntext4:', 2035: 'inaugural', 2036: 'corpus\\ntext5:', 2037: 'chat', 2038: 'corpus\\ntext6:', 2039: 'monty', 2040: 'holy', 2041: 'grail\\ntext7:', 2042: 'wall', 2043: 'street', 2044: 'journal\\ntext8:', 2045: 'personals', 2046: 'corpus\\ntext9:', 2047: 'man', 2048: 'thursday', 2049: 'g', 2050: 'k', 2051: 'chesterton', 2052: '1908\\n>>>\\n\\n\\n\\nany', 2053: 'have\\nto', 2054: 'enter', 2055: 'prompt:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2056: 'text1\\n<text:', 2057: '1851>\\n>>>', 2058: 'text2\\n<text:', 2059: '1811>\\n>>>\\n\\n\\n\\nnow', 2060: \"with,\\nwe're\", 2061: '.3\\xa0\\xa0\\xa0searching', 2062: 'text\\nthere', 2063: 'from\\nsimply', 2064: 'concordance', 2065: 'occurrence', 2066: 'word,', 2067: 'together\\nwith', 2068: 'monstrous', 2069: 'moby\\ndick', 2070: 'text1', 2071: 'period,', 2072: 'term\\nconcordance,', 2073: 'placing', 2074: 'parentheses:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2075: '.concordance(monstrous)\\ndisplaying', 2076: '11', 2077: 'matches:\\nong', 2078: 'former', 2079: ',', 2080: 'came', 2081: 'towards', 2082: ',\\non', 2083: 'psalms', 2084: 'touching', 2085: 'bulk', 2086: 'whale', 2087: 'ork', 2088: 'r\\nll', 2089: 'heathenish', 2090: 'array', 2091: 'clubs', 2092: 'spears', 2093: 'were', 2094: 'thick\\nd', 2095: 'gazed', 2096: 'wondered', 2097: 'cannibal', 2098: 'savage', 2099: 'hav\\nthat', 2100: 'survived', 2101: 'flood', 2102: ';', 2103: 'mountainous', 2104: '!', 2105: 'himmal\\nthey', 2106: 'might', 2107: 'scout', 2108: 'fable', 2109: 'still', 2110: 'worse', 2111: 'de\\nth', 2112: 'radney', 2113: \".'\", 2114: '55', 2115: 'pictures', 2116: 'whales', 2117: 'shall', 2118: 'ere', 2119: 'l\\ning', 2120: 'scenes', 2121: 'connexion', 2122: 'am', 2123: 'strongly\\nere', 2124: 'fo\\nght', 2125: 'rummaged', 2126: 'cabinet', 2127: 'telling', 2128: 'but\\nof', 2129: 'bones', 2130: 'oftentimes', 2131: 'cast', 2132: 'dead', 2133: 'u\\n>>>\\n\\n\\n\\nthe', 2134: 'particular', 2135: 'takes', 2136: 'a\\nfew', 2137: 'extra', 2138: 'seconds', 2139: 'searches', 2140: 'fast', 2141: 'turn:\\ntry', 2142: 'searching', 2143: 'words;', 2144: 're-typing,', 2145: 'to\\nuse', 2146: 'up-arrow,', 2147: 'ctrl-up-arrow', 2148: 'alt-p', 2149: 'modify', 2150: 'searched', 2151: 'included', 2152: '.\\nfor', 2153: 'search', 2154: 'word\\naffection,', 2155: 'text2', 2156: '.concordance(affection)', 2157: 'genesis\\nto', 2158: 'lived,', 2159: 'using\\ntext3', 2160: '.concordance(lived)', 2161: 'text4,', 2162: 'the\\ninaugural', 2163: 'corpus,', 2164: 'english', 2165: 'going\\nback', 2166: '1789,', 2167: 'nation,', 2168: 'terror,', 2169: 'god\\nto', 2170: 'differently', 2171: \".\\nwe've\", 2172: 'text5,', 2173: 'nps', 2174: 'corpus:', 2175: 'for\\nunconventional', 2176: 'im,', 2177: 'ur,', 2178: 'lol', 2179: 'uncensored!)\\n\\nonce', 2180: 'spent', 2181: 'little', 2182: 'examining', 2183: 'new\\nsense', 2184: 'richness', 2185: 'diversity', 2186: 'chapter\\nyou', 2187: 'broader', 2188: 'in\\nlanguages', 2189: 'saw', 2190: 'that\\nmonstrous', 2191: 'contexts', 2192: '___', 2193: 'pictures\\nand', 2194: 'appear', 2195: 'similar', 2196: 'range\\nof', 2197: 'contexts?', 2198: 'out\\nby', 2199: 'appending', 2200: 'term', 2201: 'in\\nquestion,', 2202: 'inserting', 2203: '.similar(monstrous)\\nmean', 2204: 'maddens', 2205: 'doleful', 2206: 'gamesome', 2207: 'subtly', 2208: 'uncommon', 2209: 'untoward\\nexasperate', 2210: 'loving', 2211: 'passing', 2212: 'mouldy', 2213: 'christian', 2214: 'true', 2215: 'mystifying\\nimperial', 2216: 'modifies', 2217: 'contemptible\\n>>>', 2218: '.similar(monstrous)\\nvery', 2219: 'heartily', 2220: 'exceedingly', 2221: 'remarkably', 2222: 'vast', 2223: 'great', 2224: 'amazingly\\nextremely', 2225: 'sweet\\n>>>\\n\\n\\n\\nobserve', 2226: '.\\nausten', 2227: 'quite', 2228: 'melville;', 2229: 'her,', 2230: 'has\\npositive', 2231: 'connotations,', 2232: 'intensifier', 2233: 'word\\nvery', 2234: 'common_contexts', 2235: 'the\\ncontexts', 2236: 'shared', 2237: 'words,', 2238: 'monstrous\\nand', 2239: 'enclose', 2240: 'square', 2241: 'brackets', 2242: 'as\\nwell', 2243: 'parentheses,', 2244: 'separate', 2245: 'comma:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2246: '.common_contexts([monstrous,', 2247: 'very])\\na_pretty', 2248: 'is_pretty', 2249: 'am_glad', 2250: 'be_glad', 2251: 'a_lucky\\n>>>\\n\\n\\n\\n\\nnote\\nyour', 2252: 'turn:\\npick', 2253: 'pair', 2254: 'using\\nthe', 2255: 'similar()', 2256: 'common_contexts()', 2257: '.\\n\\nit', 2258: 'thing', 2259: 'detect', 2260: 'occurs', 2261: 'text,\\nand', 2262: 'display', 2263: 'however,', 2264: 'determine\\nthe', 2265: 'location', 2266: 'text:', 2267: 'beginning', 2268: 'appears', 2269: 'positional', 2270: 'displayed', 2271: 'dispersion', 2272: 'plot', 2273: 'stripe', 2274: 'represents', 2275: 'instance\\nof', 2276: 'row', 2277: 'entire', 2278: 'we\\nsee', 2279: 'striking', 2280: 'patterns', 2281: 'last', 2282: '220', 2283: 'years\\n(in', 2284: 'constructed', 2285: 'joining\\nthe', 2286: 'end-to-end)', 2287: 'below', 2288: '(e', 2289: '.g', 2290: 'liberty,', 2291: 'constitution),\\nand', 2292: 'predict', 2293: 'the\\ndispersion', 2294: 'it?', 2295: 'before,', 2296: 'take\\ncare', 2297: 'quotes,', 2298: 'commas,', 2299: 'exactly', 2300: 'right', 2301: 'text4', 2302: '.dispersion_plot([citizens,', 2303: 'democracy,', 2304: 'freedom,', 2305: 'duties,', 2306: 'america])\\n>>>\\n\\n\\n\\n\\n\\nfigure', 2307: '.2:', 2308: 'presidential', 2309: 'addresses:\\nthis', 2310: 'investigate', 2311: '.\\n\\n\\nnote\\nimportant:\\nyou', 2312: 'numpy', 2313: 'matplotlib', 2314: 'installed\\nin', 2315: 'plots', 2316: '.\\nplease', 2317: 'installation', 2318: '.\\n\\n\\nnote\\nyou', 2319: 'frequency', 2320: 'using\\nhttps://books', 2321: '.google', 2322: '.com/ngrams\\n\\nnow,', 2323: 'fun,', 2324: 'generating', 2325: 'random', 2326: 'various\\nstyles', 2327: 'seen', 2328: 'text\\nfollowed', 2329: 'generate', 2330: '(we', 2331: 'the\\nparentheses,', 2332: \"there's\", 2333: 'nothing', 2334: 'goes', 2335: '.)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2336: 'text3', 2337: '.generate()\\nin', 2338: 'brother', 2339: 'hairy', 2340: 'whose', 2341: 'top', 2342: 'reach\\nunto', 2343: 'heaven', 2344: 'ye', 2345: 'sow', 2346: 'land', 2347: 'egypt', 2348: 'bread', 2349: 'in\\nall', 2350: 'month', 2351: 'earth', 2352: 'thy\\nwages', 2353: '?', 2354: 'made', 2355: 'father', 2356: 'isaac', 2357: 'old', 2358: 'kissed\\nhim', 2359: ':', 2360: 'laban', 2361: 'cattle', 2362: 'midst', 2363: 'esau', 2364: 'thy\\nfirst', 2365: 'born', 2366: 'phichol', 2367: 'chief', 2368: 'butler', 2369: 'unto', 2370: 'son', 2371: 'she\\n>>>\\n\\n\\n\\n\\nnote\\nthe', 2372: 'generate()', 2373: 'be\\nreinstated', 2374: '.\\n\\n\\n\\n\\n\\n1', 2375: '.4\\xa0\\xa0\\xa0counting', 2376: 'vocabulary\\nthe', 2377: 'obvious', 2378: 'fact', 2379: 'emerges', 2380: 'that\\nthey', 2381: 'differ', 2382: 'vocabulary', 2383: 'the\\ncomputer', 2384: 'count', 2385: '.\\nas', 2386: 'jump', 2387: 'experiment', 2388: 'with\\nthe', 2389: 'though', 2390: 'studied', 2391: 'systematically\\nyet', 2392: 'examples,', 2393: 'trying', 2394: 'the\\nexercises', 2395: \".\\nlet's\", 2396: 'finding', 2397: 'length', 2398: 'finish,\\nin', 2399: 'symbols', 2400: 'the\\nterm', 2401: 'len', 2402: 'something,', 2403: 'apply', 2404: 'the\\nbook', 2405: 'genesis:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2406: 'len(text3)\\n44764\\n>>>\\n\\n\\n\\nso', 2407: 'genesis', 2408: '44,764', 2409: 'symbols,', 2410: 'tokens', 2411: 'token', 2412: 'technical', 2413: 'characters\\n—', 2414: 'hairy,', 2415: 'his,', 2416: ':)', 2417: 'a\\ngroup', 2418: 'say,', 2419: 'phrase\\nto', 2420: 'be,', 2421: 'counting', 2422: 'occurrences', 2423: 'these\\nsequences', 2424: 'thus,', 2425: 'phrase', 2426: 'to,\\ntwo', 2427: 'are\\nonly', 2428: 'distinct', 2429: '.\\nhow', 2430: 'contain?\\nto', 2431: 'pose', 2432: 'slightly\\ndifferently', 2433: 'tokens\\nthat', 2434: 'uses,', 2435: 'set,', 2436: 'duplicates', 2437: 'collapsed\\ntogether', 2438: 'the\\ncommand:', 2439: 'set(text3)', 2440: 'screens', 2441: 'will\\nfly', 2442: 'following:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2443: 'sorted(set(text3))', 2444: \"\\n['!',\", 2445: \"',\", 2446: \"'(',\", 2447: \"')',\", 2448: \"',',\", 2449: \"',)',\", 2450: \"'\", 2451: \".',\", 2452: \".)',\", 2453: \"':',\", 2454: \"';',\", 2455: \"';)',\", 2456: \"'?',\", 2457: \"'?)',\\n'a',\", 2458: \"'abel',\", 2459: \"'abelmizraim',\", 2460: \"'abidah',\", 2461: \"'abide',\", 2462: \"'abimael',\", 2463: \"'abimelech',\\n'abr',\", 2464: \"'abrah',\", 2465: \"'abraham',\", 2466: \"'abram',\", 2467: \"'accad',\", 2468: \"'achbor',\", 2469: \"'adah',\", 2470: '.]\\n>>>', 2471: 'len(set(text3))', 2472: '\\n2789\\n>>>\\n\\n\\n\\nby', 2473: 'wrapping', 2474: 'sorted()', 2475: 'around', 2476: 'set(text3)\\n,', 2477: 'sorted', 2478: 'items,', 2479: 'continuing', 2480: 'all\\ncapitalized', 2481: 'precede', 2482: 'lowercase', 2483: 'discover', 2484: 'indirectly,', 2485: 'asking\\nfor', 2486: 'again', 2487: 'to\\nobtain', 2488: 'although', 2489: 'tokens,', 2490: 'book\\nhas', 2491: 'only', 2492: '2,789', 2493: 'form', 2494: 'independently', 2495: 'its\\nspecific', 2496: 'is,', 2497: 'the\\nword', 2498: 'considered', 2499: 'item', 2500: 'items\\nwill', 2501: 'generally', 2502: 'call', 2503: 'these\\nunique', 2504: '.\\nnow,', 2505: 'calculate', 2506: 'measure', 2507: 'lexical\\nrichness', 2508: 'of\\ndistinct', 2509: '6%', 2510: 'total', 2511: 'equivalently\\nthat', 2512: '16', 2513: 'average\\n(remember', 2514: \"you're\", 2515: '2,', 2516: 'division)', 2517: 'len(text3)\\n0', 2518: '.06230453042623537\\n>>>\\n\\n\\n\\nnext,', 2519: 'occurs\\nin', 2520: 'compute', 2521: 'percentage', 2522: 'specific', 2523: 'word:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2524: '.count(smote)\\n5\\n>>>', 2525: '100', 2526: \".count('a')\", 2527: 'len(text4)\\n1', 2528: '.4643016433938312\\n>>>\\n\\n\\n\\n\\nnote\\nyour', 2529: 'turn:\\nhow', 2530: 'text5?\\nhow', 2531: 'words\\nin', 2532: 'text?\\n\\nyou', 2533: 'calculations', 2534: 'texts,\\nbut', 2535: 'tedious', 2536: 'keep', 2537: 'retyping', 2538: 'formula', 2539: 'instead,\\nyou', 2540: 'task,', 2541: 'like\\nlexical_diversity', 2542: 'percentage,', 2543: 'block', 2544: 'short\\nname', 2545: 'and\\nyou', 2546: 're-use', 2547: 'a\\ntask', 2548: 'function,', 2549: 'and\\nwe', 2550: 'define', 2551: 'keyword', 2552: 'def', 2553: 'the\\nnext', 2554: 'functions,\\nlexical_diversity()', 2555: 'percentage():\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2556: 'lexical_diversity(text):', 2557: '\\n', 2558: 'len(set(text))', 2559: 'len(text)', 2560: '.\\n>>>', 2561: 'percentage(count,', 2562: 'total):', 2563: 'total\\n', 2564: '.\\n\\n\\n\\n\\ncaution!\\nthe', 2565: 'from\\n>>>', 2566: 'encountering', 2567: 'colon', 2568: 'the\\nend', 2569: 'indicates\\nthat', 2570: 'expects', 2571: 'indented', 2572: 'indentation,', 2573: 'four\\nspaces', 2574: 'hitting', 2575: 'finish', 2576: 'just\\nenter', 2577: 'blank', 2578: '.\\n\\nin', 2579: 'definition', 2580: 'lexical_diversity()', 2581: 'we\\nspecify', 2582: 'parameter', 2583: 'is\\na', 2584: 'placeholder', 2585: 'actual', 2586: 'to\\ncompute,', 2587: 'reoccurs', 2588: 'the\\nfunction', 2589: 'similarly,', 2590: 'percentage()', 2591: 'to\\ntake', 2592: 'parameters,', 2593: 'knows', 2594: 'percentage()\\nare', 2595: 'blocks\\nof', 2596: 'go', 2597: 'ahead', 2598: 'functions:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2599: 'lexical_diversity(text3)\\n0', 2600: '.06230453042623537\\n>>>', 2601: 'lexical_diversity(text5)\\n0', 2602: '.13477005109975562\\n>>>', 2603: 'percentage(4,', 2604: '5)\\n80', 2605: '.0\\n>>>', 2606: 'percentage(text4', 2607: \".count('a'),\", 2608: 'len(text4))\\n1', 2609: '.4643016433938312\\n>>>\\n\\n\\n\\nto', 2610: 'recap,', 2611: 'followed\\nby', 2612: 'parenthesis,', 2613: 'close\\nparenthesis', 2614: 'show', 2615: 'often;', 2616: 'separate\\nthe', 2617: 'data\\nthat', 2618: 'place', 2619: 'a\\nfunction', 2620: 'already', 2621: 'encountered', 2622: 'such\\nas', 2623: 'len(),', 2624: 'set(),', 2625: 'convention,', 2626: 'will\\nalways', 2627: 'add', 2628: 'empty', 2629: 'in\\nlen(),', 2630: 'talking', 2631: 'rather', 2632: '.\\nfunctions', 2633: 'concept', 2634: 'only\\nmention', 2635: 'outset', 2636: 'newcomers', 2637: 'the\\npower', 2638: 'creativity', 2639: 'worry', 2640: 'bit\\nconfusing', 2641: '.\\nlater', 2642: 'tabulating', 2643: 'table', 2644: 'but\\nwith', 2645: 'repetitive', 2646: '.\\n\\n\\n\\n\\n\\n\\n\\n\\ngenre\\ntokens\\ntypes\\nlexical', 2647: 'diversity\\n\\n\\n\\nskill', 2648: 'hobbies\\n82345\\n11935\\n0', 2649: '.145\\n\\nhumor\\n21695\\n5017\\n0', 2650: '.231\\n\\nfiction:', 2651: 'science\\n14470\\n3233\\n0', 2652: '.223\\n\\npress:', 2653: 'reportage\\n100554\\n14394\\n0', 2654: '.143\\n\\nfiction:', 2655: 'romance\\n70022\\n8452\\n0', 2656: '.121\\n\\nreligion\\n39399\\n6373\\n0', 2657: '.162\\n\\n\\ntable', 2658: 'genres', 2659: 'brown', 2660: 'corpus\\n\\n\\n\\n\\n\\n2\\xa0\\xa0\\xa0a', 2661: 'python:', 2662: \"words\\n\\nyou've\", 2663: 'elements', 2664: 'moments', 2665: '.\\n\\n2', 2666: '.1\\xa0\\xa0\\xa0lists\\n\\nwhat', 2667: 'text?', 2668: 'level,', 2669: 'page', 2670: 'up\\nof', 2671: 'sections,', 2672: 'paragraphs,\\nand', 2673: 'purposes,', 2674: 'nothing\\nmore', 2675: 'represent\\ntext', 2676: 'case', 2677: 'opening', 2678: 'dick:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2679: 'sent1', 2680: '=', 2681: \"['call',\", 2682: \"'me',\", 2683: \"'ishmael',\", 2684: \".']\\n>>>\\n\\n\\n\\nafter\", 2685: \"we've\", 2686: 'equals', 2687: 'sign,', 2688: 'quoted', 2689: 'separated', 2690: 'with\\ncommas,', 2691: 'surrounded', 2692: 'bracketed', 2693: 'material\\nis', 2694: 'store', 2695: 'inspect', 2696: 'ask', 2697: \"\\n['call',\", 2698: \".']\\n>>>\", 2699: 'len(sent1)', 2700: '\\n4\\n>>>', 2701: 'lexical_diversity(sent1)', 2702: '\\n1', 2703: '.0\\n>>>\\n\\n\\n\\nsome', 2704: 'you,\\none', 2705: 'texts,\\nsent2', 2706: '…', 2707: 'sent9', 2708: 'them\\nhere;', 2709: 'interpreter\\n(if', 2710: 'sent2', 2711: 'defined,', 2712: 'you\\nneed', 2713: '*)', 2714: \"sent2\\n['the',\", 2715: \"'family',\", 2716: \"'of',\", 2717: \"'dashwood',\", 2718: \"'had',\", 2719: \"'long',\\n'been',\", 2720: \"'settled',\", 2721: \"'in',\", 2722: \"'sussex',\", 2723: \"sent3\\n['in',\", 2724: \"'the',\", 2725: \"'beginning',\", 2726: \"'god',\", 2727: \"'created',\", 2728: \"'the',\\n'heaven',\", 2729: \"'and',\", 2730: \"'earth',\", 2731: \".']\\n>>>\\n\\n\\n\\n\\nnote\\nyour\", 2732: 'turn:\\nmake', 2733: 'sentences', 2734: 'own,', 2735: 'equals\\nsign,', 2736: 'this:\\nex1', 2737: \"['monty',\", 2738: \"'python',\", 2739: \"'holy',\", 2740: \"'grail']\", 2741: '.\\nrepeat', 2742: 'operations', 2743: 'earlier', 2744: 'in\\n1,\\ne', 2745: 'sorted(ex1),', 2746: 'len(set(ex1)),', 2747: 'ex1', 2748: \".count('the')\", 2749: '.\\n\\na', 2750: 'pleasant', 2751: 'surprise', 2752: '.\\nadding', 2753: 'creates', 2754: 'list\\nwith', 2755: 'everything', 2756: 'list,', 2757: 'list:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2758: \"'python']\", 2759: \"['and',\", 2760: \"\\n['monty',\", 2761: \"'grail']\\n>>>\\n\\n\\n\\n\\nnote\\nthis\", 2762: 'operation', 2763: 'concatenation;\\nit', 2764: 'combines', 2765: 'concatenate\\nsentences', 2766: 'either;', 2767: 'short\\nnames', 2768: 'pre-defined', 2769: 'sent4', 2770: \"sent1\\n['fellow',\", 2771: \"'-',\", 2772: \"'citizens',\", 2773: \"'senate',\", 2774: \"'the',\\n'house',\", 2775: \"'representatives',\", 2776: \"'call',\", 2777: \".']\\n>>>\\n\\n\\n\\nwhat\", 2778: 'list?', 2779: '.\\nwhen', 2780: 'append()', 2781: 'itself', 2782: 'result\\nof', 2783: '.append(some)\\n>>>', 2784: \"sent1\\n['call',\", 2785: \"'some']\\n>>>\\n\\n\\n\\n\\n\\n2\", 2786: '.2\\xa0\\xa0\\xa0indexing', 2787: 'lists\\n\\n\\nas', 2788: 'seen,', 2789: 'represented\\nusing', 2790: 'combination', 2791: 'quotes', 2792: 'ordinary\\npage', 2793: 'text1\\nwith', 2794: 'len(text1),', 2795: 'a\\nparticular', 2796: \"'heaven'\", 2797: \".count('heaven')\", 2798: '.\\nwith', 2799: 'pick', 2800: '1st,', 2801: '173rd,', 2802: '14,278th\\nword', 2803: 'printed', 2804: 'analogously,', 2805: 'identify', 2806: 'a\\npython', 2807: 'that\\nrepresents', 2808: \"item's\", 2809: 'instruct', 2810: 'python\\nto', 2811: '173', 2812: 'text\\nby', 2813: 'brackets:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2814: \"text4[173]\\n'awaken'\\n>>>\\n\\n\\n\\nwe\", 2815: 'converse;', 2816: 'first\\noccurs:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2817: \".index('awaken')\\n173\\n>>>\\n\\n\\n\\nindexes\", 2818: 'common', 2819: 'text,\\nor,', 2820: 'generally,', 2821: 'sublists', 2822: 'well,', 2823: 'extracting\\nmanageable', 2824: 'pieces', 2825: 'technique\\nknown', 2826: 'slicing', 2827: \"text5[16715:16735]\\n['u86',\", 2828: \"'thats',\", 2829: \"'why',\", 2830: \"'something',\", 2831: \"'like',\", 2832: \"'gamefly',\", 2833: \"'is',\", 2834: \"'so',\", 2835: \"'good',\\n'because',\", 2836: \"'you',\", 2837: \"'can',\", 2838: \"'actually',\", 2839: \"'play',\", 2840: \"'a',\", 2841: \"'full',\", 2842: \"'game',\", 2843: \"'without',\\n'buying',\", 2844: \"'it']\\n>>>\", 2845: \"text6[1600:1625]\\n['we',\", 2846: \"'re',\", 2847: \"'an',\", 2848: \"'anarcho',\", 2849: \"'syndicalist',\", 2850: \"'commune',\", 2851: \"'we',\\n'take',\", 2852: \"'it',\", 2853: \"'turns',\", 2854: \"'to',\", 2855: \"'act',\", 2856: \"'as',\", 2857: \"'sort',\", 2858: \"'executive',\\n'officer',\", 2859: \"'for',\", 2860: \"'week']\\n>>>\\n\\n\\n\\nindexes\", 2861: 'subtleties,', 2862: 'sentence:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2863: 'sent', 2864: \"['word1',\", 2865: \"'word2',\", 2866: \"'word3',\", 2867: \"'word4',\", 2868: \"'word5',\\n\", 2869: \"'word6',\", 2870: \"'word7',\", 2871: \"'word8',\", 2872: \"'word9',\", 2873: \"'word10']\\n>>>\", 2874: \"sent[0]\\n'word1'\\n>>>\", 2875: \"sent[9]\\n'word10'\\n>>>\\n\\n\\n\\nnotice\", 2876: 'indexes', 2877: 'zero:', 2878: 'element', 2879: 'zero,', 2880: 'sent[0],\\nis', 2881: \"'word1',\", 2882: '9', 2883: \"'word10'\", 2884: 'simple:', 2885: 'moment', 2886: 'accesses', 2887: 'from\\nthe', 2888: \"computer's\", 2889: 'memory,', 2890: 'element;\\nwe', 2891: 'tell', 2892: 'forward', 2893: '.\\nthus,', 2894: 'zero', 2895: 'steps', 2896: 'leaves', 2897: 'practice', 2898: 'initially', 2899: 'confusing,\\nbut', 2900: 'typical', 2901: 'modern', 2902: 'hang', 2903: \"if\\nyou've\", 2904: 'mastered', 2905: 'system', 2906: 'centuries', 2907: '19xy', 2908: 'year\\nin', 2909: '20th', 2910: 'century,', 2911: 'live', 2912: 'country', 2913: 'floors', 2914: 'of\\na', 2915: 'numbered', 2916: '1,', 2917: 'walking', 2918: 'n-1', 2919: 'flights', 2920: 'of\\nstairs', 2921: 'n', 2922: '.\\n\\nnow,', 2923: 'accidentally', 2924: 'too', 2925: 'large,', 2926: 'error:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2927: 'sent[10]\\ntraceback', 2928: '(most', 2929: 'recent', 2930: 'last):\\n', 2931: '?\\nindexerror:', 2932: 'range\\n>>>\\n\\n\\n\\nthis', 2933: 'error,', 2934: 'syntactically', 2935: 'correct', 2936: '.\\ninstead,', 2937: 'produces', 2938: 'traceback', 2939: 'message', 2940: 'that\\nshows', 2941: 'error,\\nindexerror,', 2942: 'brief', 2943: 'explanation', 2944: 'slicing,', 2945: 'verify', 2946: 'slice', 2947: '5:8', 2948: 'at\\nindexes', 2949: '5,', 2950: '6,', 2951: '7:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2952: \"sent[5:8]\\n['word6',\", 2953: \"'word8']\\n>>>\", 2954: \"sent[5]\\n'word6'\\n>>>\", 2955: \"sent[6]\\n'word7'\\n>>>\", 2956: \"sent[7]\\n'word8'\\n>>>\\n\\n\\n\\nby\", 2957: 'm:n', 2958: 'm…n-1', 2959: 'shows,\\nwe', 2960: 'omit', 2961: 'begins', 2962: ':\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 2963: 'sent[:3]', 2964: \"\\n['word1',\", 2965: \"'word3']\\n>>>\", 2966: 'text2[141525:]', 2967: \"\\n['among',\", 2968: \"'merits',\", 2969: \"'happiness',\", 2970: \"'elinor',\", 2971: \"'marianne',\\n',',\", 2972: \"'let',\", 2973: \"'not',\", 2974: \"'be',\", 2975: \"'ranked',\", 2976: \"'least',\", 2977: \"'considerable',\", 2978: \"',',\\n'that',\", 2979: \"'though',\", 2980: \"'sisters',\", 2981: \"'living',\", 2982: \"'almost',\", 2983: \"'within',\", 2984: \"'sight',\", 2985: \"'of',\\n'each',\", 2986: \"'other',\", 2987: \"'they',\", 2988: \"'could',\", 2989: \"'live',\", 2990: \"'without',\", 2991: \"'disagreement',\", 2992: \"'between',\\n'themselves',\", 2993: \"'or',\", 2994: \"'producing',\", 2995: \"'coolness',\", 2996: \"'between',\", 2997: \"'their',\", 2998: \"'husbands',\", 2999: \".',\\n'the',\", 3000: \"'end']\\n>>>\\n\\n\\n\\nwe\", 3001: 'assigning', 3002: 'put', 3003: 'sent[0]', 3004: 'left', 3005: 'also\\nreplace', 3006: 'consequence', 3007: 'this\\nlast', 3008: 'change', 3009: 'elements,', 3010: 'value\\ngenerates', 3011: \"'first'\", 3012: '\\n>>>', 3013: 'sent[9]', 3014: \"'last'\\n>>>\", 3015: 'len(sent)\\n10\\n>>>', 3016: 'sent[1:9]', 3017: \"['second',\", 3018: \"'third']\", 3019: \"sent\\n['first',\", 3020: \"'second',\", 3021: \"'third',\", 3022: \"'last']\\n>>>\", 3023: '\\ntraceback', 3024: 'range\\n>>>\\n\\n\\n\\n\\nnote\\nyour', 3025: 'turn:\\ntake', 3026: 'minutes', 3027: 'and\\ngroups', 3028: '(slices)', 3029: 'understanding\\nby', 3030: '.\\n\\n\\n\\n2', 3031: '.3\\xa0\\xa0\\xa0variables\\nfrom', 3032: 'had\\naccess', 3033: 'text2,', 3034: 'saved', 3035: 'lot\\nof', 3036: '250,000-word', 3037: 'name\\nlike', 3038: 'this!', 3039: 'anything', 3040: 'care\\nto', 3041: 'did', 3042: 'ourselves', 3043: 'e', 3044: '.,\\ndefining', 3045: 'follows:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 3046: \".']\\n>>>\\n\\n\\n\\nsuch\", 3047: 'form:', 3048: 'evaluate\\nthe', 3049: 'result', 3050: 'process', 3051: 'is\\ncalled', 3052: 'assignment', 3053: 'output;\\nyou', 3054: 'its\\nown', 3055: 'contents', 3056: 'slightly', 3057: 'misleading,\\nsince', 3058: 'moving', 3059: 'side', 3060: 'left-arrow', 3061: 'like,', 3062: 'my_sent,', 3063: 'xyzzy', 3064: 'must', 3065: 'letter,', 3066: 'numbers', 3067: 'underscores', 3068: 'variables', 3069: 'assignments:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 3070: 'my_sent', 3071: \"['bravely',\", 3072: \"'bold',\", 3073: \"'sir',\", 3074: \"'robin',\", 3075: \"'rode',\\n\", 3076: \"'forth',\", 3077: \"'from',\", 3078: \"'camelot',\", 3079: 'noun_phrase', 3080: 'my_sent[1:4]\\n>>>', 3081: \"noun_phrase\\n['bold',\", 3082: \"'robin']\\n>>>\", 3083: 'sorted(noun_phrase)\\n>>>', 3084: \"words\\n['robin',\", 3085: \"'bold']\\n>>>\\n\\n\\n\\nremember\", 3086: 'capitalized', 3087: '.\\n\\nnote\\nnotice', 3088: 'split', 3089: 'definition\\nof', 3090: 'across\\nmultiple', 3091: 'lines,', 3092: 'happens', 3093: 'indicate', 3094: 'is\\nexpected', 3095: 'matter', 3096: 'indentation', 3097: 'these\\ncontinuation', 3098: 'makes', 3099: 'easier', 3100: 'choose', 3101: 'meaningful', 3102: 'remind', 3103: 'anyone\\nelse', 3104: 'reads', 3105: 'meant', 3106: 'names;', 3107: 'blindly', 3108: 'follows', 3109: 'instructions,\\nand', 3110: 'confusing,', 3111: \"'two'\", 3112: 'restriction', 3113: 'that\\na', 3114: 'cannot', 3115: 'reserved', 3116: 'as\\ndef,', 3117: 'if,', 3118: 'not,\\nand', 3119: \"'camelot'\", 3120: '\\nfile', 3121: \"'camelot'\\n\", 3122: 'syntax\\n>>>\\n\\n\\n\\nwe', 3123: 'hold', 3124: 'computation,', 3125: 'especially\\nwhen', 3126: 'follow', 3127: 'len(set(text1))', 3128: 'written:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 3129: 'vocab', 3130: 'set(text1)\\n>>>', 3131: 'vocab_size', 3132: 'len(vocab)\\n>>>', 3133: 'vocab_size\\n19317\\n>>>\\n\\n\\n\\n\\ncaution!\\ntake', 3134: 'choice', 3135: 'identifiers)', 3136: 'python\\nvariables', 3137: 'first,', 3138: 'optionally\\nfollowed', 3139: 'digits', 3140: '(0', 3141: '9)', 3142: 'letters', 3143: 'abc23', 3144: 'fine,', 3145: 'but\\n23abc', 3146: 'cause', 3147: '.\\nnames', 3148: 'case-sensitive,', 3149: 'myvar', 3150: 'myvar\\nare', 3151: 'contain', 3152: 'whitespace,\\nbut', 3153: 'underscore,', 3154: '.,\\nmy_var', 3155: 'insert', 3156: 'hyphen', 3157: 'an\\nunderscore:', 3158: 'my-var', 3159: 'wrong,', 3160: 'interprets', 3161: 'the\\n-', 3162: 'minus', 3163: '.4\\xa0\\xa0\\xa0strings\\nsome', 3164: 'words,\\nor', 3165: 'assign', 3166: ',\\nindex', 3167: \"'monty'\", 3168: 'name[0]', 3169: \"\\n'm'\\n>>>\", 3170: 'name[:4]', 3171: \"\\n'mont'\\n>>>\\n\\n\\n\\nwe\", 3172: 'perform', 3173: 'strings:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 3174: \"2\\n'montymonty'\\n>>>\", 3175: \"'!'\\n'monty!'\\n>>>\\n\\n\\n\\nwe\", 3176: 'join', 3177: 'string,', 3178: \".join(['monty',\", 3179: \"'python'])\\n'monty\", 3180: \"python'\\n>>>\", 3181: \"'monty\", 3182: \"python'\", 3183: \".split()\\n['monty',\", 3184: \"'python']\\n>>>\\n\\n\\n\\nwe\", 3185: 'back', 3186: 'topic', 3187: 'being,', 3188: 'blocks\\n—', 3189: '—\\nand', 3190: '.\\n\\n\\n\\n3\\xa0\\xa0\\xa0computing', 3191: \"statistics\\n\\nlet's\", 3192: 'bring', 3193: 'computational\\nresources', 3194: 'bear', 3195: 'began', 3196: 'in\\n1,', 3197: 'context,', 3198: 'compile', 3199: 'random\\ntext', 3200: 'distinct,\\nand', 3201: 'automatic', 3202: 'characteristic', 3203: 'expressions\\nof', 3204: 'try\\nnew', 3205: 'interpreter,\\nand', 3206: \"you'll\", 3207: '.\\nbefore', 3208: 'further,', 3209: 'the\\nlast', 3210: 'predicting', 3211: 'use\\nthe', 3212: 'whether', 3213: 'got', 3214: 'sure', 3215: 'how\\nto', 3216: 'section\\nbefore', 3217: 'saying', 3218: \"['after',\", 3219: \"'all',\", 3220: \"'said',\", 3221: \"'done',\\n\", 3222: \"'more',\", 3223: \"'than',\", 3224: \"'done']\\n>>>\", 3225: 'set(saying)\\n>>>', 3226: 'sorted(tokens)\\n>>>', 3227: 'tokens[-2:]\\nwhat', 3228: 'expect', 3229: 'here?\\n>>>\\n\\n\\n\\n\\n3', 3230: '.1\\xa0\\xa0\\xa0frequency', 3231: 'distributions\\nhow', 3232: 'most\\ninformative', 3233: 'genre', 3234: 'imagine', 3235: 'might\\ngo', 3236: '50', 3237: 'frequent', 3238: 'method\\nwould', 3239: 'tally', 3240: 'item,', 3241: 'thousands', 3242: 'rows,', 3243: 'exceedingly\\nlaborious', 3244: 'laborious', 3245: 'appearing', 3246: '(a', 3247: 'distribution)\\n\\nthe', 3248: 'distribution,\\nand', 3249: '.\\n(in', 3250: 'observable', 3251: 'event', 3252: '.)\\nit', 3253: 'distribution\\nbecause', 3254: 'text\\nare', 3255: '.\\nsince', 3256: 'distributions', 3257: 'built-in', 3258: 'freqdist', 3259: 'the\\n50', 3260: 'fdist1', 3261: 'freqdist(text1)', 3262: 'print(fdist1)', 3263: '\\n<freqdist', 3264: '19317', 3265: '260819', 3266: 'outcomes>\\n>>>', 3267: '.most_common(50)', 3268: \"\\n[(',',\", 3269: '18713),', 3270: \"('the',\", 3271: '13721),', 3272: \"('\", 3273: '6862),', 3274: \"('of',\", 3275: '6536),', 3276: \"('and',\", 3277: \"6024),\\n('a',\", 3278: '4569),', 3279: \"('to',\", 3280: '4542),', 3281: \"(';',\", 3282: '4072),', 3283: \"('in',\", 3284: '3916),', 3285: \"('that',\", 3286: \"2982),\\n(',\", 3287: '2684),', 3288: \"('-',\", 3289: '2552),', 3290: \"('his',\", 3291: '2459),', 3292: \"('it',\", 3293: '2209),', 3294: \"('i',\", 3295: \"2124),\\n('s',\", 3296: '1739),', 3297: \"('is',\", 3298: '1695),', 3299: \"('he',\", 3300: '1661),', 3301: \"('with',\", 3302: '1659),', 3303: \"('was',\", 3304: \"1632),\\n('as',\", 3305: '1620),', 3306: \"('',\", 3307: '1478),', 3308: \"('all',\", 3309: '1462),', 3310: \"('for',\", 3311: '1414),', 3312: \"('this',\", 3313: \"1280),\\n('!',\", 3314: '1269),', 3315: \"('at',\", 3316: '1231),', 3317: \"('by',\", 3318: '1137),', 3319: \"('but',\", 3320: '1113),', 3321: \"('not',\", 3322: \"1103),\\n('--',\", 3323: '1070),', 3324: \"('him',\", 3325: '1058),', 3326: \"('from',\", 3327: '1052),', 3328: \"('be',\", 3329: '1030),', 3330: \"('on',\", 3331: \"1005),\\n('so',\", 3332: '918),', 3333: \"('whale',\", 3334: '906),', 3335: \"('one',\", 3336: '889),', 3337: \"('you',\", 3338: '841),', 3339: \"('had',\", 3340: \"767),\\n('have',\", 3341: '760),', 3342: \"('there',\", 3343: '715),', 3344: '705),', 3345: \"('or',\", 3346: '697),', 3347: \"('were',\", 3348: \"680),\\n('now',\", 3349: '646),', 3350: \"('which',\", 3351: '640),', 3352: \"('?',\", 3353: '637),', 3354: \"('me',\", 3355: '627),', 3356: \"('like',\", 3357: '624)]\\n>>>', 3358: \"fdist1['whale']\\n906\\n>>>\\n\\n\\n\\nwhen\", 3359: 'invoke', 3360: 'freqdist,', 3361: 'an\\nargument', 3362: '(outcomes)\\nthat', 3363: 'counted', 3364: '260,819', 3365: 'the\\ncase', 3366: 'most_common(50)', 3367: 'frequently', 3368: 'occurring', 3369: 'distribution', 3370: 'for\\ntext2', 3371: 'uppercase', 3372: 'nameerror:', 3373: \"'freqdist'\", 3374: 'defined,\\nyou', 3375: '*\\n\\n\\ndo', 3376: 'text?\\nonly', 3377: 'whale,', 3378: 'informative!', 3379: '900', 3380: 'text;', 3381: \"they're\", 3382: 'plumbing', 3383: 'proportion', 3384: 'words?\\nwe', 3385: 'cumulative', 3386: 'words,\\nusing', 3387: '.plot(50,', 3388: 'cumulative=true),', 3389: 'graph', 3390: 'account', 3391: 'nearly', 3392: 'half', 3393: 'book!\\n\\n\\nfigure', 3394: 'dick:\\nthese', 3395: '.\\n\\nif', 3396: 'us,', 3397: 'occur', 3398: 'once\\nonly,', 3399: 'so-called', 3400: 'hapaxes?', 3401: '.hapaxes()', 3402: 'lexicographer,', 3403: 'cetological,\\ncontraband,', 3404: 'expostulations,', 3405: '9,000', 3406: 'others', 3407: 'seems', 3408: 'rare', 3409: 'seeing', 3410: 'the\\ncontext', 3411: \"can't\", 3412: 'guess', 3413: 'hapaxes', 3414: 'case!\\nsince', 3415: 'neither', 3416: 'nor', 3417: 'infrequent', 3418: 'try\\nsomething', 3419: '.\\n\\n\\n3', 3420: '.2\\xa0\\xa0\\xa0fine-grained', 3421: 'selection', 3422: 'words\\nnext,', 3423: 'perhaps', 3424: 'be\\nmore', 3425: 'informative', 3426: 'adapt', 3427: 'notation\\nfrom', 3428: 'vocabulary\\nof', 3429: 'call\\nthis', 3430: 'property', 3431: 'p,', 3432: 'p(w)', 3433: 'true\\nif', 3434: 'w', 3435: 'express', 3436: 'interest', 3437: 'mathematical\\nset', 3438: 'notation', 3439: '(1a)', 3440: 'an\\nelement', 3441: 'v', 3442: '(the', 3443: 'vocabulary)', 3444: 'p', 3445: '.\\n\\n', 3446: '(1)\\n', 3447: '.{w', 3448: '|', 3449: '∈', 3450: '&', 3451: 'p(w)}\\n\\n', 3452: 'b', 3453: '.[w', 3454: 'p(w)]\\n\\nthe', 3455: '(1b)', 3456: '.)\\nobserve', 3457: 'notations', 3458: 'and\\nwrite', 3459: 'executable', 3460: 'code:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 3461: 'long_words', 3462: '[w', 3463: 'len(w)', 3464: '>', 3465: '15]\\n>>>', 3466: \"sorted(long_words)\\n['circumnavigation',\", 3467: \"'physiognomically',\", 3468: \"'apprehensiveness',\", 3469: \"'cannibalistically',\\n'characteristically',\", 3470: \"'circumnavigating',\", 3471: \"'circumnavigation',\", 3472: \"'circumnavigations',\\n'comprehensiveness',\", 3473: \"'hermaphroditical',\", 3474: \"'indiscriminately',\", 3475: \"'indispensableness',\\n'irresistibleness',\", 3476: \"'preternaturalness',\", 3477: \"'responsibilities',\\n'simultaneousness',\", 3478: \"'subterraneousness',\", 3479: \"'supernaturalness',\", 3480: \"'superstitiousness',\\n'uncomfortableness',\", 3481: \"'uncompromisedness',\", 3482: \"'undiscriminating',\", 3483: \"'uninterpenetratingly']\\n>>>\\n\\n\\n\\nfor\", 3484: 'v,', 3485: 'whether\\nlen(w)', 3486: 'greater', 3487: '15;', 3488: 'will\\nbe', 3489: 'ignored', 3490: 'discuss', 3491: 'statements', 3492: 'changing', 3493: 'condition', 3494: '.\\ndoes', 3495: 'difference', 3496: 'the\\nvariable', 3497: '[word', 3498: \".]?\\n\\nlet's\", 3499: 'characterize', 3500: '.\\nnotice', 3501: 'reflect', 3502: 'focus\\n—', 3503: 'constitutionally,', 3504: 'transcontinental', 3505: '—\\nwhereas', 3506: 'text5', 3507: 'informal', 3508: 'content:\\nboooooooooooglyyyyyy', 3509: 'yuuuuuuuuuuuummmmmmmmmmmm', 3510: '.\\nhave', 3511: 'succeeded', 3512: 'typify\\na', 3513: 'unique)\\nand', 3514: 'better', 3515: 'occurring\\nlong', 3516: 'promising', 3517: 'eliminates\\nfrequent', 3518: 'the)', 3519: 'words\\n(e', 3520: 'antiphilosophists)', 3521: 'corpus\\nthat', 3522: 'longer', 3523: 'seven', 3524: 'characters,', 3525: 'times:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 3526: 'fdist5', 3527: 'freqdist(text5)\\n>>>', 3528: 'sorted(w', 3529: 'set(text5)', 3530: '7', 3531: 'fdist5[w]', 3532: \"7)\\n['#14-19teens',\", 3533: \"'#talkcity_adults',\", 3534: \"'((((((((((',\", 3535: \"'question',\\n'actually',\", 3536: \"'anything',\", 3537: \"'computer',\", 3538: \"'cute\", 3539: \".-ass',\", 3540: \"'everyone',\", 3541: \"'football',\\n'innocent',\", 3542: \"'listening',\", 3543: \"'remember',\", 3544: \"'seriously',\", 3545: \"'together',\\n'tomorrow',\", 3546: \"'watching']\\n>>>\\n\\n\\n\\nnotice\", 3547: 'conditions:', 3548: 'the\\nwords', 3549: 'letters,', 3550: 'that\\nthese', 3551: 'to\\nautomatically', 3552: 'frequently-occurring', 3553: 'content-bearing\\nwords', 3554: 'modest', 3555: 'milestone:', 3556: 'tiny', 3557: 'piece', 3558: 'code,\\nprocessing', 3559: 'tens', 3560: '.3\\xa0\\xa0\\xa0collocations', 3561: 'bigrams\\na', 3562: 'collocation', 3563: 'together\\nunusually', 3564: 'red', 3565: 'wine', 3566: 'collocation,', 3567: 'the\\nwine', 3568: 'collocations', 3569: 'are\\nresistant', 3570: 'substitution', 3571: 'senses;\\nfor', 3572: 'maroon', 3573: 'sounds', 3574: 'definitely', 3575: 'odd', 3576: 'handle', 3577: 'collocations,', 3578: 'off', 3579: 'text\\na', 3580: 'pairs,', 3581: 'bigrams', 3582: 'easily\\naccomplished', 3583: 'bigrams():\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 3584: \"list(bigrams(['more',\", 3585: \"'done']))\\n[('more',\", 3586: \"'is'),\", 3587: \"'said'),\", 3588: \"('said',\", 3589: \"'than'),\", 3590: \"('than',\", 3591: \"'done')]\\n>>>\\n\\n\\n\\n\\nnote\\nif\", 3592: 'omitted', 3593: 'list()', 3594: 'above,', 3595: \"bigrams(['more',\", 3596: '.]),\\nyou', 3597: '<generator', 3598: '0x10fb8b3a8>', 3599: 'compute\\na', 3600: 'case,', 3601: 'need\\nto', 3602: 'know', 3603: '.\\n\\nhere', 3604: 'than-done', 3605: 'bigram,', 3606: 'write\\nit', 3607: \"'done')\", 3608: 'essentially\\njust', 3609: 'bigrams,', 3610: 'except', 3611: 'pay', 3612: 'attention', 3613: 'the\\ncases', 3614: 'particular,', 3615: 'find\\nbigrams', 3616: 'collocations()', 3617: 'function\\ndoes', 3618: '.collocations()\\nunited', 3619: 'states;', 3620: 'fellow', 3621: 'citizens;', 3622: 'years;', 3623: 'ago;', 3624: 'federal\\ngovernment;', 3625: 'government;', 3626: 'american', 3627: 'people;', 3628: 'vice', 3629: 'president;', 3630: 'old\\nworld;', 3631: 'almighty', 3632: 'god;', 3633: 'magistrate;', 3634: 'justice;\\ngod', 3635: 'bless;', 3636: 'citizen;', 3637: 'indian', 3638: 'tribes;', 3639: 'public', 3640: 'debt;', 3641: 'another;\\nforeign', 3642: 'nations;', 3643: 'political', 3644: 'parties\\n>>>', 3645: 'text8', 3646: '.collocations()\\nwould', 3647: 'like;', 3648: 'medium', 3649: 'build;', 3650: 'social', 3651: 'drinker;', 3652: 'quiet', 3653: 'nights;', 3654: 'non', 3655: 'smoker;\\nlong', 3656: 'term;', 3657: 'age', 3658: 'open;', 3659: 'going;', 3660: 'financially', 3661: 'secure;', 3662: 'fun\\ntimes;', 3663: 'interests;', 3664: 'weekends', 3665: 'away;', 3666: 'poss', 3667: 'rship;', 3668: 'well\\npresented;', 3669: 'married;', 3670: 'mum;', 3671: 'permanent', 3672: 'relationship;', 3673: 'slim\\nbuild\\n>>>\\n\\n\\n\\nthe', 3674: 'emerge', 3675: 'the\\ntexts', 3676: 'would\\nneed', 3677: 'larger', 3678: 'body', 3679: 'things\\ncounting', 3680: 'useful,', 3681: 'can\\nlook', 3682: 'lengths', 3683: 'creating', 3684: 'freqdist\\nout', 3685: 'numbers,', 3686: 'corresponding\\nword', 3687: 'text:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 3688: '[len(w)', 3689: 'text1]', 3690: '\\n[1,', 3691: '4,', 3692: '8,', 3693: '9,', 3694: '11,', 3695: '7,', 3696: 'fdist', 3697: 'freqdist(len(w)', 3698: 'text1)', 3699: 'print(fdist)', 3700: '19', 3701: 'fdist\\nfreqdist({3:', 3702: '50223,', 3703: '1:', 3704: '47933,', 3705: '4:', 3706: '42345,', 3707: '2:', 3708: '38513,', 3709: '5:', 3710: '26597,', 3711: '6:', 3712: '17111,', 3713: '7:', 3714: '14399,\\n', 3715: '8:', 3716: '9966,', 3717: '9:', 3718: '6428,', 3719: '10:', 3720: '3528,', 3721: '.})\\n>>>\\n\\n\\n\\nwe', 3722: 'deriving', 3723: 'text1\\n,\\nand', 3724: 'counts', 3725: 'these\\noccurs', 3726: 'containing\\na', 3727: 'quarter', 3728: 'million', 3729: 'a\\nword', 3730: '20', 3731: 'distinct\\nitems', 3732: 'counted,', 3733: '20,', 3734: '20\\ndifferent', 3735: 'character,\\ntwo', 3736: 'twenty', 3737: 'none', 3738: 'more\\ncharacters', 3739: 'wonder', 3740: 'are\\n(e', 3741: 'five\\nthan', 3742: 'four,', 3743: 'etc)', 3744: '.most_common()\\n[(3,', 3745: '50223),', 3746: '(1,', 3747: '47933),', 3748: '(4,', 3749: '42345),', 3750: '(2,', 3751: '38513),', 3752: '(5,', 3753: '26597),', 3754: '(6,', 3755: '17111),', 3756: '(7,', 3757: '14399),\\n(8,', 3758: '9966),', 3759: '(9,', 3760: '6428),', 3761: '(10,', 3762: '3528),', 3763: '(11,', 3764: '1873),', 3765: '(12,', 3766: '1053),', 3767: '(13,', 3768: '567),', 3769: '(14,', 3770: '177),\\n(15,', 3771: '70),', 3772: '(16,', 3773: '22),', 3774: '(17,', 3775: '12),', 3776: '(18,', 3777: '1),', 3778: '(20,', 3779: '1)]\\n>>>', 3780: '.max()\\n3\\n>>>', 3781: 'fdist[3]\\n50223\\n>>>', 3782: '.freq(3)\\n0', 3783: '.19255882431878046\\n>>>\\n\\n\\n\\nfrom', 3784: 'that\\nwords', 3785: 'roughly', 3786: '50,000', 3787: '20%)', 3788: 'making', 3789: 'pursue', 3790: 'word\\nlength', 3791: 'differences', 3792: 'authors,', 3793: 'genres,', 3794: 'or\\nlanguages', 3795: '.\\n3', 3796: 'summarizes', 3797: '.\\n\\n\\n\\n\\n\\n\\nexample\\ndescription\\n\\n\\n\\nfdist', 3798: 'freqdist(samples)\\ncreate', 3799: 'containing', 3800: 'samples\\n\\nfdist[sample]', 3801: '+=', 3802: '1\\nincrement', 3803: \"sample\\n\\nfdist['monstrous']\\ncount\", 3804: 'sample', 3805: 'occurred\\n\\nfdist', 3806: \".freq('monstrous')\\nfrequency\", 3807: 'sample\\n\\nfdist', 3808: '.n()\\ntotal', 3809: 'samples\\n\\nfdist', 3810: '.most_common(n)\\nthe', 3811: 'frequencies\\n\\nfor', 3812: 'fdist:\\niterate', 3813: '.max()\\nsample', 3814: 'greatest', 3815: 'count\\n\\nfdist', 3816: '.tabulate()\\ntabulate', 3817: 'distribution\\n\\nfdist', 3818: '.plot()\\ngraphical', 3819: '.plot(cumulative=true)\\ncumulative', 3820: 'distribution\\n\\nfdist1', 3821: '|=', 3822: 'fdist2\\nupdate', 3823: 'fdist2\\n\\nfdist1', 3824: '<', 3825: 'fdist2\\ntest', 3826: 'fdist2\\n\\n\\ntable', 3827: 'distributions\\n\\n\\nour', 3828: 'concepts,\\nand', 3829: '.\\n\\n\\n\\n\\n4\\xa0\\xa0\\xa0back', 3830: 'decisions', 3831: 'taking', 3832: 'control\\n\\nso', 3833: 'far,', 3834: 'qualities:\\nthe', 3835: 'ability', 3836: 'and\\nthe', 3837: 'potential', 3838: 'automation', 3839: 'machines', 3840: 'to\\nmake', 3841: 'behalf,', 3842: 'executing', 3843: 'when\\ncertain', 3844: 'conditions', 3845: 'met,', 3846: 'repeatedly', 3847: 'looping', 3848: 'through\\ntext', 3849: 'until', 3850: 'satisfied', 3851: 'feature\\nis', 3852: 'control,', 3853: '.\\n\\n4', 3854: '.1\\xa0\\xa0\\xa0conditionals\\npython', 3855: 'supports', 3856: 'operators,', 3857: '>=,', 3858: 'for\\ntesting', 3859: 'relationship', 3860: 'relational\\noperators', 3861: '.\\n\\n\\n\\n\\n\\n\\noperator\\nrelationship\\n\\n\\n\\n<\\nless', 3862: 'than\\n\\n<=\\nless', 3863: 'equal', 3864: 'to\\n\\n==\\nequal', 3865: '(note', 3866: 'signs,', 3867: 'one)\\n\\n!=\\nnot', 3868: 'to\\n\\n>\\ngreater', 3869: 'than\\n\\n>=\\ngreater', 3870: 'to\\n\\n\\ntable', 3871: 'numerical', 3872: 'comparison', 3873: 'operators\\n\\n\\nwe', 3874: 'news', 3875: 'changed', 3876: 'one\\nline', 3877: 'sent7,', 3878: 'text7\\n(wall', 3879: 'journal)', 3880: 'sent7\\nis', 3881: 'undefined,', 3882: '*\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 3883: \"sent7\\n['pierre',\", 3884: \"'vinken',\", 3885: \"'61',\", 3886: \"'years',\", 3887: \"'old',\", 3888: \"'will',\", 3889: \"'join',\", 3890: \"'the',\\n'board',\", 3891: \"'nonexecutive',\", 3892: \"'director',\", 3893: \"'nov\", 3894: \"'29',\", 3895: 'sent7', 3896: \"4]\\n[',',\", 3897: '<=', 3898: '==', 3899: \"4]\\n['will',\", 3900: '!=', 3901: \"4]\\n['pierre',\", 3902: \"'board',\\n'as',\", 3903: \".']\\n>>>\\n\\n\\n\\nthere\", 3904: 'pattern', 3905: 'examples:\\n[w', 3906: '],', 3907: 'yields', 3908: 'either', 3909: 'false', 3910: 'cases', 3911: 'always', 3912: '.\\nhowever,', 3913: 'properties', 3914: 'listed', 3915: '.\\n\\n\\n\\n\\n\\n\\nfunction\\nmeaning\\n\\n\\n\\ns', 3916: '.startswith(t)\\ntest', 3917: 's', 3918: 't\\n\\ns', 3919: '.endswith(t)\\ntest', 3920: 't\\n\\nt', 3921: 's\\ntest', 3922: 't', 3923: 'substring', 3924: 's\\n\\ns', 3925: '.islower()\\ntest', 3926: 'cased', 3927: 'lowercase\\n\\ns', 3928: '.isupper()\\ntest', 3929: 'uppercase\\n\\ns', 3930: '.isalpha()\\ntest', 3931: 'non-empty', 3932: 'alphabetic\\n\\ns', 3933: '.isalnum()\\ntest', 3934: 'alphanumeric\\n\\ns', 3935: '.isdigit()\\ntest', 3936: 'digits\\n\\ns', 3937: '.istitle()\\ntest', 3938: 'titlecased\\n(i', 3939: 'initial', 3940: 'capitals)\\n\\n\\ntable', 3941: 'operators\\n\\n\\nhere', 3942: 'operators', 3943: 'to\\nselect', 3944: 'texts:\\nwords', 3945: '-ableness;\\nwords', 3946: 'gnt;\\nwords', 3947: 'capital;\\nand', 3948: 'entirely', 3949: 'set(text1)', 3950: \".endswith('ableness'))\\n['comfortableness',\", 3951: \"'honourableness',\", 3952: \"'immutableness',\", 3953: \"'indispensableness',\", 3954: 'sorted(term', 3955: 'set(text4)', 3956: \"'gnt'\", 3957: \"term)\\n['sovereignty',\", 3958: \"'sovereignties',\", 3959: \"'sovereignty']\\n>>>\", 3960: 'sorted(item', 3961: 'set(text6)', 3962: \".istitle())\\n['a',\", 3963: \"'aaaaaaaaah',\", 3964: \"'aaaaaaaah',\", 3965: \"'aaaaaah',\", 3966: \"'aaaah',\", 3967: \"'aaaaugh',\", 3968: \"'aaagh',\", 3969: 'set(sent7)', 3970: \".isdigit())\\n['29',\", 3971: \"'61']\\n>>>\\n\\n\\n\\nwe\", 3972: 'a\\ncondition,', 3973: 'c1', 3974: 'c2,\\nthen', 3975: 'combine', 3976: 'conjunction', 3977: 'disjunction:\\nc1', 3978: 'c2,\\nc1', 3979: 'c2', 3980: 'turn:\\nrun', 3981: 'explain', 3982: '.\\nnext,', 3983: '.\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 3984: 'set(text7)', 3985: \"'-'\", 3986: \"'index'\", 3987: 'w)\\n>>>', 3988: 'sorted(wd', 3989: 'wd', 3990: '.istitle()', 3991: 'len(wd)', 3992: '10)\\n>>>', 3993: '.islower())\\n>>>', 3994: 'sorted(t', 3995: 'set(text2)', 3996: \"'cie'\", 3997: \"'cei'\", 3998: 't)\\n\\n\\n\\n\\n\\n4', 3999: '.2\\xa0\\xa0\\xa0operating', 4000: 'element\\nin', 4001: 'of\\ncounting', 4002: 'used:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 4003: 'text1]\\n[1,', 4004: '.upper()', 4005: \"text1]\\n['[',\", 4006: \"'moby',\", 4007: \"'dick',\", 4008: \"'by',\", 4009: \"'herman',\", 4010: \"'melville',\", 4011: \"'1851',\", 4012: \"']',\", 4013: \"'etymology',\", 4014: '.]\\n>>>\\n\\n\\n\\nthese', 4015: '[f(w)', 4016: '.]', 4017: '.f()', 4018: '.],', 4019: 'where\\nf', 4020: 'operates', 4021: 'length,', 4022: 'to\\nconvert', 4023: 'f(w)', 4024: 'and\\nw', 4025: 'instead,', 4026: 'idiom', 4027: 'the\\nsame', 4028: 'through\\neach', 4029: 'turn', 4030: 'and\\nperforming', 4031: '.\\n\\nnote\\nthe', 4032: 'described', 4033: 'comprehension', 4034: 'example\\nof', 4035: 'idiom,', 4036: 'fixed', 4037: 'habitually', 4038: 'bothering', 4039: 'to\\nanalyze', 4040: 'mastering', 4041: 'a\\nfluent', 4042: 'programmer', 4043: \".\\n\\nlet's\", 4044: 'size,', 4045: 'here:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 4046: 'len(text1)\\n260819\\n>>>', 4047: 'len(set(text1))\\n19317\\n>>>', 4048: 'len(set(word', 4049: '.lower()', 4050: 'text1))\\n17231\\n>>>\\n\\n\\n\\nnow', 4051: 'double-counting', 4052: 'only\\nin', 4053: 'capitalization,', 4054: 'wiped', 4055: '2,000', 4056: 'count!', 4057: 'further\\nand', 4058: 'eliminate', 4059: 'filtering', 4060: 'any\\nnon-alphabetic', 4061: 'items:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 4062: '.isalpha()))\\n16948\\n>>>\\n\\n\\n\\nthis', 4063: 'complicated:', 4064: 'lowercases', 4065: 'purely', 4066: 'alphabetic', 4067: '.\\nperhaps', 4068: 'simpler', 4069: 'lowercase-only', 4070: 'this\\ngives', 4071: 'wrong', 4072: '(why?)', 4073: \".\\ndon't\", 4074: 'confident', 4075: 'yet,\\nsince', 4076: 'explanations', 4077: '.\\n\\n\\n4', 4078: '.3\\xa0\\xa0\\xa0nested', 4079: 'blocks\\nmost', 4080: 'execute', 4081: 'a\\nconditional', 4082: 'statement,', 4083: 'we\\nalready', 4084: 'conditional', 4085: 'tests', 4086: 'in\\nsent7', 4087: '4]', 4088: 'program,', 4089: 'a\\nvariable', 4090: \"'cat'\", 4091: 'the\\nif', 4092: 'checks', 4093: 'len(word)', 4094: 'invoked', 4095: 'the\\nprint', 4096: 'executed,', 4097: '.\\nremember', 4098: 'indent', 4099: 'spaces', 4100: \"'cat'\\n>>>\", 4101: '5:\\n', 4102: \"print('word\", 4103: \"5')\\n\", 4104: '\\nword', 4105: '5\\n>>>\\n\\n\\n\\nwhen', 4106: '\\nin', 4107: 'nested', 4108: '.\\n\\nnote\\nif', 4109: '.7,', 4110: 'following\\nline', 4111: 'above', 4112: 'recognized:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 4113: 'print_function\\n\\n\\n\\n\\nif', 4114: '>=', 4115: '5,\\nto', 4116: '5,\\nthen', 4117: 'time,', 4118: 'executed,\\nand', 4119: 'user:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 4120: '.\\n>>>\\n\\n\\n\\nan', 4121: 'structure\\nbecause', 4122: 'controls', 4123: '.\\nanother', 4124: 'loop', 4125: '.\\ntry', 4126: 'following,', 4127: 'remember', 4128: 'spaces:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 4129: \".']:\\n\", 4130: 'print(word)\\n', 4131: '.\\ncall\\nme\\nishmael\\n', 4132: '.\\n>>>\\n\\n\\n\\nthis', 4133: 'executes', 4134: 'in\\ncircular', 4135: 'fashion', 4136: 'the\\nassignment', 4137: \"'call',\\neffectively\", 4138: 'first\\nitem', 4139: 'then,', 4140: 'displays', 4141: 'word\\nto', 4142: 'next,', 4143: 'statement,\\nand', 4144: 'value\\nto', 4145: 'user,', 4146: 'until\\nevery', 4147: '.4\\xa0\\xa0\\xa0looping', 4148: 'conditions\\nnow', 4149: 'print\\nthe', 4150: 'letter', 4151: 'l', 4152: 'another\\nname', 4153: \"doesn't\\ntry\", 4154: 'sent1:\\n', 4155: \".endswith('l'):\\n\", 4156: 'print(xyzzy)\\n', 4157: '.\\ncall\\nishmael\\n>>>\\n\\n\\n\\nyou', 4158: 'notice', 4159: 'statements\\nhave', 4160: 'line,\\nbefore', 4161: 'fact,', 4162: 'python\\ncontrol', 4163: 'colon\\nindicates', 4164: 'current', 4165: 'relates', 4166: 'the\\nindented', 4167: 'specify', 4168: 'action', 4169: 'if\\nthe', 4170: 'met', 4171: 'elif', 4172: '(else', 4173: 'if)', 4174: 'have\\ncolons', 4175: '.islower():\\n', 4176: 'print(token,', 4177: \"'is\", 4178: \"word')\\n\", 4179: '.istitle():\\n', 4180: 'titlecase', 4181: 'else:\\n', 4182: \"punctuation')\\n\", 4183: '.\\ncall', 4184: 'word\\nme', 4185: 'word\\nishmael', 4186: 'word\\n', 4187: 'punctuation\\n>>>\\n\\n\\n\\nas', 4188: 'see,', 4189: 'small', 4190: 'knowledge,\\nyou', 4191: 'multiline', 4192: \".\\nit's\", 4193: 'pieces,\\ntesting', 4194: 'before\\ncombining', 4195: 'python\\ninteractive', 4196: 'invaluable,', 4197: 'get\\ncomfortable', 4198: 'exploring', 4199: '.\\nfirst,', 4200: 'cie', 4201: 'cei', 4202: 'words,\\nthen', 4203: 'the\\nextra', 4204: 'statement:', 4205: \"end='\", 4206: '(not', 4207: 'default', 4208: 'newline)', 4209: 'tricky', 4210: 'tricky:\\n', 4211: 'print(word,', 4212: \"')\\nancient\", 4213: 'ceiling', 4214: 'conceit', 4215: 'conceited', 4216: 'conceive', 4217: 'conscience\\nconscientious', 4218: 'conscientiously', 4219: 'deceitful', 4220: 'deceive', 4221: '.\\n>>>\\n\\n\\n\\n\\n\\n\\n5\\xa0\\xa0\\xa0automatic', 4222: 'understanding\\n\\nwe', 4223: 'bottom-up,', 4224: 'exploiting', 4225: 'computation\\nby', 4226: 'opportunity\\nnow', 4227: 'nitty-gritty', 4228: 'paint', 4229: 'a\\nbigger', 4230: 'picture', 4231: 'navigate', 4232: 'universe', 4233: 'information\\nlocked', 4234: 'crucial', 4235: 'the\\ngrowth', 4236: 'popularity', 4237: 'web,', 4238: 'shortcomings', 4239: 'skill,', 4240: 'knowledge,', 4241: 'luck,\\nto', 4242: 'answers', 4243: 'as:', 4244: 'tourist', 4245: 'sites', 4246: 'i\\nvisit', 4247: 'philadelphia', 4248: 'pittsburgh', 4249: 'limited', 4250: 'budget?\\nwhat', 4251: 'experts', 4252: 'say', 4253: 'digital', 4254: 'slr', 4255: 'cameras?', 4256: 'what\\npredictions', 4257: 'steel', 4258: 'market', 4259: 'credible', 4260: 'commentators\\nin', 4261: 'week?', 4262: 'automatically\\ninvolves', 4263: 'extraction,\\ninference,', 4264: 'summarization,', 4265: 'carried', 4266: 'scale\\nand', 4267: 'robustness', 4268: 'beyond', 4269: 'capabilities', 4270: 'philosophical', 4271: 'long-standing', 4272: 'intelligence\\nhas', 4273: 'intelligent', 4274: 'machines,', 4275: 'major', 4276: 'behaviour', 4277: 'understanding\\nlanguage', 4278: 'goal', 4279: 'become', 4280: 'mature,', 4281: 'for\\nanalyzing', 4282: 'unrestricted', 4283: 'widespread,', 4284: 'prospect', 4285: 'of\\nnatural', 4286: 're-emerged', 4287: 'plausible', 4288: 'technologies,\\nto', 4289: '.\\n\\n5', 4290: '.1\\xa0\\xa0\\xa0word', 4291: 'disambiguation\\nin', 4292: 'disambiguation', 4293: 'out\\nwhich', 4294: 'consider', 4295: 'the\\nambiguous', 4296: 'serve', 4297: 'dish:\\n\\n', 4298: '(2)\\n', 4299: '.serve:', 4300: 'food', 4301: 'drink;', 4302: 'office;', 4303: 'ball', 4304: 'play\\n\\n', 4305: '.dish:', 4306: 'plate;', 4307: 'meal;', 4308: 'communications', 4309: 'device\\n\\nin', 4310: 'phrase:', 4311: 'served', 4312: 'dish,', 4313: 'you\\ncan', 4314: 'dish', 4315: 'with\\ntheir', 4316: 'meanings', 4317: \"it's\", 4318: 'unlikely', 4319: 'discussion\\nshifted', 4320: 'sports', 4321: 'crockery', 4322: 'invent', 4323: 'bizarre', 4324: 'images,', 4325: 'tennis', 4326: 'pro\\ntaking', 4327: 'her', 4328: 'frustrations', 4329: 'china', 4330: 'tea-set', 4331: 'laid', 4332: 'beside', 4333: 'court', 4334: 'disambiguate', 4335: 'exploiting\\nthe', 4336: 'nearby', 4337: 'related', 4338: 'contextual', 4339: 'effect,', 4340: 'word\\nby,', 4341: 'meanings,', 4342: '.:', 4343: 'by\\nchesterton', 4344: '(agentive', 4345: 'author', 4346: 'book);\\nthe', 4347: 'cup', 4348: 'stove', 4349: '(locative', 4350: 'the\\ncup', 4351: 'is);', 4352: 'submit', 4353: 'friday', 4354: '(temporal', 4355: 'the\\ntime', 4356: 'submitting)', 4357: '.\\nobserve', 4358: '(3c)', 4359: 'italicized', 4360: 'helps', 4361: 'us\\ninterpret', 4362: '(3)\\n', 4363: '.the', 4364: 'lost', 4365: 'searchers', 4366: '(agentive)\\n\\n', 4367: 'mountain', 4368: '(locative)\\n\\n', 4369: 'afternoon', 4370: '(temporal)\\n\\n\\n\\n5', 4371: '.2\\xa0\\xa0\\xa0pronoun', 4372: 'resolution\\na', 4373: 'deeper', 4374: 'whom', 4375: '—\\ni', 4376: 'subjects', 4377: 'verbs', 4378: 'in\\nelementary', 4379: 'school,', 4380: 'harder', 4381: 'thieves', 4382: 'stole', 4383: 'paintings\\nit', 4384: 'stealing', 4385: '.\\nconsider', 4386: '(4c),', 4387: 'determine\\nwhat', 4388: 'sold,', 4389: 'caught,', 4390: '(one', 4391: 'ambiguous)', 4392: '(4)\\n', 4393: 'paintings', 4394: 'subsequently', 4395: 'sold', 4396: 'caught', 4397: '.\\n\\nanswering', 4398: 'antecedent', 4399: 'pronoun', 4400: 'they,\\neither', 4401: 'tackling', 4402: 'problem\\ninclude', 4403: 'anaphora', 4404: 'resolution', 4405: 'noun', 4406: 'phrase\\nrefers', 4407: 'labeling', 4408: 'phrase\\nrelates', 4409: 'verb', 4410: '(as', 4411: 'agent,', 4412: 'patient,', 4413: 'instrument,', 4414: 'on)', 4415: '.\\n\\n\\n5', 4416: '.3\\xa0\\xa0\\xa0generating', 4417: 'output\\nif', 4418: 'understanding,', 4419: 'move', 4420: 'output,', 4421: 'as\\nquestion', 4422: 'case,\\na', 4423: \"user's\", 4424: 'relating', 4425: 'texts:\\n\\n', 4426: '(5)\\n', 4427: '.text:', 4428: '.human:', 4429: 'sold?\\n\\n', 4430: '.machine:', 4431: '.\\n\\nthe', 4432: \"machine's\", 4433: 'demonstrates', 4434: 'they\\nrefers', 4435: 'translate', 4436: 'accurately\\nconveying', 4437: 'original', 4438: 'translating', 4439: 'french,\\nwe', 4440: 'forced', 4441: 'gender', 4442: 'sentence:\\nils', 4443: '(masculine)', 4444: 'found,', 4445: 'elles', 4446: '(feminine)', 4447: 'actually', 4448: 'depends', 4449: '(6)\\n', 4450: '.les', 4451: 'voleurs', 4452: 'ont', 4453: 'volé', 4454: 'les', 4455: 'peintures', 4456: 'ils', 4457: 'été', 4458: 'trouvés', 4459: 'tard', 4460: 'thieves)\\n\\n', 4461: 'trouvées', 4462: 'paintings)\\n\\nin', 4463: 'subject', 4464: 'verb,', 4465: 'the\\nantecedent', 4466: 'establishing', 4467: 'things\\nwe', 4468: '.4\\xa0\\xa0\\xa0machine', 4469: 'translation\\nfor', 4470: '(mt)', 4471: 'grail', 4472: 'understanding,\\nultimately', 4473: 'seeking', 4474: 'high-quality,\\nidiomatic', 4475: '.\\nits', 4476: 'roots', 4477: 'days', 4478: 'cold', 4479: 'war,', 4480: 'promise\\nof', 4481: 'government', 4482: 'sponsorship,\\nand', 4483: '.\\ntoday,', 4484: 'pairs\\nof', 4485: 'languages,', 4486: 'integrated', 4487: 'serious', 4488: 'shortcomings,', 4489: 'which\\nare', 4490: 'starkly', 4491: 'revealed', 4492: 'forth\\nbetween', 4493: 'equilibrium', 4494: 'reached,', 4495: '.:\\n\\n0>', 4496: 'flight', 4497: 'alice', 4498: 'springs?\\n1>', 4499: 'wie', 4500: 'lang', 4501: 'vor', 4502: 'dem', 4503: 'folgenden', 4504: 'flug', 4505: 'zu', 4506: 'springs?\\n2>', 4507: 'jump?\\n3>', 4508: 'springen', 4509: 'sie?\\n4>', 4510: 'jump?\\n5>', 4511: 'lang,', 4512: 'bevor', 4513: 'der', 4514: 'folgende', 4515: 'tun,', 4516: 'sie', 4517: 'springen?\\n6>', 4518: 'long,', 4519: 'does,', 4520: 'jump?\\n7>', 4521: 'tut,', 4522: 'tun', 4523: 'springen?\\n8>', 4524: 'jump?\\n9>', 4525: 'springen?\\n10>', 4526: 'alice,', 4527: 'jump?\\n11>', 4528: 'sprung?\\n12>', 4529: 'leap', 4530: 'you?\\n\\nobserve', 4531: 'translates', 4532: 'springs', 4533: 'english\\nto', 4534: 'german', 4535: '(in', 4536: '1>),', 4537: 'jump\\n(line', 4538: '2)', 4539: 'preposition', 4540: 'translated', 4541: 'corresponding\\ngerman', 4542: 'vor,', 4543: '5)', 4544: '(but', 4545: 'phrasings\\nindicated', 4546: 'leap)', 4547: 'proper', 4548: 'name,\\nand', 4549: 'misinterpreted', 4550: 'grammatical', 4551: 'turn:', 4552: 'http://translationparty', 4553: '.com/\\n\\nmachine', 4554: 'possible\\ntranslations', 4555: '(depending', 4556: 'meaning),', 4557: 'changed\\nin', 4558: 'keeping', 4559: 'target', 4560: '.\\ntoday', 4561: 'difficulties', 4562: 'faced', 4563: 'collecting', 4564: 'massive', 4565: 'of\\nparallel', 4566: 'websites', 4567: 'publish', 4568: 'documents\\nin', 4569: 'possibly\\na', 4570: 'bilingual', 4571: 'dictionary,', 4572: 'sentences,\\na', 4573: 'alignment', 4574: 'once', 4575: 'sentence\\npairs,', 4576: 'phrases,', 4577: 'model\\nthat', 4578: '.5\\xa0\\xa0\\xa0spoken', 4579: 'dialog', 4580: 'systems\\nin', 4581: 'history', 4582: 'intelligence,', 4583: 'one,', 4584: 'namely', 4585: 'turing', 4586: 'test:', 4587: 'system,\\nresponding', 4588: 'input,', 4589: 'naturally', 4590: 'distinguish\\nit', 4591: 'human-generated', 4592: 'response?', 4593: 'contrast,', 4594: \"today's\", 4595: 'commercial', 4596: 'systems\\nare', 4597: 'limited,', 4598: 'narrowly-defined', 4599: 'domains,\\nas', 4600: 'here:\\n\\ns:', 4601: 'you?\\nu:', 4602: 'saving', 4603: 'private', 4604: 'ryan', 4605: 'playing?\\ns:', 4606: 'theater?\\nu:', 4607: 'paramount', 4608: 'theater', 4609: '.\\ns:', 4610: 'playing', 4611: 'theater,', 4612: \"but\\nit's\", 4613: 'madison', 4614: '3:00,', 4615: '5:30,', 4616: '8:00,', 4617: '10:30', 4618: '.\\n\\nyou', 4619: 'driving', 4620: 'or\\ndetails', 4621: 'restaurants', 4622: 'information\\nhad', 4623: 'suitable', 4624: 'question-answer', 4625: 'pairs\\nhad', 4626: 'incorporated', 4627: 'goals:\\nthe', 4628: 'asks', 4629: 'movie', 4630: 'showing', 4631: 'system\\ncorrectly', 4632: 'determines', 4633: 'wants', 4634: 'see\\nthe', 4635: \"probably\\ndidn't\", 4636: 'made,', 4637: 'system\\nneeds', 4638: 'endowed', 4639: 'capability', 4640: 'interact\\nnaturally', 4641: 'asked', 4642: 'private\\nryan', 4643: 'playing?,', 4644: 'unhelpfully', 4645: 'respond', 4646: 'yes', 4647: 'developers', 4648: 'use\\ncontextual', 4649: 'assumptions', 4650: 'logic', 4651: 'ensure', 4652: 'might\\nexpress', 4653: 'requests', 4654: 'handled', 4655: 'that\\nmakes', 4656: 'application', 4657: 'so,', 4658: 'type\\nwhen', 4659: 'me\\nwhen', 4660: 'yield', 4661: 'screening', 4662: 'is\\nenough', 4663: 'service', 4664: 'pipeline', 4665: 'system:\\nspoken', 4666: '(top', 4667: 'left)', 4668: 'analyzed,', 4669: 'recognized,', 4670: 'parsed', 4671: 'and\\ninterpreted', 4672: 'application-specific', 4673: 'actions', 4674: 'right);\\na', 4675: 'response', 4676: 'planned,', 4677: 'realized', 4678: 'suitably\\ninflected', 4679: 'finally', 4680: 'output;', 4681: 'of\\nlinguistic', 4682: 'inform', 4683: 'stage', 4684: '.\\n\\ndialogue', 4685: 'opportunity', 4686: 'mention', 4687: 'the\\ncommonly', 4688: 'assumed', 4689: '.\\n5', 4690: '.\\nalong', 4691: 'diagram,', 4692: 'a\\npipeline', 4693: 'map', 4694: 'via', 4695: 'parsing\\nto', 4696: 'representation', 4697: 'middle,', 4698: 'from\\nright', 4699: 'left,', 4700: 'reverse', 4701: 'converting\\nconcepts', 4702: 'aspects', 4703: 'bottom', 4704: 'diagram', 4705: 'representative', 4706: 'of\\nstatic', 4707: 'information:', 4708: 'repositories', 4709: 'language-related', 4710: 'draw', 4711: 'turn:\\nfor', 4712: 'primitive', 4713: 'having\\na', 4714: 'conversation', 4715: 'chatbot', 4716: 'chatbots,\\nrun', 4717: '.chat', 4718: '.chatbots()', 4719: '.\\n(remember', 4720: '.)\\n\\n\\n\\n5', 4721: '.6\\xa0\\xa0\\xa0textual', 4722: 'entailment\\nthe', 4723: 'brought', 4724: 'public\\nshared', 4725: 'recognizing', 4726: 'textual', 4727: 'entailment', 4728: '(rte)', 4729: 'basic\\nscenario', 4730: 'suppose', 4731: 'evidence', 4732: 'hypothesis:', 4733: 'sandra', 4734: 'goudie', 4735: 'defeated', 4736: 'max', 4737: 'purnell,', 4738: 'and\\nthat', 4739: 'relevant,', 4740: 'example,\\nsandra', 4741: 'elected', 4742: 'parliament', 4743: '2002', 4744: 'elections,\\nnarrowly', 4745: 'winning', 4746: 'seat', 4747: 'coromandel', 4748: 'defeating', 4749: 'labour', 4750: 'candidate\\nmax', 4751: 'purnell', 4752: 'pushing', 4753: 'incumbent', 4754: 'green', 4755: 'mp', 4756: 'jeanette', 4757: 'fitzsimons', 4758: 'into\\nthird', 4759: 'to\\naccept', 4760: 'hypothesis?', 4761: 'conclusion', 4762: 'easily,', 4763: 'with\\nautomated', 4764: 'decision', 4765: 'rte\\nchallenges', 4766: 'allow', 4767: 'competitors', 4768: 'their\\nsystems,', 4769: 'brute', 4770: 'topic\\nwe', 4771: 'chap-data-intensive)', 4772: 'consequently,', 4773: 'some\\nlinguistic', 4774: 'important\\nfor', 4775: 'person', 4776: 'being\\ndefeated', 4777: 'hypothesis,', 4778: 'doing', 4779: 'the\\ntext', 4780: 'illustration', 4781: 'difficulty', 4782: 'consider\\nthe', 4783: 'text-hypothesis', 4784: 'pair:\\n\\n', 4785: '(7)\\n', 4786: 'david', 4787: 'golinkin', 4788: 'editor', 4789: 'eighteen', 4790: 'books,', 4791: '150', 4792: 'responsa,', 4793: 'articles,', 4794: 'sermons', 4795: 'books\\n\\n', 4796: '.hypothesis:', 4797: 'books\\n\\nin', 4798: 'determine', 4799: 'hypothesis', 4800: 'supported', 4801: 'the\\ntext,', 4802: 'needs', 4803: 'background', 4804: 'knowledge:\\n(i)', 4805: 'someone', 4806: 'he/she', 4807: 'that\\nbook;', 4808: '(ii)', 4809: 'not\\nwritten', 4810: '(all', 4811: 'of)', 4812: 'book;', 4813: '(iii)', 4814: 'eighteen\\nbooks,', 4815: 'conclude', 4816: '.7\\xa0\\xa0\\xa0limitations', 4817: 'nlp\\ndespite', 4818: 'research-led', 4819: 'advances', 4820: 'rte,', 4821: 'language\\nsystems', 4822: 'real-world', 4823: 'perform\\ncommon-sense', 4824: 'reasoning', 4825: 'and\\nrobust', 4826: 'manner', 4827: 'wait', 4828: 'artificial\\nintelligence', 4829: 'solved,', 4830: 'meantime', 4831: 'is\\nnecessary', 4832: 'severe', 4833: 'limitations', 4834: 'and\\nknowledge', 4835: 'accordingly,', 4836: 'right\\nfrom', 4837: 'beginning,', 4838: 'progress', 4839: 'that\\nunderstand', 4840: 'superficial', 4841: 'of\\nunrestricted', 4842: '.\\nindeed,', 4843: 'equip', 4844: 'systems,', 4845: 'to\\ncontribute', 4846: 'long-term', 4847: 'aspiration', 4848: '.\\n\\n\\n\\n6\\xa0\\xa0\\xa0summary\\n\\ntexts', 4849: 'represented', 4850: \"lists:\\n['monty',\", 4851: 'indexing,', 4852: 'slicing,\\nand', 4853: 'len()', 4854: 'appearance', 4855: 'text;\\na', 4856: 'sequence\\nof', 4857: 'using\\nlen(set(text))', 4858: 'sorted(set(t))', 4859: 'operate', 4860: '[f(x)', 4861: 'x', 4862: 'text]', 4863: 'derive', 4864: 'vocabulary,', 4865: 'collapsing', 4866: 'distinctions', 4867: 'ignoring', 4868: 'punctuation,\\nwe', 4869: 'set(w', 4870: '.isalpha())', 4871: 't:', 4872: 'character\\nand', 4873: 'executed', 4874: 'character', 4875: 'of\\ncode,', 4876: 'counts\\n(e', 4877: 'appearance)', 4878: 'assigned', 4879: 'reused', 4880: 'keyword,', 4881: 'in\\ndef', 4882: 'mult(x,', 4883: 'y);', 4884: 'y', 4885: 'function,\\nand', 4886: 'placeholders', 4887: 'specifying', 4888: 'more\\narguments', 4889: 'this:', 4890: 'texts(),', 4891: 'mult(3,', 4892: '4),', 4893: 'len(text1)', 4894: '.\\n\\n\\n\\n7\\xa0\\xa0\\xa0further', 4895: 'reading\\nthis', 4896: 'processing,\\nand', 4897: 'linguistics,', 4898: 'mixed', 4899: '.\\nmany', 4900: 'consolidated', 4901: 'to\\nconsult', 4902: '.org/),', 4903: 'links\\nto', 4904: 'links', 4905: 'on\\nsome', 4906: 'nlp-related', 4907: 'wikipedia', 4908: 'collocations,\\nthe', 4909: 'test,', 4910: 'type-token', 4911: 'distinction)', 4912: 'acquaint', 4913: '.org/,\\nincluding', 4914: 'tutorials', 4915: \"beginner's\", 4916: 'guide', 4917: 'http://wiki', 4918: '.org/moin/beginnersguide', 4919: '.\\nmiscellaneous', 4920: 'answered', 4921: 'faq', 4922: 'at\\nhttp://python', 4923: '.org/doc/faq/general/', 4924: 'delve', 4925: 'subscribe', 4926: 'mailing', 4927: 'new\\nreleases', 4928: 'announced', 4929: 'nltk-users', 4930: 'list,\\nwhere', 4931: 'for\\nlanguage', 4932: 'covered', 4933: '5,\\nand', 4934: 'excellent\\nbooks:\\n\\nindurkhya,', 4935: 'nitin', 4936: 'fred', 4937: 'damerau', 4938: '(eds,', 4939: '2010)', 4940: 'handbook', 4941: 'processing\\n(second', 4942: 'edition)', 4943: 'chapman', 4944: 'hall/crc', 4945: '2010', 4946: '(indurkhya', 4947: 'damerau,', 4948: '(dale,', 4949: 'moisl,', 4950: 'somers,', 4951: '2000)\\njurafsky,', 4952: 'daniel', 4953: 'james', 4954: '(2008)', 4955: '(second', 4956: 'prentice', 4957: 'hall', 4958: '.\\n(jurafsky', 4959: 'martin,', 4960: '2008)\\nmitkov,', 4961: 'ruslan', 4962: '(ed,', 4963: '2003)', 4964: 'oxford', 4965: 'press', 4966: '.\\n(second', 4967: '(mitkov,', 4968: '2002)\\n\\nthe', 4969: 'international', 4970: 'organization', 4971: 'acl', 4972: '(http://www', 4973: '.aclweb', 4974: '.org/)', 4975: 'hosts', 4976: 'resources,', 4977: 'including:\\ninformation', 4978: 'regional', 4979: 'conferences', 4980: 'workshops;\\nthe', 4981: 'wiki', 4982: 'resources;\\nand', 4983: 'anthology,', 4984: 'literature\\nfrom', 4985: '50+', 4986: 'years,', 4987: 'fully', 4988: 'indexed', 4989: '.\\nsome', 4990: 'excellent', 4991: 'textbooks', 4992: 'are:\\n[finegan2007]_,', 4993: \"(o'grady\", 4994: 'et', 4995: 'al,', 4996: '2004),', 4997: '(osu,', 4998: '2007)', 4999: 'consult\\nlanguagelog,', 5000: 'popular', 5001: 'blog', 5002: 'occasional', 5003: 'posts', 5004: 'that\\nuse', 5005: '.\\n\\n\\n8\\xa0\\xa0\\xa0exercises\\n\\n\\n☼', 5006: 'calculator,', 5007: 'and\\ntyping', 5008: '12', 5009: '(4', 5010: '1)', 5011: '.\\n\\n☼', 5012: 'alphabet', 5013: '26', 5014: 'power\\n10,', 5015: '**', 5016: '10,', 5017: 'ten-letter', 5018: 'out\\nto', 5019: '141167095653376', 5020: 'hundred-letter', 5021: 'possible?\\n\\n☼', 5022: 'applied', 5023: '20,\\nor', 5024: 'sent1?\\n\\n☼', 5025: 'on\\ncomputing', 5026: 'text2?\\nhow', 5027: 'there?\\n\\n☼', 5028: 'scores', 5029: 'humor\\nand', 5030: 'romance', 5031: 'fiction', 5032: 'is\\nmore', 5033: 'lexically', 5034: 'diverse?\\n\\n☼', 5035: 'protagonists', 5036: 'in\\nsense', 5037: 'sensibility:', 5038: 'elinor,', 5039: 'marianne,', 5040: 'edward,', 5041: 'willoughby', 5042: 'observe', 5043: 'roles', 5044: 'played', 5045: 'males\\nand', 5046: 'females', 5047: 'novel?', 5048: 'couples?\\n\\n☼', 5049: 'expression:', 5050: 'len(set(text4))', 5051: '.\\nstate', 5052: 'purpose', 5053: 'steps\\ninvolved', 5054: '2\\non', 5055: '.\\n\\ndefine', 5056: 'variable,', 5057: '.,\\nmy_string', 5058: \"'my\", 5059: \"string'\", 5060: 'string)', 5061: '.\\nprint', 5062: 'ways,', 5063: 'first\\nby', 5064: 'pressing', 5065: 'enter,', 5066: 'then\\nby', 5067: 'adding', 5068: 'my_string', 5069: 'my_string,', 5070: 'multiplying\\nit', 5071: 'number,', 5072: 'strings\\nare', 5073: 'joined', 5074: 'fix', 5075: 'this?\\n\\n\\n☼', 5076: '[my,', 5077: 'sent]', 5078: 'favorite', 5079: 'saying)', 5080: '.\\n\\nuse', 5081: '.join(my_sent)', 5082: '.\\nuse', 5083: 'split()', 5084: 'form\\nyou', 5085: '.\\n\\n\\n☼', 5086: 'phrase1,\\nphrase2,', 5087: 'combinations', 5088: '(using', 5089: 'operator)\\nto', 5090: 'whole', 5091: 'between\\nlen(phrase1', 5092: 'phrase2)', 5093: 'len(phrase1)', 5094: 'len(phrase2)?\\n\\n☼', 5095: 'expressions,', 5096: 'same\\nvalue', 5097: 'typically', 5098: 'nlp?', 5099: 'why?\\n\\nmonty', 5100: 'python[6:12]\\n[monty,', 5101: 'python][1]\\n\\n\\n☼', 5102: 'represent', 5103: 'where\\neach', 5104: 'sent1[2][2]', 5105: 'do?\\nwhy?', 5106: 'sent3', 5107: 'sent3[1]\\ngives', 5108: \"'the'\", 5109: 'occurrences\\nof', 5110: 'sent3?\\n\\n☼', 5111: '.\\nfind', 5112: '(text5)\\nstarting', 5113: 'alphabetical', 5114: 'list(range(10))', 5115: 'list(range(10,', 5116: '20)),', 5117: '2)),', 5118: 'list(range(20,', 5119: '-2))', 5120: '.\\n\\n◑', 5121: '.index()', 5122: 'sunset', 5123: '.\\nby', 5124: 'trial', 5125: 'that\\ncontains', 5126: 'addition,', 5127: 'operations,', 5128: 'the\\nvocabulary', 5129: 'sent8', 5130: 'lines?\\nwhich', 5131: 'value?', 5132: 'texts?\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 5133: 'sorted(set(w', 5134: 'text1))\\n>>>', 5135: 'set(text1))\\n\\n\\n\\n\\n◑', 5136: 'tests:\\nw', 5137: '.isupper()', 5138: '.islower()?\\n\\n◑', 5139: 'extracts', 5140: 'four-letter', 5141: '(text5)', 5142: '(freqdist),', 5143: 'these\\nwords', 5144: 'decreasing', 5145: 'script', 5146: '(text6)\\nand', 5147: 'text6', 5148: 'that\\nmeet', 5149: 'words:', 5150: '.\\n\\nending', 5151: 'ise\\ncontaining', 5152: 'z\\ncontaining', 5153: 'pt\\nhaving', 5154: 'capital', 5155: 'titlecase)\\n\\n\\n◑', 5156: \"words\\n['she',\", 5157: \"'sells',\", 5158: \"'sea',\", 5159: \"'shells',\", 5160: \"'shore']\", 5161: 'tasks:\\n\\nprint', 5162: 'sh\\nprint', 5163: 'characters\\n\\n\\n◑', 5164: 'do?', 5165: 'sum(len(w)', 5166: 'text1)\\ncan', 5167: 'average', 5168: 'text?\\n\\n◑', 5169: 'vocab_size(text)', 5170: 'single\\nparameter', 5171: 'percent(word,', 5172: 'text)', 5173: 'calculates\\nhow', 5174: 'expresses', 5175: 'result\\nas', 5176: 'vocabularies', 5177: 'following\\npython', 5178: 'set(sent3)', 5179: 'using\\ndifferent', 5180: 'set()', 5181: 'do?\\ncan', 5182: 'this?\\n\\n\\n\\n\\nabout', 5183: 'acst\\n\\n\\n\\ndocutils', 5184: 'messages\\n\\nsystem', 5185: 'message:', 5186: 'error/3', 5187: '(ch01', 5188: '.rst2,', 5189: '1889);', 5190: 'backlink\\nunknown', 5191: 'name:', 5192: 'finegan2007', 5193: 'resources\\n\\n\\n\\n\\n\\n2', 5194: 'resources\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\npractical', 5195: 'uses\\nlarge', 5196: 'python?\\nwhich', 5197: 'helpful', 5198: 'work?\\nhow', 5199: 'repeating', 5200: 'code?\\n\\nthis', 5201: 'present', 5202: 'before\\nexploring', 5203: 'see\\nan', 5204: 'unfamiliar;', 5205: 'see\\nwhat', 5206: 'game', 5207: 'substituting\\nsome', 5208: 'will\\nassociate', 5209: 'hows', 5210: 'whys', 5211: '.\\n\\n1\\xa0\\xa0\\xa0accessing', 5212: 'corpora\\nas', 5213: 'mentioned,', 5214: 'many\\ncorpora', 5215: 'material\\nin', 5216: 'examined', 5217: 'in\\n1', 5218: 'speeches', 5219: 'presidential\\ninaugural', 5220: 'addresses', 5221: 'dozens\\nof', 5222: 'convenience\\nwe', 5223: 'glued', 5224: 'end-to-end', 5225: 'treated', 5226: '.\\n1', 5227: 'that\\nwe', 5228: 'accessed', 5229: 'want\\nto', 5230: 'examines', 5231: 'a\\nvariety', 5232: '.1\\xa0\\xa0\\xa0gutenberg', 5233: 'corpus\\nnltk', 5234: 'project', 5235: 'gutenberg\\nelectronic', 5236: 'archive,', 5237: 'contains\\nsome', 5238: '25,000', 5239: 'electronic', 5240: 'hosted', 5241: 'http://www', 5242: '.gutenberg', 5243: 'begin\\nby', 5244: 'package,\\nthen', 5245: '.corpus', 5246: '.fileids(),', 5247: 'identifiers', 5248: 'corpus:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 5249: \".fileids()\\n['austen-emma\", 5250: \".txt',\", 5251: \"'austen-persuasion\", 5252: \"'austen-sense\", 5253: \"'bible-kjv\", 5254: \".txt',\\n'blake-poems\", 5255: \"'bryant-stories\", 5256: \"'burgess-busterbrown\", 5257: \".txt',\\n'carroll-alice\", 5258: \"'chesterton-ball\", 5259: \"'chesterton-brown\", 5260: \".txt',\\n'chesterton-thursday\", 5261: \"'edgeworth-parents\", 5262: \"'melville-moby_dick\", 5263: \".txt',\\n'milton-paradise\", 5264: \"'shakespeare-caesar\", 5265: \"'shakespeare-hamlet\", 5266: \".txt',\\n'shakespeare-macbeth\", 5267: \"'whitman-leaves\", 5268: \".txt']\\n\\n\\n\\nlet's\", 5269: 'emma', 5270: 'and\\ngive', 5271: 'emma,', 5272: 'contains:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 5273: \".words('austen-emma\", 5274: \".txt')\\n>>>\", 5275: 'len(emma)\\n192427\\n\\n\\n\\n\\nnote\\nin', 5276: 'showed', 5277: 'you\\ncould', 5278: 'carry', 5279: 'concordancing', 5280: '.concordance()', 5281: 'are\\nusing', 5282: 'nine', 5283: 'obtained', 5284: 'from\\nnltk', 5285: '.corpus,', 5286: 'the\\nfollowing', 5287: 'other\\ntasks', 5288: '1:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 5289: '.text(nltk', 5290: \".txt'))\\n>>>\", 5291: '.concordance(surprize)\\n\\n\\n\\n\\nwhen', 5292: 'words()', 5293: 'gutenberg\\nobject', 5294: 'package', 5295: 'cumbersome', 5296: 'provides\\nanother', 5297: 'gutenberg\\n>>>', 5298: 'gutenberg', 5299: \".txt')\\n\\n\\n\\nlet's\", 5300: 'each\\ntext,', 5301: 'fileid', 5302: 'computing\\nstatistics', 5303: 'compact', 5304: 'display,', 5305: 'round\\neach', 5306: 'nearest', 5307: 'integer,', 5308: 'round()', 5309: '.fileids():\\n', 5310: 'num_chars', 5311: 'len(gutenberg', 5312: '.raw(fileid))', 5313: 'num_words', 5314: '.words(fileid))\\n', 5315: 'num_sents', 5316: '.sents(fileid))\\n', 5317: 'num_vocab', 5318: 'len(set(w', 5319: '.words(fileid)))\\n', 5320: 'print(round(num_chars/num_words),', 5321: 'round(num_words/num_sents),', 5322: 'round(num_words/num_vocab),', 5323: 'fileid)\\n', 5324: '25', 5325: 'austen-emma', 5326: '.txt\\n5', 5327: '17', 5328: 'austen-persuasion', 5329: '28', 5330: '22', 5331: 'austen-sense', 5332: '.txt\\n4', 5333: '34', 5334: '79', 5335: 'bible-kjv', 5336: 'blake-poems', 5337: '14', 5338: 'bryant-stories', 5339: '18', 5340: 'burgess-busterbrown', 5341: '13', 5342: 'carroll-alice', 5343: 'chesterton-ball', 5344: '23', 5345: 'chesterton-brown', 5346: 'chesterton-thursday', 5347: '21', 5348: 'edgeworth-parents', 5349: 'melville-moby_dick', 5350: '52', 5351: 'milton-paradise', 5352: 'shakespeare-caesar', 5353: '8', 5354: 'shakespeare-hamlet', 5355: 'shakespeare-macbeth', 5356: '36', 5357: 'whitman-leaves', 5358: '.txt\\n\\n\\n\\nthis', 5359: 'statistics', 5360: 'text:\\naverage', 5361: 'vocabulary\\nitem', 5362: '(our', 5363: 'score)', 5364: 'since\\nit', 5365: 'recurrent', 5366: 'really\\n3', 5367: '.)\\nby', 5368: 'diversity\\nappear', 5369: 'characteristics', 5370: ',\\nnot', 5371: 'raw()', 5372: 'file\\nwithout', 5373: \".raw('blake-poems\", 5374: \".txt'))\\ntells\", 5375: 'sents()', 5376: 'divides', 5377: 'sentences,', 5378: 'words:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 5379: 'macbeth_sentences', 5380: \".sents('shakespeare-macbeth\", 5381: \"macbeth_sentences\\n[['[',\", 5382: \"'tragedie',\", 5383: \"'macbeth',\", 5384: \"'william',\", 5385: \"'shakespeare',\\n'1603',\", 5386: \"']'],\", 5387: \"['actus',\", 5388: \"'primus',\", 5389: \".'],\", 5390: \"macbeth_sentences[1116]\\n['double',\", 5391: \"'double',\", 5392: \"'toile',\", 5393: \"'trouble',\", 5394: \"';',\\n'fire',\", 5395: \"'burne',\", 5396: \"'cauldron',\", 5397: \"'bubble']\\n>>>\", 5398: 'longest_len', 5399: 'max(len(s)', 5400: 'macbeth_sentences)\\n>>>', 5401: '[s', 5402: 'len(s)', 5403: \"longest_len]\\n[['doubtfull',\", 5404: \"'stood',\", 5405: \"'two',\", 5406: \"'spent',\", 5407: \"'swimmers',\", 5408: \"'that',\\n'doe',\", 5409: \"'cling',\", 5410: \"'together',\", 5411: \"'choake',\", 5412: \"'art',\", 5413: \"'the',\\n'mercilesse',\", 5414: \"'macdonwald',\", 5415: '.]]\\n\\n\\n\\n\\nnote\\nmost', 5416: 'methods\\napart', 5417: 'words(),', 5418: 'raw(),', 5419: 'richer\\nlinguistic', 5420: 'corpora,', 5421: 'part-of-speech\\ntags,', 5422: 'tags,', 5423: 'trees,', 5424: 'forth;', 5425: 'these\\nin', 5426: '.\\n\\n\\n\\n1', 5427: '.2\\xa0\\xa0\\xa0web', 5428: 'text\\nalthough', 5429: 'established\\nliterature', 5430: 'formal', 5431: \"nltk's\\nsmall\", 5432: 'firefox', 5433: 'forum,\\nconversations', 5434: 'overheard', 5435: 'york,', 5436: 'pirates', 5437: 'carribean,\\npersonal', 5438: 'advertisements,', 5439: 'reviews:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 5440: 'webtext\\n>>>', 5441: 'webtext', 5442: 'print(fileid,', 5443: '.raw(fileid)[:65],', 5444: \".')\\n\", 5445: '.\\nfirefox', 5446: '.txt', 5447: 'cookie', 5448: 'manager:', 5449: 'cookies', 5450: 'se', 5451: '.\\ngrail', 5452: 'scene', 5453: '[wind]', 5454: '[clop', 5455: 'clop', 5456: 'clop]', 5457: 'king', 5458: 'arthur:', 5459: 'whoa', 5460: 'there!', 5461: '.\\noverheard', 5462: 'white', 5463: 'guy:', 5464: 'evening?', 5465: 'asian', 5466: 'girl', 5467: '.\\npirates', 5468: 'carribean:', 5469: \"man's\", 5470: 'chest,', 5471: 'ted', 5472: 'elliott', 5473: 'terr', 5474: '.\\nsingles', 5475: 'sexy', 5476: 'male,', 5477: 'seeks', 5478: 'attrac', 5479: 'lady,', 5480: 'discreet', 5481: 'encoun', 5482: '.\\nwine', 5483: 'lovely', 5484: 'delicate,', 5485: 'fragrant', 5486: 'rhone', 5487: 'polished', 5488: 'leather', 5489: 'strawb', 5490: '.\\n\\n\\n\\nthere', 5491: 'instant', 5492: 'messaging', 5493: 'sessions,', 5494: 'collected\\nby', 5495: 'naval', 5496: 'detection', 5497: 'internet', 5498: 'predators', 5499: '10,000', 5500: 'posts,', 5501: 'anonymized', 5502: 'replacing', 5503: 'usernames', 5504: 'generic\\nnames', 5505: 'usernnn,', 5506: 'manually', 5507: 'edited', 5508: 'remove', 5509: 'hundred', 5510: 'posts\\ncollected', 5511: 'date,', 5512: 'age-specific', 5513: 'chatroom', 5514: '(teens,', 5515: '20s,', 5516: '30s,', 5517: '40s,', 5518: 'a\\ngeneric', 5519: 'adults', 5520: 'chatroom)', 5521: 'filename', 5522: 'chatroom,\\nand', 5523: 'posts;', 5524: '10-19-20s_706posts', 5525: '.xml', 5526: '706', 5527: 'gathered', 5528: '20s', 5529: 'room', 5530: '10/19/2006', 5531: 'nps_chat\\n>>>', 5532: 'nps_chat', 5533: \".posts('10-19-20s_706posts\", 5534: \".xml')\\n>>>\", 5535: \"chatroom[123]\\n['i',\", 5536: \"'do',\", 5537: \"n't,\", 5538: \"'want',\", 5539: \"'hot',\", 5540: \"'pics',\", 5541: \"'female',\", 5542: \"',',\\n'i',\", 5543: \"'look',\", 5544: \"'mirror',\", 5545: \".']\\n\\n\\n\\n\\n\\n1\", 5546: '.3\\xa0\\xa0\\xa0brown', 5547: 'corpus\\nthe', 5548: 'million-word', 5549: 'electronic\\ncorpus', 5550: '1961', 5551: '500', 5552: 'sources,', 5553: 'sources\\nhave', 5554: 'categorized', 5555: 'genre,', 5556: 'news,', 5557: 'editorial,', 5558: 'genre\\n(for', 5559: 'http://icame', 5560: '.uib', 5561: '.no/brown/bcm-los', 5562: '.html)', 5563: '.\\n\\n\\n\\n\\n\\n\\n\\n\\nid\\nfile\\ngenre\\ndescription\\n\\n\\n\\na16\\nca16\\nnews\\nchicago', 5564: 'tribune:', 5565: 'reportage\\n\\nb02\\ncb02\\neditorial\\nchristian', 5566: 'monitor:', 5567: 'editorials\\n\\nc17\\ncc17\\nreviews\\ntime', 5568: 'magazine:', 5569: 'reviews\\n\\nd12\\ncd12\\nreligion\\nunderwood:', 5570: 'probing', 5571: 'ethics', 5572: 'realtors\\n\\ne36\\nce36\\nhobbies\\nnorling:', 5573: 'renting', 5574: 'car', 5575: 'europe\\n\\nf25\\ncf25\\nlore\\nboroff:', 5576: 'jewish', 5577: 'teenage', 5578: 'culture\\n\\ng22\\ncg22\\nbelles_lettres\\nreiner:', 5579: 'coping', 5580: 'runaway', 5581: 'technology\\n\\nh15\\nch15\\ngovernment\\nus', 5582: 'office', 5583: 'civil', 5584: 'defence', 5585: 'mobilization:', 5586: 'family', 5587: 'fallout', 5588: 'shelter\\n\\nj17\\ncj19\\nlearned\\nmosteller:', 5589: 'statistical', 5590: 'applications\\n\\nk04\\nck04\\nfiction\\nw', 5591: '.b', 5592: 'du', 5593: 'bois:', 5594: 'worlds', 5595: 'color\\n\\nl13\\ncl13\\nmystery\\nhitchens:', 5596: 'footsteps', 5597: 'night\\n\\nm01\\ncm01\\nscience_fiction\\nheinlein:', 5598: 'stranger', 5599: 'strange', 5600: 'land\\n\\nn14\\ncn15\\nadventure\\nfield:', 5601: 'rattlesnake', 5602: 'ridge\\n\\np12\\ncp12\\nromance\\ncallaghan:', 5603: 'passion', 5604: 'rome\\n\\nr06\\ncr06\\nhumor\\nthurber:', 5605: 'future,', 5606: 'any,', 5607: 'comedy\\n\\n\\ntable', 5608: 'corpus\\n\\n\\nwe', 5609: '(where', 5610: 'sentence\\nis', 5611: 'words)', 5612: 'optionally', 5613: 'categories', 5614: 'read:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 5615: 'brown\\n>>>', 5616: \".categories()\\n['adventure',\", 5617: \"'belles_lettres',\", 5618: \"'editorial',\", 5619: \"'fiction',\", 5620: \"'government',\", 5621: \"'hobbies',\\n'humor',\", 5622: \"'learned',\", 5623: \"'lore',\", 5624: \"'mystery',\", 5625: \"'news',\", 5626: \"'religion',\", 5627: \"'reviews',\", 5628: \"'romance',\\n'science_fiction']\\n>>>\", 5629: \".words(categories='news')\\n['the',\", 5630: \"'fulton',\", 5631: \"'county',\", 5632: \"'grand',\", 5633: \"'jury',\", 5634: \".words(fileids=['cg22'])\\n['does',\", 5635: \"'our',\", 5636: \"'society',\", 5637: \"'have',\", 5638: \"'runaway',\", 5639: \".sents(categories=['news',\", 5640: \"'reviews'])\\n[['the',\", 5641: \"'county'\", 5642: \"['the',\", 5643: \"'further'\", 5644: '.]\\n\\n\\n\\nthe', 5645: 'convenient', 5646: 'resource', 5647: 'studying', 5648: 'between\\ngenres,', 5649: 'inquiry', 5650: 'stylistics', 5651: 'modal', 5652: 'step\\nis', 5653: 'to\\nimport', 5654: 'news_text', 5655: \".words(categories='news')\\n>>>\", 5656: '.freqdist(w', 5657: 'news_text)\\n>>>', 5658: 'modals', 5659: \"['can',\", 5660: \"'may',\", 5661: \"'might',\", 5662: \"'must',\", 5663: \"'will']\\n>>>\", 5664: 'm', 5665: 'modals:\\n', 5666: 'print(m', 5667: 'fdist[m],', 5668: \"')\\n\", 5669: '.\\ncan:', 5670: '94', 5671: 'could:', 5672: '87', 5673: 'may:', 5674: '93', 5675: 'might:', 5676: '38', 5677: 'must:', 5678: '53', 5679: 'will:', 5680: '389\\n\\n\\n\\n\\nnote\\nwe', 5681: 'to\\nput', 5682: '.\\n\\n\\nnote\\nyour', 5683: 'turn:\\nchoose', 5684: 'previous\\nexample', 5685: 'wh', 5686: 'what,\\nwhen,', 5687: 'where,', 5688: 'who,', 5689: '.\\n\\nnext,', 5690: \"use\\nnltk's\", 5691: 'are\\npresented', 5692: '2,\\nwhere', 5693: 'unpick', 5694: 'moment,\\nyou', 5695: 'ignore', 5696: 'concentrate', 5697: 'cfd', 5698: '.conditionalfreqdist(\\n', 5699: '(genre,', 5700: 'word)\\n', 5701: '.categories()\\n', 5702: '.words(categories=genre))\\n>>>', 5703: \"['news',\", 5704: \"'hobbies',\", 5705: \"'science_fiction',\", 5706: \"'romance',\", 5707: \"'humor']\\n>>>\", 5708: '.tabulate(conditions=genres,', 5709: 'samples=modals)\\n', 5710: 'will\\n', 5711: '86', 5712: '66', 5713: '389\\n', 5714: 'religion', 5715: '82', 5716: '59', 5717: '78', 5718: '54', 5719: '71\\n', 5720: 'hobbies', 5721: '268', 5722: '58', 5723: '131', 5724: '83', 5725: '264\\nscience_fiction', 5726: '49', 5727: '16\\n', 5728: '74', 5729: '193', 5730: '51', 5731: '45', 5732: '43\\n', 5733: 'humor', 5734: '13\\n\\n\\n\\nobserve', 5735: 'will,\\nwhile', 5736: '.\\nwould', 5737: 'predicted', 5738: 'this?', 5739: 'counts\\nmight', 5740: 'distinguish', 5741: 'chap-data-intensive', 5742: '.4\\xa0\\xa0\\xa0reuters', 5743: 'reuters', 5744: '10,788', 5745: 'documents', 5746: 'totaling', 5747: '.3', 5748: 'classified', 5749: '90', 5750: 'topics,', 5751: 'grouped\\ninto', 5752: 'training', 5753: 'test;', 5754: 'with\\nfileid', 5755: \"'test/14826'\", 5756: 'drawn', 5757: 'for\\ntraining', 5758: 'document,\\nas', 5759: 'reuters\\n>>>', 5760: \".fileids()\\n['test/14826',\", 5761: \"'test/14828',\", 5762: \"'test/14829',\", 5763: \"'test/14832',\", 5764: \".categories()\\n['acq',\", 5765: \"'alum',\", 5766: \"'barley',\", 5767: \"'bop',\", 5768: \"'carcass',\", 5769: \"'castor-oil',\", 5770: \"'cocoa',\\n'coconut',\", 5771: \"'coconut-oil',\", 5772: \"'coffee',\", 5773: \"'copper',\", 5774: \"'copra-cake',\", 5775: \"'corn',\\n'cotton',\", 5776: \"'cotton-oil',\", 5777: \"'cpi',\", 5778: \"'cpu',\", 5779: \"'crude',\", 5780: \"'dfl',\", 5781: \"'dlr',\", 5782: '.]\\n\\n\\n\\nunlike', 5783: 'overlap', 5784: 'with\\neach', 5785: 'other,', 5786: 'story', 5787: 'multiple', 5788: 'documents,', 5789: 'the\\ndocuments', 5790: 'convenience,', 5791: 'the\\ncorpus', 5792: 'accept', 5793: 'fileids', 5794: \".categories('training/9865')\\n['barley',\", 5795: \"'corn',\", 5796: \"'grain',\", 5797: \"'wheat']\\n>>>\", 5798: \".categories(['training/9865',\", 5799: \"'training/9880'])\\n['barley',\", 5800: \"'money-fx',\", 5801: \".fileids('barley')\\n['test/15618',\", 5802: \"'test/15649',\", 5803: \"'test/15676',\", 5804: \"'test/15728',\", 5805: \"'test/15871',\", 5806: \".fileids(['barley',\", 5807: \"'corn'])\\n['test/14832',\", 5808: \"'test/14858',\", 5809: \"'test/15033',\", 5810: \"'test/15043',\", 5811: \"'test/15106',\\n'test/15287',\", 5812: \"'test/15341',\", 5813: \"'test/15618',\", 5814: \"'test/15648',\", 5815: '.]\\n\\n\\n\\nsimilarly,', 5816: 'of\\nfiles', 5817: 'handful', 5818: 'the\\ntitles,', 5819: 'convention', 5820: 'upper', 5821: \".words('training/9865')[:14]\\n['french',\", 5822: \"'free',\", 5823: \"'market',\", 5824: \"'cereal',\", 5825: \"'export',\", 5826: \"'bids',\\n'detailed',\", 5827: \"'french',\", 5828: \"'operators',\", 5829: \"'requested',\", 5830: \"'licences',\", 5831: \"'export']\\n>>>\", 5832: \".words(['training/9865',\", 5833: \"'training/9880'])\\n['french',\", 5834: \".words(categories='barley')\\n['french',\", 5835: \".words(categories=['barley',\", 5836: \"'corn'])\\n['thai',\", 5837: \"'trade',\", 5838: \"'deficit',\", 5839: \"'widens',\", 5840: \"'first',\", 5841: '.]\\n\\n\\n\\n\\n\\n1', 5842: '.5\\xa0\\xa0\\xa0inaugural', 5843: 'corpus\\n\\nin', 5844: 'looked', 5845: 'at\\nthe', 5846: 'corpus,\\nbut', 5847: 'fig-inaugural\\nused', 5848: 'offset', 5849: 'axes;', 5850: 'dimension:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 5851: 'inaugural\\n>>>', 5852: \".fileids()\\n['1789-washington\", 5853: \"'1793-washington\", 5854: \"'1797-adams\", 5855: '[fileid[:4]', 5856: \".fileids()]\\n['1789',\", 5857: \"'1793',\", 5858: \"'1797',\", 5859: \"'1801',\", 5860: \"'1805',\", 5861: \"'1809',\", 5862: \"'1813',\", 5863: \"'1817',\", 5864: \"'1821',\", 5865: '.]\\n\\n\\n\\nnotice', 5866: 'year', 5867: 'year\\nout', 5868: 'filename,', 5869: 'extracted', 5870: 'fileid[:4]', 5871: 'america', 5872: 'citizen', 5873: 'code\\nconverts', 5874: 'corpus\\nto', 5875: ',\\nthen', 5876: 'targets\\namerica', 5877: 'startswith()', 5878: '.\\nthus', 5879: \"american's\", 5880: 'citizens', 5881: 'in\\n2;', 5882: '(target,', 5883: 'fileid[:4])\\n', 5884: '.fileids()\\n', 5885: '.words(fileid)\\n', 5886: \"['america',\", 5887: \"'citizen']\\n\", 5888: '.startswith(target))', 5889: '.plot()\\n\\n\\n\\n\\n\\nfigure', 5890: 'distribution:', 5891: 'address\\ncorpus', 5892: 'counted;', 5893: 'counts\\nare', 5894: 'kept', 5895: 'address;', 5896: 'plotted', 5897: 'trends', 5898: 'observed;', 5899: 'normalized', 5900: '.6\\xa0\\xa0\\xa0annotated', 5901: 'corpora\\nmany', 5902: 'annotations,', 5903: 'pos', 5904: 'tags,\\nnamed', 5905: 'entities,', 5906: 'roles,', 5907: 'provides\\nconvenient', 5908: 'corpora\\nand', 5909: 'samples,', 5910: 'about\\ndownloading', 5911: 'them,', 5912: '.org/data', 5913: 'corpora,\\nplease', 5914: 'howto', 5915: '.org/howto', 5916: '.\\n\\n\\n\\n\\n\\n\\n\\ncorpus\\ncompiler\\ncontents\\n\\n\\n\\nbrown', 5917: 'corpus\\nfrancis,', 5918: 'kucera\\n15', 5919: '.15m', 5920: 'tagged,', 5921: 'categorized\\n\\ncess', 5922: 'treebanks\\nclic-ub\\n1m', 5923: 'tagged', 5924: '(catalan,', 5925: 'spanish)\\n\\nchat-80', 5926: 'files\\npereira', 5927: 'warren\\nworld', 5928: 'geographic', 5929: 'database\\n\\ncmu', 5930: 'pronouncing', 5931: 'dictionary\\ncmu\\n127k', 5932: 'entries\\n\\nconll', 5933: '2000', 5934: 'chunking', 5935: 'data\\nconll\\n270k', 5936: 'chunked\\n\\nconll', 5937: 'entity\\nconll\\n700k', 5938: 'pos-', 5939: 'named-entity-tagged', 5940: '(dutch,', 5941: 'spanish)\\n\\nconll', 5942: '2007', 5943: 'dependency', 5944: 'treebanks', 5945: '(sel)\\nconll\\n150k', 5946: '(basque,', 5947: 'catalan)\\n\\ndependency', 5948: 'treebank\\nnarad\\ndependency', 5949: 'penn', 5950: 'treebank', 5951: 'sample\\n\\nframenet\\nfillmore,', 5952: 'baker', 5953: 'al\\n10k', 5954: 'senses,', 5955: '170k', 5956: 'sentences\\n\\nfloresta', 5957: 'treebank\\ndiana', 5958: 'santos', 5959: 'al\\n9k', 5960: '(portuguese)\\n\\ngazetteer', 5961: 'lists\\nvarious\\nlists', 5962: 'cities', 5963: 'countries\\n\\ngenesis', 5964: 'corpus\\nmisc', 5965: 'sources\\n6', 5966: '200k', 5967: '6', 5968: 'languages\\n\\ngutenberg', 5969: '(selections)\\nhart,', 5970: 'newby,', 5971: 'al\\n18', 5972: '2m', 5973: 'words\\n\\ninaugural', 5974: 'corpus\\ncspan\\nus', 5975: '(1789-present)\\n\\nindian', 5976: 'pos-tagged', 5977: 'corpus\\nkumaran', 5978: 'al\\n60k', 5979: '(bangla,', 5980: 'hindi,', 5981: 'marathi,', 5982: 'telugu)\\n\\nmacmorpho', 5983: 'corpus\\nnilc,', 5984: 'usp,', 5985: 'brazil\\n1m', 5986: '(brazilian', 5987: 'portuguese)\\n\\nmovie', 5988: 'reviews\\npang,', 5989: 'lee\\n2k', 5990: 'polarity', 5991: 'classification\\n\\nnames', 5992: 'corpus\\nkantrowitz,', 5993: 'ross\\n8k', 5994: 'male', 5995: 'female', 5996: 'names\\n\\nnist', 5997: '1999', 5998: 'info', 5999: 'extr', 6000: '(selections)\\ngarofolo\\n63k', 6001: 'newswire', 6002: 'named-entity', 6003: 'sgml', 6004: 'markup\\n\\nnombank\\nmeyers\\n115k', 6005: 'propositions,', 6006: '1400', 6007: 'frames\\n\\nnps', 6008: 'corpus\\nforsyth,', 6009: 'martell\\n10k', 6010: 'im', 6011: 'dialogue-act', 6012: 'tagged\\n\\nopen', 6013: 'wordnet\\nbond', 6014: 'al\\n15', 6015: 'aligned', 6016: 'wordnet\\n\\npp', 6017: 'attachment', 6018: 'corpus\\nratnaparkhi\\n28k', 6019: 'prepositional', 6020: 'modifiers\\n\\nproposition', 6021: 'bank\\npalmer\\n113k', 6022: '3300', 6023: 'frames\\n\\nquestion', 6024: 'classification\\nli,', 6025: 'roth\\n6k', 6026: 'questions,', 6027: 'categorized\\n\\nreuters', 6028: 'corpus\\nreuters\\n1', 6029: '.3m', 6030: '10k', 6031: \"categorized\\n\\nroget's\", 6032: 'thesaurus\\nproject', 6033: 'gutenberg\\n200k', 6034: 'text\\n\\nrte', 6035: 'entailment\\ndagan', 6036: 'al\\n8k', 6037: 'categorized\\n\\nsemcor\\nrus,', 6038: 'mihalcea\\n880k', 6039: 'part-of-speech', 6040: 'tagged\\n\\nsenseval', 6041: 'corpus\\npedersen\\n600k', 6042: 'tagged\\n\\nsentiwordnet\\nesuli,', 6043: 'sebastiani\\nsentiment', 6044: '145k', 6045: 'synonym', 6046: 'sets\\n\\nshakespeare', 6047: '(selections)\\nbosak\\n8', 6048: 'xml', 6049: 'format\\n\\nstate', 6050: 'union', 6051: 'corpus\\ncspan\\n485k', 6052: 'text\\n\\nstopwords', 6053: 'corpus\\nporter', 6054: 'al\\n2,400', 6055: 'stopwords', 6056: 'languages\\n\\nswadesh', 6057: 'corpus\\nwiktionary\\ncomparative', 6058: 'wordlists', 6059: '24', 6060: 'languages\\n\\nswitchboard', 6061: '(selections)\\nldc\\n36', 6062: 'phonecalls,', 6063: 'transcribed,', 6064: 'parsed\\n\\nuniv', 6065: 'decl', 6066: 'rights\\nunited', 6067: 'nations\\n480k', 6068: '300+', 6069: 'languages\\n\\npenn', 6070: '(selections)\\nldc\\n40k', 6071: 'parsed\\n\\ntimit', 6072: '(selections)\\nnist/ldc\\naudio', 6073: 'transcripts', 6074: 'speakers\\n\\nverbnet', 6075: '.1\\npalmer', 6076: 'al\\n5k', 6077: 'verbs,', 6078: 'hierarchically', 6079: 'organized,', 6080: 'wordnet\\n\\nwordlist', 6081: 'corpus\\nopenoffice', 6082: '.org', 6083: 'al\\n960k', 6084: '20k', 6085: 'affixes', 6086: 'languages\\n\\nwordnet', 6087: '(english)\\nmiller,', 6088: 'fellbaum\\n145k', 6089: 'sets\\n\\n\\ntable', 6090: 'nltk:', 6091: 'downloading\\nand', 6092: '.\\n\\n\\n\\n\\n1', 6093: '.7\\xa0\\xa0\\xa0corpora', 6094: 'languages\\nnltk', 6095: 'cases\\nyou', 6096: 'encodings', 6097: 'python\\nbefore', 6098: '.3)', 6099: '.cess_esp', 6100: \".words()\\n['el',\", 6101: \"'grupo',\", 6102: \"'estatal',\", 6103: \"'electricit\\\\xe9_de_france',\", 6104: '.floresta', 6105: \".words()\\n['um',\", 6106: \"'revivalismo',\", 6107: \"'refrescante',\", 6108: \"'o',\", 6109: \"'7_e_meio',\", 6110: '.indian', 6111: \".words('hindi\", 6112: \".pos')\\n['पूर्ण',\", 6113: \"'प्रतिबंध',\", 6114: \"'हटाओ',\", 6115: \"'इराक',\", 6116: \"'संयुक्त',\", 6117: '.udhr', 6118: \".fileids()\\n['abkhaz-cyrillic+abkh',\", 6119: \"'abkhaz-utf8',\", 6120: \"'achehnese-latin1',\", 6121: \"'achuar-shiwiar-latin1',\\n'adja-utf8',\", 6122: \"'afaan_oromo_oromiffa-latin1',\", 6123: \"'afrikaans-latin1',\", 6124: \"'aguaruna-latin1',\\n'akuapem_twi-utf8',\", 6125: \"'albanian_shqip-latin1',\", 6126: \"'amahuaca',\", 6127: \"'amahuaca-latin1',\", 6128: \".words('javanese-latin1')[11:]\\n['saben',\", 6129: \"'umat',\", 6130: \"'manungsa',\", 6131: \"'lair',\", 6132: \"'kanthi',\", 6133: \"'hak',\", 6134: 'udhr,', 6135: 'universal', 6136: 'declaration', 6137: 'rights\\nin', 6138: '300', 6139: 'include\\ninformation', 6140: 'encoding', 6141: 'file,\\nsuch', 6142: 'utf8', 6143: 'latin1', 6144: 'lengths\\nfor', 6145: 'udhr', 6146: '(run', 6147: 'color', 6148: 'plot)', 6149: 'boolean', 6150: 'udhr\\n>>>', 6151: \"['chickasaw',\", 6152: \"'english',\", 6153: \"'german_deutsch',\\n\", 6154: \"'greenlandic_inuktikut',\", 6155: \"'hungarian_magyar',\", 6156: \"'ibibio_efik']\\n>>>\", 6157: '(lang,', 6158: 'len(word))\\n', 6159: 'languages\\n', 6160: '.words(lang', 6161: \"'-latin1'))\\n>>>\", 6162: '.plot(cumulative=true)\\n\\n\\n\\n\\n\\nfigure', 6163: 'distributions:\\nsix', 6164: 'translations', 6165: 'rights', 6166: 'processed;\\nthis', 6167: 'fewer', 6168: 'about\\n80%', 6169: 'ibibio', 6170: '60%', 6171: '25%', 6172: 'inuktitut', 6173: '.\\n\\n\\n\\nnote\\nyour', 6174: 'variable\\nraw_text', 6175: '.raw(language-latin1)', 6176: 'frequency\\ndistribution', 6177: '.freqdist(raw_text)', 6178: '.plot()', 6179: '.\\n\\nunfortunately,', 6180: 'is\\ninsufficient', 6181: 'industrial', 6182: 'developing', 6183: 'individual\\nefforts', 6184: 'piecemeal', 6185: 'no\\nestablished', 6186: 'endangered', 6187: '7\\nfor', 6188: 'suggestions', 6189: '.)\\n\\n\\n1', 6190: '.8\\xa0\\xa0\\xa0text', 6191: 'structure\\nwe', 6192: 'far;', 6193: 'are\\nsummarized', 6194: 'lacks', 6195: 'structure:', 6196: '.\\noften,', 6197: 'correspond', 6198: 'source,', 6199: 'author,', 6200: 'etc', 6201: '.\\nsometimes', 6202: 'overlap,', 6203: 'notably', 6204: 'topical', 6205: 'be\\nrelevant', 6206: 'occasionally,', 6207: 'temporal', 6208: 'structure,\\nnews', 6209: '.3:', 6210: 'corpora:', 6211: 'collection\\nof', 6212: 'isolated', 6213: 'organization;', 6214: 'structured\\ninto', 6215: '(brown', 6216: 'corpus);', 6217: 'categorizations', 6218: 'as\\ntopic', 6219: '(reuters', 6220: 'time\\n(inaugural', 6221: 'corpus)', 6222: '.\\n\\n\\n\\n\\n\\n\\n\\nexample\\ndescription\\n\\n\\n\\nfileids()\\nthe', 6223: 'corpus\\n\\nfileids([categories])\\nthe', 6224: 'categories\\n\\ncategories()\\nthe', 6225: 'corpus\\n\\ncategories([fileids])\\nthe', 6226: 'files\\n\\nraw()\\nthe', 6227: 'corpus\\n\\nraw(fileids=[f1,f2,f3])\\nthe', 6228: 'files\\n\\nraw(categories=[c1,c2])\\nthe', 6229: 'categories\\n\\nwords()\\nthe', 6230: 'corpus\\n\\nwords(fileids=[f1,f2,f3])\\nthe', 6231: 'fileids\\n\\nwords(categories=[c1,c2])\\nthe', 6232: 'categories\\n\\nsents()\\nthe', 6233: 'corpus\\n\\nsents(fileids=[f1,f2,f3])\\nthe', 6234: 'fileids\\n\\nsents(categories=[c1,c2])\\nthe', 6235: 'categories\\n\\nabspath(fileid)\\nthe', 6236: 'disk\\n\\nencoding(fileid)\\nthe', 6237: '(if', 6238: 'known)\\n\\nopen(fileid)\\nopen', 6239: 'stream', 6240: 'file\\n\\nroot\\nif', 6241: 'path', 6242: 'root', 6243: 'locally', 6244: 'corpus\\n\\nreadme()\\nthe', 6245: 'readme', 6246: 'corpus\\n\\n\\ntable', 6247: 'using\\nhelp(nltk', 6248: '.reader)', 6249: \".\\n\\n\\nnltk's\", 6250: 'functionality\\nprovided', 6251: 'illustrate', 6252: 'some\\nof', 6253: 'below:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 6254: '.raw(burgess-busterbrown', 6255: '.txt)\\n>>>', 6256: \"raw[1:20]\\n'the\", 6257: 'adventures', 6258: \"b'\\n>>>\", 6259: '.words(burgess-busterbrown', 6260: \"words[1:20]\\n['the',\", 6261: \"'adventures',\", 6262: \"'buster',\", 6263: \"'bear',\", 6264: \"'thornton',\", 6265: \"'w',\", 6266: \".',\\n'burgess',\", 6267: \"'1920',\", 6268: \"'i',\", 6269: \"'goes',\", 6270: \"'fishing',\", 6271: \"'buster',\\n'bear']\\n>>>\", 6272: 'sents', 6273: '.sents(burgess-busterbrown', 6274: \"sents[1:20]\\n[['i'],\", 6275: \"['buster',\", 6276: \"'fishing'],\", 6277: \"'yawned',\", 6278: \"'as',\\n'he',\", 6279: \"'lay',\", 6280: \"'on',\", 6281: \"'his',\", 6282: \"'comfortable',\", 6283: \"'bed',\", 6284: \"'leaves',\", 6285: \"'watched',\\n'the',\", 6286: \"'early',\", 6287: \"'morning',\", 6288: \"'sunbeams',\", 6289: \"'creeping',\", 6290: \"'through',\", 6291: '.9\\xa0\\xa0\\xa0loading', 6292: 'corpus\\nif', 6293: 'methods,', 6294: \"nltk's\\nplaintextcorpusreader\", 6295: 'system;', 6296: 'directory\\n/usr/share/dict', 6297: 'whatever', 6298: 'location,', 6299: 'of\\ncorpus_root', 6300: 'plaintextcorpusreader', 6301: 'initializer', 6302: '\\ncan', 6303: 'fileids,', 6304: \"['a\", 6305: \"'test/b\", 6306: \".txt'],\\nor\", 6307: 'matches', 6308: \"'[abc]/\", 6309: '.*\\\\', 6310: \".txt'\\n(see\", 6311: 'information\\nabout', 6312: 'regular', 6313: 'expressions)', 6314: 'plaintextcorpusreader\\n>>>', 6315: 'corpus_root', 6316: \"'/usr/share/dict'\", 6317: 'plaintextcorpusreader(corpus_root,', 6318: \".*')\", 6319: \".fileids()\\n['readme',\", 6320: \"'connectives',\", 6321: \"'propernames',\", 6322: \"'web2',\", 6323: \"'web2a',\", 6324: \"'words']\\n>>>\", 6325: \".words('connectives')\\n['the',\", 6326: \"'that',\", 6327: '.]\\n\\n\\n\\nas', 6328: 'local', 6329: '(release', 6330: '3),\\nin', 6331: 'c:\\\\corpora', 6332: 'bracketparsecorpusreader', 6333: 'this\\ncorpus', 6334: 'street\\njournal', 6335: 'component', 6336: 'file_pattern\\nthat', 6337: 'contained', 6338: 'subfolders', 6339: 'slashes)', 6340: 'bracketparsecorpusreader\\n>>>', 6341: 'rc:\\\\corpora\\\\penntreebank\\\\parsed\\\\mrg\\\\wsj', 6342: 'file_pattern', 6343: 'r', 6344: '.*/wsj_', 6345: '.mrg', 6346: 'ptb', 6347: 'bracketparsecorpusreader(corpus_root,', 6348: 'file_pattern)\\n>>>', 6349: \".fileids()\\n['00/wsj_0001\", 6350: \".mrg',\", 6351: \"'00/wsj_0002\", 6352: \"'00/wsj_0003\", 6353: \"'00/wsj_0004\", 6354: 'len(ptb', 6355: '.sents())\\n49208\\n>>>', 6356: \".sents(fileids='20/wsj_2013\", 6357: \".mrg')[19]\\n['the',\", 6358: \"'55-year-old',\", 6359: \"'mr\", 6360: \"'noriega',\", 6361: \"'smooth',\", 6362: \"'the',\\n'shah',\", 6363: \"'iran',\", 6364: \"'well-born',\", 6365: \"'nicaragua',\", 6366: \"'s,\", 6367: \"'anastasio',\\n'somoza',\", 6368: \"'imperial',\", 6369: \"'ferdinand',\", 6370: \"'marcos',\", 6371: \"'philippines',\\n'or',\", 6372: \"'bloody',\", 6373: \"'haiti',\", 6374: \"'baby',\", 6375: \"doc',\", 6376: \"'duvalier',\", 6377: \".']\\n\\n\\n\\n\\n\\n\\n2\\xa0\\xa0\\xa0conditional\", 6378: 'distributions\\nwe', 6379: 'mylist', 6380: 'items,\\nfreqdist(mylist)', 6381: 'each\\nitem', 6382: 'generalize', 6383: 'several\\ncategories,', 6384: 'topic,', 6385: 'etc,', 6386: 'maintain', 6387: 'separate\\nfrequency', 6388: 'category', 6389: 'to\\nstudy', 6390: 'previous\\nsection', 6391: 'achieved', 6392: 'conditionalfreqdist', 6393: 'data\\ntype', 6394: 'of\\nfrequency', 6395: 'the\\ncondition', 6396: '.1\\ndepicts', 6397: 'just\\ntwo', 6398: 'conditions,', 6399: 'distribution)\\n\\n\\n2', 6400: '.1\\xa0\\xa0\\xa0conditions', 6401: 'events\\na', 6402: 'events,\\nsuch', 6403: 'conditional\\nfrequency', 6404: '.\\nso', 6405: ',\\nwe', 6406: 'pairs', 6407: \"[('news',\", 6408: \"'the'),\", 6409: \"('news',\", 6410: \"'fulton'),\", 6411: \"'county'),\", 6412: '\\n\\n\\n\\neach', 6413: '(condition,', 6414: 'event)', 6415: 'the\\nentire', 6416: 'genre),\\nand', 6417: '1,161,192', 6418: 'events', 6419: 'word)', 6420: '.\\n\\n\\n2', 6421: '.2\\xa0\\xa0\\xa0counting', 6422: 'genre\\nin', 6423: 'the\\nbrown', 6424: 'whereas\\nfreqdist()', 6425: 'conditionalfreqdist()\\ntakes', 6426: \".words(categories=genre))\\n\\n\\n\\nlet's\", 6427: 'down,', 6428: ',\\nproducing', 6429: 'genre_word', 6430: '[(genre,', 6431: \"'romance']\", 6432: '.words(categories=genre)]', 6433: 'len(genre_word)\\n170576\\n\\n\\n\\nso,', 6434: 'below,\\npairs', 6435: \"form\\n('news',\", 6436: \"form\\n('romance',\", 6437: \"genre_word[:4]\\n[('news',\", 6438: \"'grand')]\", 6439: '#', 6440: '[_start-genre]\\n>>>', 6441: \"genre_word[-4:]\\n[('romance',\", 6442: \"'afraid'),\", 6443: \"('romance',\", 6444: \"'not'),\", 6445: \"''),\", 6446: \".')]\", 6447: '[_end-genre]\\n\\n\\n\\nwe', 6448: 'conditionalfreqdist,', 6449: 'and\\nsave', 6450: 'usual,', 6451: '.conditionalfreqdist(genre_word)\\n>>>', 6452: '\\n<conditionalfreqdist', 6453: 'conditions>\\n>>>', 6454: \".conditions()\\n['news',\", 6455: \"[_conditions-cfd]\\n\\n\\n\\nlet's\", 6456: 'satisfy', 6457: 'just\\na', 6458: 'distribution:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 6459: \"print(cfd['news'])\\n<freqdist\", 6460: '14394', 6461: '100554', 6462: \"print(cfd['romance'])\\n<freqdist\", 6463: '8452', 6464: '70022', 6465: \"cfd['romance']\", 6466: \".most_common(20)\\n[(',',\", 6467: '3899),', 6468: '3736),', 6469: '2758),', 6470: '1776),', 6471: \"1502),\\n('a',\", 6472: '1335),', 6473: '1186),', 6474: \"('``',\", 6475: '1045),', 6476: '1044),', 6477: \"993),\\n('i',\", 6478: '951),', 6479: '875),', 6480: '702),', 6481: '692),', 6482: \"690),\\n('her',\", 6483: '651),', 6484: '583),', 6485: '573),', 6486: '559),', 6487: \"('she',\", 6488: '496)]\\n>>>', 6489: \"cfd['romance']['could']\\n193\\n\\n\\n\\n\\n\\n2\", 6490: '.3\\xa0\\xa0\\xa0plotting', 6491: 'distributions\\napart', 6492: 'initialize,\\na', 6493: 'tabulation', 6494: 'distribution\\nreproduced', 6495: ',\\nand', 6496: 'occured', 6497: 'exploits', 6498: 'speech,', 6499: '1865-lincoln', 6500: '.txt\\ncontains', 6501: 'generates', 6502: \"('america',\", 6503: \"'1865')\", 6504: 'for\\nevery', 6505: 'instance', 6506: 'lowercased', 6507: 'america\\n—', 6508: 'americans', 6509: 'fileid[:4])', 6510: \"'citizen']\", 6511: '.startswith(target))\\n\\n\\n\\nthe', 6512: 'distribution,\\nreproduced', 6513: 'language\\nand', 6514: 'derived', 6515: \"'-latin1'\", 6516: 'encoding)', 6517: 'len(word))', 6518: \"'-latin1'))\\n\\n\\n\\nin\", 6519: 'plot()', 6520: 'tabulate()', 6521: 'can\\noptionally', 6522: 'conditions=', 6523: 'limit', 6524: 'the\\nsamples', 6525: 'samples=', 6526: 'to\\nload', 6527: 'distribution,', 6528: 'then\\nto', 6529: 'selected', 6530: 'also\\ngives', 6531: 'tabulate', 6532: 'two\\nlanguages,', 6533: '10', 6534: 'interpret', 6535: 'cell', 6536: '1,638', 6537: 'the\\nenglish', 6538: \".tabulate(conditions=['english',\", 6539: \"'german_deutsch'],\\n\", 6540: 'samples=range(10),', 6541: 'cumulative=true)\\n', 6542: '0', 6543: '9\\n', 6544: '185', 6545: '525', 6546: '883', 6547: '997', 6548: '1166', 6549: '1283', 6550: '1440', 6551: '1558', 6552: '1638\\ngerman_deutsch', 6553: '171', 6554: '263', 6555: '614', 6556: '717', 6557: '894', 6558: '1013', 6559: '1110', 6560: '1213', 6561: '1275\\n\\n\\n\\n\\nnote\\nyour', 6562: 'turn:\\nworking', 6563: 'corpus,\\nfind', 6564: 'week', 6565: 'newsworthy,', 6566: 'romantic', 6567: '.\\ndefine', 6568: 'week,', 6569: \".\\n['monday',\", 6570: 'using\\ncfd', 6571: '.tabulate(samples=days)', 6572: \"parameter:\\nsamples=['monday',\", 6573: 'noticed', 6574: 'multi-line', 6575: 'been\\nusing', 6576: 'list\\ncomprehensions,', 6577: 'general,\\nwhen', 6578: 'function,\\nlike', 6579: 'set([w', 6580: 't]),', 6581: 'permitted', 6582: 'omit\\nthe', 6583: 'write:', 6584: 't)', 6585: '.\\n(see', 6586: 'generator', 6587: '.2\\nfor', 6588: '.)\\n\\n\\n2', 6589: '.4\\xa0\\xa0\\xa0generating', 6590: 'bigrams\\nwe', 6591: 'of\\nbigrams', 6592: '(word', 6593: 'pairs)', 6594: 'introducted', 6595: 'in\\n3', 6596: '.)\\nthe', 6597: 'bigrams()', 6598: 'of\\nwords', 6599: 'builds', 6600: 'consecutive', 6601: 'that,', 6602: 'cryptic\\ngenerator', 6603: 'function:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 6604: \"['in',\", 6605: \"'heaven',\\n\", 6606: 'list(nltk', 6607: \".bigrams(sent))\\n[('in',\", 6608: \"'beginning'),\", 6609: \"('beginning',\", 6610: \"'god'),\", 6611: \"('god',\", 6612: \"'created'),\\n('created',\", 6613: \"'heaven'),\", 6614: \"('heaven',\", 6615: \"'and'),\", 6616: \"'the'),\\n('the',\", 6617: \"'earth'),\", 6618: \"('earth',\", 6619: \".')]\\n\\n\\n\\nin\", 6620: '.2,', 6621: 'condition,', 6622: 'one\\nwe', 6623: 'following\\nwords', 6624: 'generate_model()', 6625: 'to\\ngenerate', 6626: '(such', 6627: \"as\\n'living')\", 6628: 'loop,', 6629: 'we\\nprint', 6630: 'reset', 6631: 'likely', 6632: 'max());', 6633: 'next\\ntime', 6634: 'inspecting', 6635: 'approach', 6636: 'text\\ngeneration', 6637: 'tends', 6638: 'stuck', 6639: 'loops;', 6640: 'to\\nrandomly', 6641: 'among', 6642: '.\\n\\n\\n\\n\\n\\xa0\\n\\ndef', 6643: 'generate_model(cfdist,', 6644: 'num=15):\\n', 6645: 'range(num):\\n', 6646: 'cfdist[word]', 6647: '.max()\\n\\ntext', 6648: '.genesis', 6649: \".words('english-kjv\", 6650: \".txt')\\nbigrams\", 6651: '.bigrams(text)\\ncfd', 6652: '.conditionalfreqdist(bigrams)', 6653: '\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 6654: \"cfd['living']\\nfreqdist({'creature':\", 6655: \"'thing':\", 6656: \"'substance':\", 6657: \"',':\", 6658: \".':\", 6659: \"'soul':\", 6660: '1})\\n>>>', 6661: 'generate_model(cfd,', 6662: \"'living')\\nliving\", 6663: 'creature', 6664: 'said', 6665: 'land\\n\\n\\nexample', 6666: '(code_random_text', 6667: '.py):', 6668: 'figure', 6669: 'obtains', 6670: 'bigrams\\nfrom', 6671: 'genesis,', 6672: 'record', 6673: 'which\\nwords', 6674: 'word;', 6675: 'after\\nthe', 6676: 'living,', 6677: 'is\\ncreature;', 6678: 'this\\ndata,', 6679: 'seed', 6680: '.\\n\\nconditional', 6681: '.\\ntheir', 6682: 'commonly-used', 6683: 'summarized', 6684: '.\\n\\n\\n\\n\\n\\n\\nexample\\ndescription\\n\\n\\n\\ncfdist', 6685: 'conditionalfreqdist(pairs)\\ncreate', 6686: 'pairs\\n\\ncfdist', 6687: '.conditions()\\nthe', 6688: 'conditions\\n\\ncfdist[condition]\\nthe', 6689: 'condition\\n\\ncfdist[condition][sample]\\nfrequency', 6690: 'condition\\n\\ncfdist', 6691: 'distribution\\n\\ncfdist', 6692: '.tabulate(samples,', 6693: 'conditions)\\ntabulation', 6694: 'conditions\\n\\ncfdist', 6695: '.plot(samples,', 6696: 'conditions)\\ngraphical', 6697: 'conditions\\n\\ncfdist1', 6698: 'cfdist2\\ntest', 6699: 'cfdist1', 6700: 'cfdist2\\n\\n\\ntable', 6701: 'distributions:', 6702: 'defining,\\naccessing,', 6703: 'counters', 6704: '.\\n\\n\\n\\n\\n\\n3\\xa0\\xa0\\xa0more', 6705: 'reusing', 6706: 'code\\nby', 6707: 'retyped', 6708: 'lot', 6709: 'mess', 6710: 'arrow', 6711: 'keys', 6712: 'so\\nfar', 6713: 'reuse', 6714: 'code:', 6715: 'editors', 6716: '.\\n\\n3', 6717: '.1\\xa0\\xa0\\xa0creating', 6718: 'editor\\nthe', 6719: 'type\\nthem', 6720: 'often,', 6721: 'compose', 6722: 'editor,\\nthen', 6723: 'idle,', 6724: 'do\\nthis', 6725: 'menu', 6726: 'window', 6727: 'and\\nenter', 6728: 'one-line', 6729: \"program:\\n\\nprint('monty\", 6730: \"python')\\n\\nsave\", 6731: '.py,', 6732: 'then\\ngo', 6733: 'menu,', 6734: \".\\n(we'll\", 6735: 'shortly', 6736: 'idle', 6737: 'this:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 6738: '================================', 6739: 'restart', 6740: '================================\\n>>>\\nmonty', 6741: 'python\\n>>>\\n\\n\\n\\nyou', 6742: '.\\nfrom', 6743: 'on,', 6744: 'a\\ntext', 6745: 'ideas\\nusing', 6746: 'revising', 6747: 'ready,', 6748: 'paste', 6749: 'code\\n(minus', 6750: 'prompts)', 6751: 'editor,\\ncontinue', 6752: 'program\\nin', 6753: '.\\ngive', 6754: 'descriptive', 6755: 'separating\\nwords', 6756: 'extension,', 6757: 'monty_python', 6758: '.\\n\\nnote\\nimportant:\\nour', 6759: 'inline', 6760: 'prompts\\nas', 6761: 'interacting', 6762: 'complicated,\\nyou', 6763: 'editor,', 6764: 'prompts,', 6765: 'them\\nfrom', 6766: 'book,\\nwe', 6767: 'prompts', 6768: 'rather\\nthan', 6769: 'couple', 6770: 'prompt;\\nthis', 6771: 'downloadable\\nfrom', 6772: '.\\n\\n\\n\\n3', 6773: '.2\\xa0\\xa0\\xa0functions\\nsuppose', 6774: 'forms\\nof', 6775: 'out\\nthe', 6776: 'plural', 6777: 'singular', 6778: 'this\\nwork', 6779: 'places,', 6780: 'again\\nwhen', 6781: '.\\nrather', 6782: 'over,', 6783: 'more\\nefficient', 6784: 'reliable', 6785: 'localize', 6786: 'well-defined\\ntask,', 6787: 'inputs,', 6788: 'parameters,\\nand', 6789: 'result,', 6790: '1\\n(including', 6791: 'needed', 6792: 'behave', 6793: 'expected):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 6794: 'division\\n>>>', 6795: 'lexical_diversity(text):\\n', 6796: 'len(set(text))\\n\\n\\n\\nwe', 6797: 'is\\nproduced', 6798: 'example,\\nall', 6799: \".\\nhere's\", 6800: 'equivalent', 6801: 'work\\nusing', 6802: 'name\\nfrom', 6803: 'my_text_data', 6804: 'arbitrary', 6805: 'choice:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 6806: 'lexical_diversity(my_text_data):\\n', 6807: 'word_count', 6808: 'len(my_text_data)\\n', 6809: 'len(set(my_text_data))\\n', 6810: 'diversity_score', 6811: 'word_count\\n', 6812: 'diversity_score\\n\\n\\n\\nnotice', 6813: 'lexical_diversity', 6814: 'just\\ndefining', 6815: \"won't\", 6816: 'output!\\nfunctions', 6817: 'invoked):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 6818: 'genesis\\n>>>', 6819: 'kjv', 6820: 'lexical_diversity(kjv)\\n0', 6821: \".06230453042623537\\n\\n\\n\\nlet's\", 6822: 'scenario,', 6823: 'simple\\nfunction', 6824: 'plurals', 6825: 'plural()', 6826: '.1\\ntakes', 6827: 'form,', 6828: 'always\\ncorrect', 6829: \"(we'll\", 6830: '.)\\n\\n\\n\\n\\n\\xa0\\n\\ndef', 6831: 'plural(word):\\n', 6832: \".endswith('y'):\\n\", 6833: 'word[:-1]', 6834: \"'ies'\\n\", 6835: 'word[-1]', 6836: \"'sx'\", 6837: 'word[-2:]', 6838: \"['sh',\", 6839: \"'ch']:\\n\", 6840: \"'es'\\n\", 6841: \".endswith('an'):\\n\", 6842: 'word[:-2]', 6843: \"'en'\\n\", 6844: \"'s'\\n\\n\\n\\n\\n\\xa0\\n\\n>>>\", 6845: \"plural('fairy')\\n'fairies'\\n>>>\", 6846: \"plural('woman')\\n'women'\\n\\n\\nexample\", 6847: '(code_plural', 6848: 'function:', 6849: 'tries', 6850: 'the\\nplural', 6851: 'noun;', 6852: '(define)\\nis', 6853: 'inside\\nparentheses,', 6854: 'colon;', 6855: 'code;', 6856: 'patterns\\nwithin', 6857: 'accordingly;', 6858: 'y,', 6859: 'delete', 6860: 'ies', 6861: 'endswith()', 6862: 'object\\n(e', 6863: '.1)', 6864: 'give\\nthe', 6865: '.3\\xa0\\xa0\\xa0modules\\nover', 6866: 'functions,\\nand', 6867: 'the\\nlatest', 6868: 'use?\\nit', 6869: 'collect', 6870: 'and\\naccess', 6871: 'previously', 6872: 'copies', 6873: 'function(s)', 6874: '(say)', 6875: 'text_proc', 6876: 'importing', 6877: 'file:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 6878: 'plural\\n>>>', 6879: \"plural('wish')\\nwishes\\n>>>\", 6880: \"plural('fan')\\nfen\\n\\n\\n\\nour\", 6881: 'obviously', 6882: 'of\\nfan', 6883: 'fans', 6884: '.\\ninstead', 6885: 'can\\nsimply', 6886: 'edit', 6887: 'existing', 6888: 'every\\nstage,', 6889: 'confusion', 6890: 'about\\nwhich', 6891: 'definitions', 6892: 'python\\nmodule', 6893: \".\\nnltk's\", 6894: 'module,\\nand', 6895: 'is\\nan', 6896: 'sometimes\\ncalled', 6897: '.\\n\\ncaution!\\nif', 6898: 'python\\ncode,', 6899: '.py:', 6900: 'imported', 6901: 'in\\nplace', 6902: 'imports', 6903: 'modules,', 6904: 'python\\nfirst', 6905: 'looks', 6906: 'directory', 6907: '(folder)', 6908: '.\\n\\n\\n\\n\\n4\\xa0\\xa0\\xa0lexical', 6909: 'resources\\na', 6910: 'lexicon,', 6911: 'resource,', 6912: 'and/or', 6913: 'along\\nwith', 6914: '.\\nlexical', 6915: 'secondary', 6916: 'enriched', 6917: 'help\\nof', 6918: 'my_text,', 6919: 'then\\nvocab', 6920: 'sorted(set(my_text))', 6921: 'my_text,\\nwhile', 6922: 'word_freq', 6923: 'freqdist(my_text)', 6924: 'both\\nof', 6925: 'concordance\\nlike', 6926: '1\\ngives', 6927: 'preparation', 6928: 'dictionary', 6929: 'terminology', 6930: 'lexicons', 6931: 'entry', 6932: 'consists', 6933: 'headword', 6934: '(also', 6935: 'lemma)\\nalong', 6936: 'sense\\ndefinition', 6937: 'homonyms', 6938: 'lexicon', 6939: 'terminology:', 6940: 'entries', 6941: 'lemmas\\nhaving', 6942: '(homonyms),', 6943: 'speech\\nand', 6944: 'gloss', 6945: '.\\nsophisticated', 6946: 'across\\nthe', 6947: 'resources\\nincluded', 6948: '.1\\xa0\\xa0\\xa0wordlist', 6949: 'corpora\\n\\nnltk', 6950: '/usr/share/dict/words', 6951: 'unix,', 6952: 'by\\nsome', 6953: 'spell', 6954: 'checkers', 6955: 'mis-spelt\\nwords', 6956: 'unusual_words(text):\\n', 6957: 'text_vocab', 6958: '.isalpha())\\n', 6959: 'english_vocab', 6960: '.words', 6961: '.words())\\n', 6962: 'english_vocab\\n', 6963: 'sorted(unusual)\\n\\n>>>', 6964: 'unusual_words(nltk', 6965: \".words('austen-sense\", 6966: \".txt'))\\n['abbeyland',\", 6967: \"'abhorred',\", 6968: \"'abilities',\", 6969: \"'abounded',\", 6970: \"'abridgement',\", 6971: \"'abused',\", 6972: \"'abuses',\\n'accents',\", 6973: \"'accepting',\", 6974: \"'accommodations',\", 6975: \"'accompanied',\", 6976: \"'accounted',\", 6977: \"'accounts',\\n'accustomary',\", 6978: \"'aches',\", 6979: \"'acknowledging',\", 6980: \"'acknowledgment',\", 6981: \"'acknowledgments',\", 6982: '.nps_chat', 6983: \".words())\\n['aaaaaaaaaaaaaaaaa',\", 6984: \"'aaahhhh',\", 6985: \"'abortions',\", 6986: \"'abou',\", 6987: \"'abourted',\", 6988: \"'abs',\", 6989: \"'ack',\\n'acros',\", 6990: \"'actualy',\", 6991: \"'adams',\", 6992: \"'adds',\", 6993: \"'adduser',\", 6994: \"'adjusts',\", 6995: \"'adoted',\", 6996: \"'adreniline',\\n'ads',\", 6997: \"'adults',\", 6998: \"'afe',\", 6999: \"'affairs',\", 7000: \"'affari',\", 7001: \"'affects',\", 7002: \"'afk',\", 7003: \"'agaibn',\", 7004: \"'ages',\", 7005: '.]\\n\\n\\nexample', 7006: '(code_unusual', 7007: 'computes', 7008: 'text,\\nthen', 7009: 'removes', 7010: 'wordlist,\\nleaving', 7011: 'mis-spelt', 7012: '.\\n\\nthere', 7013: 'stopwords,', 7014: 'high-frequency\\nwords', 7015: 'the,', 7016: 'sometimes\\nwant', 7017: 'filter', 7018: 'stopwords\\nusually', 7019: 'content,', 7020: 'presence', 7021: 'fails\\nto', 7022: 'stopwords\\n>>>', 7023: \".words('english')\\n['i',\", 7024: \"'my',\", 7025: \"'myself',\", 7026: \"'we',\", 7027: \"'ours',\", 7028: \"'ourselves',\", 7029: \"'your',\", 7030: \"'yours',\\n'yourself',\", 7031: \"'yourselves',\", 7032: \"'he',\", 7033: \"'him',\", 7034: \"'himself',\", 7035: \"'she',\", 7036: \"'her',\", 7037: \"'hers',\\n'herself',\", 7038: \"'its',\", 7039: \"'itself',\", 7040: \"'them',\", 7041: \"'theirs',\", 7042: \"'themselves',\\n'what',\", 7043: \"'which',\", 7044: \"'who',\", 7045: \"'whom',\", 7046: \"'this',\", 7047: \"'these',\", 7048: \"'those',\", 7049: \"'am',\", 7050: \"'are',\\n'was',\", 7051: \"'were',\", 7052: \"'been',\", 7053: \"'being',\", 7054: \"'has',\", 7055: \"'having',\", 7056: \"'does',\\n'did',\", 7057: \"'doing',\", 7058: \"'but',\", 7059: \"'if',\", 7060: \"'because',\", 7061: \"'until',\\n'while',\", 7062: \"'at',\", 7063: \"'with',\", 7064: \"'about',\", 7065: \"'against',\", 7066: \"'into',\\n'through',\", 7067: \"'during',\", 7068: \"'before',\", 7069: \"'after',\", 7070: \"'above',\", 7071: \"'below',\", 7072: \"'up',\", 7073: \"'down',\\n'in',\", 7074: \"'out',\", 7075: \"'off',\", 7076: \"'over',\", 7077: \"'under',\", 7078: \"'again',\", 7079: \"'further',\", 7080: \"'then',\", 7081: \"'once',\", 7082: \"'here',\\n'there',\", 7083: \"'when',\", 7084: \"'where',\", 7085: \"'how',\", 7086: \"'any',\", 7087: \"'both',\", 7088: \"'each',\", 7089: \"'few',\", 7090: \"'more',\\n'most',\", 7091: \"'some',\", 7092: \"'such',\", 7093: \"'no',\", 7094: \"'nor',\", 7095: \"'only',\", 7096: \"'own',\", 7097: \"'same',\", 7098: \"'so',\\n'than',\", 7099: \"'too',\", 7100: \"'very',\", 7101: \"'s',\", 7102: \"'t',\", 7103: \"'just',\", 7104: \"'don',\", 7105: \"'should',\", 7106: \"'now']\\n\\n\\n\\nlet's\", 7107: 'the\\nstopwords', 7108: 'content_fraction(text):\\n', 7109: '.stopwords', 7110: \".words('english')\\n\", 7111: 'stopwords]\\n', 7112: 'len(content)', 7113: 'len(text)\\n', 7114: 'content_fraction(nltk', 7115: '.reuters', 7116: '.words())\\n0', 7117: '.7364374824583169\\n\\n\\n\\nthus,', 7118: 'lexical\\nresource', 7119: 'puzzle:', 7120: 'grid', 7121: 'randomly', 7122: 'chosen', 7123: 'for\\ncreating', 7124: 'letters;', 7125: 'puzzle', 7126: 'wordlist', 7127: 'solving', 7128: 'puzzles,', 7129: '.\\nour', 7130: 'iterates', 7131: 'and,', 7132: 'meets\\nthe', 7133: 'obligatory', 7134: '\\nand', 7135: 'constraints', 7136: '(and', 7137: \"we'll\\nonly\", 7138: 'six', 7139: 'here)', 7140: 'trickier', 7141: 'candidate', 7142: 'solutions', 7143: 'the\\nsupplied', 7144: 'especially', 7145: 'letters\\nappear', 7146: 'twice', 7147: '(here,', 7148: 'v)', 7149: 'equal\\nto', 7150: 'puzzle_letters', 7151: \".freqdist('egivrvonl')\\n>>>\", 7152: \"'r'\\n>>>\", 7153: '.words()\\n>>>', 7154: '.freqdist(w)', 7155: 'puzzle_letters]', 7156: \"\\n['glover',\", 7157: \"'gorlin',\", 7158: \"'govern',\", 7159: \"'grovel',\", 7160: \"'ignore',\", 7161: \"'involver',\", 7162: \"'lienor',\\n'linger',\", 7163: \"'longer',\", 7164: \"'lovering',\", 7165: \"'noiler',\", 7166: \"'overling',\", 7167: \"'region',\", 7168: \"'renvoi',\\n'revolving',\", 7169: \"'ringle',\", 7170: \"'roving',\", 7171: \"'violer',\", 7172: \"'virole']\\n\\n\\n\\none\", 7173: '8,000', 7174: 'appear\\nin', 7175: 'ambiguous', 7176: 'gender:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 7177: '.names\\n>>>', 7178: \".fileids()\\n['female\", 7179: \"'male\", 7180: \".txt']\\n>>>\", 7181: 'male_names', 7182: \".words('male\", 7183: 'female_names', 7184: \".words('female\", 7185: \"female_names]\\n['abbey',\", 7186: \"'abbie',\", 7187: \"'abby',\", 7188: \"'addie',\", 7189: \"'adrian',\", 7190: \"'adrien',\", 7191: \"'ajay',\", 7192: \"'alex',\", 7193: \"'alexis',\\n'alfie',\", 7194: \"'ali',\", 7195: \"'alix',\", 7196: \"'allie',\", 7197: \"'allyn',\", 7198: \"'andie',\", 7199: \"'andrea',\", 7200: \"'andy',\", 7201: \"'angel',\\n'angie',\", 7202: \"'ariel',\", 7203: \"'ashley',\", 7204: \"'aubrey',\", 7205: \"'augustine',\", 7206: \"'austin',\", 7207: \"'averil',\", 7208: '.]\\n\\n\\n\\nit', 7209: 'almost', 7210: '.4,\\nproduced', 7211: 'name[-1]', 7212: 'letter\\nof', 7213: '(fileid,', 7214: 'name[-1])\\n', 7215: '.words(fileid))\\n>>>', 7216: '.4:', 7217: 'names\\nending', 7218: 'alphabet;', 7219: 'a,', 7220: 'i\\nare', 7221: 'female;', 7222: 'h', 7223: 'equally', 7224: 'female;\\nnames', 7225: 'k,', 7226: 'o,', 7227: 'r,', 7228: 's,', 7229: '.\\n\\n\\n\\n4', 7230: '.2\\xa0\\xa0\\xa0a', 7231: 'dictionary\\na', 7232: 'richer', 7233: 'spreadsheet),', 7234: 'word\\nplus', 7235: 'cmu', 7236: 'pronouncing\\ndictionary', 7237: 'for\\nuse', 7238: 'synthesizers', 7239: '.cmudict', 7240: '.entries()\\n>>>', 7241: 'len(entries)\\n133737\\n>>>', 7242: 'entries[42371:42379]:\\n', 7243: 'print(entry)\\n', 7244: \".\\n('fir',\", 7245: \"['f',\", 7246: \"'er1'])\\n('fire',\", 7247: \"'ay1',\", 7248: \"'er0'])\\n('fire',\", 7249: \"'r'])\\n('firearm',\", 7250: \"'er0',\", 7251: \"'aa2',\", 7252: \"'r',\", 7253: \"'m'])\\n('firearm',\", 7254: \"'m'])\\n('firearms',\", 7255: \"'m',\", 7256: \"'z'])\\n('firearms',\", 7257: \"'z'])\\n('fireball',\", 7258: \"'b',\", 7259: \"'ao2',\", 7260: \"'l'])\\n\\n\\n\\nfor\", 7261: 'phonetic\\ncodes', 7262: 'labels', 7263: 'contrastive', 7264: 'sound', 7265: '—\\nknown', 7266: 'fire', 7267: 'pronunciations\\n(in', 7268: 'english):\\nthe', 7269: 'one-syllable', 7270: 'f', 7271: 'ay1', 7272: 'two-syllable', 7273: 'er0', 7274: 'arpabet,\\ndescribed', 7275: 'detail', 7276: 'http://en', 7277: '.wikipedia', 7278: '.org/wiki/arpabet\\n\\neach', 7279: 'parts,', 7280: 'can\\nprocess', 7281: 'individually', 7282: 'entries:,', 7283: 'replace\\nentry', 7284: 'pron', 7285: 'the\\nentry,', 7286: 'entry:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 7287: 'entries:', 7288: 'len(pron)', 7289: '3:', 7290: 'ph1,', 7291: 'ph2,', 7292: 'ph3', 7293: 'ph1', 7294: \"'p'\", 7295: \"'t':\\n\", 7296: '.\\npait', 7297: 'ey1', 7298: 'pat', 7299: 'ae1', 7300: 'pate', 7301: 'patt', 7302: 'peart', 7303: 'er1', 7304: 'peat', 7305: 'iy1', 7306: 'peet', 7307: 'peete', 7308: 'pert', 7309: 'er1\\npet', 7310: 'eh1', 7311: 'pete', 7312: 'pett', 7313: 'piet', 7314: 'piette', 7315: 'pit', 7316: 'ih1', 7317: 'pitt', 7318: 'pot', 7319: 'aa1', 7320: 'pote', 7321: 'ow1\\npott', 7322: 'pout', 7323: 'aw1', 7324: 'puett', 7325: 'uw1', 7326: 'purt', 7327: 'uh1', 7328: 'putt', 7329: 'ah1\\n\\n\\n\\nthe', 7330: 'scans', 7331: 'looking', 7332: 'pronunciation', 7333: 'of\\nthree', 7334: 'true,', 7335: 'assigns', 7336: 'contents\\nof', 7337: 'ph2', 7338: 'unusual\\nform', 7339: 'list\\ncomprehension', 7340: 'finds', 7341: 'syllable\\nsounding', 7342: 'nicks', 7343: 'rhyming', 7344: 'syllable', 7345: \"['n',\", 7346: \"'ih0',\", 7347: \"'k',\", 7348: \"'s']\\n>>>\", 7349: 'pron[-4:]', 7350: \"syllable]\\n[atlantic's,\", 7351: \"'audiotronics',\", 7352: \"'avionics',\", 7353: \"'beatniks',\", 7354: \"'calisthenics',\", 7355: \"'centronics',\\n'chamonix',\", 7356: \"'chetniks',\", 7357: \"clinic's,\", 7358: \"'clinics',\", 7359: \"'conics',\", 7360: \"'cryogenics',\\n'cynics',\", 7361: \"'diasonics',\", 7362: \"dominic's,\", 7363: \"'ebonics',\", 7364: \"'electronics',\", 7365: \"electronics',\", 7366: 'spelt', 7367: 'ways:', 7368: 'nics,', 7369: 'niks,', 7370: 'nix,\\neven', 7371: \"ntic's\", 7372: 'silent', 7373: 't,', 7374: \"atlantic's\", 7375: 'other\\nmismatches', 7376: 'summarize', 7377: 'work?\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 7378: 'w,', 7379: 'pron[-1]', 7380: \"'m'\", 7381: 'w[-1]', 7382: \"'n']\\n['autumn',\", 7383: \"'column',\", 7384: \"'condemn',\", 7385: \"'damn',\", 7386: \"'goddamn',\", 7387: \"'hymn',\", 7388: \"'solemn']\\n>>>\", 7389: 'sorted(set(w[:2]', 7390: 'pron[0]', 7391: \"'n'\", 7392: 'w[0]', 7393: \"'n'))\\n['gn',\", 7394: \"'kn',\", 7395: \"'mn',\", 7396: \"'pn']\\n\\n\\n\\nthe\", 7397: 'represent\\nprimary', 7398: 'stress', 7399: '(1),', 7400: '(2)', 7401: '(0)', 7402: 'digits\\nand', 7403: 'scan', 7404: 'stress(pron):\\n', 7405: '[char', 7406: 'phone', 7407: 'char', 7408: '.isdigit()]\\n>>>', 7409: 'stress(pron)', 7410: \"['0',\", 7411: \"'1',\", 7412: \"'0',\", 7413: \"'2',\", 7414: \"'0']]\\n['abbreviated',\", 7415: \"'abbreviated',\", 7416: \"'abbreviating',\", 7417: \"'accelerated',\", 7418: \"'accelerating',\\n'accelerator',\", 7419: \"'accelerators',\", 7420: \"'accentuated',\", 7421: \"'accentuating',\", 7422: \"'accommodated',\\n'accommodating',\", 7423: \"'accommodative',\", 7424: \"'accumulated',\", 7425: \"'accumulating',\", 7426: \"'accumulative',\", 7427: \"'0']]\\n['abbreviation',\", 7428: \"'abbreviations',\", 7429: \"'abomination',\", 7430: \"'abortifacient',\", 7431: \"'abortifacients',\\n'academicians',\", 7432: \"'accommodation',\", 7433: \"'accreditation',\", 7434: \"'accreditations',\\n'accumulation',\", 7435: \"'accumulations',\", 7436: \"'acetylcholine',\", 7437: \"'adjudication',\", 7438: '.]\\n\\n\\n\\n\\nnote\\na', 7439: 'subtlety', 7440: 'our\\nuser-defined', 7441: 'stress()', 7442: 'doubly-nested', 7443: \".\\nthere's\", 7444: 'minimally-contrasting\\nsets', 7445: 'p-words', 7446: 'p3', 7447: \"[(pron[0]+'-'+pron[2],\", 7448: '(word,', 7449: 'pron)', 7450: 'entries\\n', 7451: '3]', 7452: '.conditionalfreqdist(p3)\\n>>>', 7453: 'template', 7454: 'sorted(cfd', 7455: '.conditions()):\\n', 7456: 'len(cfd[template])', 7457: '10:\\n', 7458: 'sorted(cfd[template])\\n', 7459: 'wordstring', 7460: '.join(words)\\n', 7461: 'print(template,', 7462: 'wordstring[:70]', 7463: '.)\\n', 7464: '.\\np-ch', 7465: 'patch', 7466: 'pautsch', 7467: 'peach', 7468: 'perch', 7469: 'petsch', 7470: 'petsche', 7471: 'piche', 7472: 'piech', 7473: 'pietsch', 7474: 'pitch', 7475: '.\\np-k', 7476: 'pac', 7477: 'pack', 7478: 'paek', 7479: 'paik', 7480: 'pak', 7481: 'pake', 7482: 'paque', 7483: 'peak', 7484: 'peake', 7485: 'pech', 7486: 'peck', 7487: 'peek', 7488: 'perc', 7489: 'perk', 7490: '.\\np-l', 7491: 'pahl', 7492: 'pail', 7493: 'paille', 7494: 'pal', 7495: 'pale', 7496: 'pall', 7497: 'paul', 7498: 'paule', 7499: 'paull', 7500: 'peal', 7501: 'peale', 7502: 'pearl', 7503: '.\\np-n', 7504: 'paign', 7505: 'pain', 7506: 'paine', 7507: 'pan', 7508: 'pane', 7509: 'pawn', 7510: 'payne', 7511: 'peine', 7512: 'pen', 7513: 'penh', 7514: 'pin', 7515: 'pine', 7516: 'pinn', 7517: '.\\np-p', 7518: 'paap', 7519: 'paape', 7520: 'pap', 7521: 'pape', 7522: 'papp', 7523: 'paup', 7524: 'peep', 7525: 'pep', 7526: 'pip', 7527: 'pipe', 7528: 'pipp', 7529: 'poop', 7530: 'pop', 7531: 'pope', 7532: '.\\np-r', 7533: 'paar', 7534: 'par', 7535: 'pare', 7536: 'parr', 7537: 'pear', 7538: 'peer', 7539: 'pier', 7540: 'poor', 7541: 'poore', 7542: 'por', 7543: 'pore', 7544: 'porr', 7545: 'pour', 7546: '.\\np-s', 7547: 'pasts', 7548: 'peace', 7549: 'pearse', 7550: 'pease', 7551: 'perce', 7552: 'pers', 7553: 'perse', 7554: 'pesce', 7555: 'piss', 7556: '.\\np-t', 7557: 'pait', 7558: 'pet', 7559: 'piett', 7560: '.\\np-uw1', 7561: 'peru', 7562: 'peugh', 7563: 'pew', 7564: 'plew', 7565: 'plue', 7566: 'prew', 7567: 'pru', 7568: 'prue', 7569: 'prugh', 7570: 'pshew', 7571: 'pugh', 7572: '.\\n\\n\\n\\nrather', 7573: 'iterating', 7574: 'it\\nby', 7575: 'data\\nstructure,', 7576: 'key\\n(such', 7577: \"'fire')\", 7578: 'prondict', 7579: '.dict()\\n>>>', 7580: \"prondict['fire']\", 7581: \"\\n[['f',\", 7582: \"'er0'],\", 7583: \"'r']]\\n>>>\", 7584: \"prondict['blog']\", 7585: '<module>\\nkeyerror:', 7586: \"'blog'\\n>>>\", 7587: \"[['b',\", 7588: \"'l',\", 7589: \"'aa1',\", 7590: \"'g']]\", 7591: \"prondict['blog']\\n[['b',\", 7592: \"'g']]\\n\\n\\n\\nif\", 7593: 'non-existent', 7594: 'keyerror', 7595: 'an\\ninteger', 7596: 'producing', 7597: 'indexerror', 7598: 'missing', 7599: 'dictionary,\\nso', 7600: 'tweak', 7601: '\\n(this', 7602: 'effect', 7603: 'corpus;', 7604: 'it,\\nblog', 7605: 'absent)', 7606: 'having\\nsome', 7607: '(like', 7608: 'nouns),', 7609: 'mapping', 7610: 'text-to-speech', 7611: 'word\\nof', 7612: \"['natural',\", 7613: \"'language',\", 7614: \"'processing']\\n>>>\", 7615: '[ph', 7616: 'ph', 7617: \"prondict[w][0]]\\n['n',\", 7618: \"'ae1',\", 7619: \"'ch',\", 7620: \"'ah0',\", 7621: \"'ng',\", 7622: \"'g',\", 7623: \"'jh',\\n'p',\", 7624: \"'eh0',\", 7625: \"'ng']\\n\\n\\n\\n\\n\\n\\n4\", 7626: '.3\\xa0\\xa0\\xa0comparative', 7627: 'wordlists\\nanother', 7628: 'tabular', 7629: 'comparative', 7630: 'swadesh', 7631: 'wordlists,', 7632: '200', 7633: 'identified', 7634: 'iso', 7635: '639', 7636: 'two-letter', 7637: 'swadesh\\n>>>', 7638: \".fileids()\\n['be',\", 7639: \"'bg',\", 7640: \"'bs',\", 7641: \"'ca',\", 7642: \"'cs',\", 7643: \"'cu',\", 7644: \"'de',\", 7645: \"'en',\", 7646: \"'es',\", 7647: \"'fr',\", 7648: \"'hr',\", 7649: \"'la',\", 7650: \"'mk',\\n'nl',\", 7651: \"'pl',\", 7652: \"'pt',\", 7653: \"'ro',\", 7654: \"'ru',\", 7655: \"'sk',\", 7656: \"'sl',\", 7657: \"'sr',\", 7658: \"'sw',\", 7659: \"'uk']\\n>>>\", 7660: \".words('en')\\n['i',\", 7661: \"'you\", 7662: '(singular),', 7663: \"thou',\", 7664: \"(plural)',\", 7665: \"'that',\\n'here',\", 7666: \"'there',\", 7667: \"'what',\", 7668: \"'many',\", 7669: \"'some',\\n'few',\", 7670: \"'one',\", 7671: \"'three',\", 7672: \"'four',\", 7673: \"'five',\", 7674: \"'big',\", 7675: \"'long',\", 7676: \"'wide',\", 7677: '.]\\n\\n\\n\\nwe', 7678: 'cognate', 7679: 'entries()', 7680: 'method,\\nspecifying', 7681: 'into\\na', 7682: 'dict()', 7683: '3)', 7684: 'fr2en', 7685: \".entries(['fr',\", 7686: \"'en'])\\n>>>\", 7687: \"fr2en\\n[('je',\", 7688: \"'i'),\", 7689: \"('tu,\", 7690: \"vous',\", 7691: \"thou'),\", 7692: \"('il',\", 7693: \"'he'),\", 7694: 'dict(fr2en)\\n>>>', 7695: \"translate['chien']\\n'dog'\\n>>>\", 7696: \"translate['jeter']\\n'throw'\\n\\n\\n\\nwe\", 7697: 'translator', 7698: 'source', 7699: 'german-english', 7700: 'spanish-english', 7701: 'a\\ndictionary', 7702: 'dict(),', 7703: 'update', 7704: 'dictionary\\nwith', 7705: 'mappings:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 7706: 'de2en', 7707: \".entries(['de',\", 7708: \"'en'])\", 7709: 'german-english\\n>>>', 7710: 'es2en', 7711: \".entries(['es',\", 7712: 'spanish-english\\n>>>', 7713: '.update(dict(de2en))\\n>>>', 7714: '.update(dict(es2en))\\n>>>', 7715: \"translate['hund']\\n'dog'\\n>>>\", 7716: \"translate['perro']\\n'dog'\\n\\n\\n\\nwe\", 7717: 'germanic', 7718: 'languages:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 7719: \"['en',\", 7720: \"'nl',\", 7721: \"'la']\\n>>>\", 7722: '[139,', 7723: '140,', 7724: '141,', 7725: '142]:\\n', 7726: 'print(swadesh', 7727: '.entries(languages)[i])\\n', 7728: \".\\n('say',\", 7729: \"'sagen',\", 7730: \"'zeggen',\", 7731: \"'decir',\", 7732: \"'dire',\", 7733: \"'dizer',\", 7734: \"'dicere')\\n('sing',\", 7735: \"'singen',\", 7736: \"'zingen',\", 7737: \"'cantar',\", 7738: \"'chanter',\", 7739: \"'canere')\\n('play',\", 7740: \"'spielen',\", 7741: \"'spelen',\", 7742: \"'jugar',\", 7743: \"'jouer',\", 7744: \"'jogar,\", 7745: \"brincar',\", 7746: \"'ludere')\\n('float',\", 7747: \"'schweben',\", 7748: \"'zweven',\", 7749: \"'flotar',\", 7750: \"'flotter',\", 7751: \"'flutuar,\", 7752: \"boiar',\", 7753: \"'fluctuare')\\n\\n\\n\\n\\n\\n4\", 7754: '.4\\xa0\\xa0\\xa0shoebox', 7755: 'lexicons\\nperhaps', 7756: 'tool', 7757: 'linguists', 7758: 'data\\nis', 7759: 'toolbox,', 7760: 'shoebox', 7761: 'replaces\\nthe', 7762: \"linguist's\", 7763: 'traditional', 7764: 'cards', 7765: '.\\ntoolbox', 7766: '.sil', 7767: '.org/computing/toolbox/', 7768: 'entries,\\nwhere', 7769: 'fields', 7770: '.\\nmost', 7771: 'optional', 7772: 'repeatable,', 7773: 'of\\nlexical', 7774: 'spreadsheet', 7775: 'rotokas', 7776: 'entry,\\nfor', 7777: 'kaa', 7778: 'gag:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 7779: 'toolbox\\n>>>', 7780: \".entries('rotokas\", 7781: \".dic')\\n[('kaa',\", 7782: \"[('ps',\", 7783: \"'v'),\", 7784: \"('pt',\", 7785: \"'a'),\", 7786: \"('ge',\", 7787: \"'gag'),\", 7788: \"('tkp',\", 7789: \"'nek\", 7790: \"pas'),\\n('dcsv',\", 7791: \"'true'),\", 7792: \"('vx',\", 7793: \"'1'),\", 7794: \"('sc',\", 7795: \"'???'),\", 7796: \"('dt',\", 7797: \"'29/oct/2005'),\\n('ex',\", 7798: \"'apoka\", 7799: 'ira', 7800: 'kaaroi', 7801: 'aioa-ia', 7802: 'reoreopaoro', 7803: \".'),\\n('xp',\", 7804: \"'kaikai\", 7805: 'pas', 7806: 'nek', 7807: 'bilong', 7808: 'apoka', 7809: 'bikos', 7810: 'em', 7811: 'kaikai', 7812: 'na', 7813: 'toktok', 7814: \".'),\\n('xe',\", 7815: 'gagging', 7816: \".')]),\", 7817: '.]\\n\\n\\n\\nentries', 7818: 'consist', 7819: 'attribute-value', 7820: \"('ps',\", 7821: \"'v')\\nto\", 7822: \"'v'\", 7823: '(verb),', 7824: \"'gag')\\nto\", 7825: 'gloss-into-english', 7826: \"'gag'\", 7827: 'contain\\nan', 7828: 'tok', 7829: 'pisin', 7830: 'loose', 7831: 'them\\nat', 7832: 'island', 7833: 'bougainville,', 7834: 'papua', 7835: 'guinea', 7836: 'contributed', 7837: 'stuart', 7838: 'robinson', 7839: '.\\nrotokas', 7840: 'notable', 7841: 'inventory', 7842: 'phonemes', 7843: '(contrastive', 7844: 'sounds),\\nhttp://en', 7845: '.org/wiki/rotokas_language\\n\\n\\n\\n\\n5\\xa0\\xa0\\xa0wordnet\\n\\nwordnet', 7846: 'semantically-oriented', 7847: 'english,\\nsimilar', 7848: 'thesaurus', 7849: 'wordnet,', 7850: '155,287', 7851: 'words\\nand', 7852: '117,659', 7853: 'by\\nlooking', 7854: 'synonyms', 7855: '.1\\xa0\\xa0\\xa0senses', 7856: 'synonyms\\n\\n\\nconsider', 7857: 'replace', 7858: 'motorcar', 7859: 'automobile,\\nto', 7860: '(1b),', 7861: 'stays', 7862: 'pretty', 7863: 'same:\\n\\n', 7864: '.benz', 7865: 'credited', 7866: 'invention', 7867: 'automobile', 7868: '.\\n\\nsince', 7869: 'remained', 7870: 'unchanged,', 7871: 'can\\nconclude', 7872: 'meaning,', 7873: 'wordnet:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 7874: 'wn\\n>>>', 7875: 'wn', 7876: \".synsets('motorcar')\\n[synset('car\", 7877: '.n', 7878: \".01')]\\n\\n\\n\\nthus,\", 7879: '.01,\\nthe', 7880: 'entity', 7881: '.01', 7882: 'synset,\\nor', 7883: 'synonymous', 7884: 'lemmas):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 7885: \".synset('car\", 7886: \".01')\", 7887: \".lemma_names()\\n['car',\", 7888: \"'auto',\", 7889: \"'automobile',\", 7890: \"'machine',\", 7891: \"'motorcar']\\n\\n\\n\\neach\", 7892: 'synset', 7893: 'signify\\na', 7894: 'train', 7895: 'carriage,', 7896: 'gondola,', 7897: 'elevator', 7898: 'interested\\nin', 7899: 'synsets\\nalso', 7900: 'prose', 7901: 'sentences:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 7902: \".definition()\\n'a\", 7903: 'motor', 7904: 'vehicle', 7905: 'wheels;', 7906: 'propelled', 7907: 'internal', 7908: 'combustion', 7909: \"engine'\\n>>>\", 7910: \".examples()\\n['he\", 7911: \"work']\\n\\n\\n\\nalthough\", 7912: 'humans', 7913: 'synset,\\nthe', 7914: 'ambiguity,', 7915: 'as\\ncar', 7916: '.automobile,', 7917: '.motorcar,', 7918: 'pairing', 7919: 'lemma', 7920: 'lemmas', 7921: ',\\nlook', 7922: ',\\nget', 7923: '.lemmas()', 7924: \"\\n[lemma('car\", 7925: \".car'),\", 7926: \"lemma('car\", 7927: \".auto'),\", 7928: \".automobile'),\\nlemma('car\", 7929: \".machine'),\", 7930: \".motorcar')]\\n>>>\", 7931: \".lemma('car\", 7932: \".automobile')\", 7933: \"\\nlemma('car\", 7934: \".automobile')\\n>>>\", 7935: '.synset()', 7936: \"\\nsynset('car\", 7937: \".01')\\n>>>\", 7938: '.name()', 7939: \"\\n'automobile'\\n\\n\\n\\nunlike\", 7940: 'motorcar,', 7941: 'unambiguous', 7942: 'one\\nsynset,', 7943: 'ambiguous,', 7944: 'five', 7945: 'synsets:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 7946: \".synsets('car')\\n[synset('car\", 7947: \".01'),\", 7948: \"synset('car\", 7949: \".02'),\", 7950: \".03'),\", 7951: \".04'),\\nsynset('cable_car\", 7952: \".01')]\\n>>>\", 7953: \".synsets('car'):\\n\", 7954: 'print(synset', 7955: '.lemma_names())\\n', 7956: \".\\n['car',\", 7957: \"'motorcar']\\n['car',\", 7958: \"'railcar',\", 7959: \"'railway_car',\", 7960: \"'railroad_car']\\n['car',\", 7961: \"'gondola']\\n['car',\", 7962: \"'elevator_car']\\n['cable_car',\", 7963: \"'car']\\n\\n\\n\\nfor\", 7964: 'involving', 7965: 'car\\nas', 7966: \".lemmas('car')\\n[lemma('car\", 7967: '.02', 7968: '.03', 7969: \".car'),\\nlemma('car\", 7970: '.04', 7971: \"lemma('cable_car\", 7972: \".car')]\\n\\n\\n\\n\\nnote\\nyour\", 7973: 'turn:\\nwrite', 7974: 'senses', 7975: 'this\\nword', 7976: '.\\n\\n\\n\\n5', 7977: '.2\\xa0\\xa0\\xa0the', 7978: 'hierarchy\\nwordnet', 7979: 'synsets', 7980: 'abstract', 7981: 'always\\nhave', 7982: 'hierarchy', 7983: 'entity,', 7984: 'state,', 7985: 'called\\nunique', 7986: 'beginners', 7987: 'others,', 7988: 'gas', 7989: 'guzzler', 7990: 'and\\nhatchback,', 7991: 'concept\\nhierarchy', 7992: 'hierarchy:', 7993: 'nodes', 7994: 'synsets;\\nedges', 7995: 'hypernym/hyponym', 7996: 'relation,', 7997: 'relation', 7998: 'between\\nsuperordinate', 7999: 'subordinate', 8000: '.\\n\\nwordnet', 8001: 'motorcar,\\nwe', 8002: 'specific;\\nthe', 8003: '(immediate)', 8004: 'hyponyms', 8005: 'types_of_motorcar', 8006: '.hyponyms()\\n>>>', 8007: \"types_of_motorcar[0]\\nsynset('ambulance\", 8008: 'sorted(lemma', 8009: \".lemmas())\\n['model_t',\", 8010: \"'s\", 8011: '.u', 8012: '.v', 8013: \"'suv',\", 8014: \"'stanley_steamer',\", 8015: \"'ambulance',\", 8016: \"'beach_waggon',\\n'beach_wagon',\", 8017: \"'bus',\", 8018: \"'cab',\", 8019: \"'compact',\", 8020: \"'compact_car',\", 8021: \"'convertible',\\n'coupe',\", 8022: \"'cruiser',\", 8023: \"'electric',\", 8024: \"'electric_automobile',\", 8025: \"'electric_car',\\n'estate_car',\", 8026: \"'gas_guzzler',\", 8027: \"'hack',\", 8028: \"'hardtop',\", 8029: \"'hatchback',\", 8030: \"'heap',\\n'horseless_carriage',\", 8031: \"'hot-rod',\", 8032: \"'hot_rod',\", 8033: \"'jalopy',\", 8034: \"'jeep',\", 8035: \"'landrover',\\n'limo',\", 8036: \"'limousine',\", 8037: \"'loaner',\", 8038: \"'minicar',\", 8039: \"'minivan',\", 8040: \"'pace_car',\", 8041: \"'patrol_car',\\n'phaeton',\", 8042: \"'police_car',\", 8043: \"'police_cruiser',\", 8044: \"'prowl_car',\", 8045: \"'race_car',\", 8046: \"'racer',\\n'racing_car',\", 8047: \"'roadster',\", 8048: \"'runabout',\", 8049: \"'saloon',\", 8050: \"'secondhand_car',\", 8051: \"'sedan',\\n'sport_car',\", 8052: \"'sport_utility',\", 8053: \"'sport_utility_vehicle',\", 8054: \"'sports_car',\", 8055: \"'squad_car',\\n'station_waggon',\", 8056: \"'station_wagon',\", 8057: \"'stock_car',\", 8058: \"'subcompact',\", 8059: \"'subcompact_car',\\n'taxi',\", 8060: \"'taxicab',\", 8061: \"'tourer',\", 8062: \"'touring_car',\", 8063: \"'two-seater',\", 8064: \"'used-car',\", 8065: \"'waggon',\\n'wagon']\\n\\n\\n\\nwe\", 8066: 'visiting', 8067: 'hypernyms', 8068: 'words\\nhave', 8069: 'paths,', 8070: 'paths', 8071: 'because\\nwheeled_vehicle', 8072: 'container', 8073: \".hypernyms()\\n[synset('motor_vehicle\", 8074: '.hypernym_paths()\\n>>>', 8075: 'len(paths)\\n2\\n>>>', 8076: '[synset', 8077: \"paths[0]]\\n['entity\", 8078: \".01',\", 8079: \"'physical_entity\", 8080: \"'object\", 8081: \"'whole\", 8082: \".02',\", 8083: \"'artifact\", 8084: \".01',\\n'instrumentality\", 8085: \".03',\", 8086: \"'container\", 8087: \"'wheeled_vehicle\", 8088: \".01',\\n'self-propelled_vehicle\", 8089: \"'motor_vehicle\", 8090: \"'car\", 8091: \".01']\\n>>>\", 8092: \"paths[1]]\\n['entity\", 8093: \"'conveyance\", 8094: \"'vehicle\", 8095: \".01']\\n\\n\\n\\nwe\", 8096: 'hypernyms)', 8097: \".root_hypernyms()\\n[synset('entity\", 8098: \".01')]\\n\\n\\n\\n\\nnote\\nyour\", 8099: 'browser:', 8100: '.app', 8101: '.wordnet()', 8102: '.\\nexplore', 8103: 'hypernym', 8104: 'hyponym', 8105: '.3\\xa0\\xa0\\xa0more', 8106: 'relations\\nhypernyms', 8107: 'relations', 8108: 'relate', 8109: 'one\\nsynset', 8110: 'is-a', 8111: 'their\\ncomponents', 8112: '(meronyms)', 8113: '(holonyms)', 8114: 'parts', 8115: 'tree', 8116: 'trunk,', 8117: 'crown,', 8118: 'on;\\nthe', 8119: 'part_meronyms()', 8120: 'substance', 8121: 'heartwood', 8122: 'sapwood;\\nthe', 8123: 'substance_meronyms()', 8124: 'trees', 8125: 'forms', 8126: 'forest;', 8127: 'member_holonyms():\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 8128: \".synset('tree\", 8129: \".part_meronyms()\\n[synset('burl\", 8130: \"synset('crown\", 8131: \".07'),\", 8132: \"synset('limb\", 8133: \".02'),\\nsynset('stump\", 8134: \"synset('trunk\", 8135: \".substance_meronyms()\\n[synset('heartwood\", 8136: \"synset('sapwood\", 8137: \".member_holonyms()\\n[synset('forest\", 8138: \".01')]\\n\\n\\n\\nto\", 8139: 'intricate', 8140: 'get,', 8141: 'mint,', 8142: 'which\\nhas', 8143: 'closely-related', 8144: 'mint', 8145: 'of\\nmint', 8146: '.05', 8147: \".synsets('mint',\", 8148: '.noun):\\n', 8149: '.definition())\\n', 8150: '.\\nbatch', 8151: '.02:', 8152: '(often', 8153: \"`of')\", 8154: 'extent\\nmint', 8155: 'north', 8156: 'temperate', 8157: 'plant', 8158: 'genus', 8159: 'mentha', 8160: 'aromatic', 8161: 'and\\n', 8162: 'mauve', 8163: 'flowers\\nmint', 8164: '.03:', 8165: 'plants\\nmint', 8166: '.04:', 8167: 'fresh', 8168: 'candied\\nmint', 8169: '.05:', 8170: 'candy', 8171: 'flavored', 8172: 'oil\\nmint', 8173: '.06:', 8174: 'money', 8175: 'coined', 8176: 'authority', 8177: 'government\\n>>>', 8178: \".synset('mint\", 8179: \".04')\", 8180: \".part_holonyms()\\n[synset('mint\", 8181: \".02')]\\n>>>\", 8182: \".substance_holonyms()\\n[synset('mint\", 8183: \".05')]\\n\\n\\n\\nthere\", 8184: 'relationships', 8185: 'stepping,\\nso', 8186: 'entails', 8187: 'stepping', 8188: 'entailments:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 8189: \".synset('walk\", 8190: \".entailments()\\n[synset('step\", 8191: \".synset('eat\", 8192: \".entailments()\\n[synset('chew\", 8193: \"synset('swallow\", 8194: \".synset('tease\", 8195: \".03')\", 8196: \".entailments()\\n[synset('arouse\", 8197: \"synset('disappoint\", 8198: \".01')]\\n\\n\\n\\nsome\", 8199: 'lemmas,', 8200: 'antonymy:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 8201: \".lemma('supply\", 8202: \".supply')\", 8203: \".antonyms()\\n[lemma('demand\", 8204: \".demand')]\\n>>>\", 8205: \".lemma('rush\", 8206: \".rush')\", 8207: \".antonyms()\\n[lemma('linger\", 8208: \".linger')]\\n>>>\", 8209: \".lemma('horizontal\", 8210: '.a', 8211: \".horizontal')\", 8212: \".antonyms()\\n[lemma('inclined\", 8213: \".inclined'),\", 8214: \"lemma('vertical\", 8215: \".vertical')]\\n>>>\", 8216: \".lemma('staccato\", 8217: '.r', 8218: \".staccato')\", 8219: \".antonyms()\\n[lemma('legato\", 8220: \".legato')]\\n\\n\\n\\nyou\", 8221: 'relations,', 8222: 'defined\\non', 8223: 'synset,', 8224: 'dir(),', 8225: 'dir(wn', 8226: \".synset('harmony\", 8227: \".02'))\", 8228: '.4\\xa0\\xa0\\xa0semantic', 8229: 'similarity\\n\\nwe', 8230: 'traverse\\nthe', 8231: '.\\nknowing', 8232: 'semantically', 8233: 'related\\nis', 8234: 'indexing', 8235: 'so\\nthat', 8236: 'match', 8237: 'documents\\ncontaining', 8238: 'limousine', 8239: '.\\nrecall', 8240: 'link', 8241: 'it\\nto', 8242: 'common\\n(cf', 8243: 'low\\ndown', 8244: \".synset('right_whale\", 8245: 'orca', 8246: \".synset('orca\", 8247: 'minke', 8248: \".synset('minke_whale\", 8249: 'tortoise', 8250: \".synset('tortoise\", 8251: 'novel', 8252: \".synset('novel\", 8253: \".lowest_common_hypernyms(minke)\\n[synset('baleen_whale\", 8254: \".lowest_common_hypernyms(orca)\\n[synset('whale\", 8255: \".lowest_common_hypernyms(tortoise)\\n[synset('vertebrate\", 8256: \".lowest_common_hypernyms(novel)\\n[synset('entity\", 8257: \".01')]\\n\\n\\n\\nof\", 8258: 'baleen', 8259: 'so),\\nwhile', 8260: 'vertebrate', 8261: 'quantify', 8262: 'generality', 8263: 'depth', 8264: 'synset:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 8265: \".synset('baleen_whale\", 8266: '.min_depth()\\n14\\n>>>', 8267: \".synset('whale\", 8268: \".02')\", 8269: '.min_depth()\\n13\\n>>>', 8270: \".synset('vertebrate\", 8271: '.min_depth()\\n8\\n>>>', 8272: \".synset('entity\", 8273: '.min_depth()\\n0\\n\\n\\n\\nsimilarity', 8274: 'measures', 8275: 'synsets\\nwhich', 8276: 'incorporate', 8277: 'insight', 8278: 'example,\\npath_similarity', 8279: 'score', 8280: '0–1', 8281: 'shortest', 8282: 'connects', 8283: 'hypernym\\nhierarchy', 8284: '(-1', 8285: 'returned', 8286: 'be\\nfound)', 8287: 'comparing', 8288: 'similarity', 8289: 'scores,', 8290: 'whale\\nto', 8291: 'orca,', 8292: 'tortoise,', 8293: '.\\nalthough', 8294: 'much,', 8295: 'decrease', 8296: 'as\\nwe', 8297: 'sea', 8298: 'creatures', 8299: 'inanimate', 8300: '.path_similarity(minke)\\n0', 8301: '.25\\n>>>', 8302: '.path_similarity(orca)\\n0', 8303: '.16666666666666666\\n>>>', 8304: '.path_similarity(tortoise)\\n0', 8305: '.07692307692307693\\n>>>', 8306: '.path_similarity(novel)\\n0', 8307: '.043478260869565216\\n\\n\\n\\n\\nnote\\nseveral', 8308: 'available;', 8309: 'help(wn)\\nfor', 8310: 'verbnet,', 8311: 'hierarhical', 8312: '.verbnet', 8313: '.\\n\\n\\n\\n\\n6\\xa0\\xa0\\xa0summary\\n\\na', 8314: 'corpora,\\ne', 8315: '.brown', 8316: 'categorized,', 8317: 'topic;', 8318: 'the\\ncategories', 8319: 'distributions,\\neach', 8320: 'frequencies,\\ngiven', 8321: 'entered', 8322: 'editor,\\nsaved', 8323: 'code,\\nand', 8324: 'necessary', 8325: 'object\\nname', 8326: 'period', 8327: '.funct(y),\\ne', 8328: '.isalpha()', 8329: 'v,\\ntype', 8330: 'help(v)', 8331: '.\\nwordnet', 8332: 'default,', 8333: \"using\\npython's\", 8334: 'reading\\nextra', 8335: 'freely\\navailable', 8336: 'howto,', 8337: '.org/howto,', 8338: 'documented', 8339: 'extensively', 8340: '.\\nsignificant', 8341: 'sources', 8342: 'published', 8343: '(ldc)', 8344: 'agency', 8345: '(elra)', 8346: 'speech\\ncorpora', 8347: 'non-commercial', 8348: 'licences', 8349: 'to\\nbe', 8350: 'licenses', 8351: 'available\\n(but', 8352: 'higher', 8353: 'fee)', 8354: 'brat,\\nand', 8355: 'http://brat', 8356: '.nlplab', 8357: 'olac', 8358: 'metadata,', 8359: 'homepage', 8360: '.language-archives', 8361: 'list\\nfor', 8362: 'discussions', 8363: 'archives\\nor', 8364: 'posting', 8365: \"world's\", 8366: 'ethnologue,', 8367: '.ethnologue', 8368: '.com/', 8369: '.\\nof', 8370: '7,000', 8371: 'dozen', 8372: 'touched', 8373: 'this\\narea', 8374: '(biber,', 8375: 'conrad,', 8376: 'reppen,', 8377: '1998),', 8378: '(mcenery,', 8379: '2006),', 8380: '(meyer,', 8381: '2002),', 8382: '(sampson', 8383: 'mccarthy,', 8384: '2005),', 8385: '(scott', 8386: 'tribble,', 8387: '2006)', 8388: '.\\nfurther', 8389: 'readings', 8390: 'quantitative', 8391: 'are:\\n(baayen,', 8392: '2008),', 8393: '(gries,', 8394: '2009),', 8395: '(woods,', 8396: 'fletcher,', 8397: 'hughes,', 8398: '1986)', 8399: 'description', 8400: '(fellbaum,', 8401: '1998)', 8402: 'research\\nin', 8403: 'psycholinguistics,', 8404: 'retrieval', 8405: '.\\nwordnets', 8406: 'documented\\nat', 8407: '.globalwordnet', 8408: 'measures,', 8409: '(budanitsky', 8410: 'hirst,', 8411: '.\\nother', 8412: 'phonetics', 8413: 'semantics,\\nand', 8414: '(jurafsky', 8415: '.\\n\\n\\n8\\xa0\\xa0\\xa0exercises\\n\\n☼', 8416: '.\\nreview', 8417: 'addition,\\nmultiplication,', 8418: 'sorting', 8419: '.\\n☼', 8420: 'have?', 8421: 'types?\\n☼', 8422: 'reader', 8423: '.words()', 8424: 'corpus\\nreader', 8425: '.webtext', 8426: 'state', 8427: 'addresses,', 8428: 'the\\nstate_union', 8429: 'men,', 8430: 'women,\\nand', 8431: 'happened', 8432: 'time?\\n☼', 8433: 'holonym-meronym', 8434: 'nouns', 8435: 'relation,\\nso', 8436: 'use:\\nmember_meronyms(),', 8437: 'part_meronyms(),', 8438: 'substance_meronyms(),\\nmember_holonyms(),', 8439: 'part_holonyms(),', 8440: 'substance_holonyms()', 8441: 'object\\ncalled', 8442: 'spanish\\nin', 8443: 'arise', 8444: 'approach?\\ncan', 8445: 'suggest', 8446: 'problem?\\n☼', 8447: 'strunk', 8448: \"white's\", 8449: 'style,\\nthe', 8450: 'sentence,\\nmeans', 8451: 'extent,', 8452: 'not\\nnevertheless', 8453: 'usage:\\nhowever', 8454: 'advise', 8455: 'him,', 8456: 'thinks', 8457: 'best', 8458: '.\\n(http://www', 8459: '.bartleby', 8460: '.com/141/strunk3', 8461: '.html)\\nuse', 8462: 'word\\nin', 8463: 'considering', 8464: '.\\nsee', 8465: 'languagelog', 8466: 'fossilized', 8467: 'prejudices', 8468: \"'however'\\nat\", 8469: 'http://itre', 8470: '.cis', 8471: '.upenn', 8472: '.edu/~myl/languagelog/archives/001913', 8473: '.html\\n◑', 8474: 'males\\nvs', 8475: '(cf', 8476: '.4)', 8477: '.\\n◑', 8478: 'them,\\nin', 8479: 'richness,', 8480: 'you\\nfind', 8481: 'the\\ntwo', 8482: 'sensibility?\\n◑', 8483: 'bbc', 8484: 'article:', 8485: \"uk's\", 8486: 'vicky', 8487: 'pollards', 8488: \"'left\", 8489: \"behind'\", 8490: 'http://news', 8491: '.bbc', 8492: '.co', 8493: '.uk/1/hi/education/6173441', 8494: '.stm', 8495: 'article', 8496: 'statistic', 8497: 'teen', 8498: 'language:\\nthe', 8499: 'used,', 8500: 'yeah,', 8501: 'no,', 8502: 'third', 8503: 'third\\nof', 8504: 'sources?', 8505: 'statistic?\\nread', 8506: 'languagelog,', 8507: '.edu/~myl/languagelog/archives/003993', 8508: 'impressionistic', 8509: 'understanding\\nof', 8510: 'closed', 8511: 'that\\nexhibit', 8512: 'genres?\\n◑', 8513: 'pronunciations\\nfor', 8514: 'contain?', 8515: 'fraction\\nof', 8516: 'pronunciation?\\n◑', 8517: 'hyponyms?\\nyou', 8518: \".all_synsets('n')\", 8519: 'supergloss(s)', 8520: 'argument\\nand', 8521: 'concatenation', 8522: 'token/type', 8523: 'ratios),', 8524: '(nltk', 8525: '.categories())', 8526: '.\\nwhich', 8527: 'lowest', 8528: '(greatest', 8529: 'type)?\\nis', 8530: 'expected?\\n◑', 8531: 'words\\nof', 8532: 'bigrams\\n(pairs', 8533: 'adjacent', 8534: 'omitting', 8535: 'genre,\\nlike', 8536: '.\\nchoose', 8537: 'presence\\n(or', 8538: 'absence)', 8539: 'findings', 8540: 'word_freq()', 8541: 'section\\nof', 8542: 'arguments,', 8543: 'syllables', 8544: 'text,\\nmaking', 8545: 'hedge(text)', 8546: \"word\\n'like'\", 8547: '.\\n★', 8548: \"zipf's\", 8549: 'law:\\nlet', 8550: 'that\\nall', 8551: 'ranked', 8552: 'frequency,\\nwith', 8553: 'law', 8554: 'the\\nfrequency', 8555: 'inversely', 8556: 'proportional', 8557: 'rank\\n(i', 8558: '×', 8559: 'constant', 8560: 'k)', 8561: '50th', 8562: 'most\\ncommon', 8563: 'the\\n150th', 8564: '.\\nwrite', 8565: 'word\\nfrequency', 8566: 'against', 8567: 'rank', 8568: 'pylab', 8569: '.plot', 8570: 'do\\nyou', 8571: 'confirm', 8572: 'law?', 8573: '(hint:', 8574: 'logarithmic', 8575: 'scale)', 8576: 'extreme', 8577: 'line?\\ngenerate', 8578: '.choice(abcdefg', 8579: '),\\ntaking', 8580: 'string\\nconcatenation', 8581: 'accumulate', 8582: '(very)\\nlong', 8583: 'tokenize', 8584: 'zipf\\nplot', 8585: \"of\\nzipf's\", 8586: 'light', 8587: 'this?\\n\\n\\n★', 8588: 'to\\ndo', 8589: 'tasks:\\nstore', 8590: 'randomly\\nchoose', 8591: '.choice()', 8592: '(you', 8593: '.)\\nselect', 8594: 'corpus,\\nor', 8595: 'translation,', 8596: 'train\\nthe', 8597: 'you\\nmay', 8598: 'intelligible\\nis', 8599: 'strengths', 8600: 'weaknesses', 8601: 'of\\ngenerating', 8602: 'experiment\\nwith', 8603: 'hybrid', 8604: 'observations', 8605: '.\\n\\n\\n★', 8606: 'find_language()', 8607: 'string\\nas', 8608: 'argument,', 8609: 'that\\nstring', 8610: 'searches\\nto', 8611: 'latin-1', 8612: 'branching', 8613: 'factor', 8614: 'hierarchy?\\ni', 8615: 'the\\nhypernym', 8616: 'average?\\nyou', 8617: 'polysemy', 8618: 'dog', 8619: 'senses\\nwith:', 8620: 'len(wn', 8621: \".synsets('dog',\", 8622: \"'n'))\", 8623: '.\\ncompute', 8624: 'nouns,', 8625: 'adjectives', 8626: 'and\\nadverbs', 8627: 'predefined', 8628: 'score\\nthe', 8629: '.\\nrank', 8630: 'close', 8631: 'ranking', 8632: 'here,\\nan', 8633: 'experimentally\\nby', 8634: '(miller', 8635: 'charles,', 8636: '1998):\\ncar-automobile,', 8637: 'gem-jewel,', 8638: 'journey-voyage,', 8639: 'boy-lad,\\ncoast-shore,', 8640: 'asylum-madhouse,', 8641: 'magician-wizard,', 8642: 'midday-noon,\\nfurnace-stove,', 8643: 'food-fruit,', 8644: 'bird-cock,', 8645: 'bird-crane,', 8646: 'tool-implement,\\nbrother-monk,', 8647: 'lad-brother,', 8648: 'crane-implement,', 8649: 'journey-car,\\nmonk-oracle,', 8650: 'cemetery-woodland,', 8651: 'food-rooster,', 8652: 'coast-hill,\\nforest-graveyard,', 8653: 'shore-woodland,', 8654: 'monk-slave,', 8655: 'coast-forest,\\nlad-wizard,', 8656: 'chord-smile,', 8657: 'glass-magician,', 8658: 'rooster-voyage,', 8659: 'noon-string', 8660: 'acstch03', 8661: '.rst2\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3\\xa0\\xa0\\xa0processing', 8662: 'text\\nthe', 8663: 'undoubtedly', 8664: 'convenient\\nto', 8665: 'explore,', 8666: 'saw\\nin', 8667: 'sources\\nin', 8668: 'mind,', 8669: 'questions:\\n\\nhow', 8670: 'and\\nfrom', 8671: 'unlimited', 8672: 'of\\nlanguage', 8673: 'material?\\nhow', 8674: 'and\\npunctuation', 8675: 'of\\nanalysis', 8676: 'chapters?\\nhow', 8677: 'output\\nand', 8678: 'file?\\n\\nin', 8679: 'covering\\nkey', 8680: 'nlp,', 8681: 'tokenization', 8682: 'stemming', 8683: 'consolidate', 8684: 'and\\nlearn', 8685: 'since\\nso', 8686: 'html', 8687: 'format,', 8688: 'also\\nsee', 8689: 'dispense', 8690: 'markup', 8691: '.\\n\\nnote\\nimportant:\\nfrom', 8692: 'onwards,', 8693: 'assume', 8694: 'you\\nbegin', 8695: 'session', 8696: 'import\\nstatements:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 8697: 'only\\n>>>', 8698: 're,', 8699: 'pprint\\n>>>', 8700: 'word_tokenize\\n\\n\\n\\n\\n\\n3', 8701: '.1\\xa0\\xa0\\xa0accessing', 8702: 'disk\\n\\nelectronic', 8703: 'books\\na', 8704: 'catalog', 8705: 'at\\nhttp://www', 8706: '.org/catalog/,', 8707: 'url', 8708: 'ascii', 8709: '90%', 8710: 'it\\nincludes', 8711: 'catalan,', 8712: 'chinese,', 8713: 'dutch,\\nfinnish,', 8714: 'french,', 8715: 'german,', 8716: 'italian,', 8717: 'portuguese', 8718: 'spanish', 8719: '(with', 8720: 'than\\n100', 8721: 'each)', 8722: '.\\ntext', 8723: '2554', 8724: 'crime', 8725: 'punishment,\\nand', 8726: 'urllib', 8727: 'request\\n>>>', 8728: '.org/files/2554/2554-0', 8729: '.txt\\n>>>', 8730: 'request', 8731: '.urlopen(url)\\n>>>', 8732: '.read()', 8733: \".decode('utf8')\\n>>>\", 8734: 'type(raw)\\n<class', 8735: \"'str'>\\n>>>\", 8736: 'len(raw)\\n1176893\\n>>>', 8737: \"raw[:75]\\n'the\", 8738: 'ebook', 8739: 'punishment,', 8740: 'fyodor', 8741: \"dostoevsky\\\\r\\\\n'\\n\\n\\n\\n\\nnote\\nthe\", 8742: 'read()', 8743: 'downloads', 8744: 'proxy', 8745: 'detected', 8746: 'python,\\nyou', 8747: 'manually,', 8748: 'urlopen,', 8749: 'proxies', 8750: \"{'http':\", 8751: \"'http://www\", 8752: '.someproxy', 8753: \".com:3128'}\\n>>>\", 8754: '.proxyhandler(proxies)\\n\\n\\n\\n\\nthe', 8755: '1,176,893', 8756: '.\\n(we', 8757: 'type(raw)', 8758: 'book,\\nincluding', 8759: 'as\\nwhitespace,', 8760: 'breaks', 8761: '\\\\r', 8762: '\\\\n\\nin', 8763: 'file,', 8764: 'the\\nspecial', 8765: 'carriage', 8766: 'feed', 8767: 'must\\nhave', 8768: 'machine)', 8769: 'language\\nprocessing,', 8770: 'into\\nwords', 8771: 'punctuation,', 8772: 'tokenization,', 8773: 'word_tokenize(raw)\\n>>>', 8774: 'type(tokens)\\n<class', 8775: \"'list'>\\n>>>\", 8776: 'len(tokens)\\n254354\\n>>>', 8777: \"tokens[:10]\\n['the',\", 8778: \"'project',\", 8779: \"'gutenberg',\", 8780: \"'ebook',\", 8781: \"'crime',\", 8782: \"'punishment',\", 8783: \"'by']\\n\\n\\n\\nnotice\", 8784: 'the\\nearlier', 8785: 'this\\nlist,', 8786: 'operations\\nlike', 8787: 'slicing:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 8788: '.text(tokens)\\n>>>', 8789: 'type(text)\\n<class', 8790: \"'nltk\", 8791: '.text', 8792: \".text'>\\n>>>\", 8793: \"text[1024:1062]\\n['chapter',\", 8794: \"'exceptionally',\", 8795: \"'evening',\", 8796: \"'in',\\n\", 8797: \"'july',\", 8798: \"'young',\", 8799: \"'man',\", 8800: \"'came',\", 8801: \"'garret',\", 8802: \"'lodged',\", 8803: \"'place',\", 8804: \"'walked',\", 8805: \"'slowly',\\n\", 8806: \"'hesitation',\", 8807: \"'towards',\", 8808: \"'k\", 8809: \"'bridge',\", 8810: '.collocations()\\nkaterina', 8811: 'ivanovna;', 8812: 'pyotr', 8813: 'petrovitch;', 8814: 'pulcheria', 8815: 'alexandrovna;', 8816: 'avdotya\\nromanovna;', 8817: 'rodion', 8818: 'romanovitch;', 8819: 'marfa', 8820: 'petrovna;', 8821: 'sofya', 8822: 'semyonovna;', 8823: 'old\\nwoman;', 8824: 'gutenberg-tm;', 8825: 'porfiry', 8826: 'amalia', 8827: 'ivanovna;\\ngreat', 8828: 'deal;', 8829: 'nikodim', 8830: 'fomitch;', 8831: 'young', 8832: 'man;', 8833: 'ilya', 8834: \"n't\", 8835: 'know;\\nproject', 8836: 'gutenberg;', 8837: 'dmitri', 8838: 'prokofitch;', 8839: 'andrey', 8840: 'semyonovitch;', 8841: 'hay', 8842: 'market\\n\\n\\n\\nnotice', 8843: 'header', 8844: 'the\\nname', 8845: 'scanned', 8846: 'and\\ncorrected', 8847: 'license,', 8848: 'information\\nappears', 8849: 'footer', 8850: 'reliably', 8851: 'detect\\nwhere', 8852: 'ends,', 8853: 'resort', 8854: 'manual\\ninspection', 8855: 'beginning\\nand', 8856: 'end,', 8857: 'trimming', 8858: 'else:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 8859: '.find(part', 8860: 'i)\\n5338\\n>>>', 8861: '.rfind(end', 8862: \"gutenberg's\", 8863: 'crime)\\n1157743\\n>>>', 8864: 'raw[5338:1157743]', 8865: 'i)\\n0\\n\\n\\n\\nthe', 8866: 'find()', 8867: 'rfind()', 8868: '(reverse', 8869: 'find)', 8870: 'get\\nthe', 8871: 'overwrite', 8872: 'slice,', 8873: 'begins\\nwith', 8874: 'including)\\nthe', 8875: 'marks', 8876: 'brush', 8877: 'reality', 8878: 'web:\\ntexts', 8879: 'unwanted', 8880: 'material,\\nand', 8881: '.\\n\\n\\ndealing', 8882: 'html\\nmuch', 8883: 'browser', 8884: 'local\\nfile,', 8885: 'easiest', 8886: 'before,\\nusing', 8887: 'urlopen', 8888: 'fun', 8889: 'story\\ncalled', 8890: 'blondes', 8891: 'die', 8892: 'urban', 8893: 'legend\\npassed', 8894: 'fact:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 8895: '.uk/2/hi/health/2284783', 8896: '.stm\\n>>>', 8897: '.urlopen(url)', 8898: \"html[:60]\\n'<!doctype\", 8899: '-//w3c//dtd', 8900: \"transitional//en'\\n\\n\\n\\nyou\", 8901: 'print(html)', 8902: 'glory,\\nincluding', 8903: 'meta', 8904: 'image', 8905: 'map,', 8906: 'javascript,', 8907: 'forms,', 8908: 'tables', 8909: 'beautifulsoup,\\navailable', 8910: '.crummy', 8911: '.com/software/beautifulsoup/:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 8912: 'bs4', 8913: 'beautifulsoup\\n>>>', 8914: 'beautifulsoup(html,', 8915: \"'html\", 8916: \".parser')\", 8917: '.get_text()\\n>>>', 8918: \"tokens\\n['bbc',\", 8919: \"'|',\", 8920: \"'health',\", 8921: \"'blondes',\", 8922: \"'to,\", 8923: \"'die',\", 8924: '.]\\n\\n\\n\\nthis', 8925: 'concerning', 8926: 'site', 8927: 'navigation', 8928: 'related\\nstories', 8929: 'the\\ncontent', 8930: 'interest,', 8931: 'initialize', 8932: 'tokens[110:390]\\n>>>', 8933: \".concordance('gene')\\ndisplaying\", 8934: 'matches:\\nhey', 8935: 'gene', 8936: 'next\\nblonde', 8937: 'hair', 8938: 'caused', 8939: 'recessive', 8940: 'child', 8941: 'blond\\nhave', 8942: 'blonde', 8943: 'g\\nere', 8944: 'disadvantage', 8945: 'chance', 8946: 'disappear\\ndes', 8947: 'disappear', 8948: 'thin\\n\\n\\n\\n\\n\\nprocessing', 8949: 'engine', 8950: 'results\\nthe', 8951: 'thought', 8952: 'unannotated', 8953: 'large\\nquantity', 8954: 'advantage\\nof', 8955: 'size:', 8956: 'of\\ndocuments,', 8957: 'you\\nare', 8958: 'furthermore,', 8959: 'specific\\npatterns,', 8960: 'smaller\\nexample,', 8961: 'run\\non', 8962: 'advantage', 8963: 'are\\nvery', 8964: 'for\\nquickly', 8965: 'checking', 8966: 'theory,', 8967: 'reasonable', 8968: '.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ngoogle', 8969: 'hits\\nadore\\nlove\\nlike\\nprefer\\n\\n\\n\\nabsolutely\\n289,000\\n905,000\\n16,200\\n644\\n\\ndefinitely\\n1,460\\n51,000\\n158,000\\n62,600\\n\\nratio\\n198:1\\n18:1\\n1:10\\n1:97\\n\\n\\ntable', 8970: 'google', 8971: 'hits', 8972: 'collocations:', 8973: 'collocations\\ninvolving', 8974: 'absolutely', 8975: 'definitely,', 8976: 'adore,', 8977: 'prefer', 8978: '.\\n(liberman,', 8979: '2005)', 8980: '.\\n\\n\\nunfortunately,', 8981: 'allowable', 8982: 'severely', 8983: 'restricted', 8984: '.\\nunlike', 8985: 'for\\narbitrarily', 8986: 'patterns,', 8987: 'generally\\nonly', 8988: 'of\\nwords,', 8989: 'wildcards', 8990: 'give\\ninconsistent', 8991: 'results,', 8992: 'figures', 8993: 'used\\nat', 8994: 'geographical', 8995: 'regions', 8996: 'been\\nduplicated', 8997: 'sites,', 8998: 'boosted', 8999: 'unpredictably,\\nbreaking', 9000: 'pattern-based', 9001: 'locating', 9002: 'problem\\nwhich', 9003: 'ameliorated', 9004: 'apis)', 9005: 'turn:\\nsearch', 9006: '(inside', 9007: 'quotes)', 9008: 'large\\ncount,', 9009: 'collocation\\nin', 9010: 'english?\\n\\n\\n\\nprocessing', 9011: 'rss', 9012: 'feeds\\n\\n\\nthe', 9013: 'blogosphere', 9014: 'registers', 9015: 'parser,\\navailable', 9016: 'https://pypi', 9017: '.org/pypi/feedparser,', 9018: 'content\\nof', 9019: 'blog,', 9020: 'feedparser\\n>>>', 9021: 'llog', 9022: 'feedparser', 9023: '.parse(http://languagelog', 9024: '.ldc', 9025: '.edu/nll/?feed=atom)\\n>>>', 9026: \"llog['feed']['title']\\n'language\", 9027: \"log'\\n>>>\", 9028: 'len(llog', 9029: '.entries)\\n15\\n>>>', 9030: 'post', 9031: '.entries[2]\\n>>>', 9032: \".title\\nhe's\", 9033: 'my', 9034: 'bf\\n>>>', 9035: '.content[0]', 9036: '.value\\n>>>', 9037: \"content[:70]\\n'<p>today\", 9038: 'chatting', 9039: \"f'\\n>>>\", 9040: 'beautifulsoup(content,', 9041: \"word_tokenize(raw)\\n['today',\", 9042: \"'was',\", 9043: \"'chatting',\", 9044: \"'visiting',\\n'graduate',\", 9045: \"'students',\", 9046: \"'prc',\", 9047: \"'thinking',\", 9048: \"'i',\\n'was',\", 9049: \"'au',\", 9050: \"'courant',\", 9051: \"'mentioned',\", 9052: \"'expression',\\n'dui4xiang4',\", 9053: \"'\\\\u5c0d\\\\u8c61',\", 9054: \"'boy',\", 9055: \"'/',\", 9056: \"'girl',\", 9057: \"'friend',\", 9058: \"'',\", 9059: '.]\\n\\n\\n\\nwith', 9060: 'work,', 9061: 'posts,\\nand', 9062: '.\\n\\n\\n\\nreading', 9063: 'files\\n\\nin', 9064: 'open()', 9065: 'function,\\nfollowed', 9066: '.txt,', 9067: \"open('document\", 9068: '.read()\\n\\n\\n\\n\\nnote\\nyour', 9069: 'turn:\\ncreate', 9070: 'plain', 9071: 'typing\\nthe', 9072: 'window,', 9073: 'as\\ndocument', 9074: 'offers', 9075: 'pop-up', 9076: 'box', 9077: \".txt'),\", 9078: 'then\\ninspect', 9079: 'print(f', 9080: '.read())', 9081: '.\\n\\nvarious', 9082: 'gone', 9083: \"couldn't\", 9084: 'an\\nerror', 9085: \".txt')\\ntraceback\", 9086: 'last):\\nfile', 9087: '<pyshell#7>,', 9088: '-toplevel-\\nf', 9089: \".txt')\\nioerror:\", 9090: '[errno', 9091: '2]', 9092: 'directory:', 9093: \"'document\", 9094: \".txt'\\n\\n\\n\\nto\", 9095: 'really', 9096: 'the\\nright', 9097: 'directory,', 9098: \"idle's\", 9099: 'menu;\\nthis', 9100: 'where\\nidle', 9101: 'alternative', 9102: 'current\\ndirectory', 9103: 'python:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 9104: 'os\\n>>>', 9105: 'os', 9106: \".listdir('\", 9107: \".')\\n\\n\\n\\nanother\", 9108: 'file\\nis', 9109: 'newline', 9110: 'conventions,', 9111: 'operating', 9112: 'controlling', 9113: 'how\\nthe', 9114: 'opened:', 9115: \"'ru')\", 9116: \"—\\n'r'\", 9117: 'default),', 9118: \"and\\n'u'\", 9119: 'universal,', 9120: 'lets', 9121: 'different\\nconventions', 9122: 'marking', 9123: 'newlines', 9124: '.\\nassuming', 9125: \".read()\\n'time\", 9126: 'flies', 9127: '.\\\\nfruit', 9128: 'banana', 9129: \".\\\\n'\\n\\n\\n\\nrecall\", 9130: \"'\\\\n'\", 9131: 'newlines;', 9132: 'this\\nis', 9133: 'keyboard', 9134: 'loop:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 9135: \"'ru')\\n>>>\", 9136: 'f:\\n', 9137: 'print(line', 9138: '.strip())\\ntime', 9139: '.\\nfruit', 9140: '.\\n\\n\\n\\nhere', 9141: 'strip()', 9142: 'simply\\nhave', 9143: '.data', 9144: '.find()', 9145: '.\\nthen', 9146: 'demonstrated', 9147: 'above:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 9148: \".find('corpora/gutenberg/melville-moby_dick\", 9149: 'open(path,', 9150: '.read()\\n\\n\\n\\n\\n\\nextracting', 9151: 'pdf,', 9152: 'msword', 9153: 'binary', 9154: 'formats\\nascii', 9155: 'formats', 9156: 'binary\\nformats', 9157: 'pdf', 9158: 'opened', 9159: 'specialized\\nsoftware', 9160: 'third-party', 9161: 'libraries', 9162: 'pypdf', 9163: 'pywin32\\nprovide', 9164: 'to\\nthese', 9165: 'multi-column', 9166: 'particularly\\nchallenging', 9167: 'once-off', 9168: 'conversion', 9169: 'documents,\\nit', 9170: 'text\\nto', 9171: 'drive,', 9172: \"google's\", 9173: 'document,\\nwhich', 9174: '.\\n\\n\\ncapturing', 9175: 'input\\nsometimes', 9176: 'capture', 9177: 'inputs', 9178: 'she', 9179: 'is\\ninteracting', 9180: 'user\\nto', 9181: 'input()', 9182: 'can\\nmanipulate', 9183: 'input(enter', 9184: ')\\nenter', 9185: 'exceptionally', 9186: 'hot', 9187: 'evening', 9188: 'july\\n>>>', 9189: 'print(you', 9190: 'typed,', 9191: 'len(word_tokenize(s)),', 9192: '.)\\nyou', 9193: '.\\n\\n\\n\\n\\n\\nthe', 9194: 'pipeline\\n3', 9195: 'section,', 9196: 'process\\nof', 9197: 'step,', 9198: 'normalization,\\nwill', 9199: 'discussed', 9200: '.)\\n\\n\\nfigure', 9201: 'pipeline:', 9202: 'content,\\nremove', 9203: 'characters;\\nthis', 9204: 'tokenized', 9205: 'converted', 9206: 'object;\\nwe', 9207: \".\\n\\nthere's\", 9208: 'properly,', 9209: 'be\\nclear', 9210: 'mentions', 9211: 'type\\nof', 9212: 'type(x),', 9213: 'type(1)', 9214: '<int>\\nsince', 9215: 'integer', 9216: 'strip', 9217: 'markup,\\nwe', 9218: 'dealing', 9219: '<str>', 9220: '.2):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 9221: \".txt')\", 9222: '.read()\\n>>>', 9223: \"'str'>\\n\\n\\n\\nwhen\", 9224: '(of', 9225: 'words),', 9226: '<list>\\ntype', 9227: 'normalizing', 9228: 'lists:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 9229: 'tokens]\\n>>>', 9230: 'type(words)\\n<class', 9231: 'sorted(set(words))\\n>>>', 9232: 'type(vocab)\\n<class', 9233: \"'list'>\\n\\n\\n\\nthe\", 9234: '.\\nso,', 9235: 'append', 9236: 'string:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 9237: \".append('blog')\\n>>>\", 9238: \".append('blog')\\ntraceback\", 9239: '<module>\\nattributeerror:', 9240: \"'str'\", 9241: 'attribute', 9242: \"'append'\\n\\n\\n\\n\\nsimilarly,\", 9243: 'concatenate', 9244: 'with\\nlists,', 9245: 'query', 9246: \"'who\", 9247: \"knows?'\\n>>>\", 9248: 'beatles', 9249: \"['john',\", 9250: \"'paul',\", 9251: \"'george',\", 9252: \"'ringo']\\n>>>\", 9253: 'beatles\\ntraceback', 9254: '<module>\\ntypeerror:', 9255: \"'list'\", 9256: 'objects\\n\\n\\n\\n\\n\\n\\n3', 9257: '.2\\xa0\\xa0\\xa0strings:', 9258: \"level\\nit's\", 9259: 'studiously', 9260: 'avoiding\\nso', 9261: 'far', 9262: 'focused', 9263: \"didn't\\nlook\", 9264: 'interface', 9265: 'ignore\\nthe', 9266: 'and\\nof', 9267: 'fundamental\\ndata', 9268: 'strings\\nin', 9269: 'detail,', 9270: 'connection', 9271: '.\\n\\nbasic', 9272: 'strings\\nstrings', 9273: '\\nor', 9274: 'double', 9275: 'quote,', 9276: 'backslash-escape\\nthe', 9277: 'quote', 9278: 'literal', 9279: 'intended,\\nor', 9280: '.\\notherwise,', 9281: '\\nwill', 9282: 'interpreted', 9283: 'interpreter\\nwill', 9284: 'report', 9285: \"monty\\n'monty\", 9286: 'circus', 9287: 'flying', 9288: 'circus\\nmonty', 9289: 'circus\\n>>>', 9290: \"python\\\\'s\", 9291: \"circus'\", 9292: \"circus'\\n\", 9293: 'syntax\\n\\n\\n\\nsometimes', 9294: 'various\\nways', 9295: 'is\\njoined', 9296: 'backslash', 9297: 'couplet', 9298: 'thee', 9299: \"summer's\", 9300: 'day?\\\\\\n', 9301: 'thou', 9302: 'temperate:', 9303: 'print(couplet)\\nshall', 9304: 'day?thou', 9305: 'temperate:\\n>>>', 9306: '(rough', 9307: 'winds', 9308: 'shake', 9309: 'darling', 9310: 'buds', 9311: 'may,\\n', 9312: 'lease', 9313: 'hath', 9314: 'date:)', 9315: 'print(couplet)\\nrough', 9316: 'may,and', 9317: 'date:\\n\\n\\n\\nunfortunately', 9318: 'between\\nthe', 9319: 'sonnet', 9320: 'triple-quoted\\nstring', 9321: 'day?\\n', 9322: 'day?\\nthou', 9323: \"'''rough\", 9324: \"date:'''\\n>>>\", 9325: 'may,\\nand', 9326: 'date:\\n\\n\\n\\nnow', 9327: '.\\nfirst', 9328: 'operation,', 9329: 'pasted', 9330: 'that\\nconcatenation', 9331: 'multiply', 9332: \"'very'\", 9333: \"\\n'veryveryvery'\\n>>>\", 9334: \"\\n'veryveryvery'\\n\\n\\n\\n\\nnote\\nyour\", 9335: '.\\nbe', 9336: 'which\\nis', 9337: 'whitespace', 9338: 'character,', 9339: '[1,', 9340: '1]\\n>>>', 9341: \"['\", 9342: '(7', 9343: 'i)', 9344: 'a]\\n>>>', 9345: 'b:\\n', 9346: \"print(line)\\n\\n\\n\\n\\n\\nwe've\", 9347: 'to\\nstrings,', 9348: 'use\\nsubtraction', 9349: \"'y'\\ntraceback\", 9350: 'unsupported', 9351: 'operand', 9352: 'type(s)', 9353: '-:', 9354: \"'str'\\n>>>\", 9355: '2\\ntraceback', 9356: '/:', 9357: \"'int'\\n\\n\\n\\nthese\", 9358: 'messages', 9359: 'we\\nhave', 9360: 'muddle', 9361: 'told\\nthat', 9362: 'subtraction', 9363: '-)', 9364: 'to\\nobjects', 9365: 'str', 9366: '(strings),', 9367: 'told', 9368: 'that\\ndivision', 9369: 'int', 9370: 'operands', 9371: '.\\n\\n\\nprinting', 9372: 'strings\\nso', 9373: 'or\\nsee', 9374: 'calculation,', 9375: 'name\\ninto', 9376: 'variable\\nusing', 9377: 'statement:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 9378: 'print(monty)\\nmonty', 9379: 'python\\n\\n\\n\\nnotice', 9380: 'quotation', 9381: 'inspect\\na', 9382: 'prints\\nthe', 9383: 'string,\\nthe', 9384: 'the\\ninterpreter', 9385: 'see\\nquotation', 9386: 'line\\nin', 9387: \"'holy\", 9388: \"grail'\\n>>>\", 9389: 'print(monty', 9390: 'grail)\\nmonty', 9391: 'pythonholy', 9392: 'grail\\n>>>', 9393: 'print(monty,', 9394: 'grail\\n\\n\\n\\n\\n\\naccessing', 9395: 'characters\\nas', 9396: 'lists,', 9397: 'indexed,', 9398: 'letters)', 9399: \"monty[0]\\n'm'\\n>>>\", 9400: \"monty[3]\\n't'\\n>>>\", 9401: \"monty[5]\\n'\", 9402: \"'\\n\\n\\n\\nas\", 9403: 'monty[20]\\ntraceback', 9404: 'range\\n\\n\\n\\n\\nagain', 9405: 'negative', 9406: 'strings,\\nwhere', 9407: '-1', 9408: '.\\npositive', 9409: 'to\\nany', 9410: '12,\\nindexes', 9411: '-7', 9412: 'space)', 9413: '.\\n(notice', 9414: 'len(monty)', 9415: 'monty[-1]', 9416: \"\\n'n'\\n>>>\", 9417: \"'\\n>>>\", 9418: \"monty[-7]\\n'\", 9419: \"'\\n\\n\\n\\nwe\", 9420: 'loops', 9421: 'iterate', 9422: 'characters\\nin', 9423: \"'\\nparameter,\", 9424: \"'colorless\", 9425: 'ideas', 9426: 'sleep', 9427: \"furiously'\\n>>>\", 9428: 'sent:\\n', 9429: 'print(char,', 9430: '.\\nc', 9431: 'o', 9432: 'd', 9433: 'y\\n\\n\\n\\nwe', 9434: 'case\\ndistinction', 9435: 'lowercase,', 9436: 'non-alphabetic', 9437: 'characters:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 9438: \".raw('melville-moby_dick\", 9439: '.freqdist(ch', 9440: 'ch', 9441: '.isalpha())\\n>>>', 9442: \".most_common(5)\\n[('e',\", 9443: '117092),', 9444: \"('t',\", 9445: '87996),', 9446: \"('a',\", 9447: '77916),', 9448: \"('o',\", 9449: '69326),', 9450: \"('n',\", 9451: '65617)]\\n>>>', 9452: '(char,', 9453: 'count)', 9454: \".most_common()]\\n['e',\", 9455: \"'n',\", 9456: \"'h',\", 9457: \"'d',\", 9458: \"'u',\", 9459: \"'c',\", 9460: \"'w',\\n'f',\", 9461: \"'p',\", 9462: \"'y',\", 9463: \"'v',\", 9464: \"'q',\", 9465: \"'j',\", 9466: \"'x',\", 9467: \"'z']\\n\\n\\n\\n\\n\\n\\n[sb]explain\", 9468: 'tuple', 9469: 'unpacking', 9470: 'somewhere?\\n\\n\\nthis', 9471: 'alphabet,', 9472: 'letters\\nlisted', 9473: 'complicated', 9474: 'below)', 9475: 'visualize', 9476: 'relative', 9477: 'identifying\\nthe', 9478: '.\\n\\n\\naccessing', 9479: 'substrings\\n\\n\\nfigure', 9480: 'slicing:', 9481: 'positive', 9482: 'and\\nnegative', 9483: 'indexes;', 9484: 'substrings', 9485: '[m,n]', 9486: 'continuous', 9487: 'pull', 9488: 'for\\nfurther', 9489: 'notation\\nwe', 9490: '.2)', 9491: '6,\\nup', 9492: 'including)', 9493: '10:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 9494: \"monty[6:10]\\n'pyth'\\n\\n\\n\\nhere\", 9495: \"'h'\", 9496: 'correspond\\nto', 9497: 'monty[6]', 9498: 'monty[9]', 9499: 'monty[10]', 9500: 'because\\na', 9501: 'finishes', 9502: 'rule', 9503: 'starting\\nfrom', 9504: 'stopping', 9505: 'applies;\\nhere', 9506: 'stop', 9507: \"monty[-12:-7]\\n'monty'\\n\\n\\n\\nas\", 9508: 'slices,', 9509: 'value,', 9510: 'start\\nof', 9511: 'end\\nof', 9512: \"monty[:5]\\n'monty'\\n>>>\", 9513: \"monty[6:]\\n'python'\\n\\n\\n\\nwe\", 9514: 'operator,', 9515: \"'and\", 9516: \"different'\\n>>>\", 9517: \"'thing'\", 9518: 'phrase:\\n', 9519: \"print('found\", 9520: \"thing')\\nfound\", 9521: 'thing\\n\\n\\n\\nwe', 9522: 'find():\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 9523: \".find('python')\\n6\\n\\n\\n\\n\\nnote\\nyour\", 9524: 'obviously\\nnot', 9525: 'text!)\\n\\n\\n\\nmore', 9526: 'strings\\npython', 9527: 'summary,', 9528: 'operations\\nwe', 9529: 'yet,', 9530: 'type\\nhelp(str)', 9531: '.\\n\\n\\n\\n\\n\\n\\nmethod\\nfunctionality\\n\\n\\n\\ns', 9532: '.find(t)\\nindex', 9533: 'found)\\n\\ns', 9534: '.rfind(t)\\nindex', 9535: '.index(t)\\nlike', 9536: '.find(t)', 9537: 'raises', 9538: 'valueerror', 9539: 'found\\n\\ns', 9540: '.rindex(t)\\nlike', 9541: '.rfind(t)', 9542: '.join(text)\\ncombine', 9543: 'glue\\n\\ns', 9544: '.split(t)\\nsplit', 9545: 'wherever', 9546: '(whitespace', 9547: 'default)\\n\\ns', 9548: '.splitlines()\\nsplit', 9549: 'line\\n\\ns', 9550: '.lower()\\na', 9551: '.upper()\\nan', 9552: 'uppercased', 9553: '.title()\\na', 9554: 'titlecased', 9555: '.strip()\\na', 9556: 'trailing', 9557: 'whitespace\\n\\ns', 9558: '.replace(t,', 9559: 'u)\\nreplace', 9560: 'instances', 9561: 's\\n\\n\\ntable', 9562: 'methods:', 9563: 'tests\\nshown', 9564: '.2;', 9565: 'list\\n\\n\\n\\n\\nthe', 9566: 'them\\napart', 9567: 'together\\nby', 9568: 'concatenating', 9569: \"query[2]\\n'o'\\n>>>\", 9570: \"beatles[2]\\n'george'\\n>>>\", 9571: \"query[:2]\\n'wh'\\n>>>\", 9572: \"beatles[:2]\\n['john',\", 9573: \"'paul']\\n>>>\", 9574: \"don't\\nwho\", 9575: 'knows?', 9576: \"don't\\n>>>\", 9577: \"'brian'\\ntraceback\", 9578: 'str)', 9579: 'list\\n>>>', 9580: \"['brian']\\n['john',\", 9581: \"'ringo',\", 9582: \"'brian']\\n\\n\\n\\nwhen\", 9583: 'file\\nfor', 9584: 'string\\ncorresponding', 9585: 'to\\nprocess', 9586: 'the\\nindividual', 9587: 'the\\ngranularity', 9588: 'big', 9589: 'or\\nsmall', 9590: 'like:', 9591: 'paragraphs,', 9592: 'sentences,\\nphrases,', 9593: 'we\\ncan', 9594: 'contain,', 9595: 'and\\ncorrespondingly', 9596: 'downstream', 9597: '.\\nconsequently,', 9598: 'nlp\\ncode', 9599: '(3', 9600: '.7)', 9601: '.\\nconversely,', 9602: 'terminal,\\nwe', 9603: '.9)', 9604: '.\\nlists', 9605: 'power', 9606: 'elements:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 9607: 'beatles[0]', 9608: 'john', 9609: 'lennon\\n>>>', 9610: 'del', 9611: 'beatles[-1]\\n>>>', 9612: \"beatles\\n['john\", 9613: \"lennon',\", 9614: \"'george']\\n\\n\\n\\non\", 9615: 'hand', 9616: 'string\\n—', 9617: '0th', 9618: \"'f'\", 9619: 'get:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 9620: 'query[0]', 9621: \"'f'\\ntraceback\", 9622: '?\\ntypeerror:', 9623: 'assignment\\n\\n\\n\\n', 9624: 'immutable', 9625: 'a\\nstring', 9626: 'mutable,\\nand', 9627: 'modified', 9628: 'lists\\nsupport', 9629: 'turn:\\nconsolidate', 9630: 'on\\nstrings', 9631: '.\\n\\n\\n\\n\\n3', 9632: '.3\\xa0\\xa0\\xa0text', 9633: 'unicode\\nour', 9634: 'deal', 9635: 'and\\ndifferent', 9636: 'english-speaking', 9637: 'ascii,\\npossibly', 9638: 'realizing', 9639: 'europe', 9640: 'use\\none', 9641: 'extended', 9642: 'latin', 9643: 'ø', 9644: 'danish', 9645: 'norwegian,', 9646: 'ő', 9647: 'hungarian,\\nñ', 9648: 'breton,', 9649: 'ň', 9650: 'czech', 9651: 'and\\nslovak', 9652: 'overview', 9653: 'use\\nunicode', 9654: 'non-ascii', 9655: '.\\n\\nwhat', 9656: 'unicode?\\nunicode', 9657: 'each\\ncharacter', 9658: 'code\\npoints', 9659: '\\\\uxxxx,', 9660: 'xxxx', 9661: 'number\\nin', 9662: '4-digit', 9663: 'hexadecimal', 9664: 'unicode', 9665: 'normal', 9666: 'terminal,\\nthey', 9667: 'encoded', 9668: 'bytes', 9669: '(such\\nas', 9670: 'latin-2)', 9671: 'byte', 9672: 'point,', 9673: 'a\\nsmall', 9674: 'subset', 9675: 'unicode,', 9676: 'encodings\\n(such', 9677: 'utf-8)', 9678: 'of\\nunicode', 9679: 'encoding,', 9680: 'some\\nmechanism', 9681: 'into\\nunicode', 9682: 'decoding', 9683: 'conversely,', 9684: 'a\\nfile', 9685: 'terminal,', 9686: 'suitable\\nencoding', 9687: 'encoding,\\nand', 9688: 'encoding\\n\\nfrom', 9689: 'perspective,', 9690: 'entities', 9691: 'glyphs', 9692: 'a\\nscreen', 9693: 'paper', 9694: 'font', 9695: '.\\n\\n\\nextracting', 9696: \"files\\nlet's\", 9697: 'it\\nis', 9698: 'polish-lat2', 9699: 'suggests,', 9700: 'snippet', 9701: 'polish', 9702: '(from', 9703: 'wikipedia;', 9704: 'see\\nhttp://pl', 9705: '.org/wiki/biblioteka_pruska)', 9706: 'latin-2,\\nalso', 9707: 'iso-8859-2', 9708: 'locates', 9709: 'the\\nfile', 9710: \".find('corpora/unicode_samples/polish-lat2\", 9711: \".txt')\\n\\n\\n\\nthe\", 9712: 'data\\ninto', 9713: 'encoded\\nform', 9714: 'to\\nspecify', 9715: 'open\\nour', 9716: 'file\\nwith', 9717: \"'latin2'\", 9718: \"encoding='latin2')\\n>>>\", 9719: '.strip()\\n', 9720: 'print(line)\\npruska', 9721: 'biblioteka', 9722: 'państwowa', 9723: 'jej', 9724: 'dawne', 9725: 'zbiory', 9726: 'znane', 9727: 'pod', 9728: 'nazwą\\nberlinka', 9729: 'skarb', 9730: 'kultury', 9731: 'sztuki', 9732: 'niemieckiej', 9733: 'przewiezione', 9734: 'przez\\nniemców', 9735: 'koniec', 9736: 'ii', 9737: 'wojny', 9738: 'światowej', 9739: 'dolny', 9740: 'śląsk,', 9741: 'zostały\\nodnalezione', 9742: 'po', 9743: '1945', 9744: 'terytorium', 9745: 'polski', 9746: 'trafiły', 9747: 'biblioteki\\njagiellońskiej', 9748: 'krakowie,', 9749: 'obejmują', 9750: 'ponad', 9751: 'tys', 9752: 'zabytkowych\\narchiwaliów,', 9753: '.in', 9754: 'manuskrypty', 9755: 'goethego,', 9756: 'mozarta,', 9757: 'beethovena,', 9758: 'bacha', 9759: '.\\n\\n\\n\\nif', 9760: 'underlying', 9761: 'codepoints)', 9762: 'characters,\\nthen', 9763: 'two-digit', 9764: '\\\\xxx\\nand', 9765: 'four-digit', 9766: '\\\\uxxxx', 9767: 'representations:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 9768: \".encode('unicode_escape'))\\nb'pruska\", 9769: 'pa\\\\\\\\u0144stwowa', 9770: \"nazw\\\\\\\\u0105'\\nb'berlinka\", 9771: \"przez'\\nb'niemc\\\\\\\\xf3w\", 9772: '\\\\\\\\u015bwiatowej', 9773: '\\\\\\\\u015al\\\\\\\\u0105sk,', 9774: \"zosta\\\\\\\\u0142y'\\nb'odnalezione\", 9775: 'trafi\\\\\\\\u0142y', 9776: \"biblioteki'\\nb'jagiello\\\\\\\\u0144skiej\", 9777: 'obejmuj\\\\\\\\u0105', 9778: \"zabytkowych'\\nb'archiwali\\\\\\\\xf3w,\", 9779: \".'\\n\\n\\n\\nthe\", 9780: 'escape', 9781: 'string\\npreceded', 9782: '\\\\u', 9783: '\\\\u0144', 9784: 'relevant\\nunicode', 9785: 'dislayed', 9786: 'screen', 9787: 'glyph\\nń', 9788: 'see\\n\\\\xf3,', 9789: 'corresponds', 9790: 'glyph', 9791: 'ó,', 9792: 'the\\n128-255', 9793: 'utf-8', 9794: 'can\\ninclude', 9795: 'editor\\nthat', 9796: '.\\narbitrary', 9797: 'the\\n\\\\uxxxx', 9798: 'ordinal', 9799: 'ord()', 9800: 'example:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 9801: \"ord('ń')\\n324\\n\\n\\n\\nthe\", 9802: 'digit', 9803: '324', 9804: '0144', 9805: '(type', 9806: 'hex(324)', 9807: 'this),\\nand', 9808: 'appropriate', 9809: 'nacute', 9810: \"'\\\\u0144'\\n>>>\", 9811: \"nacute\\n'ń'\\n\\n\\n\\n\\nnote\\nthere\", 9812: 'factors', 9813: 'determining', 9814: 'rendered\\non', 9815: 'encoding,\\nbut', 9816: 'failing', 9817: 'you\\nexpected,', 9818: 'fonts\\ninstalled', 9819: 'configure', 9820: 'locale\\nto', 9821: 'render', 9822: 'print(nacute', 9823: \".encode('utf8'))\\nin\", 9824: 'ń', 9825: 'terminal', 9826: 'inside\\na', 9827: \".encode('utf8')\\nb'\\\\xc5\\\\x84'\\n\\n\\n\\nthe\", 9828: 'unicodedata', 9829: 'unicode\\ncharacters', 9830: 'the\\nthird', 9831: 'their\\nutf-8', 9832: 'sequence,', 9833: 'the\\nstandard', 9834: 'prefixing', 9835: 'hex', 9836: 'with\\nu+),', 9837: 'unicodedata\\n>>>', 9838: \"encoding='latin2')\", 9839: '.readlines()\\n>>>', 9840: 'lines[2]\\n>>>', 9841: \".encode('unicode_escape'))\\nb'niemc\\\\\\\\xf3w\", 9842: \"zosta\\\\\\\\u0142y\\\\\\\\n'\\n>>>\", 9843: 'line:', 9844: 'ord(c)', 9845: '127:\\n', 9846: \"print('{}\", 9847: 'u+{:04x}', 9848: \"{}'\", 9849: '.format(c', 9850: \".encode('utf8'),\", 9851: 'ord(c),', 9852: \".name(c)))\\nb'\\\\xc3\\\\xb3'\", 9853: 'u+00f3', 9854: \"acute\\nb'\\\\xc5\\\\x9b'\", 9855: 'u+015b', 9856: \"acute\\nb'\\\\xc5\\\\x9a'\", 9857: 'u+015a', 9858: \"acute\\nb'\\\\xc4\\\\x85'\", 9859: 'u+0105', 9860: \"ogonek\\nb'\\\\xc5\\\\x82'\", 9861: 'u+0142', 9862: 'stroke\\n\\n\\n\\nif', 9863: 'replace\\nc', 9864: \".encode('utf8')\", 9865: 'c,', 9866: 'utf-8,\\nyou', 9867: 'following:\\n\\nó', 9868: 'acute\\nś', 9869: 'acute\\ną', 9870: 'ogonek\\nł', 9871: 'stroke\\n\\nalternatively,', 9872: \"'utf8'\", 9873: 'the\\nexample', 9874: \"'latin2',\", 9875: 're\\nmodule', 9876: 're', 9877: '\\\\w', 9878: 'word\\ncharacter,', 9879: 'cf', 9880: \".find('zosta\\\\u0142y')\\n54\\n>>>\", 9881: '.lower()\\n>>>', 9882: \"line\\n'niemców\", 9883: \"zostały\\\\n'\\n>>>\", 9884: \".encode('unicode_escape')\\nb'niemc\\\\\\\\xf3w\", 9885: '\\\\\\\\u015bl\\\\\\\\u0105sk,', 9886: 're\\n>>>', 9887: \".search('\\\\u015b\\\\w*',\", 9888: 'line)\\n>>>', 9889: \".group()\\n'\\\\u015bwiatowej'\\n\\n\\n\\nnltk\", 9890: 'tokenizers', 9891: \"word_tokenize(line)\\n['niemców',\", 9892: \"'pod',\", 9893: \"'koniec',\", 9894: \"'ii',\", 9895: \"'wojny',\", 9896: \"'światowej',\", 9897: \"'na',\", 9898: \"'dolny',\", 9899: \"'śląsk',\", 9900: \"'zostały']\\n\\n\\n\\n\\n\\nusing\", 9901: 'python\\nif', 9902: 'local\\nencoding,', 9903: 'methods\\nfor', 9904: 'inputting', 9905: 'editing', 9906: 'this,\\nyou', 9907: \"'#\", 9908: '-*-', 9909: 'coding:', 9910: '<coding>', 9911: \"-*-'\", 9912: 'the\\nfirst', 9913: \"'latin-1',\", 9914: \"'big5'\", 9915: \"'utf-8'\", 9916: 'idle:', 9917: 'literals', 9918: 'editor;\\nthis', 9919: 'requires', 9920: 'preferences;\\nhere', 9921: 'courier', 9922: 'ce', 9923: 'use\\nencoded', 9924: '.4\\xa0\\xa0\\xa0regular', 9925: 'detecting', 9926: 'patterns\\nmany', 9927: 'matching', 9928: 'ed', 9929: \"using\\nendswith('ed')\", 9930: 'tests\\nin', 9931: '.\\nregular', 9932: 'flexible\\nmethod', 9933: 'describing', 9934: '.\\n\\nnote\\nthere', 9935: 'introductions', 9936: 'expressions,\\norganized', 9937: 'searching\\ntext', 9938: 'expressions\\nat', 9939: 'stages', 9940: 'adopt\\na', 9941: 'problem-based', 9942: 'are\\nneeded', 9943: 'mark\\nregular', 9944: 'chevrons', 9945: '«patt»', 9946: '.\\n\\nto', 9947: 're\\nlibrary', 9948: 'using:', 9949: \"search;\\nwe'll\", 9950: '(4)', 9951: 'preprocess', 9952: \".words('en')\", 9953: '.islower()]\\n\\n\\n\\n\\nusing', 9954: \"meta-characters\\nlet's\", 9955: '«ed$»', 9956: '.search(p,', 9957: 's)', 9958: 'found\\nsomewhere', 9959: 'dollar', 9960: 'a\\nspecial', 9961: 'matches\\nthe', 9962: \".search('ed$',\", 9963: \"w)]\\n['abaissed',\", 9964: \"'abandoned',\", 9965: \"'abased',\", 9966: \"'abashed',\", 9967: \"'abatised',\", 9968: \"'abed',\", 9969: \"'aborted',\", 9970: 'wildcard', 9971: 'symbol', 9972: '.\\nsuppose', 9973: 'crossword', 9974: '8-letter', 9975: 'word\\nwith', 9976: 'j', 9977: 'sixth', 9978: 'period:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 9979: \".search('^\", 9980: '.j', 9981: '.t', 9982: \".$',\", 9983: \"w)]\\n['abjectly',\", 9984: \"'adjuster',\", 9985: \"'dejected',\", 9986: \"'dejectly',\", 9987: \"'injector',\", 9988: \"'majestic',\", 9989: '.]\\n\\n\\n\\n\\nnote\\nyour', 9990: 'turn:\\nthe', 9991: 'caret', 9992: '^', 9993: '$', 9994: 'out\\nboth', 9995: 'these,', 9996: '«', 9997: '.»?\\n\\nfinally,', 9998: 'specifies', 9999: '«^e-?mail$»', 10000: 'email', 10001: 'e-mail', 10002: 'spelling)\\nin', 10003: 'sum(1', 10004: \".search('^e-?mail$',\", 10005: 'w))', 10006: '.\\n\\n\\nranges', 10007: 'closures\\n\\n\\nfigure', 10008: '.5:', 10009: 't9:', 10010: 'keys\\n\\nthe', 10011: 't9', 10012: 'mobile', 10013: '.5)', 10014: 'that\\nare', 10015: 'keystrokes', 10016: 'textonyms', 10017: 'hole', 10018: 'golf', 10019: 'pressing\\nthe', 10020: '4653', 10021: 'words\\ncould', 10022: 'sequence?', 10023: 'expression\\n«^[ghi][mno][jlk][def]$»:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 10024: \".search('^[ghi][mno][jlk][def]$',\", 10025: \"w)]\\n['gold',\", 10026: \"'golf',\", 10027: \"'hold',\", 10028: \"'hole']\\n\\n\\n\\nthe\", 10029: '«^[ghi]»,', 10030: 'g,', 10031: 'h,', 10032: 'expression,\\n«[mno]»,', 10033: 'constrains', 10034: 'be\\nm,', 10035: 'n,', 10036: 'fourth', 10037: 'constrained', 10038: '.\\nonly', 10039: 'significant,', 10040: 'we\\ncould', 10041: '«^[hig][nom][ljk][fed]$»', 10042: 'matched', 10043: 'same\\nwords', 10044: 'turn:\\nlook', 10045: 'finger-twisters,', 10046: 'part\\nof', 10047: 'number-pad', 10048: '«^[ghijklmno]+$»,', 10049: 'or\\nmore', 10050: 'concisely,', 10051: '«^[g-o]+$»,', 10052: 'words\\nthat', 10053: 'center', 10054: 'row,', 10055: '«^[a-fj-o]+$»\\nwill', 10056: 'top-right', 10057: 'corner', 10058: \"mean?\\n\\nlet's\", 10059: 'bit', 10060: 'to\\nindividual', 10061: 'letters:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 10062: 'chat_words', 10063: '.words()))\\n>>>', 10064: \".search('^m+i+n+e+$',\", 10065: \"w)]\\n['miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee',\", 10066: \"'miiiiiinnnnnnnnnneeeeeeee',\", 10067: \"'mine',\\n'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee']\\n>>>\", 10068: \".search('^[ha]+$',\", 10069: \"w)]\\n['a',\", 10070: \"'aaaaaaaaaaaaaaaaa',\", 10071: \"'ah',\", 10072: \"'ahah',\", 10073: \"'ahahah',\", 10074: \"'ahh',\\n'ahhahahaha',\", 10075: \"'ahhh',\", 10076: \"'ahhhh',\", 10077: \"'ahhhhhh',\", 10078: \"'ahhhhhhhhhhhhhh',\", 10079: \"'ha',\", 10080: \"'haaa',\\n'hah',\", 10081: \"'haha',\", 10082: \"'hahaaa',\", 10083: \"'hahah',\", 10084: \"'hahaha',\", 10085: \"'hahahaa',\", 10086: \"'hahahah',\", 10087: \"'hahahaha',\", 10088: 'item,\\nwhich', 10089: 'm,', 10090: '[fed]', 10091: '[d-f]', 10092: '*,', 10093: '«^m*i*n*e*$»', 10094: 'using\\n«^m+i+n+e+$»,', 10095: 'all,\\ne', 10096: 'me,', 10097: 'min,', 10098: 'mmmmm', 10099: 'referred', 10100: 'kleene', 10101: 'closures,\\nor', 10102: 'closures', 10103: 'first\\ncharacter', 10104: 'for\\nexample', 10105: '«[^aeiouaeiou]»', 10106: 'vowel', 10107: 'non-vowel\\ncharacters', 10108: '«^[^aeiouaeiou]+$»', 10109: 'these:\\n:):):),', 10110: 'grrr,', 10111: 'cyb3r', 10112: 'zzzzzzzz', 10113: 'includes\\nnon-alphabetic', 10114: 'pattern,', 10115: 'illustrating', 10116: 'symbols:\\n\\\\,', 10117: '{},', 10118: '(),', 10119: '|:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 10120: 'wsj', 10121: 'sorted(set(nltk', 10122: '.treebank', 10123: \".search('^[0-9]+\\\\\", 10124: \".[0-9]+$',\", 10125: \"w)]\\n['0\", 10126: \".0085',\", 10127: \"'0\", 10128: \".05',\", 10129: \".1',\", 10130: \".16',\", 10131: \".2',\", 10132: \".25',\", 10133: \".28',\", 10134: \".3',\", 10135: \".4',\", 10136: \".5',\\n'0\", 10137: \".50',\", 10138: \".54',\", 10139: \".56',\", 10140: \".60',\", 10141: \".7',\", 10142: \".82',\", 10143: \".84',\", 10144: \".9',\", 10145: \".95',\", 10146: \".99',\\n'1\", 10147: \"'1\", 10148: \".125',\", 10149: \".14',\", 10150: \".1650',\", 10151: \".17',\", 10152: \".18',\", 10153: \".19',\", 10154: \".search('^[a-z]+\\\\$$',\", 10155: \"w)]\\n['c$',\", 10156: \"'us$']\\n>>>\", 10157: \".search('^[0-9]{4}$',\", 10158: \"w)]\\n['1614',\", 10159: \"'1637',\", 10160: \"'1787',\", 10161: \"'1901',\", 10162: \"'1903',\", 10163: \"'1917',\", 10164: \"'1925',\", 10165: \"'1929',\", 10166: \"'1933',\", 10167: \".search('^[0-9]+-[a-z]{3,5}$',\", 10168: \"w)]\\n['10-day',\", 10169: \"'10-lap',\", 10170: \"'10-year',\", 10171: \"'100-share',\", 10172: \"'12-point',\", 10173: \"'12-year',\", 10174: \".search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$',\", 10175: \"w)]\\n['black-and-white',\", 10176: \"'bread-and-butter',\", 10177: \"'father-in-law',\", 10178: \"'machine-gun-toting',\\n'savings-and-loan']\\n>>>\", 10179: \".search('(ed|ing)$',\", 10180: \"w)]\\n['62%-owned',\", 10181: \"'absorbed',\", 10182: \"'according',\", 10183: \"'adopting',\", 10184: \"'advanced',\", 10185: \"'advancing',\", 10186: 'turn:\\nstudy', 10187: 'the\\n\\\\,', 10188: 'before\\nyou', 10189: 'is\\ndeprived', 10190: 'powers', 10191: 'special,', 10192: '\\\\', 10193: 'braced', 10194: '{3,5},', 10195: 'repeats', 10196: '.\\nparentheses', 10197: 'operator:', 10198: 'disjunction)', 10199: '«w(i|e|ai|oo)t»,', 10200: 'wit,\\nwet,', 10201: 'wait,', 10202: 'woot', 10203: 'instructive', 10204: 'when\\nyou', 10205: 'for\\n«ed|ing$»', 10206: 'meta-characters', 10207: '.\\n\\n\\n\\n\\n\\n\\noperator\\nbehavior\\n\\n\\n\\n', 10208: '.\\nwildcard,', 10209: 'character\\n\\n^abc\\nmatches', 10210: 'abc', 10211: 'string\\n\\nabc$\\nmatches', 10212: 'string\\n\\n[abc]\\nmatches', 10213: 'characters\\n\\n[a-z0-9]\\nmatches', 10214: 'characters\\n\\ned|ing|s\\nmatches', 10215: '(disjunction)\\n\\n*\\nzero', 10216: 'a*,', 10217: '[a-z]*', 10218: 'closure)\\n\\n+\\none', 10219: 'a+,', 10220: '[a-z]+\\n\\n?\\nzero', 10221: 'optional),', 10222: 'a?,', 10223: '[a-z]?\\n\\n{n}\\nexactly', 10224: 'non-negative', 10225: 'integer\\n\\n{n,}\\nat', 10226: 'repeats\\n\\n{,n}\\nno', 10227: 'repeats\\n\\n{m,n}\\nat', 10228: 'repeats\\n\\na(b|c)+\\nparentheses', 10229: 'operators\\n\\n\\ntable', 10230: 'meta-characters,', 10231: 'wildcards,', 10232: 'closures\\n\\n\\nto', 10233: 'will\\ninterpret', 10234: 'specially', 10235: '\\\\b', 10236: 'the\\nbackspace', 10237: 'containing\\nbackslash,', 10238: 'string\\nat', 10239: 'all,', 10240: 'that\\nit', 10241: \"r'\\\\band\\\\b'\\ncontains\", 10242: 'library\\nas', 10243: 'boundaries', 10244: 'backspace', 10245: 'habit', 10246: \"r'\", 10247: 'expressions\\n—', 10248: 'about\\nthese', 10249: 'complications', 10250: '.5\\xa0\\xa0\\xa0useful', 10251: 'expressions\\nthe', 10252: 'w\\nthat', 10253: 'regexp', 10254: '.search(regexp,', 10255: 'w)', 10256: '.\\napart', 10257: 'use\\nregular', 10258: '.\\n\\nextracting', 10259: 'pieces\\nthe', 10260: '.findall()', 10261: '(find', 10262: 'all)', 10263: '(non-overlapping)\\nmatches', 10264: 'vowels', 10265: 'them:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 10266: \"'supercalifragilisticexpialidocious'\\n>>>\", 10267: \".findall(r'[aeiou]',\", 10268: \"word)\\n['u',\", 10269: \"'e',\", 10270: \"'u']\\n>>>\", 10271: 'len(re', 10272: \"word))\\n16\\n\\n\\n\\nlet's\", 10273: 'sequences', 10274: 'frequency:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 10275: 'fd', 10276: '.freqdist(vs', 10277: 'wsj\\n', 10278: 'vs', 10279: \".findall(r'[aeiou]{2,}',\", 10280: 'word))\\n>>>', 10281: \".most_common(12)\\n[('io',\", 10282: '549),', 10283: \"('ea',\", 10284: '476),', 10285: \"('ie',\", 10286: '331),', 10287: \"('ou',\", 10288: '329),', 10289: \"('ai',\", 10290: '261),', 10291: \"('ia',\", 10292: \"253),\\n('ee',\", 10293: '217),', 10294: \"('oo',\", 10295: '174),', 10296: \"('ua',\", 10297: '109),', 10298: \"('au',\", 10299: '106),', 10300: \"('ue',\", 10301: '105),', 10302: \"('ui',\", 10303: '95)]\\n\\n\\n\\n\\nnote\\nyour', 10304: 'turn:\\nin', 10305: 'w3c', 10306: 'date', 10307: 'dates', 10308: '2009-12-31', 10309: '.\\nreplace', 10310: 'expression,\\nin', 10311: \"'2009-12-31'\", 10312: 'integers\\n[2009,', 10313: '12,', 10314: '31]:\\n[int(n)', 10315: '.findall(?,', 10316: \"'2009-12-31')]\\n\\n\\n\\ndoing\", 10317: 'pieces\\nonce', 10318: \"there's\\ninteresting\", 10319: 'pieces,', 10320: 'glue', 10321: 'or\\nplot', 10322: 'noted', 10323: 'redundant,', 10324: 'still\\neasy', 10325: 'word-internal', 10326: 'example,\\ndeclaration', 10327: 'becomes', 10328: 'dclrtn,', 10329: 'inalienable', 10330: 'inlnble,\\nretaining', 10331: 'expression\\nin', 10332: 'sequences,', 10333: 'consonants;\\neverything', 10334: 'three-way', 10335: 'disjunction', 10336: 'left-to-right,\\nif', 10337: 'regular\\nexpression', 10338: 'matching\\npieces,', 10339: \"''\", 10340: '.join()', 10341: '.9', 10342: 'for\\nmore', 10343: \"r'^[aeiouaeiou]+|[aeiouaeiou]+$|[^aeiouaeiou]'\\n>>>\", 10344: 'compress(word):\\n', 10345: '.findall(regexp,', 10346: '.join(pieces)\\n', 10347: 'english_udhr', 10348: \".words('english-latin1')\\n>>>\", 10349: 'print(nltk', 10350: '.tokenwrap(compress(w)', 10351: 'english_udhr[:75]))\\nunvrsl', 10352: 'dclrtn', 10353: 'hmn', 10354: 'rghts', 10355: 'prmble', 10356: 'whrs', 10357: 'rcgntn', 10358: 'inhrnt', 10359: 'dgnty', 10360: 'eql', 10361: 'inlnble', 10362: 'mmbrs', 10363: 'fmly', 10364: 'fndtn\\nof', 10365: 'frdm', 10366: 'jstce', 10367: 'pce', 10368: 'wrld', 10369: 'dsrgrd', 10370: 'cntmpt', 10371: 'fr', 10372: 'hmn\\nrghts', 10373: 'hve', 10374: 'rsltd', 10375: 'brbrs', 10376: 'acts', 10377: 'whch', 10378: 'outrgd', 10379: 'cnscnce', 10380: 'mnknd', 10381: 'advnt', 10382: 'bngs', 10383: 'shll', 10384: 'enjy', 10385: 'spch', 10386: 'and\\n\\n\\n\\nnext,', 10387: 'frequency\\ndistributions', 10388: 'consonant-vowel', 10389: 'sequences\\nfrom', 10390: 'rotokas,', 10391: 'ka', 10392: 'si', 10393: 'of\\nthese', 10394: 'pair,', 10395: 'pair:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 10396: 'rotokas_words', 10397: '.toolbox', 10398: \".words('rotokas\", 10399: \".dic')\\n>>>\", 10400: 'cvs', 10401: '[cv', 10402: 'cv', 10403: \".findall(r'[ptksvr][aeiou]',\", 10404: 'w)]\\n>>>', 10405: '.conditionalfreqdist(cvs)\\n>>>', 10406: '.tabulate()\\n', 10407: 'u\\nk', 10408: '418', 10409: '148', 10410: '420', 10411: '173\\np', 10412: '31', 10413: '105', 10414: '51\\nr', 10415: '187', 10416: '63', 10417: '84', 10418: '89', 10419: '79\\ns', 10420: '1\\nt', 10421: '47', 10422: '37\\nv', 10423: '27', 10424: '48', 10425: '49\\n\\n\\n\\nexamining', 10426: 'rows', 10427: 'partial\\ncomplementary', 10428: 'not\\ndistinct', 10429: 'conceivably', 10430: 'drop\\ns', 10431: 'rule\\nthat', 10432: 'pronounced', 10433: 'by\\ni', 10434: 'su,', 10435: \"kasuari,\\n'cassowary'\", 10436: 'borrowed', 10437: '.)\\nif', 10438: 'behind', 10439: 'table,\\nit', 10440: 'index,', 10441: 'allowing', 10442: \"cv_index['su']\", 10443: 'us\\nall', 10444: 'su', 10445: 'cv_word_pairs', 10446: '[(cv,', 10447: 'rotokas_words\\n', 10448: 'cv_index', 10449: '.index(cv_word_pairs)\\n>>>', 10450: \"cv_index['su']\\n['kasuari']\\n>>>\", 10451: \"cv_index['po']\\n['kaapo',\", 10452: \"'kaapopato',\", 10453: \"'kaipori',\", 10454: \"'kaiporipie',\", 10455: \"'kaiporivira',\", 10456: \"'kapo',\", 10457: \"'kapoa',\\n'kapokao',\", 10458: \"'kapokapo',\", 10459: \"'kapokapoa',\", 10460: \"'kapokapora',\", 10461: 'turn,', 10462: 'every\\nsubstring', 10463: '«[ptksvr][aeiou]»', 10464: 'kasuari,', 10465: 'ka,', 10466: 'ri', 10467: '.\\ntherefore,', 10468: \"('ka',\", 10469: \"'kasuari'),\\n('su',\", 10470: \"'kasuari')\", 10471: \"('ri',\", 10472: 'using\\nnltk', 10473: '.index(),', 10474: 'converts', 10475: '.\\n\\n\\nfinding', 10476: 'stems\\nwhen', 10477: 'engine,', 10478: 'mind', 10479: 'notice)\\nif', 10480: 'having\\ndifferent', 10481: 'endings', 10482: 'laptops', 10483: 'laptop', 10484: 'versa', 10485: 'lemma)', 10486: 'endings,', 10487: 'just\\ndeal', 10488: 'stems', 10489: 'stem', 10490: 'simple-minded\\napproach', 10491: 'strips', 10492: 'suffix:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 10493: 'stem(word):\\n', 10494: 'suffix', 10495: \"['ing',\", 10496: \"'ly',\", 10497: \"'ed',\", 10498: \"'ious',\", 10499: \"'ies',\", 10500: \"'ive',\", 10501: \"'ment']:\\n\", 10502: '.endswith(suffix):\\n', 10503: 'word[:-len(suffix)]\\n', 10504: 'word\\n\\n\\n\\nalthough', 10505: 'ultimately', 10506: 'stemmers,', 10507: 'interesting\\nto', 10508: 'is\\nto', 10509: 'suffixes', 10510: 'parentheses\\nin', 10511: \".findall(r'^\", 10512: \".*(ing|ly|ed|ious|ies|ive|es|s|ment)$',\", 10513: \"'processing')\\n['ing']\\n\\n\\n\\nhere,\", 10514: 'gave', 10515: 'expression\\nmatched', 10516: 'function,\\nto', 10517: 'disjunction,', 10518: 'output,\\nwe', 10519: '?:,\\nwhich', 10520: 'arcane', 10521: 'subtleties', 10522: \".*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$',\", 10523: \"'processing')\\n['processing']\\n\\n\\n\\nhowever,\", 10524: \"we'd\", 10525: 'parenthesize', 10526: 'expression:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 10527: \".findall(r'^(\", 10528: \".*)(ing|ly|ed|ious|ies|ive|es|s|ment)$',\", 10529: \"'processing')\\n[('process',\", 10530: \"'ing')]\\n\\n\\n\\nthis\", 10531: 'promising,', 10532: 'different\\nword,', 10533: 'processes:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 10534: \"'processes')\\n[('processe',\", 10535: \"'s')]\\n\\n\\n\\nthe\", 10536: 'incorrectly', 10537: '-s', 10538: 'of\\nan', 10539: '-es', 10540: 'subtlety:', 10541: 'star', 10542: 'operator\\nis', 10543: 'greedy', 10544: '.*', 10545: 'consume', 10546: 'input\\nas', 10547: 'non-greedy', 10548: '*?,\\nwe', 10549: 'want:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 10550: \".*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$',\", 10551: \"'processes')\\n[('process',\", 10552: \"'es')]\\n\\n\\n\\nthis\", 10553: 'suffix,', 10554: 'the\\nsecond', 10555: 'optional:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 10556: \".*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$',\", 10557: \"'language')\\n[('language',\", 10558: \"'')]\\n\\n\\n\\nthis\", 10559: '(can', 10560: 'spot', 10561: 'them?)', 10562: 'stemming,', 10563: \"r'^(\", 10564: \".*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\\n\", 10565: 'stem,', 10566: 'word)[0]\\n', 10567: 'stem\\n', 10568: 'dennis:', 10569: 'listen,', 10570: 'women', 10571: 'lying', 10572: 'ponds', 10573: 'swords\\n', 10574: 'supreme', 10575: 'executive', 10576: 'derives', 10577: 'from\\n', 10578: 'mandate', 10579: 'masses,', 10580: 'farcical', 10581: 'aquatic', 10582: 'ceremony', 10583: '[stem(t)', 10584: \"tokens]\\n['dennis',\", 10585: \"'listen',\", 10586: \"'strange',\", 10587: \"'women',\", 10588: \"'pond',\", 10589: \"'distribut',\\n'sword',\", 10590: \"'basi',\", 10591: \"'system',\", 10592: \"'supreme',\\n'execut',\", 10593: \"'power',\", 10594: \"'deriv',\", 10595: \"'mandate',\", 10596: \"'mass',\", 10597: \"',',\\n'not',\", 10598: \"'farcical',\", 10599: \"'aquatic',\", 10600: \"'ceremony',\", 10601: \".']\\n\\n\\n\\nnotice\", 10602: 'is\\nand', 10603: 'non-words', 10604: 'distribut', 10605: 'deriv,', 10606: 'these\\nare', 10607: 'acceptable', 10608: '.\\n\\n\\nsearching', 10609: 'text\\nyou', 10610: 'tokens)', 10611: '<a>', 10612: '<man>', 10613: 'all\\ninstances', 10614: 'angle', 10615: 'boundaries,\\nand', 10616: '(behaviors', 10617: 'unique\\nto', 10618: 'findall()', 10619: 'texts)', 10620: 'include\\n<', 10621: '.*>', 10622: 'token,', 10623: 'the\\nmatched', 10624: 'monied)', 10625: 'monied', 10626: 'man)\\nis', 10627: 'three-word', 10628: 'bro\\n', 10629: 'gutenberg,', 10630: '.text(gutenberg', 10631: \".words('melville-moby_dick\", 10632: '.findall(r<a>', 10633: '(<', 10634: '.*>)', 10635: '<man>)', 10636: '\\nmonied;', 10637: 'nervous;', 10638: 'dangerous;', 10639: 'white;', 10640: 'pious;', 10641: 'queer;', 10642: 'good;\\nmature;', 10643: 'cape;', 10644: 'great;', 10645: 'wise;', 10646: 'butterless;', 10647: 'fiendish;\\npale;', 10648: 'furious;', 10649: 'better;', 10650: 'certain;', 10651: 'complete;', 10652: 'dismasted;', 10653: 'younger;', 10654: 'brave;\\nbrave;', 10655: 'brave;', 10656: 'brave\\n>>>', 10657: '.text(nps_chat', 10658: '.words())\\n>>>', 10659: '.findall(r<', 10660: '<bro>)', 10661: '\\nyou', 10662: 'bro;', 10663: 'twizted', 10664: 'bro\\n>>>', 10665: '.findall(r<l', 10666: '.*>{3,})', 10667: '\\nlol', 10668: 'lol;', 10669: 'lmao', 10670: 'la', 10671: 'la;', 10672: 'la\\nla', 10673: 'love;', 10674: '.;', 10675: 'la\\n\\n\\n\\n\\nnote\\nyour', 10676: 'substitutions', 10677: '.re_show(p,', 10678: 'annotates', 10679: 'where\\npattern', 10680: 'matched,', 10681: '.nemo()', 10682: 'graphical\\ninterface', 10683: 'practice,', 10684: '.\\n\\n\\nit', 10685: 'phenomenon', 10686: \"we're\\nstudying\", 10687: 'tied', 10688: 'cases,', 10689: 'creativity\\nwill', 10690: 'for\\nexpressions', 10691: 'ys', 10692: 'discover\\nhypernyms', 10693: '5):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 10694: 'hobbies_learned', 10695: '.text(brown', 10696: \".words(categories=['hobbies',\", 10697: \"'learned']))\\n>>>\", 10698: '.findall(r<\\\\w*>', 10699: '<and>', 10700: '<other>', 10701: '<\\\\w*s>)\\nspeed', 10702: 'activities;', 10703: 'water', 10704: 'liquids;', 10705: 'tomb', 10706: 'other\\nlandmarks;', 10707: 'statues', 10708: 'monuments;', 10709: 'pearls', 10710: 'jewels;\\ncharts', 10711: 'items;', 10712: 'roads', 10713: 'features;', 10714: 'other\\nobjects;', 10715: 'military', 10716: 'areas;', 10717: 'demands', 10718: 'factors;\\nabstracts', 10719: 'compilations;', 10720: 'iron', 10721: 'metals\\n\\n\\n\\nwith', 10722: 'store\\nof', 10723: 'taxonomy', 10724: 'objects,', 10725: 'for\\nany', 10726: 'manual', 10727: 'labor', 10728: 'usually\\ncontain', 10729: 'positives,', 10730: 'exclude', 10731: 'result:', 10732: 'suggests\\nthat', 10733: 'demand', 10734: 'factor,', 10735: 'this\\nsentence', 10736: 'wage', 10737: 'nevertheless,', 10738: 'could\\nconstruct', 10739: 'ontology', 10740: 'correcting\\nthe', 10741: 'common\\nway', 10742: 'in\\n11', 10743: '.\\n\\nsearching', 10744: 'suffers', 10745: 'negatives,\\ni', 10746: 'risky', 10747: 'to\\nconclude', 10748: 'corpus\\njust', 10749: \"didn't\", 10750: 'discover\\ninformation', 10751: '.\\n\\n\\n\\n\\n\\n3', 10752: '.6\\xa0\\xa0\\xa0normalizing', 10753: 'text\\nin', 10754: 'before\\ndoing', 10755: 'lower(),', 10756: 'distinction', 10757: 'affixes,', 10758: 'resulting', 10759: 'dictionary,\\na', 10760: 'lemmatization', 10761: 'section:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 10762: 'word_tokenize(raw)\\n\\n\\n\\n\\nstemmers\\nnltk', 10763: 'off-the-shelf', 10764: 'need\\na', 10765: 'stemmer', 10766: 'preference', 10767: 'crafting', 10768: 'own\\nusing', 10769: 'irregular', 10770: 'porter', 10771: 'lancaster', 10772: 'stemmers', 10773: 'stripping', 10774: 'lying\\n(mapping', 10775: 'lie),', 10776: '.porterstemmer()\\n>>>', 10777: '.lancasterstemmer()\\n>>>', 10778: '[porter', 10779: '.stem(t)', 10780: \"tokens]\\n['denni',\", 10781: \"'strang',\", 10782: \"'lie',\", 10783: \"'pond',\\n'distribut',\", 10784: \"'sword',\", 10785: \"'govern',\\n'\", 10786: \"'suprem',\", 10787: \"'execut',\", 10788: \"'mandat',\", 10789: \"'from',\\n'the',\", 10790: \"'farcic',\", 10791: \"'aquat',\", 10792: \"'ceremoni',\", 10793: '[lancaster', 10794: \"tokens]\\n['den',\", 10795: \"'list',\", 10796: \"'wom',\", 10797: \"'lying',\", 10798: \"'bas',\", 10799: \"'suprem',\\n'execut',\", 10800: \"'pow',\", 10801: \"'der',\", 10802: \"'mand',\", 10803: \"'not',\\n'from',\", 10804: \"'som',\", 10805: \"'farc',\", 10806: \"'aqu',\", 10807: \".']\\n\\n\\n\\nstemming\", 10808: 'well-defined', 10809: 'process,', 10810: 'best\\nsuits', 10811: '(illustrated', 10812: '.6,', 10813: 'oriented\\nprogramming', 10814: 'formatting\\ntechniques', 10815: '.9,', 10816: 'enumerate()', 10817: 'function\\nto', 10818: '.\\n\\n\\n\\n\\n\\xa0\\n\\nclass', 10819: 'indexedtext(object):\\n\\n', 10820: '__init__(self,', 10821: 'stemmer,', 10822: 'text):\\n', 10823: 'self', 10824: '._text', 10825: 'text\\n', 10826: '._stemmer', 10827: 'stemmer\\n', 10828: '._index', 10829: '.index((self', 10830: '._stem(word),', 10831: 'i)\\n', 10832: '(i,', 10833: 'enumerate(text))\\n\\n', 10834: 'concordance(self,', 10835: 'width=40):\\n', 10836: '._stem(word)\\n', 10837: 'wc', 10838: 'int(width/4)', 10839: 'context\\n', 10840: '._index[key]:\\n', 10841: 'lcontext', 10842: '.join(self', 10843: '._text[i-wc:i])\\n', 10844: 'rcontext', 10845: '._text[i:i+wc])\\n', 10846: 'ldisplay', 10847: \"'{:>{width}}'\", 10848: '.format(lcontext[-width:],', 10849: 'width=width)\\n', 10850: 'rdisplay', 10851: \"'{:{width}}'\", 10852: '.format(rcontext[:width],', 10853: 'print(ldisplay,', 10854: 'rdisplay)\\n\\n', 10855: '_stem(self,', 10856: 'word):\\n', 10857: '.stem(word)', 10858: '.lower()\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 10859: \".words('grail\", 10860: 'indexedtext(porter,', 10861: 'grail)\\n>>>', 10862: \".concordance('lie')\\nr\", 10863: 'dennis', 10864: 'listen', 10865: 'swords', 10866: 'no\\n', 10867: 'beat', 10868: 'brave', 10869: 'retreat', 10870: 'robin', 10871: 'lies', 10872: 'minstrel', 10873: '[', 10874: 'singing', 10875: ']', 10876: 'bravest', 10877: 'of\\n', 10878: 'nay', 10879: 'lie', 10880: 'oh', 10881: 'wounded', 10882: '!\\ndoctors', 10883: 'immediately', 10884: 'clap', 10885: 'piglet', 10886: 'well\\nere', 10887: 'danger', 10888: 'cave', 10889: 'gorge', 10890: 'eternal', 10891: 'peril', 10892: 'which\\n', 10893: 'tim', 10894: 'caerbannog', 10895: '--\\nh', 10896: 'lived', 10897: 'fifty', 10898: 'men', 10899: 'strewn', 10900: 'lair', 10901: 'k\\nnot', 10902: 'fight', 10903: 'til', 10904: 't\\n\\n\\nexample', 10905: '(code_stemmer_indexing', 10906: '.6:', 10907: 'stemmer\\n\\n\\n\\nlemmatization\\nthe', 10908: 'lemmatizer', 10909: 'slower', 10910: 'lying,', 10911: 'woman', 10912: 'wnl', 10913: '.wordnetlemmatizer()\\n>>>', 10914: '[wnl', 10915: '.lemmatize(t)', 10916: \"'woman',\", 10917: \"'pond',\\n'distributing',\", 10918: \"'basis',\", 10919: \"'of',\\n'government',\", 10920: \"'supreme',\", 10921: \"'executive',\", 10922: \"'derives',\", 10923: \"'a',\\n'mandate',\", 10924: \"'farcical',\\n'aquatic',\", 10925: \".']\\n\\n\\n\\nthe\", 10926: 'valid', 10927: 'headwords)', 10928: '.\\n\\nnote\\nanother', 10929: 'normalization', 10930: 'non-standard', 10931: 'words\\nincluding', 10932: 'abbreviations,', 10933: 'dates,', 10934: 'tokens\\nto', 10935: 'decimal', 10936: 'be\\nmapped', 10937: 'acronym', 10938: 'mapped', 10939: 'aaa', 10940: 'keeps', 10941: 'improves', 10942: 'accuracy', 10943: 'many\\nlanguage', 10944: 'modeling', 10945: '.7\\xa0\\xa0\\xa0regular', 10946: 'tokenizing', 10947: 'text\\ntokenization', 10948: 'cutting', 10949: 'into\\nidentifiable', 10950: 'units', 10951: 'constitute', 10952: 'to\\ndelay', 10953: 'tokenized,\\nand', 10954: 'expressions,\\nyou', 10955: 'to\\nhave', 10956: '.\\n\\nsimple', 10957: 'tokenization\\nthe', 10958: \"alice's\", 10959: 'wonderland:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 10960: \"'when\", 10961: \"i'm\", 10962: \"duchess,'\", 10963: 'herself,', 10964: 'hopeful', 10965: 'tone\\n', 10966: 'though),', 10967: \"'i\", 10968: 'pepper', 10969: 'kitchen', 10970: 'soup', 10971: 'very\\n', 10972: 'without--maybe', 10973: \"hot-tempered,'\", 10974: '.\\n\\n\\n\\nwe', 10975: 'match\\nany', 10976: 'results\\nin', 10977: '\\\\n', 10978: 'character;', 10979: 'spaces,', 10980: 'tabs,', 10981: \".split(r'\", 10982: 'raw)', 10983: \"\\n['when,\", 10984: \"i'm,\", 10985: \"duchess,',\", 10986: \"'herself,',\", 10987: \"'(not',\", 10988: \"'in',\\n'a',\", 10989: \"'hopeful',\", 10990: \"'tone\\\\nthough),',\", 10991: \"'i,\", 10992: \"won't,\", 10993: \"'pepper',\\n'in',\", 10994: \"'kitchen',\", 10995: \"'all\", 10996: \"'soup',\", 10997: \"'does',\", 10998: \"'very\\\\nwell',\", 10999: \"'without--maybe',\\nit's,\", 11000: \"'always',\", 11001: \"'pepper',\", 11002: \"'makes',\", 11003: \"'people',\", 11004: \".split(r'[\", 11005: \"\\\\t\\\\n]+',\", 11006: \"'tone',\", 11007: \"'though),',\", 11008: \"'well',\", 11009: '«[', 11010: '\\\\t\\\\n]+»', 11011: 'space,', 11012: '(\\\\t)\\nor', 11013: '(\\\\n)', 11014: 'carriage-return', 11015: 'and\\nform-feed', 11016: 'built-in\\nre', 11017: 'abbreviation,', 11018: '\\\\s,', 11019: 'above\\nstatement', 11020: 'rewritten', 11021: \".split(r'\\\\s+',\", 11022: '.\\n\\nnote\\nimportant:\\nremember', 11023: 'prefix', 11024: 'r\\n(meaning', 11025: 'raw),', 11026: 'instructs', 11027: 'python\\ninterpreter', 11028: 'literally,', 11029: 'than\\nprocessing', 11030: 'backslashed', 11031: '.\\n\\nsplitting', 11032: \"'(not'\", 11033: \"'herself,'\", 11034: 'a\\ncharacter', 11035: '[a-za-z0-9_]', 11036: 'complement', 11037: '\\\\w,', 11038: 'characters\\nother', 11039: 'underscore', 11040: 'simple\\nregular', 11041: 'character:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 11042: \".split(r'\\\\w+',\", 11043: \"raw)\\n['',\", 11044: \"'duchess',\", 11045: \"'herself',\", 11046: \"'won',\", 11047: \"'without',\\n'maybe',\", 11048: \"'tempered',\\n'']\\n\\n\\n\\nobserve\", 11049: 'understand\\nwhy,', 11050: \"'xx'\", 11051: \".split('x'))\", 11052: 'strings,\\nwith', 11053: \".findall(r'\\\\w+',\", 11054: 'extend', 11055: 'expression\\nto', 11056: 'wider', 11057: '«\\\\w+|\\\\s\\\\w*»', 11058: 'any\\nnon-whitespace', 11059: '(\\\\s', 11060: '\\\\s)', 11061: 'by\\nfurther', 11062: 'following\\nletters', 11063: \"'s)\", 11064: 'punctuation\\ncharacters', 11065: \".findall(r'\\\\w+|\\\\s\\\\w*',\", 11066: \"raw)\\n['when,\", 11067: \"'m,\", 11068: \"',',\\n'(not',\", 11069: \"'t,\\n'have',\", 11070: \"'does',\\n'very',\", 11071: \"'-maybe',\", 11072: \"'that',\\n'makes',\", 11073: \"'-tempered',\", 11074: \".']\\n\\n\\n\\nlet's\", 11075: '\\\\w+', 11076: 'hyphens', 11077: 'apostrophes:', 11078: \"«\\\\w+([-']\\\\w+)*»\", 11079: \"[-']\\\\w+;\\nit\", 11080: 'hot-tempered', 11081: '?:', 11082: \".)\\nwe'll\", 11083: 'separate\\nfrom', 11084: 'print(re', 11085: \".findall(r\\\\w+(?:[-']\\\\w+)*|'|[-\", 11086: '.(]+|\\\\s\\\\w*,', 11087: \"raw))\\n[',\", 11088: \"',',\\n'(',\", 11089: \"'i',\\nwon't,\", 11090: \"'soup',\\n'does',\", 11091: \"'--',\", 11092: \"'maybe',\", 11093: \"it's,\", 11094: \"'pepper',\\n'that',\", 11095: \"'hot-tempered',\", 11096: '«[-', 11097: '.(]+»', 11098: 'causes', 11099: 'hyphen,\\nellipsis,', 11100: 'parenthesis', 11101: 'separately', 11102: 'have\\nseen', 11103: '.\\n\\n\\n\\n\\n\\n\\nsymbol\\nfunction\\n\\n\\n\\n\\\\b\\nword', 11104: 'boundary', 11105: '(zero', 11106: 'width)\\n\\n\\\\d\\nany', 11107: '(equivalent', 11108: '[0-9])\\n\\n\\\\d\\nany', 11109: 'non-digit', 11110: '[^0-9])\\n\\n\\\\s\\nany', 11111: '\\\\t\\\\n\\\\r\\\\f\\\\v])\\n\\n\\\\s\\nany', 11112: 'non-whitespace', 11113: '[^', 11114: '\\\\t\\\\n\\\\r\\\\f\\\\v])\\n\\n\\\\w\\nany', 11115: 'alphanumeric', 11116: '[a-za-z0-9_])\\n\\n\\\\w\\nany', 11117: 'non-alphanumeric', 11118: '[^a-za-z0-9_])\\n\\n\\\\t\\nthe', 11119: 'character\\n\\n\\\\n\\nthe', 11120: 'character\\n\\n\\ntable', 11121: \"symbols\\n\\n\\n\\n\\nnltk's\", 11122: 'tokenizer\\nthe', 11123: '.regexp_tokenize()', 11124: \"(as\\nwe've\", 11125: 'tokenization)', 11126: '.regexp_tokenize()\\nis', 11127: 'avoids', 11128: 'treatment', 11129: 'readability', 11130: 'lines\\nand', 11131: 'comment', 11132: '(?x)', 11133: 'verbose', 11134: 'flag\\ntells', 11135: 'embedded', 11136: \"'that\", 11137: 'poster-print', 11138: 'costs', 11139: '$12', 11140: '.40', 11141: \".'\\n>>>\", 11142: \"r'''(?x)\", 11143: 'regexps\\n', 11144: '(?:[a-z]\\\\', 11145: '.)+', 11146: '.\\n', 11147: '\\\\w+(?:-\\\\w+)*', 11148: 'hyphens\\n', 11149: '\\\\$?\\\\d+(?:\\\\', 11150: '.\\\\d+)?%?', 11151: 'currency', 11152: 'percentages,', 11153: '.40,', 11154: '82%\\n', 11155: '.\\\\', 11156: 'ellipsis\\n', 11157: '[][', 11158: \".,;'?():-_`]\", 11159: 'tokens;', 11160: '[\\n', 11161: \"'''\\n>>>\", 11162: '.regexp_tokenize(text,', 11163: \"pattern)\\n['that',\", 11164: \"'u\", 11165: \"'poster-print',\", 11166: \"'costs',\", 11167: \"'$12\", 11168: \".40',\", 11169: \".']\\n\\n\\n\\nwhen\", 11170: 'flag,', 11171: 'match\\na', 11172: '\\\\s', 11173: 'regexp_tokenize()', 11174: 'gaps', 11175: 'gaps\\nbetween', 11176: '.\\n\\nnote\\nwe', 11177: 'tokenizer', 11178: 'a\\nwordlist,', 11179: 'reporting', 11180: 'wordlist,\\nusing', 11181: 'set(tokens)', 11182: '.difference(wordlist)', 11183: 'to\\nlowercase', 11184: '.\\n\\n\\n\\nfurther', 11185: 'issues', 11186: 'tokenization\\ntokenization', 11187: 'turns', 11188: '.\\nno', 11189: 'solution', 11190: 'across-the-board,', 11191: 'we\\nmust', 11192: 'decide', 11193: 'tokenized,', 11194: 'tokenizer\\nwith', 11195: 'high-quality', 11196: 'gold-standard)', 11197: 'corpus\\ncollection', 11198: 'raw\\nwall', 11199: 'journal', 11200: '.treebank_raw', 11201: '.raw())', 11202: '.words())', 11203: 'contractions,', 11204: 'meaning\\nof', 11205: 'normalize', 11206: 'this\\nform', 11207: 'forms:', 11208: 'not)', 11209: 'lookup', 11210: '.8\\xa0\\xa0\\xa0segmentation\\nthis', 11211: 'discusses', 11212: '.\\ntokenization', 11213: 'segmentation', 11214: 'problem,\\nwhich', 11215: 'radically', 11216: 'far\\nin', 11217: '.\\n\\nsentence', 11218: 'segmentation\\nmanipulating', 11219: 'presupposes\\nthe', 11220: 'divide', 11221: 'have\\nseen,', 11222: 'per\\nsentence', 11223: 'len(nltk', 11224: '.sents())\\n20', 11225: '.250994070456922\\n\\n\\n\\nin', 11226: 'before\\ntokenizing', 11227: 'segment', 11228: 'facilitates\\nthis', 11229: 'punkt', 11230: 'segmenter', 11231: '(kiss', 11232: 'strunk,', 11233: 'segmenting', 11234: \"segmenter's\", 11235: 'output):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 11236: \".raw('chesterton-thursday\", 11237: '.sent_tokenize(text)\\n>>>', 11238: 'pprint', 11239: \".pprint(sents[79:89])\\n['nonsense!',\\n\", 11240: \"'said\", 11241: 'gregory,', 11242: 'rational', 11243: 'anyone', 11244: 'else\\\\nattempted', 11245: 'paradox', 11246: \".',\\n\", 11247: \"'why\", 11248: 'clerks', 11249: 'navvies', 11250: \"the\\\\n'\\n\", 11251: \"'railway\", 11252: 'trains', 11253: 'sad', 11254: 'tired,', 11255: \"tired?',\\n\", 11256: 'will\\\\ntell', 11257: \"'it\", 11258: \"'it\\\\n'\\n\", 11259: \"ticket\\\\n'\\n\", 11260: \"'for\", 11261: 'reach', 11262: \"have\\\\n'\\n\", 11263: \"'passed\", 11264: 'sloane', 11265: 'station', 11266: \"be\\\\n'\\n\", 11267: \"'victoria,\", 11268: 'victoria', 11269: \"'oh,\", 11270: 'wild', 11271: \"rapture!',\\n\", 11272: \"'oh,\\\\n'\\n\", 11273: \"'their\", 11274: 'eyes', 11275: 'stars', 11276: 'souls', 11277: 'eden,', 11278: \"next\\\\n'\\n\", 11279: \"'station\", 11280: 'unaccountably', 11281: \"street!',\\n\", 11282: 'unpoetical,', 11283: 'replied', 11284: 'poet', 11285: 'syme', 11286: 'mr', 11287: 'lucian', 11288: 'gregory', 11289: 'individual\\nstrings', 11290: '.\\nsentence', 11291: 'abbreviations,\\nand', 11292: 'periods', 11293: 'simultaneously', 11294: 'abbreviation', 11295: 'terminate', 11296: 'sentence,\\nas', 11297: 'acronyms', 11298: 'segmentation,', 11299: '.\\n\\n\\nword', 11300: 'segmentation\\nfor', 11301: 'there\\nis', 11302: 'visual', 11303: 'three-character', 11304: 'string:', 11305: '爱国人\\n(ai4', 11306: 'love', 11307: 'guo2', 11308: 'country,', 11309: 'ren2', 11310: 'person)', 11311: 'could\\nbe', 11312: '爱国', 11313: '人,', 11314: 'country-loving', 11315: 'person\\nor', 11316: '爱', 11317: '国人,', 11318: 'country-person', 11319: 'arises', 11320: 'the\\nhearer', 11321: 'particularly', 11322: 'challenging', 11323: \"don't\\nknow\", 11324: 'advance', 11325: 'learner,\\nsuch', 11326: 'hearing', 11327: 'utterances', 11328: 'parent', 11329: 'removed:\\n\\n', 11330: '.doyouseethekitty\\n\\n', 11331: '.seethedoggy\\n\\n', 11332: '.doyoulikethekitty\\n\\n', 11333: '.likethedoggy\\n\\nour', 11334: 'problem:', 11335: 'find\\na', 11336: 'by\\nannotating', 11337: 'or\\nnot', 11338: 'word-break', 11339: '(an', 11340: 'used\\nheavily', 11341: '.)', 11342: 'learner', 11343: 'utterance', 11344: 'breaks,\\nsince', 11345: 'pauses', 11346: 'representation,\\nincluding', 11347: 'segmentations:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 11348: 'doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\\n>>>', 11349: 'seg1', 11350: '0000000000000001000000000010000000000000000100000000000\\n>>>', 11351: 'seg2', 11352: '0100100100100001001001000010100100010010000100010010000\\n\\n\\n\\nobserve', 11353: 'zeros', 11354: 'they\\nare', 11355: 'shorter', 11356: 'length\\nn', 11357: 'broken', 11358: 'places', 11359: 'segment()', 11360: 'can\\nget', 11361: 'segmented', 11362: 'segment(text,', 11363: 'segs):\\n', 11364: '[]\\n', 11365: '0\\n', 11366: 'range(len(segs)):\\n', 11367: 'segs[i]', 11368: \"'1':\\n\", 11369: '.append(text[last:i+1])\\n', 11370: 'i+1\\n', 11371: '.append(text[last:])\\n', 11372: 'words\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 11373: '0100100100100001001001000010100100010010000100010010000\\n>>>', 11374: \"seg1)\\n['doyouseethekitty',\", 11375: \"'seethedoggy',\", 11376: \"'doyoulikethekitty',\", 11377: \"'likethedoggy']\\n>>>\", 11378: \"seg2)\\n['do',\", 11379: \"'see',\", 11380: \"'kitty',\", 11381: \"'doggy',\", 11382: \"'you',\\n'like',\", 11383: \"'doggy']\\n\\n\\nexample\", 11384: '(code_segment', 11385: '.7:', 11386: 'reconstruct', 11387: 'representation:\\nseg1', 11388: 'final\\nsegmentations', 11389: 'hypothetical', 11390: 'child-directed', 11391: 'speech;\\nthe', 11392: 'reproduce', 11393: 'the\\nsegmented', 11394: '.\\n\\nnow', 11395: 'causes\\nthe', 11396: 'acquiring', 11397: '.\\ngiven', 11398: '(brent,', 11399: '1995),', 11400: 'objective', 11401: 'function,\\na', 11402: 'scoring', 11403: 'optimize,', 11404: '(number', 11405: 'extra\\ndelimiter', 11406: 'to\\nreconstruct', 11407: '.8', 11408: '.8:', 11409: 'calculation', 11410: 'segmentation\\nof', 11411: '(on', 11412: 'left),', 11413: 'derivation\\ntable', 11414: 'reconstructed,', 11415: 'total\\nup', 11416: '(including', 11417: 'boundary\\nmarker)', 11418: 'derivation,', 11419: 'quality', 11420: 'segmentation;', 11421: 'smaller', 11422: 'implement', 11423: 'evaluate(text,', 11424: 'segs)\\n', 11425: 'text_size', 11426: 'len(words)\\n', 11427: 'lexicon_size', 11428: 'sum(len(word)', 11429: 'set(words))\\n', 11430: 'lexicon_size\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 11431: 'seg3', 11432: '0000100100000011001000000110000100010000001100010000001\\n>>>', 11433: \"seg3)\\n['doyou',\", 11434: \"'thekitt',\", 11435: \"'thedogg',\", 11436: \"'doyou',\", 11437: \"'like',\\n\", 11438: \"'y']\\n>>>\", 11439: 'seg3)\\n47\\n>>>', 11440: 'seg2)\\n48\\n>>>', 11441: 'seg1)\\n64\\n\\n\\nexample', 11442: '(code_evaluate', 11443: '.9:', 11444: 'cost', 11445: 'reconstructing', 11446: 'text\\n\\nthe', 11447: 'minimizes', 11448: 'objective\\nfunction,', 11449: '.10', 11450: 'like\\nthekitty,', 11451: '.\\n\\n\\n\\n\\n\\xa0\\n\\nfrom', 11452: 'randint\\n\\ndef', 11453: 'flip(segs,', 11454: 'pos):\\n', 11455: 'segs[:pos]', 11456: 'str(1-int(segs[pos]))', 11457: 'segs[pos+1:]\\n\\ndef', 11458: 'flip_n(segs,', 11459: 'n):\\n', 11460: 'range(n):\\n', 11461: 'segs', 11462: 'randint(0,', 11463: 'len(segs)-1))\\n', 11464: 'segs\\n\\ndef', 11465: 'anneal(text,', 11466: 'segs,', 11467: 'iterations,', 11468: 'cooling_rate):\\n', 11469: 'temperature', 11470: 'float(len(segs))\\n', 11471: '.5:\\n', 11472: 'best_segs,', 11473: 'range(iterations):\\n', 11474: 'round(temperature))\\n', 11475: 'guess)\\n', 11476: 'best:\\n', 11477: 'best,', 11478: 'best_segs', 11479: 'score,', 11480: 'guess\\n', 11481: 'best_segs\\n', 11482: 'cooling_rate\\n', 11483: 'print(evaluate(text,', 11484: 'segs),', 11485: 'segs))\\n', 11486: 'print()\\n', 11487: 'segs\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 11488: 'seg1,', 11489: '5000,', 11490: '.2)\\n61', 11491: \"['doyouseetheki',\", 11492: \"'tty',\", 11493: \"'thedoggy',\", 11494: \"'doyouliketh',\", 11495: \"'ekittylike',\", 11496: \"'thedoggy']\\n59\", 11497: \"['doy',\", 11498: \"'ouseetheki',\", 11499: \"'ttysee',\", 11500: \"'doy',\", 11501: \"'ulikethekittylike',\", 11502: \"'thedoggy']\\n57\", 11503: \"['doyou',\", 11504: \"'seetheki',\", 11505: \"'liketh',\", 11506: \"'thedoggy']\\n55\", 11507: \"'seethekit',\", 11508: \"'tysee',\", 11509: \"'likethekittylike',\", 11510: \"'thedoggy']\\n54\", 11511: \"'thekitty',\", 11512: \"'thedoggy']\\n52\", 11513: \"'seethekittysee',\", 11514: \"'thedoggy']\\n43\", 11515: \"'thedoggy']\\n'0000100100000001001000000010000100010000000100010000000'\\n\\n\\nexample\", 11516: '(code_anneal', 11517: '.10:', 11518: 'non-deterministic', 11519: 'simulated', 11520: 'annealing:', 11521: 'searching\\nwith', 11522: 'segmentations', 11523: 'only;', 11524: 'perturb', 11525: 'ones\\nproportional', 11526: 'temperature;', 11527: 'iteration', 11528: 'temperature\\nis', 11529: 'lowered', 11530: 'perturbation', 11531: 'reduced', 11532: 'this\\nsearch', 11533: 'algorithm', 11534: 'non-deterministic,', 11535: 'different\\nresult', 11536: '.\\n\\nwith', 11537: 'reasonable\\ndegree', 11538: \"that\\ndon't\", 11539: '.9\\xa0\\xa0\\xa0formatting:', 11540: 'strings\\noften', 11541: 'meets', 11542: 'criterion,', 11543: 'summary', 11544: 'statistic\\nsuch', 11545: 'word-count', 11546: 'tagger', 11547: 'program\\nto', 11548: 'result;', 11549: 'forms,\\nor', 11550: 'reformatting', 11551: 'linguistic,\\ntextual', 11552: 'numerical,\\nit', 11553: 'preferable', 11554: 'about\\na', 11555: '.\\n\\nfrom', 11556: 'strings\\nthe', 11557: 'convert\\nthese', 11558: 'join()', 11559: 'method,', 11560: 'specify\\nthe', 11561: 'silly', 11562: \"['we',\", 11563: \"'called',\", 11564: \"'tortoise',\", 11565: \"'taught',\", 11566: \"'us',\", 11567: \".join(silly)\\n'we\", 11568: 'him', 11569: \"';'\", 11570: \".join(silly)\\n'we;called;him;tortoise;because;he;taught;us;\", 11571: \".join(silly)\\n'wecalledhimtortoisebecausehetaughtus\", 11572: \".'\\n\\n\\n\\nso\", 11573: '.join(silly)', 11574: 'means:', 11575: 'and\\nconcatenate', 11576: 'spacer', 11577: '(many', 11578: 'counter-intuitive', 11579: 'calling', 11580: 'text\\n—', 11581: 'enjoys', 11582: 'privileges', 11583: '.\\n\\n\\nstrings', 11584: 'formats\\nwe', 11585: 'object:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 11586: 'hello\\n', 11587: 'world\\n>>>', 11588: 'print(word)\\ncat\\n>>>', 11589: 'print(sentence)\\nhello\\nworld\\n>>>', 11590: \"word\\n'cat'\\n>>>\", 11591: \"sentence\\n'hello\\\\nworld'\\n\\n\\n\\nthe\", 11592: 'attempt', 11593: 'human-readable', 11594: 'naming', 11595: 'string\\nthat', 11596: 'recreate', 11597: 'benefit', 11598: 'give\\nus', 11599: 'clue', 11600: 'of\\ncharacters', 11601: 'reader,', 11602: 'because\\nwe', 11603: 'export', 11604: 'use\\nin', 11605: '.\\nformatted', 11606: 'and\\npre-specified', 11607: 'fdist\\nwe', 11608: 'do:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 11609: \".freqdist(['dog',\", 11610: \"'cat',\", 11611: \"'dog',\", 11612: \"'snake',\", 11613: \"'cat'])\\n>>>\", 11614: 'sorted(fdist):\\n', 11615: \"'->',\", 11616: 'fdist[word],', 11617: \"end=';\", 11618: \"')\\ncat\", 11619: '->', 11620: '4;', 11621: 'snake', 11622: '1;\\n\\n\\n\\nprint', 11623: 'alternating', 11624: 'constants', 11625: 'and\\nmaintain', 11626: 'formatting', 11627: \"print('{}->{};'\", 11628: '.format(word,', 11629: 'fdist[word]),', 11630: \"')\\ncat->3;\", 11631: 'dog->4;', 11632: 'snake->1;\\n\\n\\n\\nto', 11633: 'the\\nformat', 11634: '(by', 11635: 'be\\nyour', 11636: 'usual', 11637: \"'{}->{};'\", 11638: '.format', 11639: \"('cat',\", 11640: \"3)\\n'cat->3;'\\n\\n\\n\\nthe\", 11641: 'curly', 11642: \"'{}'\", 11643: 'replacement\\nfield:', 11644: 'a\\nplaceholder', 11645: 'are\\npassed', 11646: '.format()', 11647: 'embed', 11648: \"'{}'\\ninside\", 11649: 'format()', 11650: 'with\\nappropriate', 11651: 'containing\\nreplacement', 11652: 'unpack', 11653: 'this\\nbehavior', 11654: 'close:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 11655: \"'{}->'\", 11656: \".format('cat')\\n'cat->'\\n>>>\", 11657: \".format(3)\\n'3'\\n>>>\", 11658: '{}', 11659: \"now'\", 11660: \".format('coffee')\\n'i\", 11661: 'coffee', 11662: \"now'\\n\\n\\n\\nwe\", 11663: 'placeholders,', 11664: 'method\\nmust', 11665: \"'{}\", 11666: \"('lee',\", 11667: \"'sandwich',\", 11668: \"lunch')\\n'lee\", 11669: 'sandwich', 11670: \"lunch'\\n>>>\", 11671: \"('sandwich',\", 11672: \"lunch')\\ntraceback\", 11673: \"lunch')\\nindexerror:\", 11674: 'range\\n\\n\\n\\narguments', 11675: 'consumed', 11676: 'any\\nsuperfluous', 11677: '.\\n\\nsystem', 11678: '(ch03', 11679: '2262)\\nunexpected', 11680: \"sandwich'\\n\\n\\n\\nthe\", 11681: 'which\\nrefers', 11682: \"like\\n'from\", 11683: \"{}'\\nis\", 11684: \"'from\", 11685: '{0}', 11686: \"{1}',\", 11687: 'get\\nnon-default', 11688: 'orders:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 11689: '{1}', 11690: \"{0}'\", 11691: \".format('a',\", 11692: \"'b')\\n'from\", 11693: \"a'\\n\\n\\n\\nwe\", 11694: 'indirectly', 11695: \"here's\\nan\", 11696: \"'lee\", 11697: \"now'\\n>>>\", 11698: \"['sandwich',\", 11699: \"'spam\", 11700: \"fritter',\", 11701: \"'pancake']\\n>>>\", 11702: 'snack', 11703: 'menu:\\n', 11704: 'print(template', 11705: '.format(snack))\\n', 11706: '.\\nlee', 11707: 'now\\nlee', 11708: 'spam', 11709: 'fritter', 11710: 'pancake', 11711: 'now\\n\\n\\n\\n\\n\\nlining', 11712: 'up\\n\\nso', 11713: 'generated', 11714: 'width\\non', 11715: 'screen)', 11716: 'padding', 11717: 'given\\nwidth', 11718: \"':'\", 11719: '{:6}\\nspecifies', 11720: 'is\\npadded', 11721: 'right-justified', 11722: ',\\nbut', 11723: 'specifier', 11724: \"'<'\", 11725: 'option', 11726: 'left-justified', 11727: \"'{:6}'\", 11728: '.format(41)', 11729: \"\\n'\", 11730: \"41'\\n>>>\", 11731: \"'{:<6}'\", 11732: \"\\n'41\", 11733: \"'\\n\\n\\n\\nstrings\", 11734: \"'>'\", 11735: '2310)\\nunexpected', 11736: \".format('dog')\", 11737: \"\\n'dog\", 11738: \"'{:>6}'\", 11739: \"dog'\\n\\n\\n\\nother\", 11740: 'precision\\nof', 11741: 'numbers;', 11742: '{:', 11743: '.4f}', 11744: 'four\\ndigits', 11745: 'floating\\npoint', 11746: 'math\\n>>>', 11747: \"'{:\", 11748: \".4f}'\", 11749: '.format(math', 11750: \".pi)\\n'3\", 11751: \".1416'\\n\\n\\n\\nthe\", 11752: 'smart', 11753: \"a\\n'%'\", 11754: 'specification,', 11755: 'percentage;', 11756: 'count,', 11757: '3205,', 11758: '9375\\n>>>', 11759: '.4%}', 11760: '.format(total,', 11761: \"total)\\n'accuracy\", 11762: '9375', 11763: \".1867%'\\n\\n\\n\\nan\", 11764: 'saw\\ndata', 11765: 'tabulated', 11766: 'ourselves,', 11767: 'exercising', 11768: 'control\\nof', 11769: 'headings', 11770: 'column', 11771: 'widths,', 11772: '.11', 11773: 'separation', 11774: 'work,\\nand', 11775: 'tabulate(cfdist,', 11776: 'categories):\\n', 11777: \"print('{:16}'\", 11778: \".format('category'),\", 11779: \"')\", 11780: 'headings\\n', 11781: 'words:\\n', 11782: \"print('{:>6}'\", 11783: '.format(word),', 11784: 'categories:\\n', 11785: '.format(category),', 11786: 'heading\\n', 11787: \"print('{:6}'\", 11788: '.format(cfdist[category][word]),', 11789: 'cell\\n', 11790: 'print()', 11791: 'row\\n\\n>>>', 11792: 'tabulate(cfd,', 11793: 'modals,', 11794: 'genres)\\ncategory', 11795: 'will\\nnews', 11796: '389\\nreligion', 11797: '71\\nhobbies', 11798: '16\\nromance', 11799: '43\\nhumor', 11800: '13\\n\\n\\nexample', 11801: '(code_modal_tabulate', 11802: '.11:', 11803: 'corpus\\n\\nrecall', 11804: 'listing', 11805: \"string\\n'{:{width}}'\", 11806: 'bound', 11807: 'in\\nformat()', 11808: \".format('monty\", 11809: \"python',\", 11810: \"width=15)\\n'monty\", 11811: 'customize', 11812: 'be\\njust', 11813: 'accommodate', 11814: 'using\\nwidth', 11815: 'max(len(w)', 11816: '.\\n\\n\\nwriting', 11817: 'file\\nwe', 11818: 'following\\ncode', 11819: 'opens', 11820: 'writing,', 11821: 'saves', 11822: 'program\\noutput', 11823: 'output_file', 11824: \"open('output\", 11825: \"'w')\\n>>>\", 11826: 'set(nltk', 11827: 'sorted(words):\\n', 11828: 'file=output_file)\\n\\n\\n\\nwhen', 11829: 'non-text', 11830: 'len(words)\\n2789\\n>>>', 11831: \"str(len(words))\\n'2789'\\n>>>\", 11832: 'print(str(len(words)),', 11833: 'file=output_file)\\n\\n\\n\\n\\ncaution!\\nyou', 11834: 'like\\noutput', 11835: 'identical', 11836: 'case\\ndistinctions,', 11837: '.\\n\\n\\n\\ntext', 11838: 'wrapping\\nwhen', 11839: 'text-like,', 11840: 'tabular,\\nit', 11841: 'wrap', 11842: 'displayed\\nconveniently', 11843: 'overflows', 11844: 'line,\\nand', 11845: \"'done',\", 11846: \"',',\\n\", 11847: 'saying:\\n', 11848: \"'('\", 11849: 'str(len(word))', 11850: \"'),',\", 11851: \"')\\nafter\", 11852: '(5),', 11853: '(3),', 11854: '(2),', 11855: '(4),', 11856: '(1),\\n\\n\\n\\nwe', 11857: 'textwrap', 11858: 'clarity', 11859: 'onto', 11860: 'line:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 11861: 'fill\\n>>>', 11862: '[{}', 11863: 'saying]\\n>>>', 11864: '.join(pieces)\\n>>>', 11865: 'wrapped', 11866: 'fill(output)\\n>>>', 11867: 'print(wrapped)\\nafter', 11868: 'more\\n(4),', 11869: '(1),\\n\\n\\n\\nnotice', 11870: 'linebreak', 11871: 'could\\nredefine', 11872: 'spaces,\\ne', 11873: \"'%s_(%d),',\", 11874: 'wrapped,\\nwe', 11875: \".replace('_',\", 11876: '.10\\xa0\\xa0\\xa0summary\\n\\nin', 11877: 'potentially\\nlong', 11878: 'formatting,', 11879: 'we\\ntypically', 11880: 'quotes:', 11881: 'indexes,', 11882: \"zero:\\n'monty\", 11883: \"python'[0]\", 11884: 'is\\nfound', 11885: '.\\nsubstrings', 11886: 'notation:', 11887: \"python'[1:5]\\ngives\", 11888: 'onty', 11889: 'omitted,', 11890: 'the\\nsubstring', 11891: 'string;', 11892: 'omitted,\\nthe', 11893: '.\\nstrings', 11894: 'lists:', 11895: \"gives\\n['monty',\", 11896: \"strings:\\n'/'\", 11897: \"'python'])\", 11898: \"'monty/python'\", 11899: \"open('input\", 11900: \".decode('utf8')\", 11901: 'open(f)', 11902: 'writing\\noutput_file', 11903: \"'w'),\", 11904: 'file=output_file)', 11905: '.\\ntexts', 11906: 'headers,', 11907: 'footers,', 11908: 'markup),\\nthat', 11909: '—\\nsuch', 11910: 'inadequate', 11911: 'it\\nbundles', 11912: '.word_tokenize()', 11913: '.\\nlemmatization', 11914: 'maps', 11915: 'appeared,', 11916: 'appears)\\nto', 11917: 'canonical', 11918: 'citation', 11919: 'lexeme', 11920: 'appear)', 11921: 'specifying\\npatterns', 11922: 'use\\nre', 11923: 'backslash,', 11924: 'to\\npreprocess', 11925: 'prefix:', 11926: \"r'regexp'\", 11927: '\\\\n,', 11928: 'on\\na', 11929: '(newline', 11930: 'character);', 11931: 'used\\nbefore', 11932: '\\\\|,', 11933: '\\\\$,\\nthese', 11934: 'lose', 11935: '%', 11936: 'arg_tuple', 11937: 'a\\nformat', 11938: 'specifiers\\nlike', 11939: '%-6s', 11940: '%0', 11941: '.2d', 11942: '.11\\xa0\\xa0\\xa0further', 11943: 'materials\\nat', 11944: '(for', 11945: 'support,\\nexplaining', 11946: 'various\\noperating', 11947: '.)\\nfor', 11948: 'the\\ntokenization,', 11949: 'howtos', 11950: '.\\nchapters', 11951: 'advanced\\nmaterial', 11952: 'morphology', 11953: 'extensive\\ndiscussion', 11954: '(mertz,', 11955: '(sproat', 11956: '2001)\\nthere', 11957: 'references', 11958: 'and\\ntheoretical', 11959: 'introductory\\ntutorial', 11960: 'python,\\nsee', 11961: \"kuchling's\", 11962: 'howto,\\nhttp://www', 11963: '.amk', 11964: '.ca/python/howto/regex/', 11965: 'manual\\nin', 11966: 'major\\nprogramming', 11967: '(friedl,', 11968: '2002)', 11969: 'presentations', 11970: '2008),\\nand', 11971: 'discussions\\nof', 11972: 'facilities', 11973: 'handling', 11974: 'are:\\n\\nned', 11975: 'batchelder,', 11976: 'http://nedbatchelder', 11977: '.com/text/unipain', 11978: '.html\\nunicode', 11979: 'documentation,\\nhttp://docs', 11980: '.org/3/howto/unicode', 11981: '.html\\ndavid', 11982: 'beazley,', 11983: 'i/o,\\nhttp://pyvideo', 11984: '.org/video/289/pycon-2010--mastering-python-3-i-o\\njoel', 11985: 'spolsky,', 11986: 'absolute', 11987: 'minimum', 11988: 'developer\\nabsolutely,', 11989: 'positively', 11990: 'sets\\n(no', 11991: 'excuses!),', 11992: '.joelonsoftware', 11993: '.com/articles/unicode', 11994: '.html\\n\\nthe', 11995: 'chinese', 11996: 'sighan,\\nthe', 11997: 'processing\\nhttp://sighan', 11998: 'text\\nfollows', 11999: '1995);', 12000: 'acquisition', 12001: '(niyogi,', 12002: '.\\ncollocations', 12003: 'multiword', 12004: 'meaning\\nand', 12005: 'alone,\\ne', 12006: '(baldwin', 12007: 'kim,', 12008: '.\\nsimulated', 12009: 'annealing', 12010: 'heuristic', 12011: 'finding\\na', 12012: 'approximation', 12013: 'optimum', 12014: 'discrete', 12015: 'space,\\nbased', 12016: 'analogy', 12017: 'metallurgy', 12018: 'technique', 12019: 'discovering', 12020: 'search\\npatterns', 12021: '(hearst,', 12022: '1992)', 12023: '.12\\xa0\\xa0\\xa0exercises\\n\\n☼', 12024: \"'colorless'\", 12025: 'statement\\nthat', 12026: 'colourless', 12027: 'and\\nconcatenation', 12028: 'morphological', 12029: 'on\\nwords', 12030: \"'dogs'[:-1]\", 12031: 'of\\ndogs,', 12032: 'leaving', 12033: 'the\\naffixes', 12034: \"(we've\", 12035: 'inserted', 12036: 'affix', 12037: 'boundary,', 12038: 'strings):\\ndish-es,', 12039: 'run-ning,', 12040: 'nation-ality,', 12041: 'un-do,\\npre-heat', 12042: 'string?\\n\\n☼', 12043: 'following\\nreturns', 12044: 'slice:', 12045: 'monty[6:11:2]', 12046: 'direction:', 12047: 'monty[10:5:-2]\\ntry', 12048: 'monty[::-1]?\\nexplain', 12049: 'regular\\nexpressions', 12050: '.\\n\\n[a-za-z]+\\n[a-z][a-z]*\\np[aeiou]{,2}t\\n\\\\d+(\\\\', 12051: '.\\\\d+)?\\n([^aeiou][aeiou][^aeiou])*\\n\\\\w+|[^\\\\w\\\\s]+\\n\\ntest', 12052: '.re_show()', 12053: 'strings:\\n\\n\\na', 12054: 'determiner', 12055: '(assume', 12056: 'an,', 12057: 'the\\nare', 12058: 'determiners)', 12059: 'arithmetic', 12060: 'integers,', 12061: 'and\\nmultiplication,', 12062: '2*3+8', 12063: '.\\n\\n\\n\\n☼', 12064: 'returns\\nthe', 12065: 'url,', 12066: 'request\\nand', 12067: \".urlopen('http://nltk\", 12068: \".org/')\", 12069: '.\\n\\n☼\\nsave', 12070: 'load(f)\\nthat', 12071: 'sole', 12072: 'string\\ncontaining', 12073: 'tokenizes\\nthe', 12074: 'multi-line\\nregular', 12075: 'comments,', 12076: 'monetary', 12077: 'amounts;', 12078: 'dates;', 12079: 'names\\nof', 12080: 'organizations', 12081: 'rewrite', 12082: 'comprehension:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 12083: \"'gave',\", 12084: \"'john',\", 12085: \"'newspaper']\\n>>>\", 12086: '[]\\n>>>', 12087: 'word_len', 12088: '.append(word_len)\\n>>>', 12089: \"result\\n[('the',\", 12090: '3),', 12091: \"('dog',\", 12092: \"('gave',\", 12093: \"('john',\", 12094: \"('newspaper',\", 12095: '9)]\\n\\n\\n\\n\\n☼', 12096: 'choosing', 12097: \"'s'\", 12098: 'string\\nwith', 12099: 'argument,\\ne', 12100: 'versus', 12101: \".split('\", 12102: \"')?\", 12103: 'happens\\nwhen', 12104: 'consecutive\\nspace', 12105: 'tabs', 12106: 'spaces?', 12107: 'you\\nwill', 12108: \"'\\\\t'\", 12109: '.)\\n\\n☼', 12110: '.\\nexperiment', 12111: '.sort()', 12112: 'sorted(words)', 12113: 'difference?\\n\\n☼', 12114: 'integers', 12115: 'prompt:', 12116: 'converting', 12117: 'using\\nint(3)', 12118: 'str(3)', 12119: 'file\\ncalled', 12120: 'prog', 12121: 'following\\n(note', 12122: 'filename):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 12123: 'monty\\n>>>', 12124: 'monty\\n\\n\\n\\nthis', 12125: 'try\\nimport', 12126: 'prog,', 12127: 'to\\nevaluate', 12128: '.monty', 12129: '%6s', 12130: '%-6s\\nare', 12131: 'characters?\\n\\n◑', 12132: 'of\\nall', 12133: 'wh-word', 12134: '(wh-words', 12135: 'english\\nare', 12136: 'clauses', 12137: 'exclamations:\\nwho,', 12138: 'which,', 12139: 'what,', 12140: 'print\\nthem', 12141: 'duplicated', 12142: 'punctuation?\\n\\n◑', 12143: '(made', 12144: 'up)', 12145: 'frequencies,', 12146: 'each\\nline', 12147: 'integer,\\ne', 12148: 'fuzzy', 12149: 'open(filename)', 12150: '.readlines()', 12151: 'split(),', 12152: 'and\\nconvert', 12153: 'int()', 12154: \"[['fuzzy',\", 12155: '53],', 12156: 'webpage', 12157: 'weather', 12158: 'forecast', 12159: 'top\\ntemperature', 12160: 'town', 12161: 'city', 12162: 'today', 12163: 'unknown()', 12164: 'argument,\\nand', 12165: 'unknown', 12166: 'letters\\n(using', 12167: '.findall())', 12168: 'occur\\nin', 12169: '.words)', 12170: 'categorize', 12171: 'words\\nmanually', 12172: 'url\\nhttp://news', 12173: '.uk/', 12174: 'suggested\\nabove', 12175: 'of\\nnon-textual', 12176: 'there,', 12177: 'javascript', 12178: 'may\\nalso', 12179: 'properly\\npreserved', 12180: 'improve', 12181: 'the\\nextraction', 12182: 'such\\na', 12183: \"n't?\\nexplain\", 12184: 'work:', 12185: \"«n't|\\\\w+»\", 12186: 'hack3r,', 12187: 'expressions\\nand', 12188: 'substitution,', 12189: 'where\\ne', 12190: '→', 12191: '3,\\ni', 12192: '1,\\no', 12193: '0,\\nl', 12194: '|,\\ns', 12195: '5,\\n', 12196: '5w33t!,\\nate', 12197: '.\\nnormalize', 12198: '.\\nadd', 12199: 'map\\ns', 12200: 'values:', 12201: 'word-initial', 12202: 's,\\nand', 12203: 'pig', 12204: 'transformation', 12205: 'follows:', 12206: 'consonant', 12207: 'cluster)\\nthat', 12208: 'end,\\nthen', 12209: 'ay,', 12210: 'ingstray,\\nidle', 12211: 'idleay', 12212: '.org/wiki/pig_latin\\n\\nwrite', 12213: '.\\nextend', 12214: 'preserve', 12215: 'qu', 12216: 'together\\n(i', 12217: 'ietquay),', 12218: 'y\\nis', 12219: 'yellow)', 12220: 'style)', 12221: '.\\n\\n\\n◑', 12222: 'harmony', 12223: 'hungarian),\\nextract', 12224: 'bigram', 12225: 'choice()', 12226: 'which\\nrandomly', 12227: 'chooses', 12228: 'choice(aehh', 12229: ')', 12230: 'will\\nproduce', 12231: 'being\\ntwice', 12232: 'expression\\nthat', 12233: 'the\\nstring', 12234: 'aehh', 12235: 'this\\nexpression', 12236: 'concatenate\\nthem', 12237: 'like\\nuncontrolled', 12238: 'sneezing', 12239: 'maniacal', 12240: 'laughter:', 12241: 'haha', 12242: 'ee', 12243: 'heheeh', 12244: 'eha', 12245: 'numeric', 12246: 'medline', 12247: 'cortisol', 12248: 'fractions', 12249: 'these\\nsera', 12250: '.53', 12251: '+/-', 12252: '.15%', 12253: '.16', 12254: '.23%,', 12255: 'respectively', 12256: '.\\nshould', 12257: 'three\\nwords?', 12258: 'compound', 12259: 'word?', 12260: 'should\\nwe', 12261: 'point\\nfive', 12262: 'three,', 12263: 'fifteen', 12264: 'percent?', 12265: \"that\\nit's\", 12266: \"wouldn't\", 12267: 'dictionary?\\ndiscuss', 12268: 'possibilities', 12269: 'domains\\nthat', 12270: 'motivate', 12271: 'answers?\\n\\n◑', 12272: 'a\\ntext,', 12273: 'purposes', 12274: 'selecting', 12275: 'difficulty\\nfor', 12276: 'learners', 12277: 'let', 12278: 'define\\nμw', 12279: 'and\\nμs', 12280: '(ari)', 12281: 'text\\nis', 12282: 'be:\\n4', 12283: '.71', 12284: 'μw', 12285: '.5', 12286: 'μs', 12287: '.43', 12288: 'ari', 12289: 'including\\nsection', 12290: '(lore)', 12291: '(learned)', 12292: 'that\\nnltk', 12293: 'while\\nnltk', 12294: '.sents()', 12295: 'calling\\nthe', 12296: 'stemmer\\nand', 12297: \"list\\n['after',\", 12298: \"'more',\\n'is',\", 12299: \".']\", 12300: 'the\\nlength', 12301: 'hint:', 12302: 'the\\nempty', 12303: 'lengths,', 12304: '[]', 12305: 'time\\nthrough', 12306: \"string:\\n'newly\", 12307: 'formed', 12308: 'bland', 12309: 'inexpressible', 12310: \"infuriating\\nway'\", 12311: 'legitimate', 12312: 'interpretation', 12313: 'that\\nbilingual', 12314: 'english-spanish', 12315: 'speakers', 12316: \"chomsky's\\nfamous\", 12317: 'nonsense', 12318: 'phrase,', 12319: 'colorless', 12320: 'furiously\\naccording', 12321: 'wikipedia)', 12322: 'tasks:\\n\\nsplit', 12323: 'per\\nword,', 12324: 'save\\nthis', 12325: '.\\nextract', 12326: 'join\\nthem', 12327: \"'eoldrnnnna'\", 12328: '.\\ncombine', 12329: '.\\nmake', 12330: 'with\\nwhitespace', 12331: 'index()', 12332: \"'inexpressible'\", 12333: \".index('e')\", 12334: 'substring,', 12335: \".index('re')?\\ndefine\", 12336: '.index()\\nto', 12337: 'exercise', 12338: 'to\\nbuild', 12339: 'not\\nincluding)', 12340: 'nationality', 12341: 'canadian', 12342: 'and\\naustralian', 12343: 'canada', 12344: 'australia\\n(see', 12345: '.org/wiki/list_of_adjectival_forms_of_place_names)', 12346: 'can\\nand', 12347: 'can,', 12348: 'this\\nphenomenon', 12349: 'method\\nfor', 12350: '.\\nhttp://itre', 12351: '.edu/~myl/languagelog/archives/002733', 12352: '.html\\n\\n◑', 12353: 'lolcat', 12354: 'genesis,\\naccessible', 12355: \".words('lolcat\", 12356: 'lolspeak', 12357: '.lolcatbible', 12358: '.com/index', 12359: '.php?title=how_to_speak_lolcat', 12360: 'into\\ncorresponding', 12361: '.sub()', 12362: 'substitution\\nusing', 12363: 'help(re', 12364: '.sub)', 12365: 'consulting\\nthe', 12366: '.sub', 12367: 'code\\nto', 12368: 'tags', 12369: '.\\n\\n★', 12370: 'been\\nsplit', 12371: 'line-break', 12372: 'split,', 12373: 'long-\\\\nterm', 12374: '.\\n\\nwrite', 12375: 'identifies', 12376: 'are\\nhyphenated', 12377: 'the\\n\\\\n', 12378: 'remain', 12379: 'hyphenated\\nonce', 12380: 'removed,', 12381: \"'encyclo-\\\\npedia'?x\\n\\n\\n★\", 12382: 'soundex', 12383: 'this\\nalgorithm', 12384: 'respective\\nreading', 12385: '.\\ne', 12386: 'rural', 12387: '.abc)', 12388: 'comprehension:\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 12389: \"['attribution',\", 12390: \"'confabulation',\", 12391: \"'elocution',\\n\", 12392: \"'sequoia',\", 12393: \"'tenacious',\", 12394: \"'unidirectional']\\n>>>\", 12395: 'vsequences', 12396: 'set()\\n>>>', 12397: 'word:\\n', 12398: \"'aeiou':\\n\", 12399: '.append(char)\\n', 12400: \".add(''\", 12401: '.join(vowels))\\n>>>', 12402: \"sorted(vsequences)\\n['aiuio',\", 12403: \"'eaiou',\", 12404: \"'eouio',\", 12405: \"'euoia',\", 12406: \"'oauaio',\", 12407: \"'uiieioa']\\n\\n\\n\\n\\n\\n\\n★\", 12408: '.6,\\nindexing', 12409: 'synset,\\ne', 12410: \".synsets('dog')[0]\", 12411: '.offset', 12412: 'the\\noffset', 12413: 'ancestors', 12414: 'hierarchy)', 12415: 'the\\nuniversal', 12416: '.udhr),\\nand', 12417: 'correlation', 12418: 'functionality\\n(nltk', 12419: '.freqdist,', 12420: '.spearman_correlation),\\ndevelop', 12421: 'guesses', 12422: 'unseen', 12423: 'simplicity,', 12424: 'few\\nlanguages', 12425: 'discovers\\ncases', 12426: 'similarity\\nbetween', 12427: 'crude\\napproach;', 12428: '.)\\n\\n★', 12429: 'words\\n(sproat', 12430: '2001),', 12431: '.\\n\\n\\n\\n\\n\\nabout', 12432: 'acstch04', 12433: '.rst2\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4\\xa0\\xa0\\xa0writing', 12434: 'programs\\nby', 12435: 'language\\nfor', 12436: 'may\\nstill', 12437: 'wrestling', 12438: \"we'll\\naddress\", 12439: 'well-structured,', 12440: 'easily?\\nhow', 12441: 'blocks', 12442: 'assignment?\\nwhat', 12443: 'pitfalls', 12444: 'them?\\n\\nalong', 12445: 'programming\\nconstructs,', 12446: 'natural\\nand', 12447: 'concise', 12448: 'and\\nexercises', 12449: 'introduce', 12450: 'material)', 12451: '.\\nreaders', 12452: 'carefully\\nand', 12453: 'necessary;\\nexperienced', 12454: 'skim', 12455: 'programming\\nconcepts', 12456: 'dictated', 12457: 'revert', 12458: 'more\\nconventional', 12459: 'for\\na', 12460: '.1\\xa0\\xa0\\xa0back', 12461: 'basics\\n\\nassignment\\nassignment', 12462: 'seem', 12463: 'elementary', 12464: 'concept,', 12465: 'not\\ndeserving', 12466: 'surprising', 12467: 'subtleties\\nhere', 12468: 'fragment:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 12469: 'foo', 12470: \"'monty'\\n>>>\", 12471: \"'python'\", 12472: \"bar\\n'monty'\\n\\n\\n\\nthis\", 12473: 'behaves', 12474: 'above\\ncode', 12475: ',\\nthe', 12476: \"'monty')\", 12477: '.\\nthat', 12478: 'foo,', 12479: 'overwrite\\nfoo', 12480: 'value\\nof', 12481: 'affected', 12482: '.\\nassignment', 12483: 'not\\nalways', 12484: 'particular,\\nthe', 12485: 'a\\nreference', 12486: 'example,\\n', 12487: 'see\\nthat', 12488: \"'python']\\n>>>\", 12489: 'foo[1]', 12490: \"'bodkin'\", 12491: \"bar\\n['monty',\", 12492: \"'bodkin']\\n\\n\\n\\n\\n\\nfigure\", 12493: 'memory:', 12494: 'reference\\nthe', 12495: 'memory;', 12496: 'updating', 12497: 'bar,\\nand', 12498: 'the\\nvariable,', 12499: 'to\\nknow', 12500: '.1,', 12501: '3133', 12502: '(which', 12503: 'is\\nitself', 12504: 'locations', 12505: 'holding', 12506: 'strings)', 12507: 'reference\\n3133', 12508: 'gets', 12509: 'copied', 12510: 'extends', 12511: 'as\\nparameter', 12512: 'more,', 12513: '[empty,', 12514: 'empty,', 12515: 'empty]\\n>>>', 12516: 'nested\\n[[],', 12517: '[],', 12518: '[]]\\n>>>', 12519: 'nested[1]', 12520: \".append('python')\\n>>>\", 12521: \"nested\\n[['python'],\", 12522: \"['python'],\", 12523: \"['python']]\\n\\n\\n\\nobserve\", 12524: 'turn:\\nuse', 12525: '[[]]', 12526: 'the\\nelements', 12527: 'id()', 12528: 'identifier', 12529: 'that\\nid(nested[0]),', 12530: 'id(nested[1]),', 12531: 'id(nested[2])', 12532: 'are\\nall', 12533: 'list,\\nit', 12534: 'propagate', 12535: 'others:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 12536: '3\\n>>>', 12537: \"['monty']\\n>>>\", 12538: \"['monty'],\", 12539: \"['python']]\\n\\n\\n\\n\\n\\n\\n\\nwe\", 12540: 'we\\nmodified', 12541: 'containing\\nthree', 12542: \"['python']\", 12543: 'overwrote', 12544: \"['monty']\", 12545: \"wasn't\", 12546: 'changed,', 12547: 'referenced', 12548: 'in\\nour', 12549: 'between\\nmodifying', 12550: 'reference,', 12551: 'overwriting', 12552: '.\\n\\nnote\\nimportant:\\nto', 12553: 'bar,', 12554: 'write\\nbar', 12555: 'foo[:]', 12556: 'references,', 12557: '.deepcopy()', 12558: '.\\n\\n\\n\\nequality\\npython', 12559: 'identity', 12560: 'to\\nverify', 12561: 'create\\na', 12562: 'demonstrate\\nthat', 12563: '==,', 12564: 'also\\nthat', 12565: '5\\n>>>', 12566: \"['python']\\n>>>\", 12567: 'snake_nest', 12568: '[python]', 12569: 'size\\n>>>', 12570: 'snake_nest[0]', 12571: 'snake_nest[1]', 12572: 'snake_nest[2]', 12573: 'snake_nest[3]', 12574: 'snake_nest[4]\\ntrue\\n>>>', 12575: 'snake_nest[4]\\ntrue\\n\\n\\n\\nnow', 12576: 'not\\nall', 12577: 'identical:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 12578: 'random\\n>>>', 12579: '.choice(range(size))\\n>>>', 12580: 'snake_nest[position]', 12581: \"snake_nest\\n[['python'],\", 12582: \"['python']]\\n>>>\", 12583: 'snake_nest[4]\\nfalse\\n\\n\\n\\nyou', 12584: 'pairwise', 12585: 'interloper,\\nbut', 12586: 'easier:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 12587: '[id(snake)', 12588: 'snake_nest]\\n[4557855488,', 12589: '4557854763,', 12590: '4557855488,', 12591: '4557855488]\\n\\n\\n\\nthis', 12592: 'reveals', 12593: 'try\\nrunning', 12594: 'list,\\nand', 12595: 'interloper', 12596: '.\\n\\n\\nhaving', 12597: 'equality', 12598: 'the\\ntype-token', 12599: 'distinction,', 12600: '.\\n\\n\\nconditionals\\nin', 12601: 'a\\nnonempty', 12602: 'evaluated', 12603: 'or\\nlist', 12604: 'evaluates', 12605: \"['cat',\", 12606: \"['dog'],\", 12607: 'mixed:\\n', 12608: 'element:\\n', 12609: 'print(element)\\n', 12610: \".\\ncat\\n['dog']\\n\\n\\n\\nthat\", 12611: 'len(element)', 12612: '0:', 12613: \".\\nwhat's\", 12614: '.elif', 12615: 'opposed', 12616: 'using\\na', 12617: 'row?', 12618: 'following\\nsituation:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 12619: 'animals', 12620: \"'dog']\\n>>>\", 12621: 'animals:\\n', 12622: 'print(1)\\n', 12623: \"'dog'\", 12624: 'print(2)\\n', 12625: '.\\n1\\n\\n\\n\\nsince', 12626: 'clause', 12627: 'satisfied,', 12628: 'never\\ntries', 12629: 'clause,', 12630: 'out\\n2', 12631: 'replaced', 12632: 'we\\nwould', 12633: 'clause\\npotentially', 12634: 'bare', 12635: 'clause;', 12636: 'when\\nit', 12637: 'is\\nsatisfied,', 12638: 'was\\nnot', 12639: 'all()', 12640: 'any()', 12641: 'sequence)', 12642: 'to\\ncheck', 12643: 'meet', 12644: 'condition:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 12645: \"['no',\", 12646: \"'good',\", 12647: \"'fish',\", 12648: \"'anywhere',\", 12649: \"'porpoise',\", 12650: 'all(len(w)', 12651: 'sent)\\nfalse\\n>>>', 12652: 'any(len(w)', 12653: 'sent)\\ntrue\\n\\n\\n\\n\\n\\n\\n4', 12654: '.2\\xa0\\xa0\\xa0sequences\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nso', 12655: 'object:', 12656: 'another\\nkind', 12657: '.\\ntuples', 12658: 'comma', 12659: 'enclosed\\nusing', 12660: 'the\\nprevious', 12661: 'since\\nthere', 12662: 'tuples', 12663: 'number\\nof', 12664: 'sliced', 12665: \"'walk',\", 12666: \"'fem',\", 12667: \"t\\n('walk',\", 12668: '3)\\n>>>', 12669: 't[0]', 12670: \"\\n'walk'\\n>>>\", 12671: 't[1:]', 12672: \"\\n('fem',\", 12673: 'len(t)', 12674: '\\n3\\n\\n\\n\\n\\ncaution!\\ntuples', 12675: 'more\\ngeneral', 12676: 'syntax,', 12677: 'grouping', 12678: \"'snark'\", 12679: 'a\\ntrailing', 12680: 'comma,', 12681: \"'snark',\", 12682: 'special\\ncase,', 12683: '()', 12684: \".\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nlet's\", 12685: 'directly,', 12686: 'length\\noperation', 12687: 'type:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 12688: 'turned', 12689: \"spectroroute'\\n>>>\", 12690: \"['i',\", 12691: \"'turned',\", 12692: \"'spectroroute']\\n>>>\", 12693: \"'turned')\\n>>>\", 12694: 'raw[2],', 12695: 'text[3],', 12696: \"pair[1]\\n('t',\", 12697: 'raw[-3:],', 12698: 'text[-3:],', 12699: \"pair[-3:]\\n('ute',\", 12700: \"['off',\", 12701: \"'spectroroute'],\", 12702: \"'turned'))\\n>>>\", 12703: 'len(raw),', 12704: 'len(text),', 12705: 'len(pair)\\n(29,', 12706: '2)\\n\\n\\n\\nnotice', 12707: 'computed', 12708: 'a\\nsingle', 12709: 'line,', 12710: 'commas', 12711: 'comma-separated', 12712: 'expressions\\nare', 12713: 'the\\nparentheses', 12714: 'ambiguity', 12715: 'a\\ntuple,', 12716: 'this\\nway,', 12717: 'implicitly', 12718: 'aggregating', 12719: '.\\n\\noperating', 12720: 'types\\nwe', 12721: 'ways,\\nas', 12722: '.\\n\\n\\n\\n\\n\\n\\npython', 12723: 'expression\\ncomment\\n\\n\\n\\nfor', 12724: 's\\niterate', 12725: 's\\n\\nfor', 12726: 'sorted(s)\\niterate', 12727: 'order\\n\\nfor', 12728: 'set(s)\\niterate', 12729: 'reversed(s)\\niterate', 12730: 'reverse\\n\\nfor', 12731: 'set(s)', 12732: '.difference(t)\\niterate', 12733: 't\\n\\n\\ntable', 12734: 'sequences\\n\\n\\nthe', 12735: 'combined\\nin', 12736: 'ways;', 12737: 'sorted\\nin', 12738: 'reverse,', 12739: 'reversed(sorted(set(s)))', 12740: 'randomize', 12741: 'over\\nthem,', 12742: '.shuffle(s)', 12743: 'example,\\ntuple(s)', 12744: 'tuple,', 12745: 'and\\nlist(s)', 12746: 'the\\njoin()', 12747: '.join(words)', 12748: 'a\\nsequence', 12749: 'sorted())', 12750: 'iteration,', 12751: \"'red\", 12752: 'lorry,', 12753: 'yellow', 12754: 'lorry', 12755: '.freqdist(text)\\n>>>', 12756: \"sorted(fdist)\\n[',',\", 12757: \"'red',\", 12758: \"'lorry',\", 12759: \"'yellow']\\n>>>\", 12760: 'fdist:\\n', 12761: 'print(key', 12762: 'fdist[key],', 12763: '.\\nlorry:', 12764: 'red:', 12765: '1;', 12766: ',:', 12767: 'yellow:', 12768: '2\\n\\n\\n\\nin', 12769: 're-arrange', 12770: 'the\\ncontents', 12771: 'parentheses\\nbecause', 12772: 'precedence', 12773: 'words[2],', 12774: 'words[3],', 12775: 'words[4]', 12776: 'words[4],', 12777: 'words[2]\\n>>>', 12778: \"words\\n['i',\", 12779: \"'spectroroute',\", 12780: \"'off']\\n\\n\\n\\nthis\", 12781: 'such\\ntasks', 12782: '(notice', 12783: 'a\\ntemporary', 12784: 'tmp)', 12785: 'tmp', 12786: 'words[2]', 12787: 'words[3]\\n>>>', 12788: 'words[3]', 12789: 'words[4]\\n>>>', 12790: 'tmp\\n\\n\\n\\nas', 12791: 'reversed()\\nthat', 12792: 'rearrange', 12793: 'that\\nmodify', 12794: 'handy', 12795: 'zip()', 12796: 'takes\\nthe', 12797: 'zips', 12798: 'enumerate(s)', 12799: \"['noun',\", 12800: \"'verb',\", 12801: \"'prep',\", 12802: \"'det',\", 12803: \"'noun']\\n>>>\", 12804: 'zip(words,', 12805: 'tags)\\n<zip', 12806: '.>\\n>>>', 12807: 'list(zip(words,', 12808: \"tags))\\n[('i',\", 12809: \"'noun'),\", 12810: \"('turned',\", 12811: \"'verb'),\", 12812: \"('off',\", 12813: \"'prep'),\\n('the',\", 12814: \"'det'),\", 12815: \"('spectroroute',\", 12816: \"'noun')]\\n>>>\", 12817: 'list(enumerate(words))\\n[(0,', 12818: \"'turned'),\", 12819: \"'off'),\", 12820: '(3,', 12821: \"'spectroroute')]\\n\\n\\n\\n\\nnote\\nit\", 12822: 'perform\\ncomputation', 12823: 'lazy', 12824: 'evaluation)', 12825: '<zip', 12826: '0x10d005448>', 12827: 'be\\nevaluated', 12828: 'putting', 12829: 'sequence,\\nlike', 12830: 'list(x),', 12831: '.\\n\\nfor', 12832: 'cut', 12833: 'it\\non', 12834: '10%', 12835: 'to\\ncut', 12836: 'int(0', 12837: 'len(text))', 12838: 'training_data,', 12839: 'test_data', 12840: 'text[:cut],', 12841: 'text[cut:]', 12842: 'training_data', 12843: '\\ntrue\\n>>>', 12844: 'len(training_data)', 12845: 'len(test_data)', 12846: '\\n9', 12847: '.0\\n\\n\\n\\nwe', 12848: 'during', 12849: 'duplicated\\n', 12850: 'ratio', 12851: 'sizes', 12852: 'what\\nwe', 12853: '.\\n\\n\\ncombining', 12854: \"types\\nlet's\", 12855: 'types,', 12856: 'by\\ntheir', 12857: \"spectroroute'\", 12858: 'wordlens', 12859: '[(len(word),', 12860: 'words]', 12861: '.join(w', 12862: '(_,', 12863: 'wordlens)', 12864: \"\\n'i\", 12865: \"spectroroute'\\n\\n\\n\\n\\neach\", 12866: ',\\nwhere', 12867: 'length)', 12868: 'the\\nword,', 12869: \"'the')\", 12870: 'sort()', 12871: '\\nto', 12872: 'sort', 12873: 'in-place', 12874: 'discard', 12875: 'length\\ninformation', 12876: '.\\n(the', 12877: 'variable,\\nbut', 12878: 'will\\nnot', 12879: '.)\\nwe', 12880: 'commonalities', 12881: 'types,\\nbut', 12882: 'their\\nroles', 12883: 'end:', 12884: 'is\\ntypical', 12885: 'and\\nproducing', 12886: 'the\\nmiddle,', 12887: 'of\\nobjects', 12888: 'type,', 12889: 'often\\nuse', 12890: 'contrast,\\na', 12891: 'of\\nfixed', 12892: 'record,\\na', 12893: 'some\\ngetting', 12894: 'to,\\nso', 12895: \"['di:',\", 12896: \"'d@']),\\n\", 12897: \"['qf',\", 12898: \"'o:f'])\\n\", 12899: ']\\n\\n\\n\\nhere,', 12900: 'a\\ncollection', 12901: '—\\nof', 12902: 'predetermined', 12903: 'a\\ntuple', 12904: 'different\\ninterpretations,', 12905: 'orthographic', 12906: 'speech,\\nand', 12907: 'pronunciations', 12908: '(represented', 12909: 'sampa', 12910: 'computer-readable\\nphonetic', 12911: '.phon', 12912: '.ucl', 12913: '.ac', 12914: '.uk/home/sampa/)', 12915: '(why?)\\n\\nnote\\na', 12916: 'whether\\nthe', 12917: 'example,\\na', 12918: 'interpretation,\\nand', 12919: 'tag', 12920: \"('grail',\", 12921: \"'noun');\\na\", 12922: \"('noun',\", 12923: \"'grail')\", 12924: 'is\\nnot', 12925: \"['venetian',\", 12926: \"'blind'];\\na\", 12927: \"['blind',\", 12928: \"'venetian']\", 12929: 'different,', 12930: 'the\\ninterpretation', 12931: 'unchanged', 12932: 'of\\nusage', 12933: 'difference:', 12934: 'python,\\nlists', 12935: 'mutable,', 12936: 'other\\nwords,', 12937: 'modified,', 12938: 'modification', 12939: '.sort()\\n>>>', 12940: 'lexicon[1]', 12941: \"'vbd',\", 12942: \"['t3:nd',\", 12943: \"'t3`nd'])\\n>>>\", 12944: 'lexicon[0]\\n\\n\\n\\n\\nnote\\nyour', 12945: 'turn:\\nconvert', 12946: 'tuple(lexicon),\\nthen', 12947: 'of\\nthem', 12948: '.\\n\\n\\n\\ngenerator', 12949: \"expressions\\nwe've\", 12950: 'heavy', 12951: 'comprehensions,', 12952: 'readable\\nprocessing', 12953: \"'''when\", 12954: 'humpty', 12955: 'dumpty', 12956: 'scornful', 12957: 'tone,\\n', 12958: \".'''\\n>>>\", 12959: \"word_tokenize(text)]\\n['``',\", 12960: \"'use',\", 12961: \"'word',\", 12962: \"'humpty',\", 12963: \"'dumpty',\", 12964: '.]\\n\\n\\n\\nsuppose', 12965: 'above\\nexpression', 12966: 'max([w', 12967: 'word_tokenize(text)])', 12968: \"\\n'word'\\n>>>\", 12969: 'max(w', 12970: 'word_tokenize(text))', 12971: \"\\n'word'\\n\\n\\n\\nthe\", 12972: 'notational', 12973: 'convenience:\\nin', 12974: 'situations,', 12975: 'storage', 12976: 'allocated\\nbefore', 12977: 'max()', 12978: 'is\\nvery', 12979: 'slow', 12980: 'streamed', 12981: 'calling\\nfunction', 12982: 'latest', 12983: 'lexicographic', 12984: 'stream\\nof', 12985: '.3\\xa0\\xa0\\xa0questions', 12986: 'style\\nprogramming', 12987: 'art', 12988: 'undisputed', 12989: 'bible', 12990: 'programming,\\na', 12991: '2,500', 12992: 'multi-volume', 12993: 'donald', 12994: 'knuth,', 12995: 'called\\nthe', 12996: 'on\\nliterate', 12997: 'humans,', 12998: 'computers,\\nmust', 12999: 'of\\nprogramming', 13000: 'ramifications', 13001: 'readability\\nof', 13002: 'layout,', 13003: 'procedural', 13004: 'declarative', 13005: 'style,\\nand', 13006: '.\\n\\npython', 13007: 'coding', 13008: 'style\\nwhen', 13009: 'subtle', 13010: 'choices', 13011: 'names,\\nspacing,', 13012: 'by\\nother', 13013: 'people,', 13014: 'needless', 13015: 'harder\\nto', 13016: 'therefore,', 13017: 'designers', 13018: 'python\\nlanguage', 13019: 'available\\nat', 13020: '.org/dev/peps/pep-0008/', 13021: 'consistency,\\nfor', 13022: 'maximizing', 13023: 'recommendations', 13024: 'refer\\nreaders', 13025: '.\\n\\ncode', 13026: 'layout', 13027: 'that\\nwhen', 13028: 'you\\navoid', 13029: 'by\\ndifferent', 13030: 'messed', 13031: '.\\nlines', 13032: '80', 13033: 'long;', 13034: 'can\\nbreak', 13035: 'brackets,', 13036: 'braces,', 13037: 'because\\npython', 13038: 'braces,\\nyou', 13039: 'broken:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 13040: '(len(syllables)', 13041: 'len(syllables[2])', 13042: 'syllables[2][2]', 13043: '[aeiou]', 13044: 'syllables[2][3]', 13045: 'syllables[1][3]):\\n', 13046: 'process(syllables)\\n>>>', 13047: 'len(syllables)', 13048: '\\\\\\n', 13049: 'syllables[1][3]:\\n', 13050: 'process(syllables)\\n\\n\\n\\n\\nnote\\ntyping', 13051: 'chore', 13052: 'programming\\neditors', 13053: 'indent\\ncode', 13054: 'highlight', 13055: 'errors', 13056: 'errors)', 13057: 'python-aware', 13058: 'editors,', 13059: 'see\\nhttp://wiki', 13060: '.org/moin/pythoneditors', 13061: '.\\n\\n\\n\\nprocedural', 13062: 'style\\n\\n\\n\\n\\n\\n\\nwe', 13063: 'different\\nways,', 13064: 'implications', 13065: 'efficiency', 13066: 'influencing\\nprogram', 13067: 'following\\nprogram', 13068: '0\\n>>>', 13069: 'tokens:\\n', 13070: 'len(token)\\n>>>', 13071: 'count\\n4', 13072: '.401545438271973\\n\\n\\n\\n\\nin', 13073: 'track', 13074: 'the\\nnumber', 13075: 'low-level', 13076: 'machine\\ncode,', 13077: 'cpu', 13078: \"cpu's\", 13079: 'registers,', 13080: 'accumulating', 13081: 'values\\nat', 13082: 'stages,', 13083: 'meaningless', 13084: 'dictating\\nthe', 13085: 'thing:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 13086: 'sum(len(t)', 13087: 'tokens)\\n>>>', 13088: 'print(total', 13089: 'len(tokens))\\n4', 13090: '.401', 13091: '.\\n\\n\\n\\n\\nthe', 13092: 'lengths,\\nwhile', 13093: 'complete,', 13094: 'which\\ncan', 13095: 'understood', 13096: 'high-level', 13097: 'like:\\ntotal', 13098: '.\\nimplementation', 13099: 'constitutes\\nprogramming', 13100: 'level;', 13101: 'word_list', 13102: 'len(tokens):\\n', 13103: 'len(word_list)', 13104: 'word_list[j]', 13105: 'tokens[i]:\\n', 13106: 'tokens[i]', 13107: 'word_list[j-1]:\\n', 13108: '.insert(j,', 13109: 'tokens[i])\\n', 13110: '.\\n\\n\\n\\nthe', 13111: 'instantly', 13112: 'recognizable:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 13113: 'sorted(set(tokens))\\n\\n\\n\\n\\nanother', 13114: 'printing\\na', 13115: 'counter', 13116: 'enumerate(),', 13117: 'which\\nprocesses', 13118: 's[i])', 13119: '(0,', 13120: 's[0])', 13121: 'enumerate', 13122: 'key-value', 13123: '(rank,', 13124: 'count))', 13125: 'rank+1', 13126: '1,\\nas', 13127: '.freqdist(nltk', 13128: 'most_common_words', 13129: '.most_common()]\\n>>>', 13130: 'rank,', 13131: 'enumerate(most_common_words):\\n', 13132: '.freq(word)\\n', 13133: 'print(%3d', 13134: '%6', 13135: '.2f%%', 13136: '%s', 13137: '(rank', 13138: '100,', 13139: 'word))\\n', 13140: '.25:\\n', 13141: 'break\\n', 13142: '.40%', 13143: 'the\\n', 13144: '.42%', 13145: ',\\n', 13146: '.67%', 13147: '.78%', 13148: '.19%', 13149: 'to\\n', 13150: '.29%', 13151: 'a\\n', 13152: '.97%', 13153: \"in\\n\\n\\n\\nit's\", 13154: 'tempting', 13155: 'value\\nseen', 13156: 'longest', 13157: \".words('milton-paradise\", 13158: \"''\\n>>>\", 13159: 'text:\\n', 13160: 'len(longest):\\n', 13161: 'word\\n>>>', 13162: \"longest\\n'unextinguishable'\\n\\n\\n\\nhowever,\", 13163: 'transparent', 13164: 'comprehensions,\\nboth', 13165: 'now:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 13166: 'maxlen', 13167: 'max(len(word)', 13168: 'text)\\n>>>', 13169: \"maxlen]\\n['unextinguishable',\", 13170: \"'transubstantiate',\", 13171: \"'inextinguishable',\", 13172: \"'incomprehensible']\\n\\n\\n\\nnote\", 13173: 'want)', 13174: 'solutions,\\nthe', 13175: 'overhead', 13176: 'pass\\nthrough', 13177: 'instantaneous', 13178: 'concerns', 13179: 'about\\nprogram', 13180: 'cryptic', 13181: 'solution\\nwill', 13182: '.\\n\\n\\nsome', 13183: 'counters\\n\\n\\nthere', 13184: 'successive', 13185: 'overlapping', 13186: 'n-grams\\nfrom', 13187: '[sent[i:i+n]', 13188: \"range(len(sent)-n+1)]\\n[['the',\", 13189: \"'gave'],\\n\", 13190: \"['dog',\", 13191: \"'john'],\\n\", 13192: \"['gave',\", 13193: \"'the'],\\n\", 13194: \"'newspaper']]\\n\\n\\n\\nit\", 13195: 'nltk\\nsupports', 13196: 'bigrams(text)', 13197: 'trigrams(text),', 13198: 'and\\na', 13199: 'ngrams(text,', 13200: 'n)', 13201: 'in\\nbuilding', 13202: 'columns,\\nwhere', 13203: '7\\n>>>', 13204: '[[set()', 13205: 'range(n)]', 13206: 'range(m)]\\n>>>', 13207: 'array[2][5]', 13208: \".add('alice')\\n>>>\", 13209: '.pprint(array)\\n[[set(),', 13210: 'set()],\\n', 13211: '[set(),', 13212: \"{'alice'},\", 13213: 'set()]]\\n\\n\\n\\nobserve', 13214: 'used\\nanywhere', 13215: 'syntactically\\ncorrect', 13216: 'usage,', 13217: 'observe\\nthat', 13218: \"['very'\", 13219: 'range(3)]', 13220: 'list\\ncontaining', 13221: 'sight', 13222: 'incorrect', 13223: 'multiplication,\\nfor', 13224: '[[set()]', 13225: 'n]', 13226: 'm\\n>>>', 13227: '.add(7)\\n>>>', 13228: '.pprint(array)\\n[[{7},', 13229: '{7},', 13230: '{7}],\\n', 13231: '[{7},', 13232: '{7}]]\\n\\n\\n\\n\\n\\niteration', 13233: 'device', 13234: 'adopt', 13235: 'elegant', 13236: 'alternatives,\\nas', 13237: '.4\\xa0\\xa0\\xa0functions:', 13238: 'foundation', 13239: 'programming\\n\\n\\n\\nfunctions', 13240: 'code,\\nas', 13241: 'steps:', 13242: 'in,', 13243: 'normalizing\\nwhitespace,', 13244: 'a\\nfunction,', 13245: 'get_text(),', 13246: '.\\n\\n\\n\\n\\n\\xa0\\n\\nimport', 13247: 're\\ndef', 13248: 'get_text(file):\\n', 13249: 'open(file)', 13250: '.read()\\n', 13251: \".sub(r'<\", 13252: \".*?>',\", 13253: 'text)\\n', 13254: \".sub('\\\\s+',\", 13255: 'text\\n\\n\\nexample', 13256: '(code_get_text', 13257: 'file\\n\\nnow,', 13258: 'cleaned-up', 13259: 'call\\nget_text()', 13260: 'return\\na', 13261: '.:\\ncontents', 13262: 'get_text(test', 13263: 'of\\nsteps', 13264: 'more\\nimportantly,', 13265: 'whenever', 13266: 'cleaned-up\\ntext', 13267: 'clutter', 13268: 'we\\nsimply', 13269: 'get_text()', 13270: 'semantic\\ninterpretation', 13271: 'docstring', 13272: 'the\\npurpose', 13273: 'programmer\\nwho', 13274: 'loaded', 13275: 'file:\\n\\n|', 13276: 'help(get_text)\\n|', 13277: 'get_text', 13278: '__main__:\\n|\\n|', 13279: 'get_text(file)\\n|', 13280: 'reusable', 13281: 'they\\nalso', 13282: 'tested,', 13283: 'risk', 13284: 'forget', 13285: 'bug', 13286: 'calls', 13287: 'increased', 13288: 'reliability', 13289: 'author\\nof', 13290: 'behave\\ntransparently', 13291: 'summarize,', 13292: 'captures', 13293: 'performs\\na', 13294: 'details,\\nto', 13295: 'bigger', 13296: 'picture,', 13297: 'the\\nmechanics', 13298: '.\\n\\nfunction', 13299: 'outputs\\nwe', 13300: \"function's\", 13301: 'parameters,\\nthe', 13302: 'parenthesized', 13303: 'following\\nthe', 13304: 'repeat(msg,', 13305: 'num):', 13306: '.join([msg]', 13307: 'num)\\n>>>', 13308: 'repeat(monty,', 13309: \"\\n'monty\", 13310: \"python'\\n\\n\\n\\nwe\", 13311: 'msg', 13312: 'num\\n', 13313: '3\\n;', 13314: 'fill', 13315: 'and\\nprovide', 13316: 'num', 13317: 'monty():\\n', 13318: 'python\\n>>>', 13319: \"monty()\\n'monty\", 13320: \"python'\\n\\n\\n\\na\", 13321: 'communicates', 13322: 'statement,\\nas', 13323: 'replaced\\nwith', 13324: '.:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 13325: 'repeat(monty(),', 13326: \"3)\\n'monty\", 13327: \"repeat('monty\", 13328: 'result,\\nmodifying', 13329: 'function\\n(such', 13330: 'procedures', 13331: 'languages)', 13332: '.\\n\\n\\n\\n\\nconsider', 13333: 'dangerous', 13334: 'could\\nuse', 13335: 'parameter\\n(my_sort1()),', 13336: '(my_sort2()),\\nnot', 13337: '(my_sort3())', 13338: 'my_sort1(mylist):', 13339: 'good:', 13340: 'value\\n', 13341: 'my_sort2(mylist):', 13342: 'touch', 13343: 'sorted(mylist)\\n>>>', 13344: 'my_sort3(mylist):', 13345: 'bad:', 13346: 'it\\n', 13347: '.sort()\\n', 13348: 'mylist\\n\\n\\n\\n\\n\\nparameter', 13349: 'passing\\nback', 13350: 'values,\\nbut', 13351: 'same\\nis', 13352: 'is\\nknown', 13353: 'call-by-value)', 13354: 'set_up()', 13355: 'parameters,\\nboth', 13356: 'string\\nto', 13357: 'unchanged,\\nwhile', 13358: 'changed:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 13359: 'set_up(word,', 13360: 'properties):\\n', 13361: \"'lolcat'\\n\", 13362: \".append('noun')\\n\", 13363: '5\\n', 13364: 'set_up(w,', 13365: 'p)\\n>>>', 13366: \"w\\n''\\n>>>\", 13367: \"p\\n['noun']\\n\\n\\n\\nnotice\", 13368: 'p),', 13369: 'to\\na', 13370: 'is\\nidentical', 13371: 'w\\n>>>', 13372: \"'lolcat'\\n>>>\", 13373: \"w\\n''\\n\\n\\n\\nlet's\", 13374: 'empty\\nlist)', 13375: 'properties,\\nso', 13376: 'properties,', 13377: 'also\\nreflected', 13378: 'also\\nassigned', 13379: '5);', 13380: 'this\\ndid', 13381: 'but\\ncreated', 13382: 'p\\n>>>', 13383: \".append('noun')\\n>>>\", 13384: \"p\\n['noun']\\n\\n\\n\\nthus,\", 13385: 'call-by-value', 13386: 'passing,\\nit', 13387: '.\\n\\n\\nvariable', 13388: 'scope\\nfunction', 13389: 'new,', 13390: 'function,\\nthe', 13391: 'not\\nvisible', 13392: 'behavior\\nmeans', 13393: 'concerned', 13394: 'about\\ncollisions', 13395: 'body\\nof', 13396: 'resolve\\nthe', 13397: 'respect', 13398: 'global\\nname', 13399: 'succeed,', 13400: 'is\\nthe', 13401: 'lgb', 13402: 'resolution:', 13403: 'local,\\nthen', 13404: 'global,', 13405: '.\\n\\ncaution!\\na', 13406: 'enable', 13407: 'global', 13408: 'the\\nglobal', 13409: 'be\\navoided', 13410: 'defining', 13411: 'variables\\ninside', 13412: 'introduces', 13413: 'dependencies', 13414: 'context\\nand', 13415: 'limits', 13416: 'portability', 13417: 'reusability)', 13418: 'inputs\\nand', 13419: 'outputs', 13420: '.\\n\\n\\n\\nchecking', 13421: 'types\\npython', 13422: 'declare', 13423: 'program,\\nand', 13424: 'flexible\\nabout', 13425: 'expect\\na', 13426: 'expressed\\nas', 13427: 'iterator,', 13428: 'is\\noutside', 13429: 'discussion)', 13430: 'defensive', 13431: 'warnings', 13432: 'functions\\nhave', 13433: 'tag()\\nfunction', 13434: 'tag(word):\\n', 13435: \"['a',\", 13436: \"'all']:\\n\", 13437: \"'det'\\n\", 13438: \"'noun'\\n\", 13439: \"tag('the')\\n'det'\\n>>>\", 13440: \"tag('knight')\\n'noun'\\n>>>\", 13441: \"tag(['tis,\", 13442: \"'scratch'])\", 13443: \"\\n'noun'\\n\\n\\n\\nthe\", 13444: 'sensible', 13445: \"'knight',\\nbut\", 13446: 'passed', 13447: 'fails', 13448: 'to\\ncomplain,', 13449: 'clearly', 13450: 'to\\nensure', 13451: 'tag()', 13452: 'using\\nif', 13453: 'type(word)', 13454: 'str,', 13455: 'simply\\nreturn', 13456: 'slight', 13457: 'improvement,', 13458: 'because\\nthe', 13459: 'diagnostic\\nvalue', 13460: 'program\\nmay', 13461: 'diagnostic\\nreturn', 13462: 'be\\npropagated', 13463: 'unpredictable', 13464: 'consequences', 13465: 'has\\ntype', 13466: 'solution,', 13467: 'assert', 13468: 'basestring\\ntype', 13469: 'generalizes', 13470: 'isinstance(word,', 13471: 'basestring),', 13472: 'string\\n', 13473: \"'noun'\\n\\n\\n\\nif\", 13474: 'fails,', 13475: 'ignored,\\nsince', 13476: 'halts', 13477: 'execution', 13478: '.\\nadditionally,', 13479: 'assertions', 13480: 'logical', 13481: 'errors,', 13482: 'function\\nusing', 13483: 'docstrings', 13484: '.\\n\\n\\n\\nfunctional', 13485: 'decomposition\\nwell-structured', 13486: 'grows', 13487: '10-20', 13488: 'a\\ngreat', 13489: 'more\\nfunctions,', 13490: 'analogous', 13491: 'essay', 13492: 'expressing', 13493: '.\\n\\n\\n\\n\\n\\nfunctions', 13494: 'abstraction', 13495: '.\\nthey', 13496: 'single,', 13497: 'action,\\nand', 13498: '.\\n(compare', 13499: 'of\\ngo', 13500: 'fetch', 13501: '.)\\nwhen', 13502: 'level\\nof', 13503: 'abstraction,', 13504: 'transparent,', 13505: 'load_corpus()\\n>>>', 13506: 'analyze(data)\\n>>>', 13507: 'present(results)\\n\\n\\n\\nappropriate', 13508: 'maintainable', 13509: 'reimplement', 13510: 'function\\n—', 13511: '—\\nwithout', 13512: 'freq_words', 13513: 'updates', 13514: 'is\\npassed', 13515: 'parameter,', 13516: 'the\\nn', 13517: 'request\\nfrom', 13518: 'beautifulsoup\\n\\ndef', 13519: 'freq_words(url,', 13520: \".decode('utf8')\\n\", 13521: '.get_text()\\n', 13522: 'word_tokenize(raw):\\n', 13523: 'freqdist[word', 13524: '.lower()]', 13525: '.most_common(n):\\n', 13526: '[word]\\n', 13527: 'print(result)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 13528: 'constitution', 13529: '.archives', 13530: '.gov/exhibits/charters/constitution_transcript', 13531: '.html\\n>>>', 13532: '.freqdist()\\n>>>', 13533: 'freq_words(constitution,', 13534: 'fd,', 13535: \"30)\\n['the',\", 13536: \"'shall',\", 13537: \"'states',\\n'or',\", 13538: \"'united',\", 13539: \"'state',\", 13540: \"'=',\", 13541: \"'president',\\n'all',\", 13542: \"'congress']\\n\\n\\nexample\", 13543: '(code_freq_words1', 13544: 'poorly', 13545: 'words\\n\\nthis', 13546: 'side-effects:', 13547: 'second\\nparameter,', 13548: 'elsewhere', 13549: 'we\\ninitialize', 13550: 'freqdist()', 13551: 'place\\nit', 13552: 'populated),', 13553: 'the\\ncalling', 13554: 'it\\nshould', 13555: 'refactor', 13556: 'simplify', 13557: 'dropping', 13558: '.freqdist(word', 13559: 'word_tokenize(text))\\n', 13560: '_)', 13561: '.most_common(n)]\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 13562: '(code_freq_words2', 13563: 'well-designed', 13564: 'words\\n\\nthe', 13565: 'usability', 13566: 'improved', 13567: '_', 13568: 'other\\nvariable', 13569: 'signals', 13570: 'use\\nfor', 13571: 'holds', 13572: '.\\n\\n\\n\\ndocumenting', 13573: 'functions\\nif', 13574: 'decomposing', 13575: 'provide\\nthis', 13576: 'statement\\nshould', 13577: 'implemented;', 13578: 're-implement', 13579: 'this\\nstatement', 13580: 'adequate', 13581: 'triple-quoted', 13582: 'line,\\nsince', 13583: 'functionality\\n(see', 13584: '.org/dev/peps/pep-0257/', 13585: 'docstring\\nconventions)', 13586: '.\\n\\ndocstrings', 13587: 'block,', 13588: 'tested', 13589: 'automatically\\nusing', 13590: 'docutils', 13591: '.\\ndocstrings', 13592: 'return\\ntype', 13593: 'minimum,', 13594: 'uses\\nthe', 13595: 'sphinx', 13596: 'format\\ncan', 13597: 'richly', 13598: 'structured\\napi', 13599: 'certain\\nfields', 13600: 'param', 13601: 'be\\nclearly', 13602: 'illustrates\\na', 13603: 'accuracy(reference,', 13604: 'test):\\n', 13605: 'values,\\n', 13606: 'indexes\\n', 13607: '{0<i<=len(test)}', 13608: 'c{test[i]', 13609: 'reference[i]}', 13610: \"accuracy(['adj',\", 13611: \"'n'],\", 13612: \"'adj'])\\n\", 13613: '.5\\n\\n', 13614: ':param', 13615: 'reference:', 13616: 'ordered', 13617: 'values\\n', 13618: ':type', 13619: 'list\\n', 13620: 'corresponding\\n', 13621: ':return:', 13622: 'score\\n', 13623: ':rtype:', 13624: 'float\\n', 13625: ':raises', 13626: 'valueerror:', 13627: 'length\\n', 13628: '\\n\\n', 13629: 'len(reference)', 13630: 'len(test):\\n', 13631: 'raise', 13632: 'valueerror(lists', 13633: 'num_correct', 13634: 'x,', 13635: 'zip(reference,', 13636: 'y:\\n', 13637: 'float(num_correct)', 13638: 'len(reference)\\n\\n\\nexample', 13639: '(code_sphinx', 13640: 'docstring,', 13641: 'summary,\\na', 13642: 'explanation,', 13643: 'markup\\nspecifying', 13644: 'exceptions', 13645: '.\\n\\n\\n\\n\\n4', 13646: '.5\\xa0\\xa0\\xa0doing', 13647: 'functions\\nthis', 13648: 'features,', 13649: '.\\n\\nfunctions', 13650: 'arguments\\nso', 13651: 'like\\nstrings,', 13652: 'as\\nan', 13653: 'show,\\nwe', 13654: 'user-defined', 13655: 'last_letter()\\nas', 13656: \"['take',\", 13657: \"'care',\", 13658: \"'sense',\", 13659: \"'the',\\n\", 13660: \"'sounds',\", 13661: \"'take',\", 13662: \"'themselves',\", 13663: 'extract_property(prop):\\n', 13664: '[prop(word)', 13665: 'sent]\\n', 13666: 'extract_property(len)\\n[4,', 13667: 'last_letter(word):\\n', 13668: 'word[-1]\\n>>>', 13669: \"extract_property(last_letter)\\n['e',\", 13670: \"'f',\", 13671: 'last_letter', 13672: 'be\\npassed', 13673: 'dictionaries', 13674: 'parentheses\\nare', 13675: 'invoking', 13676: 'function;\\nwhen', 13677: 'treating', 13678: 'arguments\\nto', 13679: 'lambda', 13680: 'supposing', 13681: 'there\\nwas', 13682: 'last_letter()', 13683: 'places,\\nand', 13684: 'equivalently', 13685: 'extract_property(lambda', 13686: 'w:', 13687: \"w[-1])\\n['e',\", 13688: \".']\\n\\n\\n\\nour\", 13689: 'latter', 13690: 'sorted),\\nit', 13691: 'cmp()', 13692: 'supply', 13693: 'decreasing\\nlength', 13694: \"sorted(sent)\\n[',',\", 13695: \"'sounds',\\n'take',\", 13696: 'sorted(sent,', 13697: \"cmp)\\n[',',\", 13698: 'y:', 13699: 'cmp(len(y),', 13700: \"len(x)))\\n['themselves',\", 13701: \"'care',\\n'the',\", 13702: \".']\\n\\n\\n\\n\\n\\naccumulative\", 13703: 'functions\\nthese', 13704: 'initializing', 13705: 'storage,', 13706: 'over\\ninput', 13707: 'returning', 13708: 'structure\\nor', 13709: 'aggregated', 13710: 'result)', 13711: 'an\\nempty', 13712: 'shown\\nin', 13713: 'search1()', 13714: 'search1(substring,', 13715: 'words):\\n', 13716: '.append(word)\\n', 13717: 'result\\n\\ndef', 13718: 'search2(substring,', 13719: 'word\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 13720: \"search1('zz',\", 13721: '.words()):\\n', 13722: 'print(item,', 13723: 'end=', 13724: \")\\ngrizzlies'\", 13725: 'fizzled', 13726: 'rizzuto', 13727: 'huzzahs', 13728: 'dazzler', 13729: 'jazz', 13730: 'pezza', 13731: \"search2('zz',\", 13732: '.\\n\\n\\nexample', 13733: '(code_search_examples', 13734: 'list\\n\\nthe', 13735: 'search2()', 13736: 'called,', 13737: 'yield\\nstatement', 13738: 'does\\nany', 13739: 'another\\nword,', 13740: 'continued', 13741: 'stopped,', 13742: 'until\\nthe', 13743: 'encounters', 13744: 'is\\ntypically', 13745: 'efficient,', 13746: 'is\\nrequired', 13747: 'allocate', 13748: 'additional\\nmemory', 13749: 'above)', 13750: 'sophisticated', 13751: 'produces\\nall', 13752: 'permutations', 13753: 'permutations()\\nfunction', 13754: 'permutations(seq):\\n', 13755: 'len(seq)', 13756: '1:\\n', 13757: 'seq\\n', 13758: 'perm', 13759: 'permutations(seq[1:]):\\n', 13760: 'range(len(perm)+1):\\n', 13761: 'perm[:i]', 13762: 'seq[0:1]', 13763: 'perm[i:]\\n', 13764: \"list(permutations(['police',\", 13765: \"'buffalo']))\", 13766: \"\\n[['police',\", 13767: \"'buffalo'],\", 13768: \"['fish',\", 13769: \"'police',\", 13770: \"'buffalo'],\\n\", 13771: \"'buffalo',\", 13772: \"'police'],\", 13773: \"['police',\", 13774: \"'fish'],\\n\", 13775: \"['buffalo',\", 13776: \"'fish'],\", 13777: \"'police']]\\n\\n\\n\\n\\nnote\\nthe\", 13778: 'recursion,\\ndiscussed', 13779: 'is\\nuseful', 13780: 'grammar', 13781: '(8', 13782: '.\\n\\n\\n\\nhigher-order', 13783: 'functions\\npython', 13784: 'higher-order', 13785: 'standard\\nfeatures', 13786: 'functional', 13787: 'haskell', 13788: 'alongside', 13789: 'expression\\nusing', 13790: 'is_content_word()\\nwhich', 13791: 'filter(),\\nwhich', 13792: 'applies', 13793: 'contained\\nin', 13794: 'retains', 13795: 'which\\nthe', 13796: 'is_content_word(word):\\n', 13797: 'list(filter(is_content_word,', 13798: \"sent))\\n['take',\", 13799: \"'themselves']\\n>>>\", 13800: \"is_content_word(w)]\\n['take',\", 13801: \"'themselves']\\n\\n\\n\\nanother\", 13802: 'map(),', 13803: 'the\\nextract_property()', 13804: 'news\\nsection', 13805: 'comprehension\\ncalculation:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 13806: 'list(map(len,', 13807: \".sents(categories='news')))\\n>>>\", 13808: 'sum(lengths)', 13809: 'len(lengths)\\n21', 13810: '.75081116158339\\n>>>', 13811: '[len(sent)', 13812: \".sents(categories='news')]\\n>>>\", 13813: '.75081116158339\\n\\n\\n\\nin', 13814: 'is_content_word()\\nand', 13815: 'list(map(lambda', 13816: 'len(filter(lambda', 13817: 'c:', 13818: 'aeiou,', 13819: 'w)),', 13820: 'sent))\\n[2,', 13821: '0,', 13822: '0]\\n>>>', 13823: '[len(c', 13824: 'aeiou)', 13825: 'sent]\\n[2,', 13826: '0]\\n\\n\\n\\nthe', 13827: 'the\\nsolutions', 13828: 'favored', 13829: 'former\\napproach', 13830: 'throughout', 13831: '.\\n\\n\\nnamed', 13832: 'arguments\\nwhen', 13833: 'confused', 13834: 'the\\ncorrect', 13835: 'assign\\nthem', 13836: 'calling\\nprogram', 13837: \"repeat(msg='<empty>',\", 13838: 'num=1):\\n', 13839: 'num\\n>>>', 13840: \"repeat(num=3)\\n'<empty><empty><empty>'\\n>>>\", 13841: \"repeat(msg='alice')\\n'alice'\\n>>>\", 13842: 'repeat(num=5,', 13843: \"msg='alice')\\n'alicealicealicealicealice'\\n\\n\\n\\n\\n\\nthese\", 13844: 'unnamed', 13845: 'takes\\nan', 13846: '*args', 13847: 'and\\nan', 13848: '**kwargs', 13849: '.\\n(dictionaries', 13850: 'generic(*args,', 13851: '**kwargs):\\n', 13852: 'print(args)\\n', 13853: 'print(kwargs)\\n', 13854: 'generic(1,', 13855: 'african', 13856: 'swallow,', 13857: 'monty=python)\\n(1,', 13858: \"'african\", 13859: \"swallow')\\n{'monty':\", 13860: \"'python'}\\n\\n\\n\\nwhen\", 13861: 'aspect', 13862: 'which\\noperates', 13863: '*song', 13864: \"that\\nthere's\", 13865: 'song', 13866: \"[['four',\", 13867: \"'calling',\", 13868: \"'birds'],\\n\", 13869: \"['three',\", 13870: \"'hens'],\\n\", 13871: \"['two',\", 13872: \"'turtle',\", 13873: \"'doves']]\\n>>>\", 13874: 'list(zip(song[0],', 13875: 'song[1],', 13876: \"song[2]))\\n[('four',\", 13877: \"'two'),\", 13878: \"('calling',\", 13879: \"'turtle'),\", 13880: \"('birds',\", 13881: \"'hens',\", 13882: \"'doves')]\\n>>>\", 13883: \"list(zip(*song))\\n[('four',\", 13884: \"'doves')]\\n\\n\\n\\nit\", 13885: 'convenient\\nshorthand,', 13886: 'song[0],', 13887: 'song[2]', 13888: 'function\\ndefinition,', 13889: 'freq_words(file,', 13890: 'min=1,', 13891: 'num=10):\\n', 13892: 'word_tokenize(text)\\n', 13893: '.freqdist(t', 13894: 'min)\\n', 13895: '.most_common(num)\\n>>>', 13896: 'fw', 13897: \"freq_words('ch01\", 13898: \".rst',\", 13899: 'min=4,', 13900: 'num=10)\\n>>>', 13901: 'num=10,', 13902: 'min=4)\\n\\n\\n\\na', 13903: 'side-effect', 13904: 'optionality', 13905: 'happy', 13906: \"value:\\nfreq_words('ch01\", 13907: 'min=4),', 13908: 'reports', 13909: 'its\\nprogress', 13910: 'set:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 13911: 'verbose=false):\\n', 13912: 'freqdist()\\n', 13913: 'verbose:', 13914: 'print(opening,', 13915: 'file)\\n', 13916: 'print(read', 13917: '%d', 13918: 'len(file))\\n', 13919: 'word_tokenize(text):\\n', 13920: 'min:\\n', 13921: 'freqdist[word]', 13922: '.n()', 13923: 'print(', 13924: 'sep=)\\n', 13925: 'print\\n', 13926: '.most_common(num)\\n\\n\\n\\n\\ncaution!\\ntake', 13927: 'mutable', 13928: 'debugging', 13929: '.\\n\\n\\ncaution!\\nif', 13930: 'to\\nclose', 13931: 'will\\nclose', 13932: 'open(lexicon', 13933: '.txt)', 13934: 'data\\n\\n\\n\\n\\n\\n\\n\\n4', 13935: '.6\\xa0\\xa0\\xa0program', 13936: 'development\\nprogramming', 13937: 'skill', 13938: 'acquired', 13939: 'of\\nexperience', 13940: 'key\\nhigh-level', 13941: 'abilities', 13942: 'design', 13943: 'manifestation', 13944: 'in\\nstructured', 13945: 'familiarity\\nwith', 13946: 'diagnostic', 13947: 'trouble-shooting', 13948: 'which\\ndoes', 13949: 'exhibit', 13950: 'describes', 13951: 'and\\nhow', 13952: 'organize', 13953: 'multi-module', 13954: 'various\\nkinds', 13955: 'development,', 13956: 'can\\ndo', 13957: 'still,', 13958: '.\\n\\nstructure', 13959: 'module\\nthe', 13960: 'logically-related', 13961: 'functions\\ntogether', 13962: 'facilitate', 13963: 'be\\nkept', 13964: 'separators,\\nor', 13965: 'extn', 13966: '.inf', 13967: 'updated,\\nyou', 13968: 'could\\ncontain', 13969: 'as\\nsyntax', 13970: 'as\\nplotting', 13971: 'some\\nexamples', 13972: 'emulate', 13973: 'your\\nsystem', 13974: '__file__', 13975: '.metrics', 13976: '.distance', 13977: \".__file__\\n'/usr/lib/python2\", 13978: '.5/site-packages/nltk/metrics/distance', 13979: \".pyc'\\n\\n\\n\\nthis\", 13980: 'compiled', 13981: '.pyc', 13982: \"and\\nyou'll\", 13983: 'same\\ndirectory', 13984: '.\\nalternatively,', 13985: 'web\\nat', 13986: 'http://code', 13987: '.com/p/nltk/source/browse/trunk/nltk/nltk/metrics/distance', 13988: '.\\nlike', 13989: 'distance', 13990: 'comment\\nlines', 13991: 'title', 13992: '.\\n(since', 13993: 'distributed,', 13994: 'the\\ncode', 13995: 'available,', 13996: 'copyright', 13997: '.)\\nnext', 13998: 'module-level', 13999: 'when\\nsomeone', 14000: 'help(nltk', 14001: '.distance)', 14002: '.\\n\\n\\n#', 14003: 'toolkit:', 14004: 'metrics\\n#\\n#', 14005: '(c)', 14006: '2001-2019', 14007: 'project\\n#', 14008: 'author:', 14009: '<edloper@gmail', 14010: '.com>\\n#', 14011: '<stevenbird1@gmail', 14012: 'tom', 14013: 'lippincott', 14014: '<tom@cs', 14015: '.columbia', 14016: '.edu>\\n#', 14017: 'url:', 14018: '<http://nltk', 14019: '.org/>\\n#', 14020: '.txt\\n#\\n\\n\\ndistance', 14021: 'metrics', 14022: '.\\n\\ncompute', 14023: '(usually', 14024: 'metrics,', 14025: 'requirements:\\n\\n1', 14026: 'd(a,', 14027: 'a)', 14028: '0\\n2', 14029: 'b)', 14030: '0\\n3', 14031: 'c)', 14032: 'd(b,', 14033: 'c)\\n\\n\\nafter', 14034: 'module,\\nthen', 14035: 'variables,\\nfollowed', 14036: 'most\\nof', 14037: 'classes,', 14038: 'block\\nof', 14039: 'object-oriented', 14040: '.\\n(most', 14041: 'demo()', 14042: '.)\\n\\nnote\\nsome', 14043: '_helper(),\\nsince', 14044: 'hide', 14045: 'one,\\nusing', 14046: 'idiom:', 14047: 'externally', 14048: '__all__', 14049: \"['edit_distance',\", 14050: \"'jaccard_distance']\", 14051: '.\\n\\n\\n\\nmulti-module', 14052: 'programs\\nsome', 14053: 'loading', 14054: 'from\\na', 14055: 'stable', 14056: 'visualizations', 14057: 'functions\\nfrom', 14058: 'scenario', 14059: 'depicted', 14060: 'program:', 14061: 'my_program', 14062: 'modules;', 14063: 'localized', 14064: 'while\\ncommon', 14065: 'visualization', 14066: '.\\n\\nby', 14067: 'dividing', 14068: 'to\\naccess', 14069: 'elsewhere,', 14070: 'simple\\nand', 14071: 'growing', 14072: 'involving\\na', 14073: 'designing', 14074: 'a\\ncomplex', 14075: 'engineering', 14076: '.\\n\\n\\nsources', 14077: 'error\\nmastery', 14078: 'problem-solving', 14079: 'to\\ndraw', 14080: 'trivial', 14081: 'as\\na', 14082: 'mis-placed', 14083: 'bugs', 14084: 'damage\\nthey', 14085: 'creep', 14086: 'unnoticed,', 14087: 'later\\nwhen', 14088: '.\\nsometimes,', 14089: 'fixing', 14090: 'another,', 14091: 'impression\\nthat', 14092: 'reassurance', 14093: 'are\\nspontaneous', 14094: 'fault', 14095: '.\\nflippancy', 14096: 'aside,', 14097: 'for\\nit', 14098: 'faulty', 14099: 'algorithm,', 14100: 'or\\neven', 14101: 'examples\\nof', 14102: 'unexpected', 14103: '.01,', 14104: 'three\\ncomponents', 14105: 'initially\\ndecomposed', 14106: \"split('\", 14107: \".')\", 14108: 'broke', 14109: 'phd,', 14110: 'synset\\nname', 14111: '.d', 14112: \"rsplit('\", 14113: 'splits,', 14114: 'rightmost', 14115: 'intact', 14116: 'tested\\nthe', 14117: 'released,', 14118: 'weeks', 14119: 'detected\\nthe', 14120: '.com/p/nltk/issues/detail?id=297)', 14121: 'the\\nauthors', 14122: 'antonyms', 14123: 'though\\nthe', 14124: 'database', 14125: 'antonym', 14126: 'misunderstanding', 14127: 'itself:', 14128: 'for\\nlemmas,', 14129: 'misunderstanding\\nof', 14130: '.com/p/nltk/issues/detail?id=98)', 14131: '.\\n\\n\\nthird,', 14132: 'assumption', 14133: 'relative\\nscope', 14134: '.%s', 14135: '.%02d', 14136: 'run-time\\nerror', 14137: 'typeerror:', 14138: 'percent', 14139: 'than\\nthe', 14140: 'to\\nforce', 14141: 'are\\ndefining', 14142: 'a\\ngiven', 14143: 'initial\\nvalue', 14144: 'parameter:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 14145: 'find_words(text,', 14146: 'wordlength,', 14147: 'result=[]):\\n', 14148: 'wordlength:\\n', 14149: 'result\\n>>>', 14150: \"find_words(['omg',\", 14151: \"'teh',\", 14152: \"'lolcat',\", 14153: \"'sitted',\", 14154: \"'mat'],\", 14155: \"\\n['omg',\", 14156: \"'mat']\\n>>>\", 14157: \"['ur'])\", 14158: \"\\n['ur',\", 14159: \"'on']\\n>>>\", 14160: \"'mat',\", 14161: \"'omg',\", 14162: \"'mat']\\n\\n\\n\\nthe\", 14163: 'find_words()', 14164: 'three-letter\\nwords', 14165: 'result,\\na', 14166: 'one-element', 14167: \"['ur'],\", 14168: 'expected,', 14169: 'the\\nother', 14170: '\\nwe', 14171: 'result!\\neach', 14172: 'will\\nsimply', 14173: 'call,', 14174: \"program's\\nbehavior\", 14175: 'default\\nvalue', 14176: 'is\\ncreated', 14177: 'once,', 14178: 'loads', 14179: '.\\n\\n\\ndebugging', 14180: 'techniques\\nsince', 14181: 'assumptions,\\nthe', 14182: '.\\nlocalize', 14183: 'variables,', 14184: 'progressed', 14185: 'exception', 14186: 'run-time', 14187: 'stack', 14188: 'trace,\\npinpointing', 14189: 'reduce', 14190: 'smallest\\nsize', 14191: 'line\\nof', 14192: 'to\\nrecreate', 14193: 'situation', 14194: 'some\\nvariables', 14195: 'copy-paste', 14196: 'offending', 14197: 'session\\nand', 14198: 'reading\\nsome', 14199: 'purport', 14200: 'do\\nthe', 14201: 'explaining', 14202: 'to\\nsomeone', 14203: 'else,', 14204: 'debugger', 14205: 'monitor', 14206: 'execution\\nof', 14207: 'breakpoints),\\nand', 14208: 'pdb\\n>>>', 14209: 'mymodule\\n>>>', 14210: 'pdb', 14211: \".run('mymodule\", 14212: \".myfunction()')\\n\\n\\n\\nit\", 14213: '(pdb)', 14214: 'instructions\\nto', 14215: '.\\ntyping', 14216: 'and\\nstop', 14217: 'function\\nand', 14218: 'similar,\\nbut', 14219: 'stops', 14220: 'the\\nbreak', 14221: 'breakpoints', 14222: 'type\\ncontinue', 14223: 'continue', 14224: 'breakpoint', 14225: '.\\ntype', 14226: 'find_words()\\nfunction', 14227: 'arose', 14228: 'was\\ncalled', 14229: ',\\nusing', 14230: 'smallest', 14231: 'the\\ndebugger', 14232: \"find_words(['cat'],\", 14233: \"\\n['cat']\\n>>>\", 14234: \".run(find_words(['dog'],\", 14235: '3))', 14236: '\\n>', 14237: '<string>(1)<module>()\\n(pdb)', 14238: 'step\\n--call--\\n>', 14239: '<stdin>(1)find_words()\\n(pdb)', 14240: 'args\\ntext', 14241: \"['dog']\\nwordlength\", 14242: '3\\nresult', 14243: \"['cat']\\n\\n\\n\\nhere\", 14244: 'debugger:', 14245: 'inside\\nthe', 14246: 'args', 14247: 'parameters)', 14248: \"['cat'],\", 14249: 'not\\nthe', 14250: 'helped', 14251: 'problem,\\nprompting', 14252: '.\\n\\n\\ndefensive', 14253: 'programming\\nin', 14254: 'debugging,', 14255: 'adopt\\nsome', 14256: 'habits', 14257: '20-line\\nprogram', 14258: 'bottom-up', 14259: 'of\\nsmall', 14260: 'these\\npieces', 14261: 'unit,', 14262: 'works\\nas', 14263: 'code,\\nspecifying', 14264: 'assert(isinstance(text,', 14265: 'list))', 14266: 'your\\ncode', 14267: 'assertionerror\\nand', 14268: 'immediate', 14269: 'notification', 14270: 'bug,', 14271: 'bugfix', 14272: 're-running', 14273: \"isn't\", 14274: 'fixed,', 14275: 'trap', 14276: 'changing\\nthe', 14277: 'magically', 14278: 'change,', 14279: 'articulate', 14280: 'what\\nis', 14281: 'undo', 14282: 'change\\nif', 14283: 'resolved', 14284: 'functionality,', 14285: 'bugs,\\nit', 14286: 'suite', 14287: 'regression', 14288: 'testing,', 14289: 'detect\\nsituations', 14290: 'regresses', 14291: 'unintended', 14292: 'breaking', 14293: 'that\\nused', 14294: 'framework\\nin', 14295: 'file\\nof', 14296: 'like\\nan', 14297: 'session,', 14298: 'seen\\nmany', 14299: 'finds,\\nand', 14300: 'original\\nfile', 14301: 'mismatch,', 14302: 'actual\\nvalues', 14303: 'at\\nhttp://docs', 14304: '.org/library/doctest', 14305: 'its\\nvalue', 14306: 'for\\nensuring', 14307: 'sync', 14308: 'strategy', 14309: 'to\\nset', 14310: 'clearly,', 14311: 'function\\nnames,', 14312: 'into\\nfunctions', 14313: 'well-documented', 14314: '.7\\xa0\\xa0\\xa0algorithm', 14315: 'design\\nthis', 14316: 'algorithmic', 14317: 'adapting\\nan', 14318: 'are\\nseveral', 14319: 'alternatives,', 14320: 'knowledge\\nabout', 14321: '.\\nwhole', 14322: 'introduce\\nsome', 14323: 'elaborate', 14324: 'prevalent\\nin', 14325: 'divide-and-conquer', 14326: 'attack', 14327: 'n/2,\\nsolve', 14328: 'problems,', 14329: 'pile', 14330: 'card', 14331: 'splitting', 14332: 'people\\nto', 14333: '(they', 14334: 'turn)', 14335: 'piles', 14336: 'back,', 14337: 'merge', 14338: 'divide-and-conquer:', 14339: 'array,', 14340: 'and\\nsort', 14341: '(recursively);', 14342: 'whole\\nlist', 14343: '(again', 14344: 'recursively);', 14345: '.\\n\\nanother', 14346: 'open\\nthe', 14347: 'somewhere', 14348: 'middle', 14349: 'current\\npage', 14350: 'first\\nhalf;', 14351: 'called\\nbinary', 14352: 'splits', 14353: 'design,', 14354: 'problem\\nby', 14355: 'transforming', 14356: 'duplicate', 14357: 'pre-sort\\nthe', 14358: 'elements\\nare', 14359: '.\\n\\nrecursion\\nthe', 14360: 'property:\\nto', 14361: 'and\\nthen', 14362: 'n/2', 14363: 'recursion', 14364: 'simplifies', 14365: 'problem,\\nand', 14366: 'instances\\nof', 14367: 'solution\\nfor', 14368: 'to\\ncalculate', 14369: '(n=1),', 14370: 'is\\njust', 14371: 'two\\nwords,', 14372: 'three\\nwords', 14373: 'words,\\nthere', 14374: '1\\nways', 14375: 'factorial', 14376: 'factorial1(n):\\n', 14377: '*=', 14378: '(i+1)\\n', 14379: 'result\\n\\n\\n\\nhowever,', 14380: 'recursive', 14381: 'problem,\\nbased', 14382: 'observation', 14383: 'to\\nconstruct', 14384: 'orderings', 14385: 'then\\nfor', 14386: 'ordering,', 14387: 'can\\ninsert', 14388: 'word:', 14389: 'start,', 14390: 'n-2\\nboundaries', 14391: 'base', 14392: 'single\\nword,', 14393: 'ordering', 14394: 'factorial2(n):\\n', 14395: 'factorial2(n-1)\\n\\n\\n\\n\\nthese', 14396: 'iteration\\nwhile', 14397: 'deeply-nested', 14398: 'the\\nwordnet', 14399: 'rooted', 14400: 'the\\nsize', 14401: 'together\\n(we', 14402: 'itself)', 14403: 'following\\nfunction', 14404: 'size1()', 14405: 'work;', 14406: 'size1():\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 14407: 'size1(s):\\n', 14408: 'sum(size1(child)', 14409: '.hyponyms())\\n\\n\\n\\nwe', 14410: 'iterative', 14411: 'processes\\nthe', 14412: 'layers', 14413: 'layer', 14414: 'the\\nhyponyms', 14415: 'layer\\nby', 14416: 'maintains', 14417: 'size2(s):\\n', 14418: '[s]', 14419: 'layer:\\n', 14420: 'len(layer)', 14421: '[h', 14422: '.hyponyms()]', 14423: 'total\\n\\n\\n\\nnot', 14424: 'longer,', 14425: 'forces', 14426: 'procedurally,', 14427: 'happening', 14428: 'ourselves\\nthat', 14429: 'import\\nstatement,', 14430: 'abbreviate', 14431: 'wn:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 14432: \".synset('dog\", 14433: 'size1(dog)\\n190\\n>>>', 14434: 'size2(dog)\\n190\\n\\n\\n\\nas', 14435: 'recursion,', 14436: 'construct\\na', 14437: 'trie', 14438: 'used\\nfor', 14439: 'name\\nis', 14440: 'retrieval)', 14441: 'trie\\ncontained', 14442: 'trie,', 14443: \"trie['c']\", 14444: 'smaller\\ntrie', 14445: 'held', 14446: '.\\n4', 14447: 'trie,\\nusing', 14448: '(3)', 14449: 'chien', 14450: '(french', 14451: 'dog),\\nwe', 14452: 'recursively', 14453: 'hien\\ninto', 14454: 'sub-trie', 14455: 'continues\\nuntil', 14456: 'we\\nstore', 14457: 'dog)', 14458: 'insert(trie,', 14459: 'key,', 14460: 'value):\\n', 14461: 'key:\\n', 14462: 'key[0],', 14463: 'key[1:]\\n', 14464: 'trie:\\n', 14465: 'trie[first]', 14466: '{}\\n', 14467: 'insert(trie[first],', 14468: 'rest,', 14469: 'value)\\n', 14470: \"trie['value']\", 14471: 'value\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 14472: '{}\\n>>>', 14473: \"'chat',\", 14474: \"'cat')\\n>>>\", 14475: \"'chien',\", 14476: \"'dog')\\n>>>\", 14477: \"'chair',\", 14478: \"'flesh')\\n>>>\", 14479: \"'chic',\", 14480: \"'stylish')\\n>>>\", 14481: 'dict(trie)', 14482: 'nicer', 14483: 'printing\\n>>>', 14484: \"trie['c']['h']['a']['t']['value']\\n'cat'\\n>>>\", 14485: '.pprint(trie,', 14486: \"width=40)\\n{'c':\", 14487: \"{'h':\", 14488: \"{'a':\", 14489: \"{'t':\", 14490: \"{'value':\", 14491: \"'cat'}},\\n\", 14492: \"{'i':\", 14493: \"{'r':\", 14494: \"'flesh'}}},\\n\", 14495: \"'i':\", 14496: \"{'e':\", 14497: \"{'n':\", 14498: \"'dog'}}}\\n\", 14499: \"{'c':\", 14500: \"'stylish'}}}}}\\n\\n\\nexample\", 14501: '(code_trie', 14502: 'trie:', 14503: 'dictionary\\nstructure;', 14504: 'nesting', 14505: 'prefix,\\nand', 14506: 'continuations', 14507: '.\\n\\n\\ncaution!\\ndespite', 14508: 'simplicity', 14509: 'be\\npushed', 14510: 'stack,', 14511: 'completed,', 14512: 'execution\\ncan', 14513: 'reason,', 14514: 'iterative\\nsolutions', 14515: '.\\n\\n\\n\\nspace-time', 14516: 'tradeoffs\\nwe', 14517: 'significantly', 14518: 'speed', 14519: 'auxiliary\\ndata', 14520: 'implements', 14521: 'simple\\ntext', 14522: 'it\\nprovides', 14523: 'faster', 14524: 'raw(file):\\n', 14525: 'contents)\\n', 14526: 'contents\\n\\ndef', 14527: 'snippet(doc,', 14528: 'term):\\n', 14529: \"'*30\", 14530: 'raw(doc)', 14531: \"'*30\\n\", 14532: '.index(term)\\n', 14533: 'text[pos-30:pos+30]\\n\\nprint(building', 14534: '.)\\nfiles', 14535: '.movie_reviews', 14536: '.abspaths()\\nidx', 14537: '.index((w,', 14538: 'f)', 14539: 'raw(f)', 14540: '.split())\\n\\nquery', 14541: \"''\\nwhile\", 14542: 'quit:\\n', 14543: 'input(query>', 14544: 'raw_input()', 14545: '2\\n', 14546: 'idx:\\n', 14547: 'doc', 14548: 'idx[query]:\\n', 14549: 'print(snippet(doc,', 14550: 'query))\\n', 14551: 'print(not', 14552: 'found)\\n\\n\\nexample', 14553: '(code_search_documents', 14554: 'system\\n\\na', 14555: 'space-time', 14556: 'tradeoff', 14557: 'corpus\\nwith', 14558: 'invert', 14559: 'its\\nidentifier', 14560: 'preprocessed,', 14561: '.\\nany', 14562: '.11\\nfor', 14563: '.\\n\\n\\n\\n\\n\\n\\xa0\\n\\ndef', 14564: 'preprocess(tagged_corpus):\\n', 14565: 'set()\\n', 14566: 'tagged_corpus:\\n', 14567: '.add(word)\\n', 14568: '.add(tag)\\n', 14569: 'wm', 14570: 'dict((w,', 14571: 'enumerate(words))\\n', 14572: 'tm', 14573: 'dict((t,', 14574: 'enumerate(tags))\\n', 14575: '[[(wm[w],', 14576: 'tm[t])', 14577: '(w,', 14578: 'tagged_corpus]\\n\\n\\nexample', 14579: '(code_strings_to_ints', 14580: 'integers\\n\\nanother', 14581: 'maintaining', 14582: 'an\\nexisting', 14583: 'membership\\nof', 14584: 'membership', 14585: 'the\\ncorresponding', 14586: 'claim', 14587: 'timeit', 14588: 'timer', 14589: 'times,', 14590: 'setup', 14591: 'executed\\nonce', 14592: 'simulate', 14593: 'of\\n100,000', 14594: '\\nof', 14595: 'random\\nitem', 14596: '50%', 14597: 'timer\\n>>>', 14598: '100000\\n>>>', 14599: 'setup_list', 14600: 'random;', 14601: 'range(%d)', 14602: 'setup_set', 14603: 'set(range(%d))', 14604: '.randint(0,', 14605: '%d)', 14606: '(vocab_size', 14607: 'print(timer(statement,', 14608: 'setup_list)', 14609: '.timeit(1000))\\n2', 14610: '.78092288971\\n>>>', 14611: 'setup_set)', 14612: '.timeit(1000))\\n0', 14613: '.0037260055542\\n\\n\\n\\nperforming', 14614: '1000', 14615: 'seconds,\\nwhile', 14616: 'mere', 14617: '.0037', 14618: 'seconds,\\nor', 14619: 'orders', 14620: 'magnitude', 14621: 'faster!\\n\\n\\ndynamic', 14622: 'programming\\ndynamic', 14623: 'algorithms\\nwhich', 14624: \"term\\n'programming'\", 14625: 'expect,\\nto', 14626: 'planning', 14627: 'scheduling', 14628: 'a\\nproblem', 14629: 'sub-problems', 14630: 'computing\\nsolutions', 14631: 'repeatedly,', 14632: 'a\\nlookup', 14633: 'remainder', 14634: 'programming,\\nbut', 14635: 'parsing', 14636: '.\\npingala', 14637: '5th', 14638: 'century', 14639: '.c', 14640: '.,\\nand', 14641: 'wrote', 14642: 'treatise', 14643: 'sanskrit', 14644: 'prosody', 14645: 'chandas', 14646: 'shastra', 14647: '.\\nvirahanka', 14648: '6th', 14649: 'meter\\nof', 14650: 'syllables,', 14651: 'marked', 14652: 'unit', 14653: 'while\\nlong', 14654: 'l,', 14655: 'meter', 14656: 'v4', 14657: '{ll,', 14658: 'ssl,', 14659: 'sls,\\nlss,', 14660: 'ssss}', 14661: 'two\\nsubsets,', 14662: 'with\\ns,', 14663: '(1)', 14664: '(1)\\nv4', 14665: '=\\n', 14666: 'll,', 14667: 'lss\\n', 14668: 'prefixed', 14669: 'v2', 14670: '{l,', 14671: 'ss}\\n', 14672: 'sls,', 14673: 'ssss\\n', 14674: 'v3', 14675: '{sl,', 14676: 'ls,', 14677: 'sss}\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\ndef', 14678: 'virahanka1(n):\\n', 14679: '0:\\n', 14680: '[s]\\n', 14681: 'virahanka1(n-1)]\\n', 14682: '[l', 14683: 'virahanka1(n-2)]\\n', 14684: 'l\\n\\ndef', 14685: 'virahanka2(n):\\n', 14686: '[[],', 14687: '[s]]\\n', 14688: 'range(n-1):\\n', 14689: 'lookup[i+1]]\\n', 14690: 'lookup[i]]\\n', 14691: '.append(s', 14692: 'l)\\n', 14693: 'lookup[n]\\n\\ndef', 14694: 'virahanka3(n,', 14695: 'lookup={0:[],', 14696: '1:[s]}):\\n', 14697: 'lookup:\\n', 14698: 'virahanka3(n-1)]\\n', 14699: 'virahanka3(n-2)]\\n', 14700: 'lookup[n]', 14701: 'l\\n', 14702: 'lookup[n]\\n\\nfrom', 14703: 'memoize\\n@memoize\\ndef', 14704: 'virahanka4(n):\\n', 14705: 'virahanka4(n-1)]\\n', 14706: 'virahanka4(n-2)]\\n', 14707: 'l\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 14708: \"virahanka1(4)\\n['ssss',\", 14709: \"'ssl',\", 14710: \"'sls',\", 14711: \"'lss',\", 14712: \"'ll']\\n>>>\", 14713: \"virahanka2(4)\\n['ssss',\", 14714: \"virahanka3(4)\\n['ssss',\", 14715: \"virahanka4(4)\\n['ssss',\", 14716: \"'ll']\\n\\n\\nexample\", 14717: '.12', 14718: '(code_virahanka', 14719: '.12:', 14720: 'meter:', 14721: '(i)', 14722: 'recursive;', 14723: 'programming;\\n(iii)', 14724: 'top-down', 14725: 'programming;', 14726: '(iv)', 14727: 'memoization', 14728: 'observation,', 14729: 'called\\nvirahanka1()', 14730: 'meters,', 14731: 'compute\\nv3', 14732: 'v3,\\nwe', 14733: 'v1', 14734: 'call\\nstructure', 14735: '(2)\\nas', 14736: 'problem,', 14737: 'but\\nit', 14738: 'wasteful', 14739: 'large:\\nto', 14740: 'v20', 14741: 'technique,', 14742: '4,181', 14743: 'times;\\nand', 14744: 'v40', 14745: '63,245,986', 14746: 'times!\\na', 14747: 'table\\nand', 14748: 'values,', 14749: 'virahanka2()', 14750: 'a\\ndynamic', 14751: 'filling', 14752: 'a\\ntable', 14753: '(called', 14754: 'lookup)', 14755: 'the\\nproblem,', 14756: 'crucially,', 14757: 'each\\nsub-problem', 14758: 'solved', 14759: 'smaller\\nproblems', 14760: 'the\\nbottom-up', 14761: 'unfortunately', 14762: 'applications,', 14763: 'it\\nmay', 14764: 'for\\nsolving', 14765: 'wasted', 14766: 'avoided\\nusing', 14767: 'is\\nillustrated', 14768: 'virahanka3()', 14769: 'approach,', 14770: 'avoids\\nthe', 14771: 'wastage', 14772: 'virahanka1()', 14773: 'has\\npreviously', 14774: 'not,', 14775: 'result\\nrecursively', 14776: 'stores', 14777: 'return\\nthe', 14778: 'virahanka4(),\\nis', 14779: 'decorator', 14780: 'memoize,\\nwhich', 14781: 'housekeeping', 14782: 'done\\nby', 14783: 'cluttering', 14784: 'previous\\ncall', 14785: 'parameters,\\nit', 14786: 'recalculating', 14787: '.\\n(this', 14788: 'encounter', 14789: '.8\\xa0\\xa0\\xa0a', 14790: 'libraries\\npython', 14791: 'libraries,', 14792: 'specialized', 14793: 'extend\\nthe', 14794: 'realize', 14795: 'power\\nof', 14796: '.\\n\\nmatplotlib\\npython', 14797: 'sophisticated\\nplotting', 14798: 'matlab-style', 14799: 'interface,', 14800: 'from\\nhttp://matplotlib', 14801: '.sourceforge', 14802: '.net/', 14803: 'print\\nstatements', 14804: 'lined', 14805: 'columns', 14806: 'display\\nnumerical', 14807: 'detect\\npatterns', 14808: 'numbers\\nshowing', 14809: 'classified\\nby', 14810: '.13', 14811: 'graphical\\nformat', 14812: '.14', 14813: 'display)', 14814: 'arange\\nfrom', 14815: 'pyplot\\n\\ncolors', 14816: \"'rgbcmyk'\", 14817: 'red,', 14818: 'green,', 14819: 'blue,', 14820: 'cyan,', 14821: 'magenta,', 14822: 'yellow,', 14823: 'black\\n\\ndef', 14824: 'bar_chart(categories,', 14825: 'counts):\\n', 14826: 'chart', 14827: 'category\\n', 14828: 'ind', 14829: 'arange(len(words))\\n', 14830: '(len(categories)', 14831: '1)\\n', 14832: 'bar_groups', 14833: 'range(len(categories)):\\n', 14834: 'bars', 14835: 'pyplot', 14836: '.bar(ind+c*width,', 14837: 'counts[categories[c]],', 14838: 'width,\\n', 14839: 'color=colors[c', 14840: 'len(colors)])\\n', 14841: '.append(bars)\\n', 14842: '.xticks(ind+width,', 14843: 'words)\\n', 14844: '.legend([b[0]', 14845: 'bar_groups],', 14846: 'categories,', 14847: \"loc='upper\", 14848: \"left')\\n\", 14849: \".ylabel('frequency')\\n\", 14850: \".title('frequency\", 14851: \"genre')\\n\", 14852: '.show()\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 14853: \"'adventure']\\n>>>\", 14854: 'cfdist', 14855: 'genres\\n', 14856: '.words(categories=genre)\\n', 14857: 'modals)\\n', 14858: 'genres:\\n', 14859: 'counts[genre]', 14860: '[cfdist[genre][word]', 14861: 'modals]\\n>>>', 14862: 'bar_chart(genres,', 14863: 'counts)\\n\\n\\nexample', 14864: '(code_modal_plot', 14865: '.13:', 14866: 'corpus\\n\\n\\n\\nfigure', 14867: '.14:', 14868: 'this\\nvisualization', 14869: '.\\n\\n\\nfrom', 14870: 'have\\nalmost', 14871: 'fly', 14872: 'visitors', 14873: 'specify\\nsearch', 14874: 'dynamically', 14875: 'generated\\nvisualization', 14876: 'agg', 14877: 'backend', 14878: 'matplotlib,\\nwhich', 14879: 'raster', 14880: '(pixel)', 14881: 'images', 14882: 'the\\nresult', 14883: '.show(),', 14884: 'file\\nusing', 14885: '.savefig()', 14886: 'filename\\nthen', 14887: 'directs', 14888: 'use,', 14889: 'pyplot\\n>>>', 14890: \"use('agg')\", 14891: \".savefig('modals\", 14892: \".png')\", 14893: \"print('content-type:\", 14894: \"text/html')\\n>>>\", 14895: 'print()\\n>>>', 14896: \"print('<html><body>')\\n>>>\", 14897: \"print('<img\", 14898: 'src=modals', 14899: \".png/>')\\n>>>\", 14900: \"print('</body></html>')\\n\\n\\n\\n\\n\\nnetworkx\\nthe\", 14901: 'networkx', 14902: 'edges,', 14903: 'is\\navailable', 14904: 'https://networkx', 14905: '.lanl', 14906: '.gov/', 14907: '.\\nnetworkx', 14908: 'to\\nvisualize', 14909: 'we\\nintroduced', 14910: '.15\\ninitializes', 14911: 'traverses\\nthe', 14912: 'traversal', 14913: ',\\napplying', 14914: 'in\\n4', 14915: 'nx\\nimport', 14916: 'matplotlib\\nfrom', 14917: 'wn\\n\\ndef', 14918: 'traverse(graph,', 14919: 'node):\\n', 14920: '.depth[node', 14921: '.name]', 14922: 'node', 14923: '.shortest_path_distance(start)\\n', 14924: '.hyponyms():\\n', 14925: '.add_edge(node', 14926: '.name,', 14927: '.name)', 14928: 'child)', 14929: '\\n\\ndef', 14930: 'hyponym_graph(start):\\n', 14931: 'nx', 14932: '.graph()', 14933: '.depth', 14934: 'traverse(g,', 14935: 'start)\\n', 14936: 'g\\n\\ndef', 14937: 'graph_draw(graph):\\n', 14938: '.draw_graphviz(graph,\\n', 14939: 'node_size', 14940: '[16', 14941: '.degree(n)', 14942: 'graph],\\n', 14943: 'node_color', 14944: '[graph', 14945: '.depth[n]', 14946: 'with_labels', 14947: 'false)\\n', 14948: '.pyplot', 14949: 'hyponym_graph(dog)\\n>>>', 14950: 'graph_draw(graph)\\n\\n\\nexample', 14951: '.15', 14952: '(code_networkx', 14953: '.15:', 14954: 'libraries\\n\\n\\n\\nfigure', 14955: '.16:', 14956: 'matplotlib:', 14957: 'displayed,', 14958: 'darkest', 14959: 'middle);\\nnode', 14960: 'node,', 14961: '.01;', 14962: 'produced\\nby', 14963: '.\\n\\n\\n\\ncsv\\nlanguage', 14964: 'tabulations,', 14965: 'study,', 14966: 'linguistic\\nfeatures', 14967: 'csv', 14968: 'format:\\n\\nsleep,', 14969: 'sli:p,', 14970: '.i,', 14971: '.\\nwalk,', 14972: 'wo:k,', 14973: '.intr,', 14974: 'lifting', 14975: 'setting', 14976: 'foot', 14977: '.\\nwake,', 14978: 'weik,', 14979: 'intrans,', 14980: 'cease', 14981: 'sleep\\n\\nwe', 14982: '.csv', 14983: 'csv\\n>>>', 14984: 'input_file', 14985: '.csv,', 14986: 'rb)', 14987: '.reader(input_file):', 14988: \"print(row)\\n['sleep',\", 14989: \"'sli:p',\", 14990: \"'v\", 14991: \".i',\", 14992: \"'a\", 14993: \".']\\n['walk',\", 14994: \"'wo:k',\", 14995: \".intr',\", 14996: \"'progress\", 14997: \".']\\n['wake',\", 14998: \"'weik',\", 14999: \"'intrans',\", 15000: \"'cease\", 15001: \"sleep']\\n\\n\\n\\neach\", 15002: 'numerical\\ndata,', 15003: 'using\\nint()', 15004: 'float()', 15005: '.\\n\\n\\nnumpy\\nthe', 15006: '.\\nnumpy', 15007: 'multi-dimensional', 15008: 'access:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 15009: 'array\\n>>>', 15010: 'cube', 15011: 'array([', 15012: '[[0,0,0],', 15013: '[1,1,1],', 15014: '[2,2,2]],\\n', 15015: '[[3,3,3],', 15016: '[4,4,4],', 15017: '[5,5,5]],\\n', 15018: '[[6,6,6],', 15019: '[7,7,7],', 15020: '[8,8,8]]', 15021: '])\\n>>>', 15022: 'cube[1,1,1]\\n4\\n>>>', 15023: 'cube[2]', 15024: '.transpose()\\narray([[6,', 15025: '8],\\n', 15026: '[6,', 15027: '8]])\\n>>>', 15028: 'cube[2,1:]\\narray([[7,', 15029: '7],\\n', 15030: '[8,', 15031: '8]])\\n\\n\\n\\nnumpy', 15032: 'linear', 15033: 'algebra', 15034: 'perform\\nsingular', 15035: 'decomposition', 15036: 'matrix,', 15037: 'used\\nin', 15038: 'latent', 15039: 'implicit\\nconcepts', 15040: 'linalg\\n>>>', 15041: 'a=array([[4,0],', 15042: '[3,-5]])\\n>>>', 15043: 'u,s,vt', 15044: 'linalg', 15045: '.svd(a)\\n>>>', 15046: 'u\\narray([[-0', 15047: '.4472136', 15048: '-0', 15049: '.89442719],\\n', 15050: '[-0', 15051: '.89442719,', 15052: ']])\\n>>>', 15053: 's\\narray([', 15054: '.32455532,', 15055: '.16227766])\\n>>>', 15056: 'vt\\narray([[-0', 15057: '.70710678,', 15058: '.70710678],\\n', 15059: \".70710678]])\\n\\n\\n\\nnltk's\", 15060: 'clustering', 15061: '.cluster', 15062: 'arrays,\\nand', 15063: 'k-means', 15064: 'gaussian', 15065: 'clustering,\\ngroup', 15066: 'agglomerative', 15067: 'dendrogram', 15068: 'details,', 15069: '.cluster)', 15070: '.\\n\\n\\nother', 15071: 'libraries\\nthere', 15072: 'the\\nhelp', 15073: 'http://pypi', 15074: 'relational', 15075: 'databases', 15076: 'mysql-python)\\nand', 15077: 'pylucene)', 15078: 'formats\\nsuch', 15079: 'msword,', 15080: '(pypdf,', 15081: 'pywin32,', 15082: '.etree),\\nrss', 15083: 'feeds', 15084: 'feedparser),\\nand', 15085: 'mail', 15086: 'imaplib,', 15087: 'email)', 15088: \".9\\xa0\\xa0\\xa0summary\\n\\npython's\", 15089: 'references;\\ne', 15090: 'operation\\non', 15091: 'b,', 15092: 'objects,\\nwhile', 15093: 'distinction\\nparallels', 15094: '.\\nstrings,', 15095: 'supporting\\ncommon', 15096: 'sorted(),', 15097: 'and\\nmembership', 15098: 'compact,\\nreadable', 15099: 'manually-incremented', 15100: 'usually\\nunnecessary;', 15101: 'enumerated,', 15102: 'essential', 15103: 'abstraction:', 15104: 'concepts\\nto', 15105: 'passing,', 15106: 'scope,', 15107: 'namespace:', 15108: 'visible\\noutside', 15109: 'declared', 15110: '.\\nmodules', 15111: 'variables\\nand', 15112: 'visible', 15113: '.\\ndynamic', 15114: 'nlp\\nthat', 15115: 'computations', 15116: 'avoid\\nunnecessary', 15117: 'recomputation', 15118: '.10\\xa0\\xa0\\xa0further', 15119: 'python,\\nand', 15120: 'scratched', 15121: 'surface,', 15122: 'for\\nthis', 15123: 'to\\nunderstand', 15124: '.org/library/functions', 15125: 'and\\nhttp://docs', 15126: '.org/library/stdtypes', 15127: 'generators', 15128: 'importance', 15129: 'efficiency;\\nfor', 15130: 'iterators,', 15131: 'topic,\\nsee', 15132: '.org/library/itertools', 15133: '.\\nconsult', 15134: 'multimedia', 15135: 'processing,\\nincluding', 15136: '(guzdial,', 15137: 'aware', 15138: 'that\\nyour', 15139: 'version\\nof', 15140: 'easily\\ncheck', 15141: 'have,', 15142: 'sys;', 15143: 'sys', 15144: '.version', 15145: '.\\nversion-specific', 15146: '.org/doc/versions/', 15147: '.\\nalgorithm', 15148: 'rich', 15149: 'some\\ngood', 15150: '(harel,', 15151: '(levitin,', 15152: '(knuth,', 15153: '.\\nuseful', 15154: 'guidance', 15155: 'provided\\nin', 15156: '(hunt', 15157: 'thomas,', 15158: '2000)', 15159: '(mcconnell,', 15160: '2004)', 15161: '.11\\xa0\\xa0\\xa0exercises\\n\\n☼', 15162: 'facility', 15163: 'help(str),', 15164: 'help(list),', 15165: 'help(tuple)', 15166: 'flanked', 15167: 'underscore;', 15168: 'shows,', 15169: 'something\\nmore', 15170: '.__getitem__(y)', 15171: 'long-winded\\nway', 15172: 'x[y]', 15173: 'tuples\\nand', 15174: 'on\\ntuples', 15175: 'generates\\na', 15176: \"['is',\", 15177: \"'nlp',\", 15178: \"'fun',\", 15179: \"'?']\", 15180: 'use\\na', 15181: 'words[1]', 15182: 'words[2])\\nand', 15183: 'temporary', 15184: 'transform', 15185: \"['nlp',\", 15186: \"'!']\", 15187: 'transformation\\nusing', 15188: 'cmp,', 15189: 'help(cmp)', 15190: 'operators?\\n\\n☼', 15191: 'sliding', 15192: 'n-grams\\nbehave', 15193: 'limiting', 15194: 'cases:', 15195: 'len(sent)?\\n\\n☼', 15196: 'pointed', 15197: 'to\\nfalse', 15198: 'a\\nboolean', 15199: 'non-boolean', 15200: 'boolean\\ncontexts,', 15201: 'inequality', 15202: \".\\n'monty'\", 15203: \"'z'\", 15204: \"'a'?\\ntry\", 15205: 'prefix,', 15206: \"'montague'\", 15207: '.\\nread', 15208: 'lexicographical', 15209: 'is\\ngoing', 15210: \".\\n('monty',\", 15211: \"('monty',\", 15212: 'expected?\\n\\n☼', 15213: 'a\\nstring,', 15214: 'normalizes', 15215: 'single\\nspace', 15216: '.\\n\\ndo', 15217: 'join()\\ndo', 15218: 'substitutions\\n\\n\\n☼', 15219: 'helper', 15220: 'function\\ncmp_len', 15221: 'cmp', 15222: 'sent1\\nand', 15223: 'sent1[:]', 15224: '.\\nmodify', 15225: 'to\\nrepresent', 15226: 'assign\\ntext2', 15227: 'text1[:],', 15228: 'words,\\ne', 15229: 'text1[1][1]', 15230: '.\\nexplain', 15231: '.\\nload', 15232: 'deepcopy()', 15233: 'deepcopy),\\nconsult', 15234: 'any\\nobject', 15235: 'n-by-m', 15236: 'list\\nmultiplication,', 15237: 'word_table', 15238: \"[['']\", 15239: 'word_table[1][2]', 15240: 'hello?\\nexplain', 15241: 'range()\\nto', 15242: 'two-dimensional', 15243: 'called\\nword_vowels', 15244: 'word_vowels[l][v]', 15245: 'novel10(text)', 15246: 'that\\nappeared', 15247: 'string,\\nsplits', 15248: \"the\\nword's\", 15249: 'frequency,', 15250: 'gematria,', 15251: 'for\\nmapping', 15252: 'hidden', 15253: 'of\\ntexts', 15254: '(http://en', 15255: '.org/wiki/gematria,', 15256: 'http://essenes', 15257: '.net/gemcal', 15258: '.htm)', 15259: 'gematria()', 15260: 'sums', 15261: 'letter_vals:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 15262: 'letter_vals', 15263: \"{'a':1,\", 15264: \"'b':2,\", 15265: \"'c':3,\", 15266: \"'d':4,\", 15267: \"'e':5,\", 15268: \"'f':80,\", 15269: \"'g':3,\", 15270: \"'h':8,\\n\", 15271: \"'i':10,\", 15272: \"'j':10,\", 15273: \"'k':20,\", 15274: \"'l':30,\", 15275: \"'m':40,\", 15276: \"'n':50,\", 15277: \"'o':70,\", 15278: \"'p':80,\", 15279: \"'q':100,\\n\", 15280: \"'r':200,\", 15281: \"'s':300,\", 15282: \"'t':400,\", 15283: \"'u':6,\", 15284: \"'v':6,\", 15285: \"'w':800,\", 15286: \"'x':60,\", 15287: \"'y':10,\", 15288: \"'z':7}\\n\\n\\n\\n\\nprocess\", 15289: '.state_union)', 15290: 'document,', 15291: 'how\\nmany', 15292: '666', 15293: 'decode()', 15294: 'gematria', 15295: 'equivalents,', 15296: '.\\n\\n\\n\\n◑', 15297: 'shorten(text,', 15298: 'n\\nmost', 15299: 'it?\\n\\n◑', 15300: 'someone\\nto', 15301: 'pronunciations;', 15302: 'whatever\\nproperties', 15303: 'entries)', 15304: 'sorts', 15305: 'for\\nproximity', 15306: 'synsets\\nminke_whale', 15307: '.01,\\nsort', 15308: 'shortest_path_distance()', 15309: 'right_whale', 15310: '(containing', 15311: 'duplicates)', 15312: 'and\\nreturns', 15313: 'chair,', 15314: 'chair', 15315: 'output\\nlist', 15316: 'arguments\\nand', 15317: '.\\ncan', 15318: '.difference()?\\n\\n◑', 15319: 'itemgetter()', 15320: \"python's\\nstandard\", 15321: 'itemgetter)', 15322: 'list\\nwords', 15323: 'calling:\\nsorted(words,', 15324: 'key=itemgetter(1)),', 15325: 'sorted(words,', 15326: 'key=itemgetter(-1))', 15327: 'lookup(trie,', 15328: 'key)', 15329: 'trie,\\nand', 15330: 'uniquely\\ndetermined', 15331: 'vanguard', 15332: 'vang-,\\nso', 15333: \"'vang')\", 15334: \"'vanguard'))\", 15335: 'linkage', 15336: '2006))', 15337: 'keywords', 15338: \"from\\nnltk's\", 15339: 'shakespeare', 15340: 'package,', 15341: 'networks', 15342: 'levenshtein', 15343: '.edit_distance()', 15344: 'programming?', 15345: 'or\\ntop-down', 15346: 'approach?\\n[see', 15347: 'http://norvig', 15348: '.com/spell-correct', 15349: '.html]\\n\\n◑', 15350: 'catalan', 15351: 'combinatorial', 15352: 'mathematics,\\nincluding', 15353: '(6)', 15354: 'series\\ncan', 15355: 'c0', 15356: 'and\\ncn+1', 15357: 'σ0', 15358: '(cicn-i)', 15359: 'nth', 15360: 'cn', 15361: 'n\\nincreases', 15362: '.\\n\\n\\n★\\nreproduce', 15363: '(zhao', 15364: 'zobel,', 15365: 'authorship', 15366: 'identification', 15367: 'gender-specific', 15368: 'choice,', 15369: 'can\\nreproduce', 15370: '.clintoneast', 15371: '.com/articles/words', 15372: '.php\\n\\n★', 15373: 'alphabetically\\nsorted', 15374: '.:\\n\\nchair:', 15375: \"'flesh'\\n---t:\", 15376: \"'cat'\\n--ic:\", 15377: \"'stylish'\\n---en:\", 15378: \"'dog'\\n\\n\\n★\", 15379: 'recursive\\nfunction', 15380: 'uniqueness', 15381: 'in\\neach', 15382: 'discarding', 15383: 'compression', 15384: 'this\\ngive?', 15385: 'text?\\n\\n★', 15386: 'justify\\nthe', 15387: 'width,', 15388: 'be\\napproximately', 15389: 'evenly', 15390: 'can\\nbegin', 15391: 'extractive', 15392: 'summarization', 15393: 'tool,', 15394: 'the\\nsentences', 15395: 'highest', 15396: 'use\\nsum', 15397: 'n\\nhighest-scoring', 15398: 'the\\ndesign', 15399: 'double\\nsorting', 15400: '.\\n\\n★\\nread', 15401: 'orientation', 15402: 'visualize\\na', 15403: 'different\\nsemantic', 15404: '.org/anthology/p97-1023\\n\\n★\\ndesign', 15405: 'statistically', 15406: 'improbable\\nphrases', 15407: '.\\nhttp://www', 15408: '.amazon', 15409: '.com/gp/search-inside/sipshelp', 15410: '.html\\n\\n★', 15411: 'brute-force', 15412: 'for\\ndiscovering', 15413: 'squares,', 15414: 'crossword\\nin', 15415: 'entry\\nin', 15416: 'discussion,', 15417: 'see\\nhttp://itre', 15418: '.edu/~myl/languagelog/archives/002679', 15419: '.html\\n\\n\\n\\n\\n\\nabout', 15420: 'acst5', 15421: 'words\\n\\n\\n\\n\\n\\n5', 15422: 'words\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nback', 15423: 'verbs,\\nadjectives,', 15424: 'adverbs', 15425: 'just\\nthe', 15426: 'grammarians,', 15427: 'analysis\\nof', 15428: 'processing?\\nwhat', 15429: 'categories?\\nhow', 15430: 'class?\\n\\nalong', 15431: 'including\\nsequence', 15432: 'labeling,', 15433: 'n-gram', 15434: 'models,', 15435: 'evaluation', 15436: 'techniques\\nare', 15437: 'which\\nto', 15438: 'typical\\nnlp', 15439: 'pipeline,', 15440: 'classifying', 15441: 'and\\nlabeling', 15442: 'accordingly', 15443: 'tagging,\\npos-tagging,', 15444: 'speech\\nare', 15445: 'tags\\nused', 15446: 'tagset', 15447: 'emphasis\\nin', 15448: '.\\n\\n1\\xa0\\xa0\\xa0using', 15449: 'tagger\\na', 15450: 'tagger,', 15451: 'pos-tagger,', 15452: 'attaches', 15453: 'a\\npart', 15454: \"(don't\", 15455: 'nltk):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 15456: 'word_tokenize(and', 15457: 'different)\\n>>>', 15458: \".pos_tag(text)\\n[('and',\", 15459: \"'cc'),\", 15460: \"('now',\", 15461: \"'rb'),\", 15462: \"'in'),\", 15463: \"('something',\", 15464: \"'nn'),\\n('completely',\", 15465: \"('different',\", 15466: \"'jj')]\\n\\n\\n\\nhere\", 15467: 'cc,', 15468: 'coordinating', 15469: 'conjunction;\\nnow', 15470: 'rb,', 15471: 'adverbs;\\nfor', 15472: 'preposition;\\nsomething', 15473: 'nn,', 15474: 'jj,', 15475: 'adjective', 15476: '.\\n\\nnote\\nnltk', 15477: 'tag,', 15478: 'queried', 15479: '.help', 15480: \".upenn_tagset('rb'),\", 15481: 'regular\\nexpression,', 15482: \".upenn_tagset('nn\", 15483: 'documentation,\\nsee', 15484: '.???', 15485: '.readme(),', 15486: 'substituting', 15487: 'name\\nof', 15488: 'homonyms:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 15489: 'word_tokenize(they', 15490: 'refuse', 15491: 'permit)\\n>>>', 15492: \".pos_tag(text)\\n[('they',\", 15493: \"'prp'),\", 15494: \"('refuse',\", 15495: \"'vbp'),\", 15496: \"'to'),\", 15497: \"('permit',\", 15498: \"'vb'),\", 15499: \"('us',\", 15500: \"'prp'),\\n('to',\", 15501: \"('obtain',\", 15502: \"'dt'),\", 15503: \"'nn'),\", 15504: \"'nn')]\\n\\n\\n\\nnotice\", 15505: 'a\\npresent', 15506: 'tense', 15507: '(vbp)', 15508: '(nn)', 15509: 'deny,', 15510: 'trash', 15511: 'homophones)', 15512: 'pronounce\\nthe', 15513: 'reason,\\ntext-to-speech', 15514: 'pos-tagging', 15515: '.)\\n\\nnote\\nyour', 15516: 'turn:\\nmany', 15517: 'ski', 15518: 'race,', 15519: 'nouns\\nor', 15520: 'of\\nothers?', 15521: 'commonplace', 15522: 'put\\nthe', 15523: 'or\\nthink', 15524: 'if\\nit', 15525: 'uses\\nof', 15526: 'pos-tagger', 15527: '.\\n\\nlexical', 15528: 'nn', 15529: 'have\\ntheir', 15530: 'obscure', 15531: 'what\\njustification', 15532: 'introducing', 15533: 'distribution\\nof', 15534: 'involving\\nwoman', 15535: 'noun),', 15536: 'bought', 15537: 'verb),\\nover', 15538: 'preposition),', 15539: 'determiner)', 15540: '.similar()', 15541: 'contexts\\nw1w', 15542: 'w2,\\nthen', 15543: \"w'\", 15544: 'context,\\ni', 15545: \"w1w'w2\", 15546: '.text(word', 15547: \".similar('woman')\\nbuilding\", 15548: 'word-context', 15549: '.\\nman', 15550: 'house', 15551: 'boy', 15552: 'job\\nstate', 15553: 'war', 15554: 'question\\n>>>', 15555: \".similar('bought')\\nmade\", 15556: 'heard', 15557: 'got\\nset', 15558: 'felt', 15559: 'told\\n>>>', 15560: \".similar('over')\\nin\", 15561: 'through\\nabout', 15562: 'is\\n>>>', 15563: \".similar('the')\\na\", 15564: 'no\\nsome', 15565: 'and\\n\\n\\n\\nobserve', 15566: 'nouns;\\nsearching', 15567: 'mostly', 15568: 'verbs;\\nsearching', 15569: 'prepositions;\\nsearching', 15570: 'determiners', 15571: '$150,000\\nworth', 15572: 'clothes', 15573: 'scrobbling', 15574: 'verb,\\nwith', 15575: 'scrobble,\\nand', 15576: '.\\n\\n\\n2\\xa0\\xa0\\xa0tagged', 15577: 'corpora\\n\\n2', 15578: '.1\\xa0\\xa0\\xa0representing', 15579: 'tokens\\nby', 15580: 'string\\nrepresentation', 15581: 'str2tuple():\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 15582: 'tagged_token', 15583: '.tag', 15584: \".str2tuple('fly/nn')\\n>>>\", 15585: \"tagged_token\\n('fly',\", 15586: \"'nn')\\n>>>\", 15587: \"tagged_token[0]\\n'fly'\\n>>>\", 15588: \"tagged_token[1]\\n'nn'\\n\\n\\n\\nwe\", 15589: 'first\\nstep', 15590: 'word/tag', 15591: 'convert\\neach', 15592: 'str2tuple())', 15593: \"'''\\n\", 15594: 'the/at', 15595: 'grand/jj', 15596: 'jury/nn', 15597: 'commented/vbd', 15598: 'on/in', 15599: 'a/at', 15600: 'number/nn', 15601: 'of/in\\n', 15602: 'other/ap', 15603: 'topics/nns', 15604: ',/,', 15605: 'among/in', 15606: 'them/ppo', 15607: 'atlanta/np', 15608: 'and/cc\\n', 15609: 'fulton/np-tl', 15610: 'county/nn-tl', 15611: 'purchasing/vbg', 15612: 'departments/nns', 15613: 'which/wdt', 15614: 'it/pps\\n', 15615: 'said/vbd', 15616: '``/``', 15617: 'are/ber', 15618: 'well/ql', 15619: 'operated/vbn', 15620: 'and/cc', 15621: 'follow/vb', 15622: 'generally/rb\\n', 15623: 'accepted/vbn', 15624: 'practices/nns', 15625: 'inure/vb', 15626: 'to/in', 15627: 'best/jjt\\n', 15628: 'interest/nn', 15629: 'of/in', 15630: 'both/abx', 15631: 'governments/nns', 15632: \"''/''\", 15633: './', 15634: '[nltk', 15635: '.str2tuple(t)', 15636: \".split()]\\n[('the',\", 15637: \"'at'),\", 15638: \"('grand',\", 15639: \"'jj'),\", 15640: \"('jury',\", 15641: \"('commented',\", 15642: \"'vbd'),\\n('on',\", 15643: \"('number',\", 15644: \".')]\\n\\n\\n\\n\\n\\n2\", 15645: '.2\\xa0\\xa0\\xa0reading', 15646: 'corpora\\nseveral', 15647: 'for\\ntheir', 15648: 'you\\nopened', 15649: 'editor:\\n\\nthe/at', 15650: 'grand/jj-tl', 15651: 'jury/nn-tl\\nsaid/vbd', 15652: 'friday/nr', 15653: 'an/at', 15654: 'investigation/nn', 15655: \"atlanta's/np$\\nrecent/jj\", 15656: 'primary/nn', 15657: 'election/nn', 15658: 'produced/vbd', 15659: 'no/at\\nevidence/nn', 15660: 'that/cs', 15661: 'any/dti', 15662: 'irregularities/nns', 15663: 'took/vbd\\nplace/nn', 15664: \"you\\ndon't\", 15665: 'above,\\nthe', 15666: 'uppercase,', 15667: 'has\\nbecome', 15668: \".tagged_words()\\n[('the',\", 15669: \"('fulton',\", 15670: \"'np-tl'),\", 15671: \".tagged_words(tagset='universal')\\n[('the',\", 15672: '.]\\n\\n\\n\\nwhenever', 15673: 'interface\\nwill', 15674: 'tagged_words()', 15675: 'format\\nillustrated', 15676: \".tagged_words())\\n[('now',\", 15677: \"('im',\", 15678: \"('left',\", 15679: \"'vbd'),\", 15680: '.conll2000', 15681: \".tagged_words()\\n[('confidence',\", 15682: \".tagged_words()\\n[('pierre',\", 15683: \"'nnp'),\", 15684: \"('vinken',\", 15685: \"(',',\", 15686: \"','),\", 15687: '.]\\n\\n\\n\\nnot', 15688: 'tags;', 15689: 'the\\ntagset', 15690: 'readme()', 15691: 'methods\\nmentioned', 15692: '.\\ninitially', 15693: 'tagsets,\\nso', 15694: 'tagset:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 15695: \".tagged_words(tagset='universal')\\n[('pierre',\", 15696: \".'),\", 15697: '.]\\n\\n\\n\\ntagged', 15698: 'nltk,\\nincluding', 15699: 'portuguese,', 15700: 'spanish,', 15701: 'dutch', 15702: 'structure\\nsuch', 15703: '.sinica_treebank', 15704: \".tagged_words()\\n[('ä',\", 15705: \"'neu'),\", 15706: \"('åæ',\", 15707: \"'nad'),\", 15708: \"('åç',\", 15709: \"'nba'),\", 15710: \".tagged_words()\\n[('মহিষের',\", 15711: \"('সন্তান',\", 15712: \"(':',\", 15713: \"'sym'),\", 15714: '.mac_morpho', 15715: \".tagged_words()\\n[('jersei',\", 15716: \"'n'),\", 15717: \"('atinge',\", 15718: \"('m\\\\xe9dia',\", 15719: '.conll2002', 15720: \".tagged_words()\\n[('sao',\", 15721: \"'nc'),\", 15722: \"('paulo',\", 15723: \"'vmi'),\", 15724: \"('(',\", 15725: \"'fpa'),\", 15726: '.cess_cat', 15727: \".tagged_words()\\n[('el',\", 15728: \"'da0ms0'),\", 15729: \"('tribunal_suprem',\", 15730: \"'np0000o'),\", 15731: '.]\\n\\n\\n\\nif', 15732: 'correctly,', 15733: 'fonts,\\nyou', 15734: 'languages:', 15735: 'bangla,', 15736: 'telugu\\n\\n\\nif', 15737: 'have\\na', 15738: 'tagged_sents()', 15739: 'into\\nsentences', 15740: 'presenting', 15741: 'taggers,\\nas', 15742: 'trained', 15743: '.3\\xa0\\xa0\\xa0a', 15744: 'tagset\\ntagged', 15745: 'started,', 15746: 'simplified', 15747: 'tagset\\n(shown', 15748: '.\\n\\n\\n\\n\\n\\n\\n\\ntag\\nmeaning\\nenglish', 15749: 'examples\\n\\n\\n\\nadj\\nadjective\\nnew,', 15750: 'good,', 15751: 'high,', 15752: 'big,', 15753: 'local\\n\\nadp\\nadposition\\non,', 15754: 'of,', 15755: 'at,', 15756: 'with,', 15757: 'by,', 15758: 'into,', 15759: 'under\\n\\nadv\\nadverb\\nreally,', 15760: 'already,', 15761: 'early,', 15762: 'now\\n\\nconj\\nconjunction\\nand,', 15763: 'or,', 15764: 'but,', 15765: 'while,', 15766: 'although\\n\\ndet\\ndeterminer,', 15767: 'article\\nthe,', 15768: 'some,', 15769: 'most,', 15770: 'every,', 15771: 'which\\n\\nnoun\\nnoun\\nyear,', 15772: 'home,', 15773: 'costs,', 15774: 'africa\\n\\nnum\\nnumeral\\ntwenty-four,', 15775: 'fourth,', 15776: '1991,', 15777: '14:24\\n\\nprt\\nparticle\\nat,', 15778: 'out,', 15779: 'per,', 15780: 'with\\n\\npron\\npronoun\\nhe,', 15781: 'their,', 15782: 'its,', 15783: 'my,', 15784: 'i,', 15785: 'us\\n\\nverb\\nverb\\nis,', 15786: 'told,', 15787: 'given,', 15788: 'playing,', 15789: 'would\\n\\n', 15790: '.\\npunctuation', 15791: 'marks\\n', 15792: '!\\n\\nx\\nother\\nersatz,', 15793: 'esprit,', 15794: 'dunno,', 15795: 'gr8,', 15796: 'univeristy\\n\\n\\ntable', 15797: \"tagset\\n\\n\\nlet's\", 15798: 'news\\ncategory', 15799: 'brown_news_tagged', 15800: \".tagged_words(categories='news',\", 15801: \"tagset='universal')\\n>>>\", 15802: 'tag_fd', 15803: '.freqdist(tag', 15804: 'tag)', 15805: 'brown_news_tagged)\\n>>>', 15806: \".most_common()\\n[('noun',\", 15807: '30640),', 15808: \"('verb',\", 15809: '14399),', 15810: \"('adp',\", 15811: '12355),', 15812: '11928),', 15813: \"('det',\", 15814: '11389),\\n', 15815: \"('adj',\", 15816: '6706),', 15817: \"('adv',\", 15818: '3349),', 15819: \"('conj',\", 15820: '2717),', 15821: \"('pron',\", 15822: '2535),', 15823: \"('prt',\", 15824: '2264),\\n', 15825: \"('num',\", 15826: '2166),', 15827: \"('x',\", 15828: '106)]\\n\\n\\n\\n\\nnote\\nyour', 15829: 'turn:\\nplot', 15830: '.plot(cumulative=true)', 15831: 'list?\\n\\nwe', 15832: 'graphical\\npos-concordance', 15833: '.\\nn', 15834: 'hit/vd,', 15835: 'hit/vn,', 15836: 'adj', 15837: '.4\\xa0\\xa0\\xa0nouns\\nnouns', 15838: 'things,', 15839: '.:\\nwoman,', 15840: 'scotland,', 15841: 'after\\ndeterminers', 15842: 'adjectives,', 15843: 'the\\nverb,', 15844: '.\\n\\n\\n\\n\\n\\n\\n\\nword\\nafter', 15845: 'determiner\\nsubject', 15846: 'verb\\n\\n\\n\\nwoman\\nthe', 15847: 'yesterday', 15848: 'sat', 15849: 'down\\n\\nscotland\\nthe', 15850: 'scotland', 15851: '.\\nscotland', 15852: 'people\\n\\nbook\\nthe', 15853: 'recounts', 15854: 'colonization', 15855: 'australia\\n\\nintelligence\\nthe', 15856: \".\\nmary's\", 15857: 'impressed', 15858: 'teachers\\n\\n\\ntable', 15859: 'nouns\\n\\n\\nthe', 15860: 'book,\\nand', 15861: 'np', 15862: 'noun,\\nwith', 15863: 'list\\nof', 15864: 'themselves', 15865: 'word-tag', 15866: \"as\\n(('the',\", 15867: \"'np'))\", 15868: \"(('fulton',\", 15869: \"'np'),\", 15870: \"('county',\", 15871: 'word_tag_pairs', 15872: '.bigrams(brown_news_tagged)\\n>>>', 15873: 'noun_preceders', 15874: '[a[1]', 15875: '(a,', 15876: 'b[1]', 15877: '.freqdist(noun_preceders)\\n>>>', 15878: '[tag', 15879: '(tag,', 15880: \".most_common()]\\n['noun',\", 15881: \"'adj',\", 15882: \"'adp',\", 15883: \"'conj',\", 15884: \"'num',\", 15885: \"'adv',\", 15886: \"'prt',\", 15887: \"'pron',\", 15888: \"'x']\\n\\n\\n\\nthis\", 15889: 'confirms', 15890: 'assertion', 15891: 'and\\nadjectives,', 15892: 'numeral', 15893: '(tagged', 15894: 'num)', 15895: '.5\\xa0\\xa0\\xa0verbs\\nverbs', 15896: 'actions,', 15897: 'fall,\\neat', 15898: 'relation\\ninvolving', 15899: 'referents', 15900: '.\\n\\n\\n\\n\\n\\n\\n\\nword\\nsimple\\nwith', 15901: 'modifiers', 15902: 'adjuncts', 15903: '(italicized)\\n\\n\\n\\nfall\\nrome', 15904: 'fell\\ndot', 15905: 'com', 15906: 'stocks', 15907: 'suddenly', 15908: 'fell', 15909: 'stone\\n\\neat\\nmice', 15910: 'eat', 15911: 'cheese\\njohn', 15912: 'ate', 15913: 'pizza', 15914: 'gusto\\n\\n\\ntable', 15915: 'verbs\\n\\n\\nwhat', 15916: \".tagged_words(tagset='universal')\\n>>>\", 15917: 'word_tag_fd', 15918: '.freqdist(wsj)\\n>>>', 15919: '[wt[0]', 15920: '(wt,', 15921: '.most_common()', 15922: 'wt[1]', 15923: \"'verb']\\n['is',\", 15924: \"'are',\", 15925: \"'says',\", 15926: \"'would',\\n\", 15927: \"'say',\", 15928: \"'make',\", 15929: \"'may',\\n\", 15930: \"'did',\", 15931: \"'rose',\", 15932: \"'made',\", 15933: \"'expected',\", 15934: \"'buy',\", 15935: \"'get',\", 15936: \"'might',\\n\", 15937: \"'sell',\", 15938: \"'added',\", 15939: \"'sold',\", 15940: \"'help',\", 15941: \"'including',\", 15942: \"'reported',\", 15943: '.]\\n\\n\\n\\nnote', 15944: 'paired,', 15945: 'tag\\nas', 15946: 'event,', 15947: 'of\\ncondition-event', 15948: 'frequency-ordered', 15949: 'cfd1', 15950: '.conditionalfreqdist(wsj)\\n>>>', 15951: \"cfd1['yield']\", 15952: \".most_common()\\n[('verb',\", 15953: '28),', 15954: '20)]\\n>>>', 15955: \"cfd1['cut']\", 15956: '25),', 15957: '3)]\\n\\n\\n\\nwe', 15958: '.tagged_words()\\n>>>', 15959: 'cfd2', 15960: '.conditionalfreqdist((tag,', 15961: 'wsj)\\n>>>', 15962: \"list(cfd2['vbn'])\\n['been',\", 15963: \"'compared',\", 15964: \"'based',\", 15965: \"'priced',\", 15966: \"'used',\", 15967: \"'sold',\\n'named',\", 15968: \"'designed',\", 15969: \"'held',\", 15970: \"'fined',\", 15971: \"'taken',\", 15972: \"'paid',\", 15973: \"'traded',\", 15974: '.]\\n\\n\\n\\nto', 15975: 'clarify', 15976: 'vbd', 15977: '(past', 15978: 'tense)', 15979: 'vbn\\n(past', 15980: 'participle),', 15981: 'and\\nvbn,', 15982: 'surrounding', 15983: '.conditions()', 15984: \"'vbd'\", 15985: 'cfd1[w]', 15986: \"'vbn'\", 15987: \"cfd1[w]]\\n['asked',\", 15988: \"'accepted',\", 15989: \"'accused',\", 15990: \"'acquired',\", 15991: \"'adopted',\", 15992: 'idx1', 15993: \".index(('kicked',\", 15994: \"'vbd'))\\n>>>\", 15995: \"wsj[idx1-4:idx1+1]\\n[('while',\", 15996: \"('program',\", 15997: \"('trades',\", 15998: \"'nns'),\", 15999: \"('swiftly',\", 16000: \"'rb'),\\n\", 16001: \"('kicked',\", 16002: \"'vbd')]\\n>>>\", 16003: 'idx2', 16004: \"'vbn'))\\n>>>\", 16005: \"wsj[idx2-4:idx2+1]\\n[('head',\", 16006: \"('state',\", 16007: \"('has',\", 16008: \"'vbz'),\", 16009: \"'vbn')]\\n\\n\\n\\nin\", 16010: 'participle', 16011: 'kicked', 16012: 'preceded', 16013: 'auxiliary', 16014: 'true?\\n\\nnote\\nyour', 16015: 'turn:\\ngiven', 16016: 'participles', 16017: \"by\\nlist(cfd2['vn']),\", 16018: 'word-tag\\npairs', 16019: '.6\\xa0\\xa0\\xa0adjectives', 16020: 'adverbs\\ntwo', 16021: '.\\nadjectives', 16022: 'modifiers\\n(e', 16023: 'pizza),', 16024: 'predicates', 16025: 'the\\npizza', 16026: 'large)', 16027: 'structure\\n(e', 16028: 'fall+ing', 16029: 'falling\\nstocks)', 16030: 'manner,', 16031: 'or\\ndirection', 16032: 'quickly)', 16033: 'adjectives\\n(e', 16034: \"mary's\", 16035: 'teacher', 16036: 'nice)', 16037: '.\\nenglish', 16038: 'to\\nprepositions,', 16039: 'articles', 16040: 'determiners)\\n(e', 16041: 'a),', 16042: 'should,\\nmay),', 16043: 'personal', 16044: 'pronouns', 16045: 'she,', 16046: 'they)', 16047: 'classifies', 16048: 'turn:\\nif', 16049: 'uncertain', 16050: '.concordance(),', 16051: 'watch', 16052: 'schoolhouse', 16053: 'rock!\\ngrammar', 16054: 'videos', 16055: 'youtube,', 16056: 'reading\\nsection', 16057: '.7\\xa0\\xa0\\xa0unsimplified', 16058: \"tags\\nlet's\", 16059: 'nn,\\nand', 16060: 'that\\nthere', 16061: 'variants', 16062: 'nn;', 16063: '$\\nfor', 16064: 'possessive', 16065: '(since', 16066: 'nouns\\ntypically', 16067: 'addition,\\nmost', 16068: 'modifiers:', 16069: '-nc', 16070: 'citations,', 16071: '-hl\\nfor', 16072: 'headlines', 16073: '-tl', 16074: 'titles', 16075: 'tags)', 16076: 'findtags(tag_prefix,', 16077: 'tagged_text):\\n', 16078: 'tagged_text\\n', 16079: '.startswith(tag_prefix))\\n', 16080: 'dict((tag,', 16081: 'cfd[tag]', 16082: '.most_common(5))', 16083: '.conditions())\\n\\n>>>', 16084: 'tagdict', 16085: \"findtags('nn',\", 16086: \".tagged_words(categories='news'))\\n>>>\", 16087: 'sorted(tagdict):\\n', 16088: 'print(tag,', 16089: 'tagdict[tag])\\n', 16090: '.\\nnn', 16091: \"[('year',\", 16092: '137),', 16093: \"('time',\", 16094: '97),', 16095: '88),', 16096: \"('week',\", 16097: '85),', 16098: \"('man',\", 16099: '72)]\\nnn$', 16100: \"[(year's,\", 16101: '13),', 16102: \"(world's,\", 16103: '8),', 16104: \"(state's,\", 16105: '7),', 16106: \"(nation's,\", 16107: '6),', 16108: \"(company's,\", 16109: '6)]\\nnn$-hl', 16110: \"[(golf's,\", 16111: \"(navy's,\", 16112: '1)]\\nnn$-tl', 16113: \"[(president's,\", 16114: '11),', 16115: \"(army's,\", 16116: \"(gallery's,\", 16117: \"(university's,\", 16118: \"(league's,\", 16119: '3)]\\nnn-hl', 16120: \"[('sp\", 16121: '2),', 16122: \"('problem',\", 16123: \"('question',\", 16124: \"('business',\", 16125: \"('salary',\", 16126: '2)]\\nnn-nc', 16127: \"[('eva',\", 16128: \"('aya',\", 16129: \"('ova',\", 16130: '1)]\\nnn-tl', 16131: \"[('president',\", 16132: \"('house',\", 16133: '68),', 16134: '59),', 16135: \"('university',\", 16136: '42),', 16137: \"('city',\", 16138: '41)]\\nnn-tl-hl', 16139: \"[('fort',\", 16140: \"('dr\", 16141: \"('oak',\", 16142: \"('street',\", 16143: \"('basin',\", 16144: '1)]\\nnns', 16145: \"[('years',\", 16146: '101),', 16147: \"('members',\", 16148: '69),', 16149: \"('people',\", 16150: '52),', 16151: \"('sales',\", 16152: '51),', 16153: \"('men',\", 16154: '46)]\\nnns$', 16155: \"[(children's,\", 16156: \"(women's,\", 16157: '5),', 16158: \"(janitors',\", 16159: \"(men's,\", 16160: \"(taxpayers',\", 16161: '2)]\\nnns$-hl', 16162: \"[(dealers',\", 16163: \"(idols',\", 16164: '1)]\\nnns$-tl', 16165: \"[(women's,\", 16166: \"(states',\", 16167: \"(giants',\", 16168: '(bros', 16169: \"(writers',\", 16170: '1)]\\nnns-hl', 16171: \"[('comments',\", 16172: \"('offenses',\", 16173: \"('sacrifices',\", 16174: \"('funds',\", 16175: \"('results',\", 16176: '1)]\\nnns-tl', 16177: \"[('states',\", 16178: '38),', 16179: \"('nations',\", 16180: \"('masters',\", 16181: \"('rules',\", 16182: '9),', 16183: \"('communists',\", 16184: '9)]\\nnns-tl-hl', 16185: \"[('nations',\", 16186: '1)]\\n\\n\\nexample', 16187: '(code_findtags', 16188: 'tags\\n\\nwhen', 16189: 'constructing', 16190: 'taggers', 16191: 'chapter,\\nwe', 16192: 'unsimplified', 16193: '.8\\xa0\\xa0\\xa0exploring', 16194: \"corpora\\nlet's\", 16195: 'chapters,\\nthis', 16196: 'often\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 16197: 'brown_learned_text', 16198: \".words(categories='learned')\\n>>>\", 16199: 'sorted(set(b', 16200: '.bigrams(brown_learned_text)', 16201: \"'often'))\\n[',',\", 16202: \"'accomplished',\", 16203: \"'analytically',\", 16204: \"'appear',\", 16205: \"'apt',\", 16206: \"'associated',\", 16207: \"'assuming',\\n'became',\", 16208: \"'become',\", 16209: \"'began',\", 16210: \"'carefully',\", 16211: \"'chose',\", 16212: '.]\\n\\n\\n\\nhowever,', 16213: 'tagged_words()\\nmethod', 16214: 'brown_lrnd_tagged', 16215: \".tagged_words(categories='learned',\", 16216: '[b[1]', 16217: '.bigrams(brown_lrnd_tagged)', 16218: 'a[0]', 16219: \"'often']\\n>>>\", 16220: '.freqdist(tags)\\n>>>', 16221: 'prt', 16222: 'adv', 16223: 'adp', 16224: 'adj\\n', 16225: '37', 16226: '6\\n\\n\\n\\nnotice', 16227: 'high-frequency', 16228: '.\\nnouns', 16229: 'involving\\nparticular', 16230: '<verb>', 16231: '<verb>)', 16232: 'code-three-word-phrase', 16233: 'criterion', 16234: 'tags\\nmatch,', 16235: 'brown\\ndef', 16236: 'process(sentence):\\n', 16237: '(w1,t1),', 16238: '(w2,t2),', 16239: '(w3,t3)', 16240: '.trigrams(sentence):', 16241: '(t1', 16242: \".startswith('v')\", 16243: 't2', 16244: \"'to'\", 16245: 't3', 16246: \".startswith('v')):\", 16247: 'print(w1,', 16248: 'w2,', 16249: 'w3)', 16250: '\\n\\n>>>', 16251: 'tagged_sent', 16252: '.tagged_sents():\\n', 16253: 'process(tagged_sent)\\n', 16254: '.\\ncombined', 16255: 'achieve\\ncontinue', 16256: 'place\\nserve', 16257: 'protect\\nwanted', 16258: 'wait\\nallowed', 16259: 'place\\nexpected', 16260: 'become\\n', 16261: '(code_three_word_phrase', 16262: 'tags\\n\\nfinally,', 16263: '.\\nunderstanding', 16264: 'clarify\\nthe', 16265: '.conditionalfreqdist((word', 16266: '.lower(),', 16267: 'tag)\\n', 16268: 'sorted(data', 16269: 'len(data[word])', 16270: '3:\\n', 16271: 'data[word]', 16272: '.most_common()]\\n', 16273: '.join(tags))\\n', 16274: '.\\nbest', 16275: 'v\\nbetter', 16276: 'det\\nclose', 16277: 'n\\ncut', 16278: 'vn', 16279: 'vd\\neven', 16280: 'det', 16281: 'v\\ngrant', 16282: '-\\nhit', 16283: 'vd', 16284: 'n\\nlay', 16285: 'vd\\nleft', 16286: 'vn\\nlike', 16287: 'cnj', 16288: '-\\nnear', 16289: 'det\\nopen', 16290: 'adv\\npast', 16291: 'p\\npresent', 16292: 'n\\nread', 16293: 'np\\nright', 16294: 'adv\\nsecond', 16295: 'n\\nset', 16296: '-\\nthat', 16297: 'det\\n\\n\\n\\n\\nnote\\nyour', 16298: 'turn:\\nopen', 16299: 'complete\\nbrown', 16300: '(simplified', 16301: 'tagset)', 16302: 'tag\\nof', 16303: 'correlates', 16304: 'near', 16305: 'together,', 16306: 'near/adj', 16307: 'adjective,', 16308: 'follows,', 16309: 'having\\nthree', 16310: '.\\n\\n\\n\\n\\n3\\xa0\\xa0\\xa0mapping', 16311: 'dictionaries\\nas', 16312: 'creating\\nprograms', 16313: 'most\\nlikely', 16314: 'as\\nmapping', 16315: 'to\\nstore', 16316: 'mappings', 16317: 'type\\n(also', 16318: 'associative', 16319: 'hash', 16320: 'array\\nin', 16321: 'can\\nrepresent', 16322: 'including\\nparts', 16323: '.1\\xa0\\xa0\\xa0indexing', 16324: 'dictionaries\\na', 16325: 'particular\\nitem', 16326: 'text1[100]', 16327: 'specify\\na', 16328: 'simple\\nkind', 16329: 'table,', 16330: 'look-up:', 16331: '.\\n\\ncontrast', 16332: '(3),\\nwhere', 16333: \"fdist['monstrous'],\", 16334: 'which\\ntells', 16335: 'look-up', 16336: 'is\\nfamiliar', 16337: 'key\\nsuch', 16338: \"someone's\", 16339: 'domain,', 16340: 'word;\\nother', 16341: 'hashmap,', 16342: 'hash,', 16343: 'phonebook,', 16344: 'browser,\\nthe', 16345: 'ip', 16346: 'to\\nnumbers,', 16347: 'between\\narbitrary', 16348: 'variety\\nof', 16349: '.\\n\\n\\n\\n\\n\\n\\n\\nlinguistic', 16350: 'object\\nmaps', 16351: 'from\\nmaps', 16352: 'to\\n\\n\\n\\ndocument', 16353: 'index\\nword\\nlist', 16354: 'found)\\n\\nthesaurus\\nword', 16355: 'sense\\nlist', 16356: 'synonyms\\n\\ndictionary\\nheadword\\nentry', 16357: '(part-of-speech,', 16358: 'definitions,', 16359: 'etymology)\\n\\ncomparative', 16360: 'wordlist\\ngloss', 16361: 'term\\ncognates', 16362: '(list', 16363: 'language)\\n\\nmorph', 16364: 'analyzer\\nsurface', 16365: 'form\\nmorphological', 16366: 'morphemes)\\n\\n\\ntable', 16367: 'values\\n\\n\\nmost', 16368: 'represent\\nas', 16369: 'string),', 16370: 'integers)', 16371: '.2\\xa0\\xa0\\xa0dictionaries', 16372: 'python\\npython', 16373: 'dictionary,\\nin', 16374: 'however,\\nas', 16375: 'illustrate,', 16376: 'four\\nentries', 16377: 'add\\nentries', 16378: 'bracket', 16379: 'notation:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 16380: 'pos\\n{}\\n>>>', 16381: \"pos['colorless']\", 16382: \"'adj'\", 16383: \"pos\\n{'colorless':\", 16384: \"'adj'}\\n>>>\", 16385: \"pos['ideas']\", 16386: \"'n'\\n>>>\", 16387: \"pos['sleep']\", 16388: \"'v'\\n>>>\", 16389: \"pos['furiously']\", 16390: \"'adv'\\n>>>\", 16391: \"\\n{'furiously':\", 16392: \"'ideas':\", 16393: \"'colorless':\", 16394: \"'sleep':\", 16395: \"'v'}\\n\\n\\n\\nso,\", 16396: 'more\\nspecifically,', 16397: \"'colorless'\\nis\", 16398: 'see\\na', 16399: 'populated', 16400: 'dictionary\\nin', 16401: 'values:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 16402: \"pos['ideas']\\n'n'\\n>>>\", 16403: \"pos['colorless']\\n'adj'\\n\\n\\n\\nof\", 16404: \"hasn't\", 16405: \"pos['green']\\ntraceback\", 16406: '?\\nkeyerror:', 16407: \"'green'\\n\\n\\n\\nthis\", 16408: 'legal', 16409: 'indexes,\\nhow', 16410: 'dictionary?', 16411: 'dictionary\\nis', 16412: 'evaluating', 16413: '),', 16414: 'gives\\nus', 16415: 'they\\nwere', 16416: 'entered;', 16417: 'sequences\\nbut', 16418: '.2),', 16419: 'inherently\\nordered', 16420: 'keys,', 16421: 'the\\ndictionary', 16422: 'expected,\\nas', 16423: ',\\nor', 16424: 'list(pos)', 16425: \"\\n['ideas',\", 16426: \"'furiously',\", 16427: \"'colorless',\", 16428: \"'sleep']\\n>>>\", 16429: 'sorted(pos)', 16430: \"\\n['colorless',\", 16431: \"'ideas',\", 16432: \".endswith('s')]\", 16433: \"'ideas']\\n\\n\\n\\n\\nnote\\nwhen\", 16434: 'order\\nto', 16435: '.\\n\\nas', 16436: 'keys\\nin', 16437: 'loop\\nas', 16438: 'sorted(pos):\\n', 16439: 'print(word', 16440: ':,', 16441: 'pos[word])\\n', 16442: '.\\ncolorless:', 16443: 'adj\\nfuriously:', 16444: 'adv\\nsleep:', 16445: 'v\\nideas:', 16446: 'n\\n\\n\\n\\nfinally,', 16447: 'keys(),', 16448: 'values()', 16449: 'and\\nitems()', 16450: 'element\\n(and', 16451: 'same,', 16452: 'elements)', 16453: 'list(pos', 16454: \".keys())\\n['colorless',\", 16455: \"'sleep',\", 16456: \"'ideas']\\n>>>\", 16457: \".values())\\n['adj',\", 16458: \"'n']\\n>>>\", 16459: \".items())\\n[('colorless',\", 16460: \"'adj'),\", 16461: \"('furiously',\", 16462: \"'adv'),\", 16463: \"('sleep',\", 16464: \"('ideas',\", 16465: \"'n')]\\n>>>\", 16466: 'val', 16467: 'sorted(pos', 16468: '.items()):', 16469: 'val)\\n', 16470: 'adv\\nideas:', 16471: 'n\\nsleep:', 16472: 'v\\n\\n\\n\\nwe', 16473: 'we\\nonly', 16474: 'now\\nsuppose', 16475: 'noun:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 16476: \"pos['sleep']\\n'v'\\n>>>\", 16477: \"pos['sleep']\\n'n'\\n\\n\\n\\ninitially,\", 16478: 'is\\nimmediately', 16479: 'overwritten', 16480: \"'sleep'\", 16481: 'in\\nthat', 16482: 'entry:', 16483: 'value,\\ne', 16484: \"'v']\", 16485: 'we\\nsaw', 16486: 'dictionary,\\nwhich', 16487: '.3\\xa0\\xa0\\xa0defining', 16488: 'dictionaries\\nwe', 16489: \"there's\\na\", 16490: 'normally', 16491: 'first:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 16492: \"{'colorless':\", 16493: \"'furiously':\", 16494: \"'adv'}\\n>>>\", 16495: \"dict(colorless='adj',\", 16496: \"ideas='n',\", 16497: \"sleep='v',\", 16498: \"furiously='adv')\\n\\n\\n\\nnote\", 16499: 'typeerror:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 16500: \"{['ideas',\", 16501: \"'blogs',\", 16502: \"'adventures']:\", 16503: \"'n'}\\ntraceback\", 16504: 'unhashable\\n\\n\\n\\n\\n\\n3', 16505: '.4\\xa0\\xa0\\xa0default', 16506: 'dictionaries\\nif', 16507: 'create\\nan', 16508: 'or\\nthe', 16509: 'dictionary\\ncalled', 16510: 'defaultdict', 16511: 'to\\ncreate', 16512: 'int,', 16513: 'float,', 16514: 'dict,\\ntuple', 16515: 'defaultdict\\n>>>', 16516: 'defaultdict(int)\\n>>>', 16517: \"frequency['colorless']\", 16518: '4\\n>>>', 16519: \"frequency['ideas']\\n0\\n>>>\", 16520: 'defaultdict(list)\\n>>>', 16521: \"'verb']\\n>>>\", 16522: \"pos['ideas']\\n[]\\n\\n\\n\\n\\nnote\\nthese\", 16523: 'other\\nobjects', 16524: 'int(2),', 16525: 'list(2))', 16526: 'int(),', 16527: 'list()\\n—', 16528: 'specify\\nany', 16529: 'function\\nthat', 16530: 'dictionary\\nwhose', 16531: ',\\nit', 16532: 'defaultdict(lambda:', 16533: \"'noun')\", 16534: \"'adj'\\n>>>\", 16535: \"pos['blog']\", 16536: \"\\n'noun'\\n>>>\", 16537: \".items())\\n[('blog',\", 16538: \"('colorless',\", 16539: \"'adj')]\", 16540: '[_automatically-added]\\n\\n\\n\\n\\nnote\\nthe', 16541: 'no\\nparameters,', 16542: 'equivalent:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 16543: 'lambda:', 16544: \"'noun'\\n>>>\", 16545: \"f()\\n'noun'\\n>>>\", 16546: 'g():\\n', 16547: \"g()\\n'noun'\\n\\n\\n\\n\\nlet's\", 16548: 'substantial\\nlanguage', 16549: 'struggle', 16550: 'process\\nthe', 16551: 'a\\nguarantee', 16552: 'replace\\nlow-frequency', 16553: 'unk,', 16554: 'without\\nreading', 16555: 'on?)\\nwe', 16556: 'replacement', 16557: '.\\neverything', 16558: 'unk', 16559: \".words('carroll-alice\", 16560: '.freqdist(alice)\\n>>>', 16561: 'v1000', 16562: '.most_common(1000)]\\n>>>', 16563: \"'unk')\\n>>>\", 16564: 'v1000:\\n', 16565: 'mapping[v]', 16566: 'v\\n', 16567: 'alice2', 16568: '[mapping[v]', 16569: 'alice]\\n>>>', 16570: \"alice2[:100]\\n['unk',\", 16571: \"'alice',\", 16572: \"'unk',\", 16573: \"'unk',\\n'unk',\", 16574: \"'chapter',\", 16575: \"'rabbit',\", 16576: \"'alice',\\n'was',\", 16577: \"'tired',\", 16578: \"'sitting',\", 16579: \"'by',\\n'her',\", 16580: \"'sister',\", 16581: \"'nothing',\\n'to',\", 16582: \"'twice',\", 16583: \"'into',\", 16584: \"'the',\\n'book',\", 16585: \"'no',\\n'pictures',\", 16586: \"'the',\\n'use',\", 16587: \"'book',\", 16588: \",',\", 16589: \"'thought',\", 16590: \"'without',\\n'pictures',\", 16591: \"'conversation',\", 16592: \"?'\", 16593: 'len(set(alice2))\\n1001\\n\\n\\n\\n\\n\\n\\n3', 16594: '.5\\xa0\\xa0\\xa0incrementally', 16595: 'dictionary\\nwe', 16596: 'occurrences,', 16597: 'emulating', 16598: 'tallying', 16599: 'fig-tally', 16600: 'defaultdict,', 16601: 'each\\npart-of-speech', 16602: 'before,\\nit', 16603: 'tag,\\nwe', 16604: 'increment', 16605: \"tagset='universal'):\\n\", 16606: 'counts[tag]', 16607: \"counts['noun']\\n30640\\n>>>\", 16608: \"sorted(counts)\\n['adj',\", 16609: \"'noun',\", 16610: \"'det']\\n\\n>>>\", 16611: 'itemgetter\\n>>>', 16612: 'sorted(counts', 16613: '.items(),', 16614: 'key=itemgetter(1),', 16615: \"reverse=true)\\n[('noun',\", 16616: '[t', 16617: \"reverse=true)]\\n['noun',\", 16618: \"'x']\\n\\n\\nexample\", 16619: '(code_dictionary', 16620: 'incrementally', 16621: 'value\\n\\nthe', 16622: 'for\\nsorting', 16623: 'decreasing\\norder', 16624: 'items\\nto', 16625: 'sort,', 16626: 'itemgetter(n)', 16627: 'element,', 16628: \"('np',\", 16629: '8336)\\n>>>', 16630: 'pair[1]\\n8336\\n>>>', 16631: 'itemgetter(1)(pair)\\n8336\\n\\n\\n\\nthe', 16632: 'returned\\nin', 16633: 'of\\n3', 16634: '.3,', 16635: 'a\\nfor', 16636: 'schematic', 16637: 'version:\\n\\n>>>', 16638: 'my_dictionary', 16639: 'defaultdict(function', 16640: 'value)\\n>>>', 16641: 'sequence:\\n', 16642: 'my_dictionary[item_key]', 16643: \"item\\n\\nhere's\", 16644: 'last_letters', 16645: \".words('en')\\n>>>\", 16646: 'word[-2:]\\n', 16647: 'last_letters[key]', 16648: \"last_letters['ly']\\n['abactinally',\", 16649: \"'abandonedly',\", 16650: \"'abasedly',\", 16651: \"'abashedly',\", 16652: \"'abashlessly',\", 16653: \"'abbreviately',\\n'abdominally',\", 16654: \"'abhorrently',\", 16655: \"'abidingly',\", 16656: \"'abiogenetically',\", 16657: \"'abiologically',\", 16658: \"last_letters['zy']\\n['blazy',\", 16659: \"'bleezy',\", 16660: \"'blowzy',\", 16661: \"'boozy',\", 16662: \"'breezy',\", 16663: \"'bronzy',\", 16664: \"'buzzy',\", 16665: \"'chazy',\", 16666: 'anagram', 16667: '.\\n(you', 16668: 'anagrams', 16669: '.join(sorted(word))\\n', 16670: 'anagrams[key]', 16671: \"anagrams['aeilnrt']\\n['entrail',\", 16672: \"'latrine',\", 16673: \"'ratline',\", 16674: \"'reliant',\", 16675: \"'retinal',\", 16676: \"'trenail']\\n\\n\\n\\nsince\", 16677: 'task,\\nnltk', 16678: 'defaultdict(list),\\nin', 16679: \".index((''\", 16680: '.join(sorted(w)),', 16681: 'words)\\n>>>', 16682: \"'trenail']\\n\\n\\n\\n\\nnote\\nnltk\", 16683: '.index', 16684: 'defaultdict(list)', 16685: 'for\\ninitialization', 16686: 'similarly,\\nnltk', 16687: '.freqdist', 16688: 'essentially', 16689: 'defaultdict(int)', 16690: 'extra\\nsupport', 16691: 'initialization', 16692: '(along', 16693: 'methods)', 16694: '.6\\xa0\\xa0\\xa0complex', 16695: 'values\\nwe', 16696: 'itself,', 16697: 'see\\nhow', 16698: 'defaultdict(int))\\n>>>', 16699: '((w1,', 16700: 't1),', 16701: '(w2,', 16702: 't2))', 16703: '.bigrams(brown_news_tagged):', 16704: 'pos[(t1,', 16705: 'w2)][t2]', 16706: \"pos[('det',\", 16707: \"'right')]\", 16708: '\\ndefaultdict(<class', 16709: \"'int'>,\", 16710: \"{'adj':\", 16711: \"'noun':\", 16712: '5})\\n\\n\\n\\nthis', 16713: 'entry\\nis', 16714: '(whose', 16715: 'zero)', 16716: 'iterated', 16717: 'tagged\\ncorpus,', 16718: \"dictionary's\\nentry\", 16719: '(t1,', 16720: 'w2),', 16721: 'determiner,', 16722: '.7\\xa0\\xa0\\xa0inverting', 16723: 'dictionary\\ndictionaries', 16724: 'lookup,', 16725: 'd[k]', 16726: 'and\\nimmediately', 16727: 'more\\ncumbersome:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 16728: \".txt'):\\n\", 16729: 'counts[word]', 16730: '[key', 16731: '(key,', 16732: 'value)', 16733: '.items()', 16734: \"32]\\n['brought',\", 16735: \"'virtue',\", 16736: \"'thine',\", 16737: \"'king',\", 16738: \"'mortal',\\n'every',\", 16739: \"'been']\\n\\n\\n\\nif\", 16740: 'have\\nthe', 16741: 'key-value\\npairs', 16742: 'value-key\\npairs', 16743: 'pos2', 16744: 'dict((value,', 16745: '.items())\\n>>>', 16746: \"pos2['n']\\n'ideas'\\n\\n\\n\\nlet's\", 16747: 'realistic\\nand', 16748: 'update()', 16749: 'the\\ntechnique', 16750: '(why\\nnot?)', 16751: 'words\\nfor', 16752: 'part-of-speech,', 16753: \".update({'cats':\", 16754: \"'scratch':\", 16755: \"'peacefully':\", 16756: \"'old':\", 16757: \"'adj'})\\n>>>\", 16758: '.items():\\n', 16759: 'pos2[value]', 16760: '.append(key)\\n', 16761: \"pos2['adv']\\n['peacefully',\", 16762: \"'furiously']\\n\\n\\n\\nnow\", 16763: 'inverted', 16764: 'find\\nall', 16765: 'even\\nmore', 16766: '.index((value,', 16767: \"'furiously']\\n\\n\\n\\na\", 16768: '.\\n\\n\\n\\n\\n\\n\\nexample\\ndescription\\n\\n\\n\\nd', 16769: '{}\\ncreate', 16770: 'd\\n\\nd[key]', 16771: 'value\\nassign', 16772: 'key\\n\\nd', 16773: '.keys()\\nthe', 16774: 'dictionary\\n\\nlist(d)\\nthe', 16775: 'dictionary\\n\\nsorted(d)\\nthe', 16776: 'sorted\\n\\nkey', 16777: 'd\\ntest', 16778: 'dictionary\\n\\nfor', 16779: 'd\\niterate', 16780: 'dictionary\\n\\nd', 16781: '.values()\\nthe', 16782: 'dictionary\\n\\ndict([(k1,v1),', 16783: '(k2,v2),', 16784: '.])\\ncreate', 16785: 'pairs\\n\\nd1', 16786: '.update(d2)\\nadd', 16787: 'd2', 16788: 'd1\\n\\ndefaultdict(int)\\na', 16789: 'zero\\n\\n\\ntable', 16790: 'idioms\\ninvolving', 16791: '.\\n\\n\\n\\n\\n\\n4\\xa0\\xa0\\xa0automatic', 16792: 'tagging\\nin', 16793: 'automatically\\nadd', 16794: 'depends\\non', 16795: '(tagged)', 16796: 'brown_tagged_sents', 16797: \".tagged_sents(categories='news')\\n>>>\", 16798: 'brown_sents', 16799: \".sents(categories='news')\\n\\n\\n\\n\\n4\", 16800: '.1\\xa0\\xa0\\xa0the', 16801: 'tagger\\nthe', 16802: 'this\\nmay', 16803: 'banal', 16804: 'establishes', 16805: 'important\\nbaseline', 16806: 'we\\ntag', 16807: 'is\\nmost', 16808: '(now', 16809: 'tagset):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 16810: \".tagged_words(categories='news')]\\n>>>\", 16811: '.freqdist(tags)', 16812: \".max()\\n'nn'\\n\\n\\n\\nnow\", 16813: 'eggs', 16814: 'ham,', 16815: 'sam', 16816: \"am!'\\n>>>\", 16817: '.word_tokenize(raw)\\n>>>', 16818: 'default_tagger', 16819: \".defaulttagger('nn')\\n>>>\", 16820: \".tag(tokens)\\n[('i',\", 16821: \"('do',\", 16822: \"('green',\", 16823: \"'nn'),\\n('eggs',\", 16824: \"('ham',\", 16825: \"'nn'),\\n('do',\", 16826: \"('them',\", 16827: \"('sam',\", 16828: \"'nn'),\\n('i',\", 16829: \"('am',\", 16830: \"('!',\", 16831: \"'nn')]\\n\\n\\n\\nunsurprisingly,\", 16832: 'eighth', 16833: 'correctly,\\nas', 16834: '.evaluate(brown_tagged_sents)\\n0', 16835: '.13089484257215028\\n\\n\\n\\ndefault', 16836: 'that\\nhave', 16837: 'happens,', 16838: 'processed\\nseveral', 16839: 'thousand', 16840: 'the\\nrobustness', 16841: 'them\\nshortly', 16842: 'of\\nmatching', 16843: 'ending\\nin', 16844: \"with\\n's\", 16845: 'of\\nregular', 16846: 'expressions:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 16847: \"(r'\", 16848: \".*ing$',\", 16849: \"'vbg'),\", 16850: 'gerunds\\n', 16851: \".*ed$',\", 16852: 'past\\n', 16853: \".*es$',\", 16854: '3rd', 16855: 'present\\n', 16856: \".*ould$',\", 16857: \"'md'),\", 16858: 'modals\\n', 16859: \".*\\\\'s$',\", 16860: \"'nn$'),\", 16861: 'nouns\\n', 16862: \".*s$',\", 16863: \"(r'^-?[0-9]+(\\\\\", 16864: \".[0-9]+)?$',\", 16865: \"'cd'),\", 16866: 'cardinal', 16867: 'numbers\\n', 16868: \".*',\", 16869: \"'nn')\", 16870: '(default)\\n', 16871: ']\\n\\n\\n\\nnote', 16872: 'fifth\\nof', 16873: 'regexp_tagger', 16874: '.regexptagger(patterns)\\n>>>', 16875: \".tag(brown_sents[3])\\n[('``',\", 16876: \"('only',\", 16877: \"('relative',\", 16878: \"('handful',\", 16879: \"'nn'),\\n('of',\", 16880: \"('such',\", 16881: \"('reports',\", 16882: \"('received',\", 16883: \"'vbd'),\\n('',\", 16884: \"'nn'),\\n('``',\", 16885: \"('considering',\", 16886: \"('widespread',\", 16887: '.20326391789486245\\n\\n\\n\\nthe', 16888: '.*»', 16889: 'catch-all', 16890: '(only', 16891: 'efficient)', 16892: 're-specifying', 16893: 'tagger,\\nis', 16894: 'tagger?', 16895: 'turn:\\nsee', 16896: 'above\\nregular', 16897: '1\\ndescribes', 16898: 'partially', 16899: 'automate', 16900: '.)\\n\\n\\n\\n4', 16901: '.3\\xa0\\xa0\\xa0the', 16902: 'tagger\\n(an', 16903: 'unigramtagger):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 16904: '.freqdist(brown', 16905: \".words(categories='news'))\\n>>>\", 16906: '.conditionalfreqdist(brown', 16907: 'most_freq_words', 16908: '.most_common(100)\\n>>>', 16909: 'likely_tags', 16910: 'dict((word,', 16911: 'cfd[word]', 16912: '.max())', 16913: 'most_freq_words)\\n>>>', 16914: 'baseline_tagger', 16915: '.unigramtagger(model=likely_tags)\\n>>>', 16916: '.45578495136941344\\n\\n\\n\\nit', 16917: 'simply\\nknowing', 16918: 'of\\ntokens', 16919: '(nearly', 16920: 'fact)', 16921: 'untagged', 16922: \".sents(categories='news')[3]\\n>>>\", 16923: \".tag(sent)\\n[('``',\", 16924: \"'``'),\", 16925: 'none),', 16926: \"none),\\n('handful',\", 16927: \"none),\\n('was',\", 16928: \"'bedz'),\", 16929: \"','),\\n('the',\", 16930: \"','),\\n('``',\", 16931: \"none),\\n('interest',\", 16932: \"('election',\", 16933: \"none),\\n(',',\", 16934: \"'in'),\\n('voters',\", 16935: \"('size',\", 16936: \"none),\\n('of',\", 16937: \".')]\\n\\n\\n\\nmany\", 16938: 'none,\\nbecause', 16939: 'first,\\nand', 16940: 'tagger,\\na', 16941: 'backoff', 16942: '(5)', 16943: 'other,\\nas', 16944: 'pairs\\nfor', 16945: '.unigramtagger(model=likely_tags,\\n', 16946: 'backoff=nltk', 16947: \".defaulttagger('nn'))\\n\\n\\n\\nlet's\", 16948: 'and\\nevaluate', 16949: 'sizes,', 16950: 'performance(cfd,', 16951: 'wordlist):\\n', 16952: 'lt', 16953: 'wordlist)\\n', 16954: '.unigramtagger(model=lt,', 16955: \".defaulttagger('nn'))\\n\", 16956: '.evaluate(brown', 16957: \".tagged_sents(categories='news'))\\n\\ndef\", 16958: 'display():\\n', 16959: 'pylab\\n', 16960: 'word_freqs', 16961: \".words(categories='news'))\", 16962: '.most_common()\\n', 16963: 'words_by_freq', 16964: 'word_freqs]\\n', 16965: \".tagged_words(categories='news'))\\n\", 16966: '.arange(15)\\n', 16967: 'perfs', 16968: '[performance(cfd,', 16969: 'words_by_freq[:size])', 16970: 'sizes]\\n', 16971: '.plot(sizes,', 16972: 'perfs,', 16973: \"'-bo')\\n\", 16974: \".title('lookup\", 16975: 'varying', 16976: \"size')\\n\", 16977: \".xlabel('model\", 16978: \".ylabel('performance')\\n\", 16979: 'display()', 16980: '\\n\\n\\nexample', 16981: '(code_baseline_tagger', 16982: 'size\\n\\n\\n\\nfigure', 16983: 'tagger\\n\\nobserve', 16984: 'increases', 16985: 'rapidly', 16986: 'grows,', 16987: 'eventually\\nreaching', 16988: 'plateau,', 16989: 'improvement\\nin', 16990: 'discussed\\nin', 16991: '.)\\n\\n\\n4', 16992: '.4\\xa0\\xa0\\xa0evaluation\\nin', 16993: 'emphasis', 16994: 'on\\naccuracy', 16995: 'of\\nsuch', 16996: 'theme', 16997: 'recall', 16998: 'processing\\npipeline', 16999: 'fig-sds;', 17000: 'one\\nmodule', 17001: 'greatly', 17002: 'multiplied', 17003: 'tags\\na', 17004: 'expert', 17005: 'access\\nto', 17006: 'impartial', 17007: 'judge,', 17008: 'with\\ngold', 17009: 'manually\\nannotated', 17010: 'accepted', 17011: 'the\\nguesses', 17012: 'assessed', 17013: 'regarded', 17014: 'as\\nbeing', 17015: 'as\\nthe', 17016: 'gold', 17017: 'the\\noriginal', 17018: 'annotation', 17019: 'further\\nanalysis', 17020: 'mistakes', 17021: 'standard,', 17022: 'may\\neventually', 17023: 'lead', 17024: 'guidelines', 17025: '.\\nnevertheless,', 17026: 'correct\\nas', 17027: '.\\n\\nnote\\ndeveloping', 17028: 'undertaking', 17029: 'tools,\\ndocumentation,', 17030: 'practices', 17031: 'ensuring', 17032: 'high', 17033: 'quality\\nannotation', 17034: 'tagsets', 17035: 'schemes', 17036: 'inevitably\\ndepend', 17037: 'by\\nall,', 17038: 'however', 17039: 'creators', 17040: 'theory-neutral', 17041: 'to\\nmaximize', 17042: 'usefulness', 17043: 'discuss\\nthe', 17044: '.\\n\\n\\n\\n\\n5\\xa0\\xa0\\xa0n-gram', 17045: 'tagging\\n\\n5', 17046: '.1\\xa0\\xa0\\xa0unigram', 17047: 'tagging\\nunigram', 17048: 'algorithm:\\nfor', 17049: 'for\\nthat', 17050: 'jj', 17051: 'any\\noccurrence', 17052: 'frequent,', 17053: 'an\\nadjective', 17054: 'a\\nverb', 17055: 'cafe)', 17056: 'unigram', 17057: '(4),\\nexcept', 17058: 'up,\\ncalled', 17059: 'sample,\\nwe', 17060: 'evaluate:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 17061: \".sents(categories='news')\\n>>>\", 17062: 'unigram_tagger', 17063: '.unigramtagger(brown_tagged_sents)\\n>>>', 17064: \".tag(brown_sents[2007])\\n[('various',\", 17065: \"('apartments',\", 17066: \"'nns'),\\n('are',\", 17067: \"'ber'),\", 17068: \"('terrace',\", 17069: \"('type',\", 17070: \"'nn'),\\n(',',\", 17071: \"('being',\", 17072: \"'beg'),\", 17073: \"('ground',\", 17074: \"'nn'),\\n('floor',\", 17075: \"('so',\", 17076: \"'ql'),\", 17077: \"'cs'),\", 17078: \"('entrance',\", 17079: \"'bez'),\\n('direct',\", 17080: \".')]\\n>>>\", 17081: '.9349006503968017\\n\\n\\n\\nwe', 17082: 'unigramtagger', 17083: 'involves\\ninspecting', 17084: '.2\\xa0\\xa0\\xa0separating', 17085: 'data\\nnow', 17086: 'memorized', 17087: 'data\\nand', 17088: 'perfect', 17089: 'also\\nbe', 17090: 'useless', 17091: '10%:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 17092: 'int(len(brown_tagged_sents)', 17093: '.9)\\n>>>', 17094: 'size\\n4160\\n>>>', 17095: 'train_sents', 17096: 'brown_tagged_sents[:size]\\n>>>', 17097: 'test_sents', 17098: 'brown_tagged_sents[size:]\\n>>>', 17099: '.unigramtagger(train_sents)\\n>>>', 17100: '.evaluate(test_sents)\\n0', 17101: '.811721', 17102: '.\\n\\n\\n\\nalthough', 17103: 'worse,', 17104: 'of\\nthis', 17105: '.3\\xa0\\xa0\\xa0general', 17106: 'tagging\\nwhen', 17107: 'unigrams,', 17108: 'using\\none', 17109: 'current\\ntoken,', 17110: 'isolation', 17111: 'model,', 17112: 'best\\nwe', 17113: 'priori', 17114: 'wind', 17115: 'tag,\\nregardless', 17116: 'or\\nto', 17117: 'generalization', 17118: 'the\\nn-1', 17119: 'be\\nchosen,', 17120: 'tn,', 17121: 'circled,', 17122: 'shaded\\nin', 17123: 'grey', 17124: '.1,\\nwe', 17125: 'n=3;', 17126: 'addition\\nto', 17127: 'tagger\\npicks', 17128: 'context\\n\\n\\nnote\\na', 17129: '1-gram', 17130: 'tagger:', 17131: '.,\\nthe', 17132: '.\\n2-gram', 17133: '3-gram', 17134: 'taggers\\nare', 17135: 'trigram', 17136: 'ngramtagger', 17137: 'which\\npart-of-speech', 17138: 'bigram_tagger', 17139: '.bigramtagger(train_sents)\\n>>>', 17140: \"'nn'),\\n('type',\", 17141: \"'at'),\\n('ground',\", 17142: \"('floor',\", 17143: \"'cs'),\\n('entrance',\", 17144: \"'bez'),\", 17145: \"('direct',\", 17146: 'unseen_sent', 17147: 'brown_sents[4203]\\n>>>', 17148: \".tag(unseen_sent)\\n[('the',\", 17149: \"('population',\", 17150: \"('congo',\", 17151: \"'np'),\\n('is',\", 17152: \"('13\", 17153: \".5',\", 17154: \"('million',\", 17155: \"('divided',\", 17156: \"none),\\n('into',\", 17157: \"('least',\", 17158: \"('seven',\", 17159: \"('major',\", 17160: \"none),\\n('``',\", 17161: \"('culture',\", 17162: \"('clusters',\", 17163: \"none),\\n('innumerable',\", 17164: \"('tribes',\", 17165: \"('speaking',\", 17166: \"('400',\", 17167: \"none),\\n('separate',\", 17168: \"('dialects',\", 17169: 'none)]\\n\\n\\n\\nnotice', 17170: 'manages', 17171: 'during\\ntraining,', 17172: 'badly', 17173: 'word\\n(i', 17174: '.5),', 17175: 'million)', 17176: 'training,', 17177: 'never\\nsaw', 17178: 'the\\ntagger', 17179: 'overall', 17180: 'low:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 17181: '.102063', 17182: '.\\n\\n\\n\\n\\nas', 17183: 'larger,', 17184: 'specificity', 17185: 'increases,\\nas', 17186: 'wish', 17187: 'that\\nwere', 17188: 'sparse\\ndata', 17189: 'consequence,', 17190: 'a\\ntrade-off', 17191: '(and\\nthis', 17192: 'precision/recall', 17193: 'trade-off', 17194: 'information\\nretrieval)', 17195: '.\\n\\ncaution!\\nn-gram', 17196: 'crosses', 17197: 'a\\nsentence', 17198: 'work\\nwith', 17199: 'tn-1', 17200: 'preceding\\ntags', 17201: '.4\\xa0\\xa0\\xa0combining', 17202: 'taggers\\none', 17203: 'accurate', 17204: 'on\\nalgorithms', 17205: 'could\\ncombine', 17206: 'follows:\\n\\ntry', 17207: 'try\\nthe', 17208: '.\\n\\nmost', 17209: 'backoff-tagger', 17210: 'tagger:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 17211: 't0', 17212: 't1', 17213: '.unigramtagger(train_sents,', 17214: 'backoff=t0)\\n>>>', 17215: '.bigramtagger(train_sents,', 17216: 'backoff=t1)\\n>>>', 17217: '.844513', 17218: '.\\n\\n\\n\\n\\nnote\\nyour', 17219: 'turn:\\nextend', 17220: 'trigramtagger', 17221: 'called\\nt3,', 17222: 'backs', 17223: '.\\n\\nnote', 17224: 'is\\ninitialized', 17225: 'context,\\nthe', 17226: 'discards', 17227: 'can\\nfurther', 17228: 'a\\ncontext', 17229: 'retain', 17230: '.bigramtagger(sents,', 17231: 'cutoff=2,', 17232: 'backoff=t1)\\nwill', 17233: '.5\\xa0\\xa0\\xa0tagging', 17234: 'words\\nour', 17235: 'regular-expression', 17236: 'tagger\\nor', 17237: 'tagger\\nencountered', 17238: 'appeared', 17239: 'out-of-vocabulary', 17240: 'items?\\na', 17241: '.\\nduring', 17242: 'to),', 17243: 'unk\\nwill', 17244: '.6\\xa0\\xa0\\xa0storing', 17245: 'taggers\\ntraining', 17246: 'tagger\\nevery', 17247: '.pkl', 17248: 'pickle', 17249: 'dump\\n>>>', 17250: \"open('t2\", 17251: \".pkl',\", 17252: \"'wb')\\n>>>\", 17253: 'dump(t2,', 17254: '-1)\\n>>>', 17255: '.close()\\n\\n\\n\\nnow,', 17256: 'load\\n>>>', 17257: \"'rb')\\n>>>\", 17258: 'load(input)\\n>>>', 17259: '.close()\\n\\n\\n\\nnow', 17260: \"board's\", 17261: 'enterprise\\n', 17262: 'maze', 17263: 'regulatory', 17264: 'laws', 17265: '.split()\\n>>>', 17266: \".tag(tokens)\\n[('the',\", 17267: \"(board's,\", 17268: \"('action',\", 17269: \"('shows',\", 17270: \"'nns'),\\n('what',\", 17271: \"'wdt'),\", 17272: \"('free',\", 17273: \"('enterprise',\", 17274: \"'bez'),\\n('up',\", 17275: \"'rp'),\", 17276: \"('against',\", 17277: \"('our',\", 17278: \"'pp$'),\", 17279: \"('complex',\", 17280: \"'jj'),\\n('maze',\", 17281: \"('regulatory',\", 17282: \"('laws',\", 17283: \".')]\\n\\n\\n\\n\\n\\n5\", 17284: '.7\\xa0\\xa0\\xa0performance', 17285: 'limitations\\nwhat', 17286: 'tagger?\\nconsider', 17287: 'it\\nencounter?', 17288: 'empirically:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 17289: '((x[1],', 17290: 'y[1],', 17291: 'z[0]),', 17292: 'z[1])\\n', 17293: 'brown_tagged_sents\\n', 17294: 'z', 17295: '.trigrams(sent))\\n>>>', 17296: 'ambiguous_contexts', 17297: '[c', 17298: 'len(cfd[c])', 17299: 'sum(cfd[c]', 17300: 'ambiguous_contexts)', 17301: '.n()\\n0', 17302: '.049297702068029296\\n\\n\\n\\nthus,', 17303: 'trigrams', 17304: '[examples]', 17305: 'the\\ncurrent', 17306: '5%', 17307: 'tag\\nthat', 17308: 'legitimately', 17309: 'in\\nsuch', 17310: 'contexts,', 17311: 'lower', 17312: 'study\\nits', 17313: 'assign,', 17314: 'and\\nit', 17315: 'pre-', 17316: 'post-processing\\nthe', 17317: 'the\\nconfusion', 17318: 'matrix', 17319: 'standard)\\nagainst', 17320: 'test_tags', 17321: \".sents(categories='editorial')\\n\", 17322: '.tag(sent)]\\n>>>', 17323: 'gold_tags', 17324: \".tagged_words(categories='editorial')]\\n>>>\", 17325: '.confusionmatrix(gold_tags,', 17326: 'test_tags))', 17327: '\\n\\n\\n\\n\\nbased', 17328: 'perhaps\\na', 17329: 'dropped,\\nsince', 17330: '100%', 17331: 'annotators', 17332: '[more]\\nin', 17333: 'collapses', 17334: 'distinctions:\\ne', 17335: 'are\\ntagged', 17336: 'prp', 17337: 'introduces\\nnew', 17338: 'ambiguities:', 17339: 'vb', 17340: 'new\\ndistinctions', 17341: 'which\\nfacilitates', 17342: 'prediction', 17343: 'finer', 17344: 'tagset,', 17345: 'gets\\nmore', 17346: 'left-context', 17347: 'deciding\\nwhat', 17348: 'tagset),\\nthe', 17349: 'smaller\\nrange', 17350: 'leads', 17351: 'limit\\nin', 17352: 'resolve', 17353: 'the\\nambiguity', 17354: '(church,', 17355: 'young,', 17356: 'bloothooft,', 17357: '1996),', 17358: 'world\\nknowledge', 17359: 'despite', 17360: 'imperfections,', 17361: 'has\\nplayed', 17362: 'rise', 17363: '1990s,', 17364: 'of\\nstatistical', 17365: 'namely\\npart-of-speech', 17366: 'disambiguation,', 17367: 'pushed', 17368: 'further?', 17369: '.,\\nwe', 17370: '.\\n\\n\\n\\n6\\xa0\\xa0\\xa0transformation-based', 17371: 'tagging\\na', 17372: 'n-gram\\ntable', 17373: 'model)', 17374: 'employed', 17375: 'devices,', 17376: 'strike', 17377: 'tagger\\nperformance', 17378: 'and\\nbigram', 17379: 'tables,', 17380: 'sparse', 17381: 'millions\\nof', 17382: 'n-gram\\ntagger', 17383: 'considers', 17384: 'words\\nthemselves', 17385: 'simply\\nimpractical', 17386: 'conditioned', 17387: 'identities', 17388: 'brill', 17389: 'tagging,\\nan', 17390: 'inductive', 17391: 'models\\nthat', 17392: '.\\nbrill', 17393: 'transformation-based', 17394: 'learning,', 17395: 'named\\nafter', 17396: 'inventor', 17397: 'the\\ngeneral', 17398: 'back\\nand', 17399: 'successively\\ntransforms', 17400: 'bad', 17401: 'n-gram\\ntagging,', 17402: 'need\\nannotated', 17403: \"tagger's\", 17404: 'a\\nmistake', 17405: 'does\\nnot', 17406: 'compiles', 17407: 'transformational\\ncorrection', 17408: 'with\\npainting', 17409: 'painting', 17410: 'of\\nboughs,', 17411: 'branches,', 17412: 'twigs', 17413: 'leaves,', 17414: 'sky-blue\\nbackground', 17415: 'paint\\nblue', 17416: 'gaps,', 17417: 'canvas', 17418: 'then\\ncorrect', 17419: 'over-painting', 17420: 'blue', 17421: 'trunk', 17422: 'over-paint', 17423: 'brushes', 17424: 'brill\\ntagging', 17425: 'idea:', 17426: 'broad', 17427: 'strokes', 17428: 'up\\nthe', 17429: 'successively', 17430: 'an\\nexample', 17431: 'sentence:\\n\\n', 17432: '(1)the', 17433: 'congress', 17434: 'increase', 17435: 'grants', 17436: 'states\\nfor', 17437: 'vocational', 17438: 'rehabilitation\\nwe', 17439: 'rules:\\n(a)', 17440: 'to;\\n(b)', 17441: 'nns', 17442: '.\\n6', 17443: '.1\\nillustrates', 17444: 'then\\napplying', 17445: '.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nphrase\\nto\\nincrease\\ngrants\\nto\\nstates\\nfor\\nvocational\\nrehabilitation\\n\\nunigram\\nto\\nnn\\nnns\\nto\\nnns\\nin\\njj\\nnn\\n\\nrule', 17446: '1\\n\\xa0\\nvb\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\nrule', 17447: '2\\n\\xa0\\n\\xa0\\n\\xa0\\nin\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\noutput\\nto\\nvb\\nnns\\nin\\nnns\\nin\\njj\\nnn\\n\\ngold\\nto\\nvb\\nnns\\nin\\nnns\\nin\\njj\\nnn\\n\\n\\ntable', 17448: 'tagging\\n\\n\\nin', 17449: 'a\\ntemplate', 17450: 'with\\nt2', 17451: 'the\\nidentity', 17452: 'the\\nappearance', 17453: '2-3', 17454: 'during\\nits', 17455: 'phase,', 17456: 't1,\\nt2', 17457: 'scored', 17458: 'net', 17459: 'benefit:', 17460: 'corrects,', 17461: 'correct\\ntags', 17462: '.\\n\\nbrill', 17463: 'property:', 17464: 'are\\nlinguistically', 17465: 'interpretable', 17466: 'taggers,\\nwhich', 17467: 'potentially', 17468: 'n-grams', 17469: 'learn\\nmuch', 17470: 'direct', 17471: 'inspection', 17472: 'the\\nrules', 17473: 'learned', 17474: '.tbl', 17475: 'demo', 17476: 'brill_demo\\n>>>', 17477: 'brill_demo', 17478: '.demo()\\ntraining', 17479: '.\\nfinding', 17480: '6555', 17481: '|\\n', 17482: 'broken\\n', 17483: 'correct\\n', 17484: 'incorrect\\n', 17485: 'e\\n------------------+-------------------------------------------------------\\n', 17486: \"'to'\\n\", 17487: \"'dt'\\n\", 17488: \"'nns'\\n\", 17489: 'nnp', 17490: 'i-2', 17491: '.i-1', 17492: \"'-none-'\\n\", 17493: \"'nnp'\\n\", 17494: \"'like'\\n\", 17495: 'vbn', 17496: \"'*-1'\\n\", 17497: 'print(open(errors', 17498: '.out)', 17499: '.read())\\n', 17500: 'word/test->gold', 17501: 'context\\n--------------------------+------------------------+--------------------------\\n', 17502: 'then/nn->rb', 17503: 'in/in', 17504: 'the/dt', 17505: 'guests/n\\n,', 17506: 'guests/nns', 17507: \"'/vbd->pos\", 17508: 'honor/nn', 17509: \"speed\\n'/pos\", 17510: 'speedway/jj->nn', 17511: 'hauled/vbd', 17512: 'out/rp', 17513: 'four/cd\\nnn', 17514: 'speedway/nn', 17515: 'hauled/nn->vbd', 17516: 'four/cd', 17517: 'drivers/nn\\ndt', 17518: 'out/nnp->rp', 17519: 'drivers/nns', 17520: 'c\\ndway/nn', 17521: 'four/nnp->cd', 17522: 'crews/nns\\nhauled/vbd', 17523: 'drivers/nnp->nns', 17524: 'crews/nns', 17525: 'even\\np', 17526: 'crews/nn->nns', 17527: 'even/rb', 17528: 'off\\nnns', 17529: 'official/nnp->jj', 17530: 'indianapolis/nnp', 17531: '500/cd', 17532: 'after/vbd->in', 17533: 'race/nn', 17534: 'fortun\\nter/in', 17535: 'fortune/in->nnp', 17536: 'executives/nns', 17537: 'dro\\ns/nns', 17538: 'drooled/vbd', 17539: 'like/in', 17540: 'schoolboys/nnp->nns', 17541: 'over/in', 17542: 'cars/nns', 17543: 'a\\nolboys/nns', 17544: 'cars/nn->nns', 17545: '(code_brill_demo', 17546: 'demonstration:', 17547: 'of\\ntemplates', 17548: 'z;\\nthe', 17549: 'templates', 17550: 'instantiated', 17551: 'particular\\nwords', 17552: 'rules;', 17553: 'corrects', 17554: 'of\\ncorrect', 17555: 'breaks;', 17556: 'the\\ndemonstration', 17557: 'residual', 17558: '.\\n\\n\\n\\n\\n\\n7\\xa0\\xa0\\xa0how', 17559: 'word\\nnow', 17560: 'more\\nbasic', 17561: 'question:', 17562: 'belongs', 17563: 'place?', 17564: 'morphological,', 17565: 'syntactic,\\nand', 17566: 'clues', 17567: '.\\n\\n7', 17568: '.1\\xa0\\xa0\\xa0morphological', 17569: 'clues\\nthe', 17570: '-ness', 17571: 'suffix\\nthat', 17572: 'noun,', 17573: '.\\nhappy', 17574: 'happiness,', 17575: 'ill', 17576: 'illness', 17577: 'so\\nif', 17578: '-ness,', 17579: 'likely\\nto', 17580: '-ment', 17581: 'combines\\nwith', 17582: '.\\ngovern', 17583: 'establish', 17584: 'establishment', 17585: 'morphologically', 17586: 'the\\npresent', 17587: '-ing,', 17588: 'expresses\\nthe', 17589: 'ongoing,', 17590: 'incomplete', 17591: 'falling,', 17592: 'eating)', 17593: '-ing', 17594: 'the\\nfalling', 17595: 'gerund)', 17596: '.\\n\\n\\n7', 17597: '.2\\xa0\\xa0\\xa0syntactic', 17598: 'clues\\nanother', 17599: 'can\\noccur', 17600: 'determined', 17601: 'the\\ncategory', 17602: 'noun,\\nor', 17603: 'according\\nto', 17604: 'tests,', 17605: 'adjective:\\n\\n', 17606: 'window\\n\\n', 17607: '(very)', 17608: '.\\n\\n\\n\\n7', 17609: '.3\\xa0\\xa0\\xa0semantic', 17610: 'clues\\nfinally,', 17611: 'lexical\\ncategory', 17612: 'best-known', 17613: 'is\\nsemantic:', 17614: 'person,', 17615: 'linguistics,\\nsemantic', 17616: 'criteria', 17617: 'suspicion,', 17618: 'mainly\\nbecause', 17619: 'formalize', 17620: 'criteria\\nunderpin', 17621: 'intuitions', 17622: 'categorization', 17623: 'unfamiliar', 17624: 'word\\nverjaardag', 17625: 'word\\nbirthday,', 17626: 'verjaardag', 17627: 'in\\ndutch', 17628: 'needed:', 17629: 'zij\\nis', 17630: 'vandaag', 17631: 'jarig', 17632: 'birthday', 17633: 'today,', 17634: 'word\\njarig', 17635: 'dutch,', 17636: 'exact\\nequivalent', 17637: '.4\\xa0\\xa0\\xa0new', 17638: 'words\\nall', 17639: 'acquire', 17640: 'recently\\nadded', 17641: 'cyberslacker,\\nfatoush,', 17642: 'blamestorm,', 17643: 'sars,', 17644: 'cantopop,', 17645: 'bupkis,', 17646: 'noughties,', 17647: 'muggle,', 17648: 'and\\nrobata', 17649: 'is\\nreflected', 17650: 'prepositions\\nare', 17651: 'belonging', 17652: 'along,', 17653: 'below,', 17654: 'beside,\\nbetween,', 17655: 'during,', 17656: 'for,', 17657: 'from,', 17658: 'near,', 17659: 'outside,', 17660: 'past,\\nthrough,', 17661: 'towards,', 17662: 'under,', 17663: 'with),', 17664: 'only\\nchanges', 17665: 'gradually', 17666: '.5\\xa0\\xa0\\xa0morphology', 17667: 'tagsets\\n\\ncommon', 17668: 'morpho-syntactic', 17669: 'information;\\nthat', 17670: 'markings', 17671: 'receive', 17672: 'virtue', 17673: 'consider,', 17674: 'for\\nexample,', 17675: 'word\\ngo', 17676: 'sentences:\\n\\n', 17677: '.go', 17678: 'away!\\n\\n', 17679: '.he', 17680: 'cafe', 17681: '.all', 17682: 'cakes', 17683: '.we', 17684: 'went', 17685: 'excursion', 17686: '.\\n\\neach', 17687: 'go,', 17688: 'goes,', 17689: 'gone,', 17690: '—\\nis', 17691: 'form,\\ngoes', 17692: 'and\\nrequires', 17693: 'ungrammatical', 17694: '.*they', 17695: '.*i', 17696: 'form;', 17697: 'required\\nafter', 17698: 'by\\ngoes),', 17699: '.*all', 17700: '.*he', 17701: 'distinct\\ngrammatical', 17702: 'although\\nthis', 17703: 'fine-grained', 17704: 'tagset\\nprovides', 17705: 'help\\nother', 17706: 'processors', 17707: 'tag\\nsequences', 17708: 'distinctions,\\nas', 17709: '.\\n\\n\\n\\n\\n\\n\\n\\nform\\ncategory\\ntag\\n\\n\\n\\ngo\\nbase\\nvb\\n\\ngoes\\n3rd', 17710: 'present\\nvbz\\n\\ngone\\npast', 17711: 'participle\\nvbn\\n\\ngoing\\ngerund\\nvbg\\n\\nwent\\nsimple', 17712: 'past\\nvbd\\n\\n\\ntable', 17713: 'morphosyntactic', 17714: 'tagset\\n\\n\\nin', 17715: 'be\\nhave', 17716: 'tags:\\nbe/be,', 17717: 'being/beg,', 17718: 'am/bem,', 17719: 'are/ber,', 17720: 'is/bez,', 17721: 'been/ben,', 17722: 'were/bed', 17723: 'and\\nwas/bedz', 17724: '(plus', 17725: 'verb)', 17726: 'told,\\nthis', 17727: 'tagger\\nthat', 17728: 'carrying', 17729: 'amount\\nof', 17730: 'categories,\\nsuch', 17731: 'tagsets\\ndiffer', 17732: 'finely', 17733: 'in\\nhow', 17734: 'tagged\\nsimply', 17735: 'tagset;', 17736: 'be\\nin', 17737: 'variation', 17738: 'is\\nunavoidable,', 17739: 'for\\ndifferent', 17740: \"'right\", 17741: \"way'\", 17742: 'assign\\ntags,', 17743: \"one's\", 17744: '.\\n\\n\\n\\n\\n8\\xa0\\xa0\\xa0summary\\n\\nwords', 17745: '.\\nparts', 17746: 'labels,', 17747: 'vb,\\nthe', 17748: '.\\nautomatic', 17749: 'pipeline,\\nand', 17750: 'situations', 17751: 'including:\\npredicting', 17752: 'words,\\nanalyzing', 17753: 'possible,', 17754: '.\\ndefault', 17755: '.\\ntaggers', 17756: '.\\nbackoff', 17757: 'models:', 17758: 'specialized\\nmodel', 17759: 'tagger)', 17760: 'given\\ncontext,', 17761: '.\\npart-of-speech', 17762: 'important,', 17763: 'sequence\\nclassification', 17764: 'nlp:', 17765: 'point\\nin', 17766: 'information,\\nsuch', 17767: 'number:', 17768: \"freq['cat']\", 17769: 'create\\ndictionaries', 17770: 'brace', 17771: '{},\\npos', 17772: \"{'furiously':\", 17773: \"'adj'}\", 17774: '.\\nn-gram', 17775: 'once\\nn', 17776: 'problem;\\neven', 17777: 'tiny\\nfraction', 17778: '.\\ntransformation-based', 17779: 'series\\nof', 17780: 'repair', 17781: 'tag\\nt', 17782: 'rule\\nfixes', 17783: 'possibly', 17784: '(smaller)', 17785: '.\\n\\n\\n\\n9\\xa0\\xa0\\xa0further', 17786: 'the\\ntagging', 17787: '(petrov,', 17788: 'das,', 17789: 'mcdonald,', 17790: '2012)', 17791: '(chap-data-intensive)', 17792: 'a\\ncontiguous', 17793: 'see\\nnltk', 17794: '.upenn_tagset()', 17795: '.brown_tagset()', 17796: 'those\\nlisted', 17797: '.\\nwords', 17798: 'directives', 17799: 'synthesizer,\\nindicating', 17800: 'emphasized', 17801: 'sense\\nnumbers,', 17802: 'indicating', 17803: '.\\nexamples', 17804: 'reasons,', 17805: 'single\\nword', 17806: 'xml-style\\ntags,', 17807: 'is\\ntagged', 17808: '.\\n\\nspeech', 17809: 'synthesis', 17810: '(w3c', 17811: 'ssml):\\nthat', 17812: '<emphasis>big</emphasis>', 17813: 'car!\\nsemcor:', 17814: 'senses:\\nspace', 17815: '<wf', 17816: 'pos=nn', 17817: 'lemma=form', 17818: 'wnsn=4>form</wf>\\nis', 17819: 'measured', 17820: 'dimensions', 17821: '.\\n(wordnet', 17822: 'form/nn', 17823: 'shape,', 17824: 'configuration,\\ncontour,', 17825: 'conformation)\\nmorphological', 17826: 'turin', 17827: 'italian', 17828: \"treebank:\\ne'\", 17829: 'italiano', 17830: 'progetto', 17831: 'realizzazione', 17832: 'il\\nprimo', 17833: '(primo', 17834: 'ordin', 17835: 'sing)', 17836: 'porto', 17837: 'turistico', 17838: \"dell'\", 17839: 'albania', 17840: '(forsyth', 17841: 'martell,', 17842: 'with\\nnltk', 17843: 'communicative\\nfunction:\\n\\nstatement', 17844: 'user117', 17845: 'dude', 17846: 'that\\nynquestion', 17847: 'user120', 17848: 'something?\\nbye', 17849: 'gonna', 17850: 'food,', 17851: \"i'll\", 17852: '.\\nsystem', 17853: 'user122', 17854: 'join\\nsystem', 17855: 'user2', 17856: 'slaps', 17857: 'trout', 17858: '.\\nstatement', 17859: 'user121', 17860: '18/m', 17861: 'pm', 17862: 'me', 17863: 'tryin', 17864: 'chat\\n\\n\\n\\n10\\xa0\\xa0\\xa0exercises\\n\\n☼\\nsearch', 17865: 'spoof', 17866: 'newspaper', 17867: 'headlines,', 17868: 'gems', 17869: 'as:\\nbritish', 17870: 'waffles', 17871: 'falkland', 17872: 'islands,', 17873: 'and\\njuvenile', 17874: 'shooting', 17875: 'defendant', 17876: '.\\nmanually', 17877: 'part-of-speech\\ntags', 17878: '.\\n☼\\nworking', 17879: 'be\\neither', 17880: 'contest);', 17881: 'opponent', 17882: 'to\\npredict', 17883: \"the\\nopponent's\", 17884: 'prediction,', 17885: '.\\n☼\\ntokenize', 17886: 'sentence:\\nthey', 17887: 'clock,', 17888: 'chase', 17889: 'involved?\\n☼', 17890: 'other\\nexamples', 17891: 'map\\nfrom', 17892: 'to?\\n☼', 17893: 'mode,', 17894: 'd,', 17895: 'add\\nsome', 17896: 'non-existent\\nentry,', 17897: \"d['xyz']?\\n☼\", 17898: 'deleting', 17899: 'syntax\\ndel', 17900: \"d['abc']\", 17901: 'deleted', 17902: 'dictionaries,', 17903: 'd1', 17904: 'd2,', 17905: 'to\\neach', 17906: '.update(d2)', 17907: 'do?\\nwhat', 17908: 'for?\\n☼', 17909: 'e,', 17910: 'entry\\nfor', 17911: 'headword,', 17912: 'sense,', 17913: 'and\\nexample,', 17914: 'are\\nrestrictions', 17915: 'went,', 17916: 'the\\nsense', 17917: 'interchanged', 17918: 'contexts\\nillustrated', 17919: '(3d)', 17920: '.\\n☼\\ntrain', 17921: 'not?\\n☼\\nlearn', 17922: '.affixtagger))', 17923: '.\\ntrain', 17924: 'settings', 17925: 'length\\nand', 17926: 'training\\ndata', 17927: 'why?\\n☼', 17928: 'be\\nsubstituted', 17929: 'library\\ndocumentation', 17930: 'strings\\nhttp://docs', 17931: '.org/lib/typesseq-strings', 17932: '.html\\nand', 17933: 'two\\ndifferent', 17934: 'brown\\ncorpus,', 17935: 'removing', 17936: 'following\\nquestions:\\nwhich', 17937: 'singular\\nform?', 17938: 'plurals,', 17939: '.)\\nwhich', 17940: 'they,', 17941: 'and\\nwhat', 17942: 'represent?\\nlist', 17943: 'represent?\\nwhich', 17944: 'commonly', 17945: 'after?', 17946: 'represent?\\n\\n\\n◑', 17947: 'tagger:\\nwhat', 17948: 'various\\nmodel', 17949: 'omitted?\\nconsider', 17950: 'curve', 17951: 'a\\ngood', 17952: 'balances', 17953: 'scenarios', 17954: 'to\\nminimize', 17955: 'maximize', 17956: 'regard', 17957: 'usage?\\n\\n\\n◑', 17958: 'tagger,\\nassuming', 17959: 'table?', 17960: 'assigned\\nthe', 17961: '.)\\n◑', 17962: 'questions:\\nwhat', 17963: 'tag?\\nhow', 17964: 'tags?\\nwhat', 17965: 'involve\\nthese', 17966: 'words?\\n\\n\\n◑', 17967: 'evaluate()', 17968: 'accurately\\nthe', 17969: 'text\\nwas', 17970: \"[('the',\", 17971: \"'nn')]\", 17972: \"output\\n[('the',\", 17973: \"'nn')],\", 17974: 'works:\\na', 17975: 'words\\nas', 17976: '.evaluate()', 17977: 'tagging?\\nonce', 17978: 'newly', 17979: 'method\\ngo', 17980: 'score?\\nnow', 17981: 'inspect\\nnltk', 17982: '.api', 17983: '.__file__', 17984: '(be', 17985: 'and\\nnot', 17986: 'file)', 17987: 'phrases\\naccording', 17988: 'questions:\\nproduce', 17989: 'alphabetically', 17990: 'md', 17991: '.\\nidentify', 17992: 'verbs\\n(e', 17993: 'deals,', 17994: 'flies)', 17995: 'nn\\n(eg', 17996: 'lab)', 17997: 'masculine', 17998: 'feminine', 17999: 'pronouns?\\n\\n\\n◑', 18000: 'for\\nthe', 18001: 'and\\npreceding', 18002: 'qualifiers', 18003: '.\\ninvestigate', 18004: '.\\n◑\\nwe', 18005: 'fall-back', 18006: 'for\\ncardinal', 18007: 'strings,\\nit', 18008: 'example,\\nwe', 18009: 'regexptagger())\\nthat', 18010: '.\\n(use', 18011: '.)\\n◑\\nconsider', 18012: 'accuracy()', 18013: 'method,\\nand', 18014: 'process?\\n◑\\nhow', 18015: 'problem?', 18016: 'the\\nperformance', 18017: '.\\ntabulate', 18018: 'estimate', 18019: 'required\\nfor', 18020: 'of\\n105', 18021: '102', 18022: 'is\\nmorphologically', 18023: 'complex,', 18024: 'clues\\n(e', 18025: 'capitalization)', 18026: 'a\\nregular', 18027: '(ordered', 18028: 'unigram\\ntagger,', 18029: 'of\\nyour', 18030: 'tagger(s)', 18031: 'data?\\ndiscuss', 18032: 'showing\\nchange', 18033: '.\\nplot', 18034: 'varied', 18035: '.\\n◑\\ninspect', 18036: 'collapse', 18037: 'mapping,', 18038: '.\\n◑\\nexperiment', 18039: 'your\\nown', 18040: 'name)', 18041: '.\\nsuch', 18042: 'make,', 18043: 'less\\ninformation', 18044: '.\\n◑\\nrecall', 18045: \"hadn't\\nseen\", 18046: 'fail', 18047: 'sentence\\neven', 18048: '(even', 18049: 'during\\ntraining)', 18050: 'circumstance', 18051: 'happen?', 18052: 'this?\\n◑\\npreprocess', 18053: 'low', 18054: 'unk,\\nbut', 18055: 'untouched', 18056: 'tagger\\non', 18057: 'help?', 18058: 'contribution', 18059: 'unigram\\ntagger', 18060: 'now?\\n◑\\nmodify', 18061: 'scale', 18062: 'x-axis,', 18063: '.semilogx()', 18064: 'shape', 18065: 'plot?', 18066: 'gradient\\ntell', 18067: 'anything?\\n◑\\nconsult', 18068: 'function,\\nusing', 18069: '.brill', 18070: '.demo)', 18071: '.\\nis', 18072: '(corpus', 18073: 'size)', 18074: 'performance?\\n◑', 18075: 'wordi', 18076: 'tagi\\n→', 18077: 'tagi+1', 18078: '264', 18079: 'exactly\\nthree', 18080: 'column,', 18081: 'tags\\nin', 18082: 'print\\nout', 18083: 'each\\npossible', 18084: 'discriminate', 18085: 'the\\nepistemic', 18086: 'deontic', 18087: 'must?\\n★\\ncreate', 18088: 'taggers,\\nincorporating', 18089: '.\\ncreate', 18090: 'the\\naccuracy', 18091: 'best?\\ntry', 18092: 'affect\\nyour', 18093: 'results?\\n\\n\\n★\\nour', 18094: 'word\\n(using', 18095: 'regexptagger()),', 18096: 'altogether', 18097: 'tag\\nit', 18098: '.defaulttagger())', 18099: 'having\\nnew', 18100: \"kim's\", 18101: 'new\\nword,', 18102: 'np$)', 18103: '.\\ni', 18104: 'sensitive', 18105: 'word,\\nand', 18106: 'ignores', 18107: 'source\\ncode', 18108: 'unigramtagger(),', 18109: 'object-oriented\\nprogramming', 18110: '.)\\nadd', 18111: 'ordinary', 18112: 'trigram\\nand', 18113: '.\\nevaluate', 18114: '.\\n\\n\\n★\\nconsider', 18115: 'which\\ndetermines', 18116: \"abney's\", 18117: 'impossibility', 18118: 'of\\nexact', 18119: '1996)', 18120: 'than\\njust', 18121: 'problem?\\n★\\nuse', 18122: 'estimation', 18123: '.probability,\\nsuch', 18124: 'lidstone', 18125: 'laplace', 18126: 'estimation,', 18127: 'statistical\\ntagger', 18128: 'where\\ncontexts', 18129: '.\\n★\\ninspect', 18130: '.out', 18131: 'and\\nerrors', 18132: 'code\\n(at', 18133: '.nltk', 18134: '.org/code)\\nand', 18135: '.\\ndelete', 18136: 'templates,', 18137: 'to\\ncorrect', 18138: '.\\n★\\ndevelop', 18139: 'anti-n-grams', 18140: 'as\\n[the,', 18141: 'the]', 18142: 'initialized', 18143: 'anti-ngram', 18144: 'prevent\\nbackoff', 18145: 'avoid\\nestimating', 18146: 'p(the', 18147: 'p(the))', 18148: '.\\n★\\ninvestigate', 18149: 'corpus:\\ngenre', 18150: '(category),', 18151: '(fileid),', 18152: '.\\ncompare', 18153: 'method\\nis', 18154: 'n-fold', 18155: 'cross', 18156: 'validation,\\ndiscussed', 18157: 'evaluations', 18158: '.)\\n★\\ndevelop', 18159: 'inherits', 18160: 'class,\\nand', 18161: 'encapsulates', 18162: 'taggers\\nhave', 18163: 'acst6', 18164: 'text\\n\\n\\n\\n\\n6', 18165: 'text\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ndetecting', 18166: 'in\\n-ed', 18167: 'tend', 18168: '(5', 18169: 'of\\nwill', 18170: 'indicative', 18171: 'observable\\npatterns', 18172: 'happen', 18173: 'to\\ncorrelate', 18174: 'looking,', 18175: 'to\\nassociate', 18176: 'meaning?\\nthe', 18177: 'salient', 18178: 'it?\\nhow', 18179: 'automatically?\\nwhat', 18180: 'models?\\n\\nalong', 18181: 'learning\\ntechniques,', 18182: \"bayes'\", 18183: 'classifiers,\\nand', 18184: 'entropy', 18185: 'classifiers', 18186: 'and\\nstatistical', 18187: 'techniques,', 18188: 'focusing', 18189: 'how\\nand', 18190: 'more\\ntechnical', 18191: 'background)', 18192: '.\\n\\n1\\xa0\\xa0\\xa0supervised', 18193: 'classification\\nclassification', 18194: 'class\\nlabel', 18195: 'each\\ninput', 18196: 'of\\nlabels', 18197: 'tasks\\nare:\\n\\ndeciding', 18198: '.\\ndeciding', 18199: 'of\\ntopic', 18200: 'sports,', 18201: 'technology,', 18202: 'politics', 18203: 'bank', 18204: 'to\\nrefer', 18205: 'river', 18206: 'bank,', 18207: 'financial', 18208: 'institution,', 18209: 'tilting\\nto', 18210: 'side,', 18211: 'depositing', 18212: 'financial\\ninstitution', 18213: 'multi-class', 18214: 'be\\nassigned', 18215: 'labels;', 18216: 'open-class', 18217: 'advance;', 18218: 'a\\nlist', 18219: 'jointly', 18220: 'classifier', 18221: 'on\\ntraining', 18222: 'label', 18223: 'the\\nframework', 18224: '(a)', 18225: 'feature\\nextractor', 18226: 'about\\neach', 18227: '.\\npairs', 18228: 'fed', 18229: 'learning\\nalgorithm', 18230: '(b)', 18231: 'same\\nfeature', 18232: 'extractor', 18233: 'generates\\npredicted', 18234: 'be\\nemployed', 18235: 'intended\\nto', 18236: 'comprehensive,', 18237: 'that\\ncan', 18238: '.1\\xa0\\xa0\\xa0gender', 18239: 'identification\\nin', 18240: 'names\\nhave', 18241: 'distinctive', 18242: 'a,\\ne', 18243: 'female,', 18244: 'in\\nk,', 18245: 'precisely', 18246: 'deciding', 18247: 'what\\nfeatures', 18248: 'encode\\nthose', 18249: 'the\\nfinal', 18250: 'extractor\\nfunction', 18251: 'name:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18252: 'gender_features(word):\\n', 18253: \"{'last_letter':\", 18254: 'word[-1]}\\n>>>', 18255: \"gender_features('shrek')\\n{'last_letter':\", 18256: \"'k'}\\n\\n\\n\\nthe\", 18257: 'from\\nfeature', 18258: 'case-sensitive\\nstrings', 18259: 'feature,', 18260: \"'last_letter'\", 18261: 'as\\nbooleans,', 18262: '.\\n\\nnote\\nmost', 18263: 'using\\nsimple', 18264: 'booleans,', 18265: 'but\\nnote', 18266: 'not\\nnecessarily', 18267: \"feature's\", 18268: 'or\\ncompute', 18269: 'indeed,', 18270: 'and\\ninformative', 18271: 'supervised\\nclassifier,', 18272: 'extractor,', 18273: 'prepare\\na', 18274: 'corresponding\\nclass', 18275: 'names\\n>>>', 18276: 'labeled_names', 18277: '([(name,', 18278: \"'male')\", 18279: \".txt')]\", 18280: '[(name,', 18281: \"'female')\", 18282: \".txt')])\\n>>>\", 18283: '.shuffle(labeled_names)\\n\\n\\n\\n\\nnext,', 18284: 'and\\ndivide', 18285: 'set\\nand', 18286: 'new\\nnaive', 18287: 'bayes', 18288: 'featuresets', 18289: '[(gender_features(n),', 18290: 'gender)', 18291: '(n,', 18292: 'labeled_names]\\n>>>', 18293: 'train_set,', 18294: 'test_set', 18295: 'featuresets[500:],', 18296: 'featuresets[:500]\\n>>>', 18297: '.naivebayesclassifier', 18298: '.train(train_set)\\n\\n\\n\\nwe', 18299: 'the\\nchapter', 18300: 'not\\nappear', 18301: 'data:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18302: \".classify(gender_features('neo'))\\n'male'\\n>>>\", 18303: \".classify(gender_features('trinity'))\\n'female'\\n\\n\\n\\nobserve\", 18304: 'correctly\\nclassified', 18305: '2199,', 18306: 'it\\nstill', 18307: 'conforms', 18308: 'expectations', 18309: 'genders', 18310: 'can\\nsystematically', 18311: 'of\\nunseen', 18312: '.classify', 18313: '.accuracy(classifier,', 18314: 'test_set))\\n0', 18315: '.77\\n\\n\\n\\nfinally,', 18316: 'it\\nfound', 18317: 'distinguishing', 18318: \"names'\", 18319: 'genders:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18320: '.show_most_informative_features(5)\\nmost', 18321: 'features\\n', 18322: \"'a'\", 18323: '33', 18324: '.0\\n', 18325: \"'k'\", 18326: '32', 18327: '.0\\n\\n\\n\\nthis', 18328: 'a\\nare', 18329: 'end\\nin', 18330: 'these\\nratios', 18331: 'likelihood', 18332: 'ratios,', 18333: 'for\\ncomparing', 18334: 'feature-outcome', 18335: 'turn:\\nmodify', 18336: 'gender_features()', 18337: 'the\\nclassifier', 18338: 'first\\nletter,', 18339: 'be\\ninformative', 18340: 'retrain', 18341: 'and\\ntest', 18342: '.\\n\\nwhen', 18343: 'list\\nthat', 18344: 'large\\namount', 18345: 'function\\nnltk', 18346: '.apply_features,', 18347: 'acts\\nlike', 18348: 'memory:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18349: 'apply_features\\n>>>', 18350: 'train_set', 18351: 'apply_features(gender_features,', 18352: 'labeled_names[500:])\\n>>>', 18353: 'labeled_names[:500])\\n\\n\\n\\n\\n\\n1', 18354: '.2\\xa0\\xa0\\xa0choosing', 18355: 'features\\nselecting', 18356: 'encode', 18357: 'a\\nlearning', 18358: 'enormous', 18359: 'impact', 18360: \"method's\\nability\", 18361: 'decent\\nperformance', 18362: 'features,\\nthere', 18363: 'gains', 18364: 'carefully\\nconstructed', 18365: 'thorough', 18366: 'at\\nhand', 18367: '.\\ntypically,', 18368: 'extractors', 18369: 'of\\ntrial-and-error,', 18370: 'guided', 18371: 'is\\nrelevant', 18372: 'a\\nkitchen', 18373: 'sink', 18374: 'think\\nof,', 18375: 'are\\nhelpful', 18376: '.\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\ndef', 18377: 'gender_features2(name):\\n', 18378: 'features[first_letter]', 18379: '.lower()\\n', 18380: 'features[last_letter]', 18381: \"'abcdefghijklmnopqrstuvwxyz':\\n\", 18382: 'features[count({})', 18383: '.format(letter)]', 18384: '.count(letter)\\n', 18385: 'features[has({})', 18386: '(letter', 18387: '.lower())\\n', 18388: 'features\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18389: \"gender_features2('john')\", 18390: \"\\n{'count(j)':\", 18391: \"'has(d)':\", 18392: 'false,', 18393: \"'count(b)':\", 18394: '.}\\n\\n\\nexample', 18395: '(code_gender_features_overfitting', 18396: 'overfits', 18397: 'a\\nlarge', 18398: 'overfitting', 18399: 'relatively', 18400: '.\\n\\nhowever,', 18401: 'features\\nthat', 18402: 'provide\\ntoo', 18403: 'of\\nrelying', 18404: 'idiosyncrasies', 18405: 'generalize\\nwell', 18406: 'overfitting,', 18407: 'and\\ncan', 18408: 'problematic', 18409: 'overfit\\nthe', 18410: 'accuracy\\nis', 18411: '1%', 18412: 'only\\npays', 18413: '[(gender_features2(n),', 18414: '.train(train_set)\\n>>>', 18415: '.768\\n\\n\\n\\n\\n\\n\\n\\nonce', 18416: 'chosen,', 18417: 'productive\\nmethod', 18418: 'refining', 18419: 'first,\\nwe', 18420: 'subdivided\\ninto', 18421: 'dev-test', 18422: 'train_names', 18423: 'labeled_names[1500:]\\n>>>', 18424: 'devtest_names', 18425: 'labeled_names[500:1500]\\n>>>', 18426: 'test_names', 18427: 'labeled_names[:500]\\n\\n\\n\\nthe', 18428: 'is\\nused', 18429: 'final\\nevaluation', 18430: 'analysis,\\nrather', 18431: 'subsets', 18432: 'sets:', 18433: 'set,\\nand', 18434: '.\\n\\nhaving', 18435: 'datasets,', 18436: 'model\\nusing', 18437: 'the\\ndev-test', 18438: 'train_names]\\n>>>', 18439: 'devtest_set', 18440: 'devtest_names]\\n>>>', 18441: 'test_names]\\n>>>', 18442: '.train(train_set)', 18443: 'devtest_set))', 18444: '\\n0', 18445: '.75\\n\\n\\n\\nusing', 18446: '(name,', 18447: 'devtest_names:\\n', 18448: '.classify(gender_features(name))\\n', 18449: 'tag:\\n', 18450: '.append(', 18451: 'guess,', 18452: ')\\n\\n\\n\\nwe', 18453: 'predicted\\nthe', 18454: 'label,', 18455: 'of\\ninformation', 18456: 'which\\nexisting', 18457: 'tricking', 18458: 'wrong\\ndecision)', 18459: 'adjusted', 18460: 'the\\nnames', 18461: 'sorted(errors):\\n', 18462: \"print('correct={:<8}\", 18463: 'guess={:<8s}', 18464: \"name={:<30}'\", 18465: '.format(tag,', 18466: 'name))\\ncorrect=female', 18467: 'guess=male', 18468: 'name=abigail\\n', 18469: '.\\ncorrect=female', 18470: 'name=cindelyn\\n', 18471: 'name=katheryn\\ncorrect=female', 18472: 'name=kathryn\\n', 18473: '.\\ncorrect=male', 18474: 'guess=female', 18475: 'name=aldrich\\n', 18476: 'name=mitch\\n', 18477: 'name=rich\\n', 18478: '.\\n\\n\\n\\nlooking', 18479: 'suffixes\\nthat', 18480: 'yn', 18481: 'predominantly', 18482: 'female,\\ndespite', 18483: 'male;', 18484: 'h\\ntend', 18485: 'therefore\\nadjust', 18486: 'two-letter\\nsuffixes:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18487: \"{'suffix1':\", 18488: 'word[-1:],\\n', 18489: \"'suffix2':\", 18490: 'word[-2:]}\\n\\n\\n\\nrebuilding', 18491: 'dataset', 18492: '2\\npercentage', 18493: '76', 18494: '.5%', 18495: '.2%):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18496: 'devtest_set))\\n0', 18497: '.782\\n\\n\\n\\nthis', 18498: 'procedure', 18499: 'repeated,', 18500: 'for\\npatterns', 18501: 'a\\ndifferent', 18502: 'dev-test/training', 18503: 'classifier\\ndoes', 18504: 'the\\nmodel,', 18505: 'trust', 18506: 'of\\nhow', 18507: 'therefore\\nimportant', 18508: 'separate,', 18509: 'unused,', 18510: 'model\\ndevelopment', 18511: '.3\\xa0\\xa0\\xa0document', 18512: 'classification\\n\\nin', 18513: 'of\\ncorpora', 18514: 'labeled', 18515: 'using\\nthese', 18516: 'tag\\nnew', 18517: 'we\\nconstruct', 18518: 'appropriate\\ncategories', 18519: 'corpus,\\nwhich', 18520: 'categorizes', 18521: 'movie_reviews\\n>>>', 18522: '[(list(movie_reviews', 18523: '.words(fileid)),', 18524: 'category)\\n', 18525: 'movie_reviews', 18526: '.fileids(category)]\\n>>>', 18527: '.shuffle(documents)\\n\\n\\n\\nnext,', 18528: 'classifier\\nwill', 18529: 'to\\n(1', 18530: 'identification,', 18531: 'can\\ndefine', 18532: 'document\\ncontains', 18533: 'the\\n2000', 18534: 'corpus\\n', 18535: 'extractor\\n', 18536: '.\\n\\n\\n\\n\\n\\xa0\\n\\nall_words', 18537: '.words())\\nword_features', 18538: 'list(all_words)[:2000]', 18539: 'document_features(document):', 18540: 'document_words', 18541: 'set(document)', 18542: 'word_features:\\n', 18543: \"features['contains({})'\", 18544: '.format(word)]', 18545: 'document_words)\\n', 18546: 'print(document_features(movie_reviews', 18547: \".words('pos/cv957_8737\", 18548: \".txt')))\", 18549: \"\\n{'contains(waste)':\", 18550: \"'contains(lot)':\", 18551: '(code_document_classify_fd', 18552: 'whose\\nfeatures', 18553: 'present\\nin', 18554: '.\\n\\n\\nnote\\nthe', 18555: 'in\\n,', 18556: 'if\\nword', 18557: 'is\\nthat', 18558: 'than\\nchecking', 18559: 'a\\nclassifier', 18560: '(1', 18561: 'its\\naccuracy', 18562: 'again,\\nwe', 18563: 'show_most_informative_features()', 18564: 'which\\nfeatures', 18565: 'informative\\n', 18566: '.\\n\\n\\n\\n\\n\\xa0\\n\\nfeaturesets', 18567: '[(document_features(d),', 18568: '(d,c)', 18569: 'documents]\\ntrain_set,', 18570: 'featuresets[100:],', 18571: 'featuresets[:100]\\nclassifier', 18572: '.train(train_set)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18573: 'test_set))', 18574: '.81\\n>>>', 18575: '.show_most_informative_features(5)', 18576: '\\nmost', 18577: 'contains(outstanding)', 18578: 'neg', 18579: 'contains(seagal)', 18580: 'contains(wonderfully)', 18581: 'contains(damon)', 18582: 'contains(wasted)', 18583: '.0\\n\\n\\nexample', 18584: '(code_document_classify_use', 18585: '.\\n\\napparently', 18586: 'seagal', 18587: '8\\ntimes', 18588: 'positive,', 18589: 'that\\nmentions', 18590: 'damon', 18591: '.4\\xa0\\xa0\\xa0part-of-speech', 18592: 'tagging\\n\\nin', 18593: 'a\\npart-of-speech', 18594: 'make-up', 18595: 'be\\nhand-crafted', 18596: 'which\\nsuffixes', 18597: 'the\\nmost', 18598: 'are:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18599: 'suffix_fdist', 18600: '.words():\\n', 18601: 'suffix_fdist[word[-1:]]', 18602: 'suffix_fdist[word[-2:]]', 18603: 'suffix_fdist[word[-3:]]', 18604: '1\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18605: 'common_suffixes', 18606: '[suffix', 18607: '(suffix,', 18608: '.most_common(100)]\\n>>>', 18609: \"print(common_suffixes)\\n['e',\", 18610: \"'nd',\", 18611: \"'l',\\n\", 18612: \"'er',\", 18613: \"'ing',\", 18614: \"'or',\\n\", 18615: \"'``',\", 18616: \"'ion',\", 18617: '.]\\n\\n\\n\\n\\nnext,', 18618: 'given\\nword', 18619: 'suffixes:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18620: 'pos_features(word):\\n', 18621: 'common_suffixes:\\n', 18622: \"features['endswith({})'\", 18623: '.format(suffix)]', 18624: '.endswith(suffix)\\n', 18625: 'features\\n\\n\\n\\nfeature', 18626: 'tinted', 18627: 'glasses,', 18628: 'highlighting\\nsome', 18629: '(colors)', 18630: 'impossible\\nto', 18631: 'rely', 18632: 'exclusively', 18633: 'on\\nthese', 18634: 'highlighted', 18635: 'on\\ninformation', 18636: 'any)', 18637: 'word\\nhas', 18638: '.\\n\\n\\nnow', 18639: 'to\\ntrain', 18640: 'in\\n4):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18641: 'tagged_words', 18642: \".tagged_words(categories='news')\\n>>>\", 18643: '[(pos_features(n),', 18644: 'g)', 18645: '(n,g)', 18646: 'tagged_words]\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18647: 'int(len(featuresets)', 18648: '.1)\\n>>>', 18649: 'featuresets[size:],', 18650: 'featuresets[:size]\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18651: '.decisiontreeclassifier', 18652: 'test_set)\\n0', 18653: '.62705121829935351\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18654: \".classify(pos_features('cats'))\\n'nns'\\n\\n\\n\\n\\none\", 18655: 'fairly\\neasy', 18656: 'them\\nout', 18657: 'pseudocode:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18658: 'print(classifier', 18659: '.pseudocode(depth=4))\\nif', 18660: 'endswith(,)', 18661: 'true:', 18662: \"','\\nif\", 18663: 'false:\\n', 18664: 'endswith(the)', 18665: \"'at'\\n\", 18666: 'endswith(s)', 18667: 'true:\\n', 18668: 'endswith(is)', 18669: \"'bez'\\n\", 18670: 'false:', 18671: \"'vbz'\\n\", 18672: 'endswith(', 18673: \".'\\n\", 18674: \"'nn'\\n\\n\\n\\n\\nhere,\", 18675: 'word\\nends', 18676: 'tag\\n,', 18677: 'the,\\nin', 18678: 'certainly', 18679: 'gets\\nused', 18680: '.\\ncontinuing', 18681: 'so,\\nthen', 18682: 'vbz', 18683: '(unless', 18684: \"it's\\nthe\", 18685: 'bez),', 18686: 'not,\\nthen', 18687: 'if-then', 18688: 'below\\nthe', 18689: 'depth=4', 18690: 'the\\ntop', 18691: '.5\\xa0\\xa0\\xa0exploiting', 18692: 'context\\n\\nby', 18693: 'augmenting', 18694: 'this\\npart-of-speech', 18695: 'leverage', 18696: 'word-internal\\nfeatures,', 18697: 'it\\ncontains,', 18698: 'extractor\\njust', 18699: 'that\\ndepend', 18700: 'contextual\\nfeatures', 18701: 'knowing', 18702: 'word\\nis', 18703: 'functioning', 18704: 'not\\na', 18705: 'depend', 18706: \"word's\", 18707: 'revise', 18708: 'a\\ncomplete', 18709: '(untagged)', 18710: 'employs', 18711: 'a\\ncontext-dependent', 18712: 'tag\\nclassifier', 18713: 'pos_features(sentence,', 18714: 'i):', 18715: '{suffix(1):', 18716: 'sentence[i][-1:],\\n', 18717: 'suffix(2):', 18718: 'sentence[i][-2:],\\n', 18719: 'suffix(3):', 18720: 'sentence[i][-3:]}\\n', 18721: 'features[prev-word]', 18722: '<start>\\n', 18723: 'sentence[i-1]\\n', 18724: 'pos_features(brown', 18725: '.sents()[0],', 18726: \"8)\\n{'suffix(3)':\", 18727: \"'prev-word':\", 18728: \"'suffix(2)':\", 18729: \"'suffix(1)':\", 18730: \"'n'}\\n\\n>>>\", 18731: 'tagged_sents', 18732: 'tagged_sents:\\n', 18733: 'untagged_sent', 18734: '.untag(tagged_sent)\\n', 18735: 'enumerate(tagged_sent):\\n', 18736: '(pos_features(untagged_sent,', 18737: 'i),', 18738: ')\\n\\n>>>', 18739: 'featuresets[:size]\\n>>>', 18740: '.train(train_set)\\n\\n>>>', 18741: '.78915962207856782\\n\\n\\nexample', 18742: '(code_suffix_pos_tag', 18743: 'detector\\nexamines', 18744: 'to\\ndetermine', 18745: 'in\\nparticular,', 18746: 'a\\nfeature', 18747: '.\\n\\n\\n\\nit', 18748: 'performance\\nof', 18749: 'learns\\nthat', 18750: 'gubernatorial', 18751: 'follows\\nan', 18752: \"word's\\npart-of-speech\", 18753: 'independent', 18754: 'this\\nmakes', 18755: 'tend\\nto', 18756: 'case-by-case', 18757: 'however,\\nthere', 18758: 'are\\ninterested', 18759: 'related\\nto', 18760: '.6\\xa0\\xa0\\xa0sequence', 18761: 'classification\\nin', 18762: 'classification\\ntasks,', 18763: 'joint', 18764: 'an\\nappropriate', 18765: 'case\\nof', 18766: 'sequence\\nclassifier', 18767: '.\\n\\none', 18768: 'strategy,', 18769: 'consecutive\\nclassification', 18770: 'to\\nfind', 18771: 'input,\\nthen', 18772: 'next\\ninput', 18773: 'repeated', 18774: 'have\\nbeen', 18775: 'bigram\\ntagger', 18776: 'chose\\nthe', 18777: 'the\\npredicted', 18778: 'must\\naugment', 18779: 'history\\nargument,', 18780: 'already\\nclassified,', 18781: 'is\\npossible', 18782: 'right\\nof', 18783: 'those\\nwords', 18784: 'yet)', 18785: '.\\nhaving', 18786: 'proceed', 18787: 'our\\nsequence', 18788: 'to\\nprovide', 18789: 'when\\ntagging', 18790: 'the\\noutput', 18791: '.\\n\\n\\n\\n\\n\\xa0\\n\\n', 18792: 'history):', 18793: 'features[prev-tag]', 18794: 'history[i-1]\\n', 18795: 'features\\n\\nclass', 18796: 'consecutivepostagger(nltk', 18797: '.taggeri):', 18798: 'train_sents):\\n', 18799: 'train_sents:\\n', 18800: 'featureset', 18801: 'pos_features(untagged_sent,', 18802: 'history)\\n', 18803: '(featureset,', 18804: ')\\n', 18805: '.append(tag)\\n', 18806: '.classifier', 18807: '.train(train_set)\\n\\n', 18808: 'tag(self,', 18809: 'sentence):\\n', 18810: 'enumerate(sentence):\\n', 18811: '.classify(featureset)\\n', 18812: 'zip(sentence,', 18813: 'history)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18814: 'int(len(tagged_sents)', 18815: 'train_sents,', 18816: 'tagged_sents[size:],', 18817: 'tagged_sents[:size]\\n>>>', 18818: 'consecutivepostagger(train_sents)\\n>>>', 18819: 'print(tagger', 18820: '.evaluate(test_sents))\\n0', 18821: '.79796012981\\n\\n\\nexample', 18822: '(code_consecutive_pos_tagger', 18823: 'classifier\\n\\n\\n\\n1', 18824: '.7\\xa0\\xa0\\xa0other', 18825: 'classification\\none', 18826: 'shortcoming', 18827: 'commit', 18828: 'decision\\nthat', 18829: 'noun,\\nbut', 18830: 'no\\nway', 18831: 'mistake', 18832: 'transformational', 18833: 'joint\\nclassifiers', 18834: 'the\\ninputs,', 18835: 'iteratively', 18836: 'to\\nrepair', 18837: 'inconsistencies', 18838: 'tagger,\\ndescribed', 18839: 'possible\\nsequences', 18840: 'sequence\\nwhose', 18841: 'by\\nhidden', 18842: 'markov', 18843: '.\\nhidden', 18844: 'predicted\\ntags', 18845: 'tag\\nfor', 18846: 'over\\ntags', 18847: 'probabilities', 18848: 'calculate\\nprobability', 18849: 'unfortunately,', 18850: 'of\\npossible', 18851: '30\\ntags,', 18852: '600', 18853: 'trillion', 18854: '(3010)', 18855: 'label\\na', 18856: '10-word', 18857: 'separately,', 18858: 'the\\nfeature', 18859: 'most\\nrecent', 18860: 'small)', 18861: 'that\\nrestriction,', 18862: '.7)\\nto', 18863: 'efficiently', 18864: 'particular,\\nfor', 18865: 'i,\\na', 18866: 'by\\ntwo', 18867: 'and\\nlinear-chain', 18868: 'models;\\nbut', 18869: '.\\n\\n\\n\\n2\\xa0\\xa0\\xa0further', 18870: 'classification\\n\\n2', 18871: '.1\\xa0\\xa0\\xa0sentence', 18872: 'segmentation\\nsentence', 18873: 'viewed', 18874: 'for\\npunctuation:', 18875: 'a\\nsentence,', 18876: 'mark,', 18877: 'decide\\nwhether', 18878: 'terminates', 18879: 'segmented\\ninto', 18880: 'for\\nextracting', 18881: 'features:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18882: '.sents()\\n>>>', 18883: 'sents:\\n', 18884: '.extend(sent)\\n', 18885: 'len(sent)\\n', 18886: '.add(offset-1)\\n\\n\\n\\nhere,', 18887: 'merged', 18888: 'individual\\nsentences,', 18889: 'all\\nsentence-boundary', 18890: 'punctuation\\nindicates', 18891: 'sentence-boundary:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18892: 'punct_features(tokens,', 18893: 'i):\\n', 18894: \"{'next-word-capitalized':\", 18895: 'tokens[i+1][0]', 18896: '.isupper(),\\n', 18897: 'tokens[i-1]', 18898: '.lower(),\\n', 18899: \"'punct':\", 18900: 'tokens[i],\\n', 18901: \"'prev-word-is-one-char':\", 18902: 'len(tokens[i-1])', 18903: '1}\\n\\n\\n\\nbased', 18904: 'labeled\\nfeaturesets', 18905: 'tagging\\nwhether', 18906: 'not:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18907: '[(punct_features(tokens,', 18908: 'boundaries))\\n', 18909: 'range(1,', 18910: 'len(tokens)-1)\\n', 18911: \".?!']\\n\\n\\n\\nusing\", 18912: 'featuresets,', 18913: 'a\\npunctuation', 18914: 'classifier:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18915: '.936026936026936\\n\\n\\n\\nto', 18916: 'simply\\ncheck', 18917: 'boundary;\\nand', 18918: 'listing\\nin', 18919: 'segment_sentences(words):\\n', 18920: 'enumerate(words):\\n', 18921: \".?!'\", 18922: '.classify(punct_features(words,', 18923: 'i))', 18924: '.append(words[start:i+1])\\n', 18925: 'len(words):\\n', 18926: '.append(words[start:])\\n', 18927: 'sents\\n\\n\\nexample', 18928: '(code_classification_based_segmenter', 18929: 'segmenter\\n\\n\\n\\n2', 18930: '.2\\xa0\\xa0\\xa0identifying', 18931: 'types\\nwhen', 18932: 'dialogue,', 18933: 'of\\nutterances', 18934: 'speaker', 18935: 'this\\ninterpretation', 18936: 'straightforward', 18937: 'performative', 18938: 'statements\\nsuch', 18939: 'forgive', 18940: 'bet', 18941: 'climb', 18942: 'hill', 18943: 'but\\ngreetings,', 18944: 'assertions,', 18945: 'clarifications', 18946: 'all\\nbe', 18947: 'speech-based', 18948: 'the\\ndialogue', 18949: 'an\\nimportant', 18950: 'from\\ninstant', 18951: 'sessions', 18952: 'with\\none', 18953: 'emotion,\\nynquestion,', 18954: 'continuer', 18955: 'therefore', 18956: 'new\\ninstant', 18957: 'basic\\nmessaging', 18958: 'xml_posts()', 18959: 'structure\\nrepresenting', 18960: 'post:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 18961: '.xml_posts()[:10000]\\n\\n\\n\\nnext,', 18962: 'words\\nthe', 18963: 'dialogue_act_features(post):\\n', 18964: '.word_tokenize(post):\\n', 18965: '.format(word', 18966: '.lower())]', 18967: 'true\\n', 18968: 'features\\n\\n\\n\\nfinally,', 18969: \".get('class')\", 18970: 'get\\na', 18971: \"post's\", 18972: 'type),', 18973: '[(dialogue_act_features(post', 18974: '.text),', 18975: \".get('class'))\\n\", 18976: 'posts]\\n>>>', 18977: '.67\\n\\n\\n\\n\\n\\n\\n\\n2', 18978: '.3\\xa0\\xa0\\xa0recognizing', 18979: 'entailment\\nrecognizing', 18980: 'determining\\nwhether', 18981: 'the\\nhypothesis', 18982: 'rte', 18983: 'challenges,', 18984: 'where\\nshared', 18985: 'teams', 18986: 'text/hypothesis', 18987: 'the\\nentailment', 18988: 'holds,', 18989: '.\\n\\nchallenge', 18990: '(true)\\n\\nt:', 18991: 'parviz', 18992: 'davudi', 18993: 'iran', 18994: 'meeting', 18995: 'shanghai\\nco-operation', 18996: 'organisation', 18997: '(sco),', 18998: 'fledgling', 18999: 'that\\nbinds', 19000: 'russia,', 19001: 'soviet', 19002: 'republics', 19003: 'central\\nasia', 19004: 'terrorism', 19005: '.\\nh:', 19006: 'sco', 19007: '81', 19008: '(false)\\n\\nt:', 19009: 'nc', 19010: 'organization,', 19011: 'llc\\ncompany', 19012: 'nelson', 19013: 'beavers,', 19014: 'iii,', 19015: 'chester', 19016: 'beavers', 19017: 'jennie\\nbeavers', 19018: 'stewart', 19019: 'jennie', 19020: 'share-holder', 19021: 'carolina', 19022: 'analytical\\nlaboratory', 19023: 'and\\nhypothesis', 19024: 'entailment,', 19025: 'rather\\nwhether', 19026: 'reasonable\\nevidence', 19027: 'true/false', 19028: 'seems\\nlikely', 19029: 'successful', 19030: 'a\\ncombination', 19031: 'world\\nknowledge,', 19032: 'attempts', 19033: 'reasonably', 19034: 'results\\nwith', 19035: 'analysis,', 19036: 'ideal', 19037: 'if\\nthere', 19038: 'hypothesis\\nshould', 19039: 'information\\nfound', 19040: 'absent', 19041: 'there\\nwill', 19042: 'detector', 19043: '(2', 19044: 'words\\n(i', 19045: 'types)', 19046: 'and\\nour', 19047: 'degree', 19048: 'which\\nthere', 19049: '(captured', 19050: 'the\\nmethod', 19051: 'hyp_extra())', 19052: '—\\nnamed', 19053: 'and\\nplaces', 19054: 'motivates', 19055: 'to\\nextract', 19056: 'nes', 19057: '(named\\nentities)', 19058: 'are\\nfiltered', 19059: '.\\n\\n\\n\\n\\n[xx]give', 19060: 'intro', 19061: 'rtefeatureextractor??\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\ndef', 19062: 'rte_features(rtepair):\\n', 19063: '.rtefeatureextractor(rtepair)\\n', 19064: \"features['word_overlap']\", 19065: 'len(extractor', 19066: \".overlap('word'))\\n\", 19067: \"features['word_hyp_extra']\", 19068: \".hyp_extra('word'))\\n\", 19069: \"features['ne_overlap']\", 19070: \".overlap('ne'))\\n\", 19071: \"features['ne_hyp_extra']\", 19072: \".hyp_extra('ne'))\\n\", 19073: 'features\\n\\n\\nexample', 19074: '(code_rte_features', 19075: 'the\\nrtefeatureextractor', 19076: 'bag\\nof', 19077: 'throwing\\naway', 19078: 'calculates', 19079: 'some\\nattributes', 19080: 'earlier:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 19081: 'rtepair', 19082: '.rte', 19083: \".pairs(['rte3_dev\", 19084: \".xml'])[33]\\n>>>\", 19085: '.rtefeatureextractor(rtepair)\\n>>>', 19086: 'print(extractor', 19087: \".text_words)\\n{'russia',\", 19088: \"'organisation',\", 19089: \"'shanghai',\", 19090: \"'asia',\", 19091: \"'at',\\n'operation',\", 19092: \"'sco',\", 19093: '.}\\n>>>', 19094: \".hyp_words)\\n{'member',\", 19095: \"'china'}\\n>>>\", 19096: \".overlap('word'))\\nset()\\n>>>\", 19097: \".overlap('ne'))\\n{'sco',\", 19098: \".hyp_extra('word'))\\n{'member'}\\n\\n\\n\\nthese\", 19099: 'are\\ncontained', 19100: 'this\\nas', 19101: '.rte_classify', 19102: 'reaches', 19103: '58%\\naccuracy', 19104: 'impressive,', 19105: 'effort,', 19106: 'more\\nlinguistic', 19107: '.4\\xa0\\xa0\\xa0scaling', 19108: 'datasets\\npython', 19109: 'text\\nprocessing', 19110: 'perform\\nthe', 19111: 'numerically', 19112: 'intensive', 19113: 'learning\\nmethods', 19114: 'thus,\\nif', 19115: 'pure-python', 19116: 'implementations\\n(such', 19117: '.naivebayesclassifier)', 19118: 'may\\nfind', 19119: 'unreasonable', 19120: 'time\\nand', 19121: 'plan', 19122: 'amounts', 19123: 'data\\nor', 19124: 'recommend', 19125: \"explore\\nnltk's\", 19126: 'interfacing', 19127: 'learning\\npackages', 19128: 'can\\ntransparently', 19129: '(via', 19130: 'calls)', 19131: 'classifier\\nmodels', 19132: 'classifier\\nimplementations', 19133: 'recommended\\nmachine', 19134: '.\\n\\n\\n\\n\\n\\n\\n3\\xa0\\xa0\\xa0evaluation\\nin', 19135: 'accurately\\ncapturing', 19136: 'this\\nevaluation', 19137: 'trustworthy', 19138: 'and\\nfor', 19139: 'tool\\nfor', 19140: 'guiding', 19141: 'improvements', 19142: 'set\\nmost', 19143: 'comparing\\nthe', 19144: 'set\\n(or', 19145: 'set)\\nwith', 19146: 'set\\ntypically', 19147: 'training\\ncorpus:', 19148: 'test\\nset,', 19149: 'learning\\nhow', 19150: 'misleadingly', 19151: 'high\\nscores', 19152: 'available\\nfor', 19153: 'small\\nnumber', 19154: 'well-balanced', 19155: 'a\\nmeaningful', 19156: 'evaluation\\ninstances', 19157: 'labels,\\nor', 19158: 'test\\nset', 19159: 'at\\nleast', 19160: 'additionally,', 19161: 'many\\nclosely', 19162: 'single\\ndocument', 19163: 'lack', 19164: 'skew', 19165: 'evaluation\\nresults', 19166: 'is\\ncommon', 19167: 'err', 19168: 'safety', 19169: 'data\\nfor', 19170: 'consideration', 19171: 'degree\\nof', 19172: 'the\\ndevelopment', 19173: 'are,', 19174: 'less\\nconfident', 19175: 'other\\ndatasets', 19176: 'at\\none', 19177: 'by\\nrandomly', 19178: 'reflects', 19179: 'single\\ngenre', 19180: '(news):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 19181: 'list(brown', 19182: \".tagged_sents(categories='news'))\\n>>>\", 19183: '.shuffle(tagged_sents)\\n>>>', 19184: 'tagged_sents[:size]\\n\\n\\n\\nin', 19185: 'training\\nset', 19186: 'same\\ngenre,', 19187: 'would\\ngeneralize', 19188: \"what's\", 19189: 'to\\nrandom', 19190: '.shuffle(),', 19191: 'are\\ntaken', 19192: 'consistent', 19193: 'word\\nappears', 19194: 'then\\nthat', 19195: 'reflected', 19196: 'the\\ntest', 19197: 'somewhat', 19198: 'documents:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 19199: 'file_ids', 19200: \".fileids(categories='news')\\n>>>\", 19201: 'int(len(file_ids)', 19202: '.tagged_sents(file_ids[size:])\\n>>>', 19203: '.tagged_sents(file_ids[:size])\\n\\n\\n\\nif', 19204: 'stringent', 19205: 'evaluation,', 19206: 'those\\nin', 19207: \".tagged_sents(categories='fiction')\\n\\n\\n\\nif\", 19208: 'set,\\nthen', 19209: 'well\\nbeyond', 19210: '.2\\xa0\\xa0\\xa0accuracy\\nthe', 19211: 'metric', 19212: 'classifier,\\naccuracy,', 19213: 'gender\\nclassifier', 19214: 'predicts', 19215: '60', 19216: '60/80', 19217: '75%', 19218: '.accuracy()', 19219: \"print('accuracy:\", 19220: '{:4', 19221: \".2f}'\", 19222: '.format(nltk', 19223: 'test_set)))', 19224: '.75\\n\\n\\n\\nwhen', 19225: 'interpreting', 19226: 'classifier,', 19227: 'important\\nto', 19228: 'class\\nlabels', 19229: 'that\\ndetermines', 19230: 'word\\nbank', 19231: 'financial-institution', 19232: '19\\ntimes', 19233: '95%', 19234: 'hardly', 19235: 'be\\nimpressive,', 19236: 'that\\nalways', 19237: 'we\\ninstead', 19238: 'balanced', 19239: '40%,', 19240: 'accuracy\\nscore', 19241: 'arises\\nwhen', 19242: 'measuring', 19243: 'inter-annotator', 19244: 'in\\n2', 19245: '.)\\n\\n\\n3', 19246: '.3\\xa0\\xa0\\xa0precision', 19247: 'recall\\nanother', 19248: 'misleading', 19249: 'in\\nsearch', 19250: 'retrieval,', 19251: 'attempting\\nto', 19252: 'irrelevant', 19253: 'outweighs', 19254: 'relevant\\ndocuments,', 19255: 'document\\nas', 19256: 'positives', 19257: 'negatives\\n\\nit', 19258: 'for\\nsearch', 19259: 'four\\ncategories', 19260: '.1:\\n\\ntrue', 19261: 'identified\\nas', 19262: '.\\ntrue', 19263: 'negatives', 19264: '.\\nfalse', 19265: 'items\\nthat', 19266: '.\\n\\ngiven', 19267: 'metrics:\\n\\nprecision,', 19268: 'we\\nidentified', 19269: 'tp/(tp+fp)', 19270: '.\\nrecall,', 19271: 'we\\nidentified,', 19272: 'tp/(tp+fn)', 19273: 'f-measure', 19274: 'f-score),', 19275: 'precision\\nand', 19276: 'harmonic\\nmean', 19277: 'precision', 19278: 'recall:\\n(2', 19279: 'recall)', 19280: '(precision', 19281: '.4\\xa0\\xa0\\xa0confusion', 19282: 'matrices\\n\\nwhen', 19283: 'subdivide', 19284: 'on\\nwhich', 19285: 'table\\nwhere', 19286: '[i,j]', 19287: 'was\\npredicted', 19288: 'diagonal\\nentries', 19289: 'cells', 19290: '|ii|)', 19291: 'were\\ncorrectly', 19292: 'predicted,', 19293: 'off-diagonal', 19294: '4:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 19295: 'tag_list(tagged_sents):\\n', 19296: 'sent]\\n>>>', 19297: 'apply_tagger(tagger,', 19298: 'corpus):\\n', 19299: '[tagger', 19300: '.tag(nltk', 19301: '.untag(sent))', 19302: 'corpus]\\n>>>', 19303: 'tag_list(brown', 19304: \".tagged_sents(categories='editorial'))\\n>>>\", 19305: 'tag_list(apply_tagger(t2,', 19306: \".tagged_sents(categories='editorial')))\\n>>>\", 19307: 'cm', 19308: '.confusionmatrix(gold,', 19309: 'test)\\n>>>', 19310: 'print(cm', 19311: '.pretty_format(sort_by_count=true,', 19312: 'show_percents=true,', 19313: 'truncate=9))\\n', 19314: '|\\n----+----------------------------------------------------------------+\\n', 19315: '<11', 19316: '.8%>', 19317: '.0%', 19318: '.2%', 19319: '.3%', 19320: '<9', 19321: '.0%>', 19322: '<8', 19323: '.6%>', 19324: '.7%', 19325: '<3', 19326: '.9%>', 19327: '<4', 19328: '|\\nnns', 19329: '.2%>', 19330: '.4%>', 19331: '.9%', 19332: '<2', 19333: '<1', 19334: '.8%>|\\n----+----------------------------------------------------------------+\\n(row', 19335: 'reference;', 19336: 'col', 19337: 'test)\\n\\n\\n\\n\\nthe', 19338: 'a\\nsubstitution', 19339: '.6%', 19340: '(for\\n1', 19341: '(', 19342: 'cells\\nwhose', 19343: 'diagonal', 19344: 'classifications', 19345: 'xxx', 19346: 'legend', 19347: '.5\\xa0\\xa0\\xa0cross-validation\\nin', 19348: 'reserve', 19349: 'the\\nannotated', 19350: 'mentioned,\\nif', 19351: 'small,', 19352: 'then\\nour', 19353: 'set\\nlarger', 19354: 'smaller,', 19355: 'a\\nsignificant', 19356: 'annotated\\ndata', 19357: '.\\none', 19358: 'on\\ndifferent', 19359: 'those\\nevaluations,', 19360: 'cross-validation', 19361: 'n\\nsubsets', 19362: 'folds', 19363: 'folds,', 19364: 'all\\nof', 19365: 'fold,', 19366: 'that\\nmodel', 19367: 'fold', 19368: 'might\\nbe', 19369: 'the\\ncombined', 19370: 'is\\ntherefore', 19371: 'cross-validation\\nis', 19372: 'varies\\nacross', 19373: 'all\\nn', 19374: 'the\\nscore', 19375: 'hand,', 19376: 'vary', 19377: 'skeptical\\nabout', 19378: '.\\n\\n\\n\\n\\n\\n4\\xa0\\xa0\\xa0decision', 19379: 'trees\\nin', 19380: 'machine\\nlearning', 19381: 'build\\nclassification', 19382: 'classifiers,', 19383: 'and\\nmaximum', 19384: 'these\\nlearning', 19385: 'black', 19386: 'boxes,', 19387: 'them\\nfor', 19388: 'lot\\nto', 19389: 'methods\\nselect', 19390: 'an\\nunderstanding', 19391: 'of\\nappropriate', 19392: 'those\\nfeatures', 19393: 'generated\\nmodels', 19394: 'features\\nare', 19395: 'informative,', 19396: '.\\n\\n\\na', 19397: 'flowchart', 19398: 'selects\\nlabels', 19399: 'decision\\nnodes,', 19400: 'leaf', 19401: 'nodes,', 19402: 'which\\nassign', 19403: \"flowchart's\", 19404: \"value's\\nfeatures,\", 19405: 'selects', 19406: 'branch', 19407: '.\\nfollowing', 19408: 'arrive', 19409: 'a\\nnew', 19410: \"value's\", 19411: \"node's\", 19412: 'condition,\\nuntil', 19413: 'input\\nvalue', 19414: 'tree\\ndiagrams', 19415: 'conventionally', 19416: 'upside', 19417: 'the\\ntop,', 19418: '.\\n\\n\\n\\nonce', 19419: 'straightforward\\nis', 19420: 'given\\ntraining', 19421: 'for\\nbuilding', 19422: 'task:', 19423: 'picking', 19424: 'the\\nbest', 19425: 'stump', 19426: 'a\\ndecision', 19427: 'decides', 19428: 'inputs\\nbased', 19429: 'possible\\nfeature', 19430: 'to\\ninputs', 19431: 'decision\\nstump,', 19432: 'the\\nsimplest', 19433: 'possible\\nfeature,', 19434: 'achieves', 19435: 'the\\ntraining', 19436: 'alternatives', 19437: 'picked', 19438: 'a\\nlabel', 19439: 'selected\\nexamples', 19440: 'selected\\nfeature', 19441: '.\\n\\n\\ngiven', 19442: 'stumps,', 19443: 'for\\ngrowing', 19444: 'we\\nbegin', 19445: 'the\\nclassification', 19446: 'we\\nthen', 19447: '.\\nleaves', 19448: 'sufficient', 19449: 'then\\nreplaced', 19450: 'training\\ncorpus', 19451: 'grow', 19452: 'the\\nleftmost', 19453: 'stump,', 19454: 'vowel\\nor', 19455: '.1\\xa0\\xa0\\xa0entropy', 19456: 'gain\\nas', 19457: 'mentioned', 19458: 'one\\npopular', 19459: 'alternative,', 19460: 'gain,', 19461: 'how\\nmuch', 19462: 'up\\nusing', 19463: 'disorganized', 19464: 'set\\nof', 19465: 'many\\ninput', 19466: 'is\\ndefined', 19467: 'log\\nprobability', 19468: 'label:\\n\\n', 19469: '(1)h', 19470: '−σl', 19471: '|in|', 19472: 'labelsp(l)', 19473: 'log2p(l)', 19474: 'female\\nnames', 19475: 'if\\np(male)', 19476: 'particular,\\nlabels', 19477: 'contribute', 19478: 'entropy\\n(since', 19479: 'p(l)', 19480: 'small),', 19481: 'do\\nnot', 19482: 'medium\\nfrequency,', 19483: 'entropy\\nof', 19484: '.\\n\\n\\n\\n\\n\\n\\xa0\\n\\nimport', 19485: 'math\\ndef', 19486: 'entropy(labels):\\n', 19487: '.freqdist(labels)\\n', 19488: 'probs', 19489: '[freqdist', 19490: '.freq(l)', 19491: 'freqdist]\\n', 19492: '-sum(p', 19493: 'math', 19494: '.log(p,2)', 19495: 'probs)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 19496: \"print(entropy(['male',\", 19497: \"'male',\", 19498: \"'male']))\", 19499: \"'male']))\\n0\", 19500: '.811', 19501: \"print(entropy(['female',\", 19502: \"'male']))\\n1\", 19503: \"'female']))\\n0\", 19504: \"'female']))\", 19505: '(code_entropy', 19506: 'labels\\n\\n\\nonce', 19507: 'calculated', 19508: \"input\\nvalues'\", 19509: 'labels\\nbecome', 19510: 'the\\nentropy', 19511: \"stump's\", 19512: 'average\\nof', 19513: '(weighted', 19514: 'leaf)', 19515: 'original\\nentropy', 19516: 'information\\ngain,', 19517: 'input\\nvalues', 19518: 'coherent', 19519: 'groups,', 19520: 'by\\nselecting', 19521: 'stumps', 19522: 'simple\\nalgorithm', 19523: 'this\\nprocess', 19524: 'decision\\ntree', 19525: 'previously\\nevaluated', 19526: '.\\n\\n\\ndecision', 19527: \"with,\\nthey're\", 19528: 'understand,', 19529: 'is\\nespecially', 19530: 'usually\\npossible', 19531: '.\\ndecision', 19532: 'suited', 19533: 'many\\nhierarchical', 19534: 'categorical', 19535: 'example,\\ndecision', 19536: 'capturing', 19537: 'phylogeny', 19538: 'disadvantages', 19539: 'is\\nthat,', 19540: 'data,\\nthe', 19541: 'tree\\ncan', 19542: 'may\\n\\noverfit', 19543: 'reflect\\nidiosyncrasies', 19544: 'significant\\npatterns', 19545: 'stop\\ndividing', 19546: 'to\\nprune', 19547: 'a\\ndev-test', 19548: 'be\\nchecked', 19549: 'relatively\\nindependently', 19550: 'documents\\ninto', 19551: 'automotive,', 19552: 'murder', 19553: 'mystery),', 19554: 'features\\nsuch', 19555: 'hasword(football)', 19556: 'specific\\nlabel,', 19557: 'regardless', 19558: 'these\\nfeatures', 19559: 'branches', 19560: 'the\\ntree', 19561: 'exponentially', 19562: 'we\\ngo', 19563: 'repetition', 19564: 'of\\nfeatures', 19565: 'weak', 19566: 'predictors', 19567: 'incremental', 19568: 'improvements,', 19569: 'to\\noccur', 19570: 'descended', 19571: 'they\\nshould', 19572: 'features\\nacross', 19573: 'some\\nconclusions', 19574: 'affect', 19575: 'checked', 19576: 'a\\nspecific', 19577: 'exploit', 19578: 'are\\nrelatively', 19579: 'classification\\nmethod,', 19580: 'overcomes', 19581: 'limitation', 19582: 'by\\nallowing', 19583: 'parallel', 19584: '.\\n\\n\\n\\n5\\xa0\\xa0\\xa0naive', 19585: 'classifiers\\nin', 19586: 'in\\ndetermining', 19587: 'to\\nchoose', 19588: 'begins\\nby', 19589: 'is\\ndetermined', 19590: 'prior\\nprobability,', 19591: 'the\\nlabel', 19592: 'the\\ninput', 19593: '.\\n\\n\\n\\nfigure', 19594: 'bayes\\nclassifier', 19595: 'training\\ncorpus,', 19596: 'out\\nat', 19597: 'automotive', 19598: 'then\\nconsiders', 19599: 'input\\ndocument', 19600: 'dark,', 19601: 'indicator', 19602: 'for\\nmurder', 19603: 'mysteries,', 19604: 'football,', 19605: 'strong', 19606: 'feature\\nhas', 19607: 'contribution,', 19608: 'is\\nclosest', 19609: 'to,', 19610: '.\\n\\nindividual', 19611: 'by\\nvoting', 19612: 'by\\nmultiplying', 19613: 'label\\nwould', 19614: '12%\\nof', 19615: 'mystery', 19616: '2%\\nof', 19617: 'sports\\nlabel', 19618: '.12;', 19619: 'murder\\nmystery', 19620: 'effect\\nwill', 19621: 'more\\nthan', 19622: 'the\\nautomotive', 19623: 'and\\n5', 19624: 'likelihoods', 19625: 'how\\nfrequently', 19626: 'feature\\nthen', 19627: 'contributes', 19628: 'label\\nwill', 19629: 'be\\nthought', 19630: 'randomly\\nselected', 19631: 'given\\nlabel', 19632: 'feature\\nprobabilities', 19633: '.1\\xa0\\xa0\\xa0underlying', 19634: 'probabilistic', 19635: 'model\\nanother', 19636: 'it\\nchooses', 19637: 'that\\nevery', 19638: 'entirely\\nindependent', 19639: 'is\\nunrealistic;', 19640: 'dependent', 19641: \"we'll\\nreturn\", 19642: 'simplifying', 19643: 'assumption,', 19644: 'the\\nnaive', 19645: 'independence', 19646: 'assumption)\\nmakes', 19647: 'much\\neasier', 19648: 'contributions', 19649: 'since\\nwe', 19650: 'interact', 19651: 'one\\nanother', 19652: 'bayesian', 19653: 'generative', 19654: 'process\\nthat', 19655: 'a\\nlabeled', 19656: \"input's\", 19657: '.\\nevery', 19658: 'other\\nfeature,', 19659: '.\\n\\nbased', 19660: 'for\\np(label|features),', 19661: 'label\\nl', 19662: 'maximizes', 19663: 'p(l|features)', 19664: 'begin,', 19665: 'p(label|features)', 19666: 'the\\nprobability', 19667: 'specified\\nset', 19668: 'features:\\n\\n', 19669: '(2)p(label|features)', 19670: 'p(features,', 19671: 'label)/p(features)\\nnext,', 19672: 'p(features)', 19673: 'every\\nchoice', 19674: 'suffices', 19675: 'label),\\nwhich', 19676: 'each\\nlabel,', 19677: 'the\\neasiest', 19678: 'sum\\nover', 19679: 'label):\\n\\n', 19680: '(3)p(features)', 19681: '=\\nσl', 19682: 'in|', 19683: 'label)\\n\\nthe', 19684: '(4)p(features,', 19685: 'label)', 19686: 'p(label)', 19687: 'p(features|label)\\nfurthermore,', 19688: 'another\\n(given', 19689: 'label),', 19690: 'each\\nindividual', 19691: 'feature:\\n\\n', 19692: '(5)p(features,', 19693: 'prodf', 19694: 'featuresp(f|label)`\\nthis', 19695: 'equation', 19696: 'likelihood:', 19697: 'p(f|label)', 19698: 'single\\nfeature', 19699: '.2\\xa0\\xa0\\xa0zero', 19700: 'smoothing\\nthe', 19701: 'p(f|label),', 19702: 'toward', 19703: 'that\\nalso', 19704: '(6)p(f|label)', 19705: 'count(f,', 19706: 'count(label)\\nhowever,', 19707: 'feature\\nnever', 19708: 'this\\ncase,', 19709: 'will\\ncause', 19710: 'fit', 19711: 'feature/label', 19712: 'combination\\noccur', 19713: 'impossible', 19714: 'that\\ncombination', 19715: \"wouldn't\\nwant\", 19716: 'to\\nexist', 19717: 'count(f,label)/count(label)', 19718: 'for\\np(f|label)', 19719: 'this\\nestimate', 19720: 'count(f)', 19721: 'employ\\nmore', 19722: 'smoothing', 19723: 'techniques,\\nfor', 19724: 'basically', 19725: 'adds', 19726: 'each\\ncount(f,label)', 19727: 'heldout', 19728: 'heldout\\ncorpus', 19729: 'and\\nfeature', 19730: '.probability', 19731: 'support\\nfor', 19732: '.\\n\\n\\n\\n\\n5', 19733: '.3\\xa0\\xa0\\xa0non-binary', 19734: 'features\\nwe', 19735: 'binary,', 19736: 'label-valued\\nfeatures', 19737: 'blue,\\nwhite,', 19738: 'orange)', 19739: 'replacing\\nthem', 19740: 'color-is-red', 19741: 'be\\nconverted', 19742: 'binning,', 19743: 'replaces', 19744: 'with\\nfeatures', 19745: '4<x<6', 19746: 'the\\nprobabilities', 19747: 'the\\nheight', 19748: 'bell', 19749: 'estimate\\np(height|label)', 19750: 'variance', 19751: 'heights', 19752: 'the\\ninputs', 19753: 'p(f=v|label)', 19754: 'not\\nbe', 19755: '.4\\xa0\\xa0\\xa0the', 19756: 'naivete', 19757: 'independence\\nthe', 19758: '(given', 19759: 'real-world\\nproblems', 19760: 'degrees', 19761: 'dependence', 19762: 'one\\nanother,', 19763: 'sets\\nthat', 19764: 'independent?\\none', 19765: 'up\\ndouble-counting', 19766: 'correlated', 19767: 'pushing\\nthe', 19768: 'justified', 19769: 'occur,', 19770: 'f1', 19771: 'f2', 19772: 'exact', 19773: 'of\\nf1,', 19774: 'the\\ncontribution', 19775: 'when\\ndeciding', 19776: 'weight', 19777: 'deserves', 19778: 'contain\\ntwo', 19779: 'contain\\nfeatures', 19780: 'the\\nfeatures', 19781: 'ends-with(a)', 19782: 'ends-with(vowel)', 19783: 'on\\none', 19784: 'it\\nmust', 19785: 'the\\nduplicated', 19786: 'by\\nthe', 19787: '.5\\xa0\\xa0\\xa0the', 19788: 'double-counting\\nthe', 19789: 'that\\nduring', 19790: 'separately;\\nbut', 19791: 'those\\nfeature', 19792: 'to\\nconsider', 19793: 'interactions', 19794: 'contributions\\nduring', 19795: 'adjust', 19796: 'the\\ncontributions', 19797: 'precise,', 19798: 'separating', 19799: 'label):\\n\\n\\n', 19800: '(7)p(features,', 19801: 'w[label]', 19802: 'w[f,', 19803: 'label]\\nhere,', 19804: 'and\\nw[f,', 19805: 'label]', 19806: 'feature\\ntowards', 19807: \"label's\", 19808: 'w[label]\\nand', 19809: 'weights', 19810: 'the\\nmodel', 19811: 'these\\nparameters', 19812: 'independently:\\n\\n', 19813: '(8)w[label]', 19814: 'p(label)\\n\\n', 19815: '(9)w[f,', 19816: 'p(f|label)\\nhowever,', 19817: 'that\\nconsiders', 19818: 'when\\nchoosing', 19819: '.\\n\\n\\n\\n6\\xa0\\xa0\\xa0maximum', 19820: 'classifiers\\nthe', 19821: 'very\\nsimilar', 19822: \"model's\", 19823: 'search\\ntechniques', 19824: 'of\\nparameters', 19825: 'as:\\n\\n', 19826: '(10)p(features)', 19827: '=\\nσx', 19828: 'p(label(x)|features(x))\\nwhere', 19829: 'p(label|features),', 19830: '(11)p(label|features)', 19831: 'p(label,', 19832: 'features)', 19833: '/\\nσlabel', 19834: 'features)\\nbecause', 19835: 'effects', 19836: 'of\\nrelated', 19837: 'model\\nparameters', 19838: 'parameters\\nusing', 19839: 'optimization', 19840: \"the\\nmodel's\", 19841: 'refine', 19842: 'those\\nparameters', 19843: 'optimal', 19844: 'these\\niterative', 19845: 'guarantee', 19846: 'refinement', 19847: 'necessarily', 19848: 'optimal\\nvalues', 19849: 'reached', 19850: 'entropy\\nclassifiers', 19851: 'they\\ncan', 19852: 'size\\nof', 19853: '.\\n\\nnote\\nsome', 19854: 'than\\nothers', 19855: 'of\\ngeneralized', 19856: 'scaling', 19857: '(gis)', 19858: 'scaling\\n(iis),', 19859: 'considerably', 19860: 'conjugate\\ngradient', 19861: '(cg)', 19862: 'bfgs', 19863: '.\\n\\n\\n\\n\\n6', 19864: 'model\\nthe', 19865: 'model\\nused', 19866: 'the\\nmaximum', 19867: 'multiplying', 19868: 'are\\napplicable', 19869: 'classifier\\nmodel', 19870: '(feature,', 19871: 'pair,\\nspecifying', 19872: \"label's\\nlikelihood\", 19873: 'the\\nuser', 19874: 'receive\\ntheir', 19875: 'label;', 19876: 'will\\nsometimes', 19877: 'the\\ndifferences', 19878: 'receives', 19879: 'own\\nparameter', 19880: 'joint-feature', 19881: 'joint-features\\nare', 19882: '(simple)', 19883: 'are\\nproperties', 19884: 'unlabeled', 19885: '.\\n\\nnote\\nin', 19886: 'literature', 19887: 'entropy\\nmodels,', 19888: 'refers', 19889: 'to\\njoint-features;', 19890: '.\\n\\ntypically,', 19891: 'joint-features', 19892: 'maximum\\nentropy', 19893: 'mirror', 19894: 'bayes\\nmodel', 19895: 'label,\\ncorresponding', 19896: 'w[label],', 19897: 'of\\n(simple)', 19898: 'w[f,label]', 19899: 'score\\nassigned', 19900: 'product', 19901: 'the\\nparameters', 19902: 'input\\nand', 19903: 'label:\\n\\n\\n', 19904: '(12)p(input,', 19905: 'prodjoint-features(input,label)', 19906: 'w[joint-feature]\\n\\n\\n6', 19907: '.2\\xa0\\xa0\\xa0maximizing', 19908: 'entropy\\nthe', 19909: 'intuition', 19910: 'we\\nshould', 19911: 'individual\\njoint-features,', 19912: 'unwarranted', 19913: 'principle', 19914: '(labeled', 19915: 'a-j)', 19916: 'at\\nfirst,', 19917: 'the\\nten', 19918: 'as:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\na\\nb\\nc\\nd\\ne\\nf\\ng\\nh\\ni\\nj\\n\\n\\n\\n(i)\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n\\n(ii)\\n5%\\n15%\\n0%\\n30%\\n0%\\n8%\\n12%\\n0%\\n6%\\n24%\\n\\n(iii)\\n0%\\n100%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n\\n\\ntable', 19919: '.1\\n\\nalthough', 19920: 'correct,', 19921: '(i),', 19922: 'information,\\nthere', 19923: 'than\\nany', 19924: 'reflect\\nassumptions', 19925: 'fair\\nthan', 19926: 'the\\ndiscussion', 19927: 'how\\ndisorganized', 19928: 'label\\ndominates', 19929: 'low,', 19930: 'evenly\\ndistributed', 19931: 'chose\\ndistribution', 19932: 'in\\ngeneral,', 19933: 'the\\ndistributions', 19934: 'know,', 19935: 'choose\\nthe', 19936: '55%', 19937: 'this\\nnew', 19938: 'as:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\na\\nb\\nc\\nd\\ne\\nf\\ng\\nh\\ni\\nj\\n\\n\\n\\n(iv)\\n55%\\n45%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n\\n(v)\\n55%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n\\n(vi)\\n55%\\n3%\\n1%\\n2%\\n9%\\n5%\\n0%\\n25%\\n0%\\n0%\\n\\n\\ntable', 19939: '.2\\n\\nbut', 19940: 'the\\nfewest', 19941: '(v)', 19942: 'the\\nnearby', 19943: '80%', 19944: 'coming', 19945: 'appropriate\\ndistribution', 19946: 'hand;', 19947: 'following\\ndistribution', 19948: 'appropriate:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\na\\nb\\nc\\nd\\ne\\nf\\ng\\nh\\ni\\nj\\n\\n\\n\\n(vii)\\n+up\\n5', 19949: '.1%\\n0', 19950: '.25%\\n2', 19951: '.9%\\n0', 19952: '.25%\\n0', 19953: '.25%\\n\\n`', 19954: '`\\n-up\\n49', 19955: '.9%\\n4', 19956: '.46%\\n4', 19957: '.46%\\n\\n\\ntable', 19958: '.3\\n\\nin', 19959: 'know:', 19960: 'we\\nadd', 19961: '55%;', 19962: '10%;', 19963: 'boxes', 19964: 'for\\nsenses', 19965: '+up', 19966: '8%', 19967: 'cases)', 19968: '.\\nfurthermore,', 19969: '.\\nthroughout', 19970: 'distributions\\nthat', 19971: 'know;', 19972: 'the\\ndistribution', 19973: 'each\\njoint-feature,', 19974: 'empirical\\nfrequency', 19975: 'which\\nmaximizes', 19976: 'for\\neach', 19977: '.\\n\\n\\n6', 19978: '.3\\xa0\\xa0\\xa0generative', 19979: 'classifiers\\n\\nan', 19980: 'be\\nused', 19981: 'a\\ngenerative', 19982: 'predicts\\np(input,', 19983: '(input,\\nlabel)', 19984: 'input?\\nhow', 19985: 'input?\\nwhat', 19986: 'value?\\nhow', 19987: 'label?\\nwhat', 19988: 'one\\nof', 19989: 'which)?\\n\\n\\nthe', 19990: 'p(label|input)', 19991: 'label\\ngiven', 19992: '3-6', 19993: 'strictly', 19994: 'than\\nconditional', 19995: 'probability\\np(label|input)', 19996: 'p(input,\\nlabel),', 19997: 'price', 19998: 'powerful,', 19999: 'a\\nmore', 20000: \"parameter's\", 20001: 'best\\nparameter', 20002: 'good\\na', 20003: 'the\\nconditional', 20004: 'efforts', 20005: '3-6,', 20006: 'no\\nchoice', 20007: 'is\\nanalogous', 20008: 'topographical', 20009: 'skyline', 20010: 'wider\\nvariety', 20011: 'generate\\nan', 20012: '.\\n\\n\\n\\n\\n7\\xa0\\xa0\\xa0modeling', 20013: 'patterns\\nclassifiers', 20014: 'that\\noccur', 20015: 'explicit\\nmodels', 20016: 'typically,', 20017: 'analytically', 20018: 'purposes:', 20019: 'understand\\nlinguistic', 20020: 'predictions', 20021: 'about\\nnew', 20022: 'insights', 20023: 'into\\nlinguistic', 20024: 'largely', 20025: 'making\\ndecisions', 20026: 'other\\nmodels,', 20027: 'multi-level', 20028: 'neural', 20029: 'opaque', 20030: 'it\\ntypically', 20031: 'unseen\\nlanguage', 20032: 'assess', 20033: 'deemed', 20034: 'sufficiently', 20035: 'accurate,', 20036: 'then\\nbe', 20037: 'language\\ndata', 20038: 'that\\nperform', 20039: 'document\\nclassification,', 20040: '.1\\xa0\\xa0\\xa0what', 20041: \"us?\\nit's\", 20042: 'an\\nautomatically', 20043: 'when\\ndealing', 20044: 'descriptive\\nmodels', 20045: 'explanatory', 20046: 'data\\ncontains', 20047: '.1,\\nthe', 20048: 'not\\ninterchangeable:', 20049: 'adore', 20050: 'definitely\\nadore,', 20051: 'and\\nrelationships', 20052: 'we\\nmight', 20053: 'polar', 20054: 'that\\nhas', 20055: 'like\\nadore', 20056: 'detest', 20057: 'would\\ncontain', 20058: 'constraint', 20059: 'with\\npolar', 20060: 'non-polar\\nverbs', 20061: 'about\\ncorrelations', 20062: 'to\\npostulate', 20063: 'causal', 20064: 'are\\ndescriptive', 20065: 'models;', 20066: 'are\\nrelevant', 20067: 'construction,', 20068: \"can't\\nnecessarily\", 20069: 'then\\nwe', 20070: 'a\\nstarting', 20071: 'experiments', 20072: 'tease', 20073: 'the\\nrelationships', 20074: \"if\\nwe're\", 20075: 'as\\npart', 20076: 'system),', 20077: 'worrying', 20078: 'details\\nof', 20079: '.\\n\\n\\n\\n8\\xa0\\xa0\\xa0summary\\n\\nmodeling', 20080: 'predictions\\nabout', 20081: '.\\nsupervised', 20082: 'of\\nthat', 20083: 'of\\nnlp', 20084: 'part-of-speech\\ntagging,', 20085: 'identification,\\nand', 20086: 'corpus\\ninto', 20087: 'datasets:', 20088: 'model;', 20089: 'helping', 20090: 'select\\nand', 20091: 'tune', 20092: 'for\\nevaluating', 20093: 'you\\nuse', 20094: 'dev-test\\nset', 20095: 'otherwise,', 20096: 'unrealistically\\noptimistic', 20097: 'tree-structured\\nflowcharts', 20098: 'on\\ntheir', 20099: 'interpret,', 20100: 'not\\nvery', 20101: 'contributes\\nto', 20102: 'feature\\nvalues', 20103: 'interact,', 20104: '.\\nmaximum', 20105: 'bayes;', 20106: 'optimization\\nto', 20107: 'corpus\\nare', 20108: 'relevant\\nto', 20109: 'any\\ninformation', 20110: 'and\\npatterns', 20111: 'reading\\nplease', 20112: 'to\\ninstall', 20113: 'weka,', 20114: 'mallet,\\ntadm,', 20115: 'megam', 20116: 'nltk,\\nplease', 20117: 'recommend\\n(alpaydin,', 20118: 'mathematically', 20119: 'intense', 20120: '(hastie,', 20121: 'tibshirani,', 20122: 'friedman,', 20123: '2009)', 20124: 'on\\nusing', 20125: '(abney,', 20126: '2008),\\n(daelemans', 20127: 'bosch,', 20128: '(feldman', 20129: 'sanger,', 20130: '2007),', 20131: '(segaran,', 20132: '(weiss', 20133: 'see\\n(manning', 20134: 'schutze,', 20135: '1999)', 20136: 'modeling,', 20137: 'especially\\nhidden', 20138: '(manning', 20139: '.\\nchapter', 20140: '(manning,', 20141: 'raghavan,', 20142: 'for\\nclassifying', 20143: 'are\\nnumerically', 20144: 'intensive,', 20145: 'slowly', 20146: 'when\\ncoded', 20147: 'naively', 20148: 'increasing', 20149: 'efficiency\\nof', 20150: '(kiusalaas,', 20151: 'applied\\nto', 20152: '(agirre', 20153: 'edmonds,', 20154: 'uses\\nclassifiers', 20155: 'word-sense', 20156: 'disambiguation;', 20157: '(melamed,', 20158: '2001)\\nuses', 20159: 'that\\ncover', 20160: '(croft,', 20161: 'metzler,', 20162: 'strohman,', 20163: '.\\nmuch', 20164: 'learning\\ntechniques', 20165: 'driven', 20166: 'government-sponsored\\nchallenges,', 20167: 'provided\\nwith', 20168: 'the\\nresulting', 20169: 'compared', 20170: 'competitions', 20171: 'conll', 20172: 'ace\\ncompetitions,', 20173: 'competitions,\\nand', 20174: 'aquaint', 20175: 'webpages', 20176: '.\\n\\n\\n\\n10\\xa0\\xa0\\xa0exercises\\n\\n☼', 20177: 'as\\nword', 20178: 'answering,', 20179: 'translation,\\nnamed', 20180: '.\\nwhy', 20181: 'required?\\n\\n☼', 20182: 'this\\nchapter,', 20183: 'name\\ngender', 20184: 'subsets:', 20185: '6900', 20186: '.\\nthen,', 20187: 'make\\nincremental', 20188: 'your\\nprogress', 20189: 'its\\nfinal', 20190: 'set?\\nis', 20191: \"you'd\", 20192: 'expect?\\n\\n☼', 20193: 'senseval', 20194: 'train\\nword-sense', 20195: 'for\\nfour', 20196: 'hard,', 20197: 'these\\nfour', 20198: 'senseval\\n>>>', 20199: \".instances('hard\", 20200: \".pos')\\n>>>\", 20201: 'int(len(instances)', 20202: 'instances[size:],', 20203: 'instances[:size]\\n\\n\\n\\nusing', 20204: 'dataset,', 20205: 'correct\\nsense', 20206: 'at\\nhttp://nltk', 20207: 'objects\\nreturned', 20208: '.\\n\\n☼\\nusing', 20209: 'classifier\\nfinds', 20210: 'particular\\nfeatures', 20211: 'informative?', 20212: 'surprising?\\n\\n☼\\nselect', 20213: 'chapter,\\nsuch', 20214: 'detection,', 20215: 'classification,\\npart-of-speech', 20216: 'extractor,\\nbuild', 20217: 'naive\\nbayes', 20218: 'compare\\nthe', 20219: 'used\\na', 20220: 'extractor?\\n\\n☼\\nthe', 20221: 'pattern\\ndifferently', 20222: '(try', 20223: 'chip', 20224: 'sales)', 20225: 'distinction?\\nbuild', 20226: '.\\n\\n◑\\nthe', 20227: 'posts,\\nwithout', 20228: 'some\\nsequences', 20229: 'ynquestion', 20230: 'be\\nanswered', 20231: 'yanswer', 20232: 'greeting', 20233: 'this\\nfact', 20234: 'code\\nfor', 20235: '.\\n\\n◑\\nword', 20236: 'a\\nstrong', 20237: 'indication', 20238: 'however,\\nmany', 20239: 'infrequently,', 20240: 'our\\ntraining', 20241: 'lexicon,\\nwhich', 20242: 'using\\nwordnet', 20243: 'augment', 20244: 'classifier\\npresented', 20245: 'will\\nmatch', 20246: '.\\n\\n★\\nthe', 20247: 'pp', 20248: 'describing\\nprepositional', 20249: 'ppattachment', 20250: 'ppattach\\n>>>', 20251: 'ppattach', 20252: \".attachments('training')\\n[ppattachment(sent='0',\", 20253: \"verb='join',\", 20254: \"noun1='board',\\n\", 20255: \"prep='as',\", 20256: \"noun2='director',\", 20257: \"attachment='v'),\\n\", 20258: \"ppattachment(sent='1',\", 20259: \"verb='is',\", 20260: \"noun1='chairman',\\n\", 20261: \"prep='of',\", 20262: \"noun2='n\", 20263: \"attachment='n'),\\n\", 20264: 'inst', 20265: \".attachments('training')[1]\\n>>>\", 20266: '(inst', 20267: '.noun1,', 20268: '.prep,', 20269: \".noun2)\\n('chairman',\", 20270: \"'n\", 20271: \".')\\n\\n\\n\\nselect\", 20272: '.attachment', 20273: 'n:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 20274: 'nattach', 20275: '[inst', 20276: \".attachments('training')\\n\", 20277: \"'n']\\n\\n\\n\\nusing\", 20278: 'sub-corpus,', 20279: 'predict\\nwhich', 20280: 'connect', 20281: 'researchers,', 20282: 'corpus\\nhowto', 20283: 'pp\\nattachment', 20284: 'scene,\\nand', 20285: 'uniquely', 20286: 'jar,\\nand', 20287: 'relating\\nvarious', 20288: 'cupboard', 20289: 'shelf', 20290: 'data;', 20291: '.\\n\\n\\n\\n', 20292: '(13)\\n', 20293: 'train\\n\\n', 20294: 'campus\\n\\n', 20295: 'screen\\n\\n', 20296: 'macbeth', 20297: 'letterman\\n\\n\\n\\nabout', 20298: '(ch06', 20299: '1264);', 20300: 'backlink\\nundefined', 20301: 'referenced:', 20302: 'text\\n\\n\\n\\n\\n\\n7', 20303: 'text\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nfor', 20304: 'question,', 20305: 'the\\nanswer', 20306: 'truly', 20307: 'staggering,', 20308: 'increasing\\nevery', 20309: 'complexity', 20310: 'it\\nvery', 20311: 'being\\nable', 20312: 'general-purpose', 20313: 'or\\nentity', 20314: 'located,\\nor', 20315: 'company,', 20316: 'as\\ntables,', 20317: 'unstructured', 20318: 'text?\\nwhich', 20319: 'use\\nthem', 20320: 'recognition', 20321: '.\\n\\n1\\xa0\\xa0\\xa0information', 20322: 'extraction\\ninformation', 20323: 'shapes', 20324: 'is\\nstructured', 20325: 'predictable\\norganization', 20326: 'the\\nrelation', 20327: 'companies', 20328: 'company,\\nwe', 20329: 'does\\nbusiness;', 20330: 'discover\\nwhich', 20331: 'tabular\\nform,', 20332: 'then\\nanswering', 20333: 'queries', 20334: '.\\n\\n\\n\\n\\n\\n\\norgname\\nlocationname\\n\\n\\n\\nomnicom\\nnew', 20335: 'york\\n\\nddb', 20336: 'needham\\nnew', 20337: 'york\\n\\nkaplan', 20338: 'thaler', 20339: 'group\\nnew', 20340: 'york\\n\\nbbdo', 20341: 'south\\natlanta\\n\\ngeorgia-pacific\\natlanta\\n\\n\\ntable', 20342: 'data\\n\\n\\nif', 20343: 'tuples\\n(entity,', 20344: 'entity),', 20345: 'question\\nwhich', 20346: 'atlanta?', 20347: 'be\\ntranslated', 20348: 'locs', 20349: \"[('omnicom',\", 20350: \"'new\", 20351: \"york'),\\n\", 20352: \"('ddb\", 20353: \"needham',\", 20354: \"('kaplan\", 20355: \"group',\", 20356: \"('bbdo\", 20357: \"south',\", 20358: \"'atlanta'),\\n\", 20359: \"('georgia-pacific',\", 20360: \"'atlanta')]\\n>>>\", 20361: '[e1', 20362: '(e1,', 20363: 'rel,', 20364: 'e2)', 20365: \"e2=='atlanta']\\n>>>\", 20366: \"print(query)\\n['bbdo\", 20367: \"'georgia-pacific']\\n\\n\\n\\n\\n\\n\\n\\n\\norgname\\n\\n\\n\\nbbdo\", 20368: 'south\\n\\ngeorgia-pacific\\n\\n\\ntable', 20369: 'atlanta\\n\\n\\nthings', 20370: 'of\\ntext', 20371: '.ieer,', 20372: 'nyt19980315', 20373: '.0085)', 20374: 'wells', 20375: 'packaged\\npaper-products', 20376: 'georgia-pacific', 20377: 'corp', 20378: 'arrived', 20379: 'at\\nwells', 20380: 'hertz', 20381: 'channel,', 20382: 'is\\nalso', 20383: 'omnicom-owned', 20384: 'agency,', 20385: 'bbdo', 20386: 'south', 20387: 'of\\nbbdo', 20388: 'worldwide', 20389: 'atlanta,', 20390: 'corporate\\nadvertising', 20391: 'georgia-pacific,', 20392: 'duties', 20393: 'for\\nbrands', 20394: 'angel', 20395: 'soft', 20396: 'toilet', 20397: 'tissue', 20398: 'sparkle', 20399: 'towels,\\nsaid', 20400: 'ken', 20401: 'haldin,', 20402: 'spokesman', 20403: 'atlanta', 20404: 'glean', 20405: 'required\\nto', 20406: 'enough\\nabout', 20407: '.2?', 20408: 'is\\nobviously', 20409: '(1)\\ncontains', 20410: 'with\\nlocation', 20411: 'general\\nrepresentation', 20412: '(10', 20413: 'approach,\\ndeciding', 20414: 'and\\nlocations', 20415: 'directly,\\nwe', 20416: 'unstructured\\ndata', 20417: 'of\\n1', 20418: 'reap', 20419: 'benefits', 20420: 'query\\ntools', 20421: 'sql', 20422: '.\\ninformation', 20423: 'including\\nbusiness', 20424: 'resume', 20425: 'harvesting,', 20426: 'media', 20427: 'detection,\\npatent', 20428: 'search,', 20429: 'scanning', 20430: 'a\\nparticularly', 20431: 'attempt\\nto', 20432: 'electronically-available', 20433: 'scientific\\nliterature,', 20434: 'biology', 20435: 'medicine', 20436: '.1\\xa0\\xa0\\xa0information', 20437: 'architecture\\n1', 20438: 'information\\nextraction', 20439: 'several\\nof', 20440: 'first,\\nthe', 20441: 'sentence\\nsegmenter,', 20442: 'subdivided', 20443: 'a\\ntokenizer', 20444: 'tags,\\nwhich', 20445: 'prove', 20446: 'entity\\ndetection', 20447: 'potentially\\ninteresting', 20448: 'relation\\ndetection', 20449: 'different\\nentities', 20450: 'and\\ngenerates', 20451: '(entity,', 20452: 'entity)', 20453: 'its\\noutput', 20454: 'the\\ncompany', 20455: 'located', 20456: 'generate\\nthe', 20457: '([org:', 20458: \"'georgia-pacific']\", 20459: \"'in'\", 20460: '[loc:', 20461: \"'atlanta'])\", 20462: 'that\\nsimply', 20463: 'segmenter\\n,', 20464: 'tagger\\n:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 20465: 'ie_preprocess(document):\\n', 20466: '.sent_tokenize(document)', 20467: '.word_tokenize(sent)', 20468: 'sentences]', 20469: '.pos_tag(sent)', 20470: '\\n\\n\\n\\n\\nnote\\nremember', 20471: 'with:', 20472: 'pprint\\n\\nnext,', 20473: 'the\\nentities', 20474: 'participate', 20475: 'definite', 20476: 'the\\nknights', 20477: 'ni,', 20478: 'indefinite', 20479: 'noun\\nchunks,', 20480: 'cats,\\nand', 20481: 'to\\nentities', 20482: 'extraction,', 20483: 'patterns\\nbetween', 20484: 'and\\nuse', 20485: 'recording', 20486: 'relationships\\nbetween', 20487: '.\\n\\n\\n\\n2\\xa0\\xa0\\xa0chunking\\nthe', 20488: 'is\\nchunking,', 20489: 'segments', 20490: 'multi-token', 20491: 'as\\nillustrated', 20492: 'the\\nword-level', 20493: 'large\\nboxes', 20494: 'higher-level', 20495: 'chunk', 20496: 'omits', 20497: 'whitespace,\\nchunking', 20498: '.\\nalso', 20499: 'chunker', 20500: 'overlap\\nin', 20501: 'levels\\n\\nin', 20502: 'chunking,', 20503: 'chunkers', 20504: 'conll-2000', 20505: 'in\\n(5)', 20506: '6\\nto', 20507: '.1\\xa0\\xa0\\xa0noun', 20508: 'chunking\\nwe', 20509: 'chunking,\\nor', 20510: 'np-chunking,', 20511: 'np-chunks', 20512: 'brackets:\\n\\n', 20513: '(2)[', 20514: 'market/nn', 20515: 'for/in', 20516: 'system-management/nn', 20517: 'software/nn', 20518: ']\\nfor/in', 20519: 'digital/nnp', 20520: \"'s/pos\", 20521: 'hardware/nn', 20522: 'is/vbz', 20523: 'fragmented/jj\\nenough/rb', 20524: 'that/in', 20525: 'a/dt', 20526: 'giant/nn', 20527: 'such/jj', 20528: 'as/in', 20529: 'computer/nnp\\nassociates/nnps', 20530: 'should/md', 20531: 'do/vb', 20532: 'well/rb', 20533: 'there/rb', 20534: 'complete\\nnoun', 20535: 'system-management', 20536: 'software\\nfor', 20537: \"digital's\", 20538: 'hardware', 20539: 'two\\nnested', 20540: 'phrases),', 20541: 'captured', 20542: 'the\\nsimpler', 20543: 'motivations', 20544: 'this\\ndifference', 20545: 'contain\\nother', 20546: 'any\\nprepositional', 20547: 'nominal\\nwill', 20548: 'np-chunk,', 20549: 'they\\nalmost', 20550: 'np-chunking', 20551: 'is\\npart-of-speech', 20552: 'for\\nperforming', 20553: 'extraction\\nsystem', 20554: 'an\\nnp-chunker,', 20555: 'grammar,', 20556: 'rules\\nthat', 20557: 'chunked', 20558: 'will\\ndefine', 20559: 'rule\\n', 20560: 'formed\\nwhenever', 20561: '(dt)', 20562: 'any\\nnumber', 20563: '(jj)', 20564: 'grammar,\\nwe', 20565: 'parser', 20566: 'example\\nsentence', 20567: 'either\\nprint', 20568: 'graphically', 20569: '[(the,', 20570: 'dt),', 20571: '(little,', 20572: 'jj),', 20573: '(yellow,', 20574: '(dog,', 20575: 'nn),', 20576: '(barked,', 20577: 'vbd),', 20578: '(at,', 20579: 'in),', 20580: '(the,', 20581: '(cat,', 20582: 'nn)]\\n\\n>>>', 20583: 'np:', 20584: '{<dt>?<jj>*<nn>}', 20585: 'cp', 20586: '.regexpparser(grammar)', 20587: '.parse(sentence)', 20588: 'print(result)', 20589: '\\n(s\\n', 20590: '(np', 20591: 'little/jj', 20592: 'yellow/jj', 20593: 'dog/nn)\\n', 20594: 'barked/vbd\\n', 20595: 'at/in\\n', 20596: 'cat/nn))\\n>>>', 20597: '.draw()', 20598: '(code_chunkex', 20599: '.\\n\\n\\n\\n\\n2', 20600: '.2\\xa0\\xa0\\xa0tag', 20601: 'patterns\\nthe', 20602: 'to\\ndescribe', 20603: 'delimited\\nusing', 20604: '<dt>?<jj>*<nn>', 20605: 'are\\nsimilar', 20606: 'journal:\\n\\nanother/dt', 20607: 'sharp/jj', 20608: 'dive/nn\\ntrade/nn', 20609: 'figures/nns\\nany/dt', 20610: 'new/jj', 20611: 'policy/nn', 20612: 'measures/nns\\nearlier/jjr', 20613: 'stages/nns\\npanamanian/jj', 20614: 'dictator/nn', 20615: 'manuel/nnp', 20616: 'noriega/nnp\\n\\nwe', 20617: 'pattern\\nabove,', 20618: '<dt>?<jj', 20619: '.*>*<nn', 20620: '.*>+', 20621: 'by\\nzero', 20622: 'relative\\nadjectives', 20623: 'earlier/jjr),', 20624: 'any\\ntype', 20625: 'which\\nthis', 20626: 'cover:\\n\\nhis/prp$', 20627: 'mansion/nnp', 20628: 'house/nnp', 20629: 'speech/nn\\nthe/dt', 20630: 'price/nn', 20631: 'cutting/vbg\\n3/cd', 20632: '%/nn', 20633: 'to/to', 20634: '4/cd', 20635: '%/nn\\nmore/jjr', 20636: 'than/in', 20637: '10/cd', 20638: '%/nn\\nthe/dt', 20639: 'fastest/jjs', 20640: 'developing/vbg', 20641: \"trends/nns\\n's/pos\", 20642: 'skill/nn\\n\\n\\nnote\\nyour', 20643: '.\\ntest', 20644: 'interface\\nnltk', 20645: '.chunkparser()', 20646: 'your\\ntag', 20647: '.3\\xa0\\xa0\\xa0chunking', 20648: 'expressions\\nto', 20649: 'regexpparser\\nchunker', 20650: 'flat', 20651: 'are\\nchunked', 20652: 'turn,\\nsuccessively', 20653: 'the\\nchunk', 20654: 'invoked,', 20655: '.\\n2', 20656: 'a\\nsimple', 20657: 'rule\\nmatches', 20658: 'pronoun,\\nzero', 20659: '.\\n\\n\\n\\n\\n\\xa0\\n\\ngrammar', 20660: 'r\\n', 20661: '{<dt|pp\\\\$>?<jj>*<nn>}', 20662: 'determiner/possessive,', 20663: 'noun\\n', 20664: '{<nnp>+}', 20665: 'nouns\\n\\ncp', 20666: '.regexpparser(grammar)\\nsentence', 20667: '[(rapunzel,', 20668: 'nnp),', 20669: '(let,', 20670: '(down,', 20671: 'rp),', 20672: '(her,', 20673: 'pp$),', 20674: '(long,', 20675: '(golden,', 20676: '(hair,', 20677: 'nn)]\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 20678: 'print(cp', 20679: '.parse(sentence))', 20680: 'rapunzel/nnp)\\n', 20681: 'let/vbd\\n', 20682: 'down/rp\\n', 20683: 'her/pp$', 20684: 'long/jj', 20685: 'golden/jj', 20686: 'hair/nn))\\n\\n\\nexample', 20687: '(code_chunker1', 20688: 'chunker\\n\\n\\nnote\\nthe', 20689: 'regular\\nexpressions,', 20690: 'escaped\\nin', 20691: 'pp$', 20692: 'locations,', 20693: 'leftmost\\nmatch', 20694: 'matches\\ntwo', 20695: 'nouns,\\nthen', 20696: 'chunked:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 20697: '[(money,', 20698: '(market,', 20699: '(fund,', 20700: 'nn)]\\n>>>', 20701: '{<nn><nn>}', 20702: 'nouns\\n>>>', 20703: '.regexpparser(grammar)\\n>>>', 20704: '.parse(nouns))\\n(s', 20705: 'money/nn', 20706: 'market/nn)', 20707: 'fund/nn)\\n\\n\\n\\nonce', 20708: 'market,', 20709: 'have\\nremoved', 20710: 'fund', 20711: 'be\\nincluded', 20712: 'with\\na', 20713: 'permissive', 20714: 'rule,', 20715: '{<nn>+}', 20716: 'optional;', 20717: 'present,', 20718: 'chunker\\nprints', 20719: 'tracing', 20720: '.4\\xa0\\xa0\\xa0exploring', 20721: 'corpora\\nin', 20722: 'interrogate\\na', 20723: 'particular\\nsequence', 20724: 'work\\nmore', 20725: 'chunker,', 20726: \".regexpparser('chunk:\", 20727: '{<v', 20728: '<to>', 20729: '<v', 20730: \".*>}')\\n>>>\", 20731: '.brown\\n>>>', 20732: '.parse(sent)\\n', 20733: 'subtree', 20734: '.subtrees():\\n', 20735: '.label()', 20736: \"'chunk':\", 20737: 'print(subtree)\\n', 20738: '.\\n(chunk', 20739: 'combined/vbn', 20740: 'achieve/vb)\\n(chunk', 20741: 'continue/vb', 20742: 'place/vb)\\n(chunk', 20743: 'serve/vb', 20744: 'protect/vb)\\n(chunk', 20745: 'wanted/vbd', 20746: 'wait/vb)\\n(chunk', 20747: 'allowed/vbn', 20748: 'expected/vbn', 20749: 'become/vb)\\n', 20750: 'seems/vbz', 20751: 'overtake/vb)\\n(chunk', 20752: 'want/vb', 20753: 'buy/vb)\\n\\n\\n\\n\\nnote\\nyour', 20754: 'turn:\\nencapsulate', 20755: 'find_chunks()\\nthat', 20756: 'chunk:', 20757: '.*>}', 20758: 'four\\nor', 20759: 'nouns:', 20760: '{<n', 20761: '.*>{4,}}\\n\\n\\n\\n2', 20762: '.5\\xa0\\xa0\\xa0chinking\\nsometimes', 20763: 'chink', 20764: 'barked/vbd', 20765: 'at/in', 20766: 'chink:\\n\\n[', 20767: 'dog/nn', 20768: 'cat/nn', 20769: ']\\n\\nchinking', 20770: 'a\\nchunk', 20771: 'spans', 20772: 'chunk,', 20773: 'the\\nwhole', 20774: 'removed;', 20775: 'the\\nmiddle', 20776: 'chunks\\nwhere', 20777: 'periphery\\nof', 20778: 'remains', 20779: '.\\n\\n\\n\\n\\n\\n\\n\\n\\n`', 20780: '`\\nentire', 20781: 'chunk\\nmiddle', 20782: 'chunk\\nend', 20783: 'chunk\\n\\n\\n\\ninput\\n[a/dt', 20784: 'little/jj\\ndog/nn]\\n[a/dt', 20785: 'little/jj\\ndog/nn]\\n\\noperation\\nchink', 20786: 'dt', 20787: 'nn\\nchink', 20788: 'jj\\nchink', 20789: 'nn\\n\\npattern\\n}dt', 20790: 'nn{\\n}jj{\\n}nn{\\n\\noutput\\na/dt', 20791: 'little/jj\\ndog/nn\\n[a/dt]', 20792: 'little/jj\\n[dog/nn]\\n[a/dt', 20793: 'little/jj]\\ndog/nn\\n\\n\\ntable', 20794: 'chinking', 20795: 'chunk\\n\\n\\nin', 20796: '.4,', 20797: 'chunk,\\nthen', 20798: 'excise', 20799: 'chinks', 20800: 'np:\\n', 20801: '{<', 20802: '.*>+}', 20803: 'everything\\n', 20804: '}<vbd|in>+{', 20805: 'in\\n', 20806: '\\nsentence', 20807: 'jj),\\n', 20808: 'nn)]\\ncp', 20809: '.regexpparser(grammar)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 20810: '.parse(sentence))\\n', 20811: '(s\\n', 20812: 'cat/nn))\\n\\n\\nexample', 20813: '(code_chinker', 20814: 'chinker\\n\\n\\n\\n\\n\\n\\n2', 20815: '.6\\xa0\\xa0\\xa0representing', 20816: 'chunks:', 20817: 'trees\\nas', 20818: 'befits', 20819: 'status', 20820: '.),\\nchunk', 20821: 'most\\nwidespread', 20822: 'iob', 20823: 'this\\nscheme,', 20824: 'tags,\\ni', 20825: '(inside),', 20826: '(outside),', 20827: '(begin)', 20828: 'tagged\\nas', 20829: 'tokens\\nwithin', 20830: 'suffixed', 20831: 'type,\\ne', 20832: 'b-np,', 20833: 'i-np', 20834: 'just\\nlabeled', 20835: 'scheme', 20836: 'structures\\n\\niob', 20837: 'in\\nfiles,', 20838: 'is\\nhow', 20839: 'file:\\n\\nwe', 20840: 'b-np\\nsaw', 20841: 'o\\nthe', 20842: 'b-np\\nyellow', 20843: 'i-np\\ndog', 20844: 'i-np\\n\\nin', 20845: 'with\\nits', 20846: 'us\\nto', 20847: 'earlier,', 20848: 'using\\ntrees', 20849: 'constituent', 20850: 'manipulated', 20851: 'structures\\n\\n\\nnote\\nnltk', 20852: 'chunks,', 20853: 'but\\nprovides', 20854: '.\\n\\n\\n\\n\\n3\\xa0\\xa0\\xa0developing', 20855: 'chunkers\\nnow', 20856: \"haven't\\nexplained\", 20857: 'suitably', 20858: 'mechanics', 20859: 'an\\nnltk', 20860: 'a\\nchunked', 20861: 'corpus,\\nthen', 20862: 'data-driven', 20863: 'expanding', 20864: '.1\\xa0\\xa0\\xa0reading', 20865: 'corpus\\nusing', 20866: 'journal\\ntext', 20867: 'np,', 20868: 'vp', 20869: 'shown\\nbelow:\\n\\nhe', 20870: 'b-np\\naccepted', 20871: 'b-vp\\nthe', 20872: 'b-np\\nposition', 20873: 'i-np\\n', 20874: '.conllstr2tree()', 20875: 'tree\\nrepresentation', 20876: 'moreover,', 20877: 'it\\npermits', 20878: 'use,\\nhere', 20879: 'chunks:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 20880: 'b-np\\n', 20881: 'b-vp\\n', 20882: 'b-pp\\n', 20883: 'chairman', 20884: 'carlyle', 20885: 'o\\n', 20886: 'merchant', 20887: 'banking', 20888: 'concern', 20889: '.chunk', 20890: '.conllstr2tree(text,', 20891: \"chunk_types=['np'])\", 20892: '.draw()\\n\\n\\n\\n\\nwe', 20893: 'chunked\\ntext', 20894: '270k', 20895: 'portions,', 20896: 'with\\npart-of-speech', 20897: 'access\\nthe', 20898: '100th', 20899: 'conll2000\\n>>>', 20900: 'print(conll2000', 20901: \".chunked_sents('train\", 20902: \".txt')[99])\\n(s\\n\", 20903: '(pp', 20904: 'over/in)\\n', 20905: 'cup/nn)\\n', 20906: 'of/in)\\n', 20907: 'coffee/nn)\\n', 20908: ',/,\\n', 20909: './nnp', 20910: 'stone/nnp)\\n', 20911: '(vp', 20912: 'told/vbd)\\n', 20913: 'his/prp$', 20914: 'story/nn)\\n', 20915: '.)\\n\\n\\n\\nas', 20916: 'types:\\nnp', 20917: 'seen;', 20918: 'as\\nhas', 20919: 'delivered;', 20920: 'the\\nchunk_types', 20921: \"chunk_types=['np'])[99])\\n(s\\n\", 20922: 'over/in\\n', 20923: 'told/vbd\\n', 20924: '.)\\n\\n\\n\\n\\n\\n3', 20925: '.2\\xa0\\xa0\\xa0simple', 20926: 'baselines\\nnow', 20927: 'baseline', 20928: 'parser\\ncp', 20929: '.regexpparser()\\n>>>', 20930: 'conll2000', 20931: \".chunked_sents('test\", 20932: \"chunk_types=['np'])\\n>>>\", 20933: '.evaluate(test_sents))\\nchunkparse', 20934: 'score:\\n', 20935: 'accuracy:', 20936: '43', 20937: '.4%\\n', 20938: 'precision:', 20939: '.0%\\n', 20940: 'recall:', 20941: 'f-measure:', 20942: '.0%\\n\\n\\n\\nthe', 20943: 'our\\ntagger', 20944: 'precision,', 20945: 'f-measure\\nare', 20946: 'that\\nlooks', 20947: 'tags\\n(e', 20948: 'cd,', 20949: 'dt,', 20950: 'jj)', 20951: 'rnp:', 20952: '{<[cdjnp]', 20953: '.*>+}\\n>>>', 20954: '.7%\\n', 20955: '70', 20956: '.6%\\n', 20957: '67', 20958: '.8%\\n', 20959: '69', 20960: '.2%\\n\\n\\n\\nas', 20961: 'decent', 20962: 'adopting', 20963: 'we\\nuse', 20964: 'b)\\nthat', 20965: 'given\\neach', 20966: 'unigramchunker', 20967: 'class,', 20968: 'which\\nuses', 20969: 'chunkparseri\\ninterface,', 20970: 'constructor\\n', 20971: 'new\\nunigramchunker;', 20972: '\\nwhich', 20973: 'unigramchunker(nltk', 20974: '.chunkparseri):\\n', 20975: 'train_sents):', 20976: 'train_data', 20977: '[[(t,c)', 20978: 'w,t,c', 20979: '.tree2conlltags(sent)]\\n', 20980: 'train_sents]\\n', 20981: '.tagger', 20982: '.unigramtagger(train_data)', 20983: 'parse(self,', 20984: 'sentence):', 20985: 'pos_tags', 20986: '[pos', 20987: '(word,pos)', 20988: 'sentence]\\n', 20989: 'tagged_pos_tags', 20990: '.tag(pos_tags)\\n', 20991: 'chunktags', 20992: '[chunktag', 20993: '(pos,', 20994: 'chunktag)', 20995: 'tagged_pos_tags]\\n', 20996: 'conlltags', 20997: '[(word,', 20998: 'pos,', 20999: '((word,pos),chunktag)\\n', 21000: 'chunktags)]\\n', 21001: '.conlltags2tree(conlltags)\\n\\n\\nexample', 21002: '(code_unigram_chunker', 21003: 'tagger\\n\\nthe', 21004: 'constructor', 21005: 'of\\ntraining', 21006: 'it\\nfirst', 21007: 'the\\ntagger,', 21008: 'tree2conlltags', 21009: 'of\\nword,tag,chunk', 21010: 'triples', 21011: 'data\\nto', 21012: 'later\\nuse', 21013: 'sentence\\nas', 21014: 'from\\nthat', 21015: 'chunk\\ntags,', 21016: 'the\\nconstructor', 21017: 'uses\\nconlltags2tree', 21018: 'unigramchunker,', 21019: 'conll\\n2000', 21020: 'performance:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 21021: 'unigram_chunker', 21022: 'unigramchunker(train_sents)\\n>>>', 21023: 'print(unigram_chunker', 21024: '92', 21025: '.9%\\n', 21026: '.2%\\n\\n\\n\\nthis', 21027: 'achieving', 21028: 'f-measure\\nscore', 21029: '83%', 21030: 'learned,', 21031: 'its\\nunigram', 21032: 'that\\nappear', 21033: 'postags', 21034: 'sorted(set(pos', 21035: 'train_sents\\n', 21036: '.leaves()))\\n>>>', 21037: \".tag(postags))\\n[('#',\", 21038: \"'b-np'),\", 21039: \"('$',\", 21040: \"'o'),\", 21041: \"(')',\", 21042: \"'o'),\\n\", 21043: \"('cc',\", 21044: \"('cd',\", 21045: \"'i-np'),\\n\", 21046: \"('ex',\", 21047: \"('fw',\", 21048: \"'i-np'),\", 21049: \"('jj',\", 21050: \"('jjr',\", 21051: \"('jjs',\", 21052: \"('md',\", 21053: \"('nn',\", 21054: \"('nnp',\", 21055: \"('nnps',\", 21056: \"('nns',\", 21057: \"('pdt',\", 21058: \"('pos',\", 21059: \"('prp',\", 21060: \"('prp$',\", 21061: \"'b-np'),\\n\", 21062: \"('rb',\", 21063: \"('rbr',\", 21064: \"('rbs',\", 21065: \"('rp',\", 21066: \"('sym',\", 21067: \"('uh',\", 21068: \"('vb',\", 21069: \"('vbd',\", 21070: \"('vbg',\", 21071: \"('vbn',\", 21072: \"('vbp',\", 21073: \"('vbz',\", 21074: \"('wdt',\", 21075: \"('wp',\", 21076: \"('wp$',\", 21077: \"('wrb',\", 21078: \"'o')]\\n\\n\\n\\nit\", 21079: 'discovered', 21080: 'np\\nchunks,', 21081: '$,', 21082: 'markers', 21083: 'and\\npossessives', 21084: '(prp$', 21085: 'wp$)', 21086: 'beginnings', 21087: 'chunks,\\nwhile', 21088: '(nn,', 21089: 'nnp,', 21090: 'nnps,', 21091: 'nns)', 21092: 'occur\\ninside', 21093: 'bigram\\nchunker:', 21094: 'bigramchunker,', 21095: 'and\\nmodify', 21096: '.1\\nto', 21097: 'bigramtagger', 21098: 'chunker:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 21099: 'bigram_chunker', 21100: 'bigramchunker(train_sents)\\n>>>', 21101: 'print(bigram_chunker', 21102: '.3%\\n', 21103: '.5%\\n\\n\\n\\n\\n\\n3', 21104: '.3\\xa0\\xa0\\xa0training', 21105: 'classifier-based', 21106: 'chunkers\\nboth', 21107: 'chunkers\\ndecide', 21108: 'tags\\nare', 21109: 'insufficient', 21110: 'statements:\\n\\n', 21111: '.joey/nn', 21112: 'sold/vbd', 21113: 'farmer/nn', 21114: 'rice/nn', 21115: '.nick/nn', 21116: 'broke/vbd', 21117: 'my/dt', 21118: 'computer/nn', 21119: 'monitor/nn', 21120: '.\\n\\nthese', 21121: 'tags,\\nyet', 21122: 'sentence,\\nthe', 21123: 'farmer', 21124: 'rice', 21125: 'monitor,', 21126: 'make\\nuse', 21127: 'just\\ntheir', 21128: 'chunking\\nperformance', 21129: 'words\\nis', 21130: 'the\\nn-gram', 21131: 'this\\nclassifier-based', 21132: 'the\\nclassifier-based', 21133: 'we\\nused', 21134: 'first\\nclass', 21135: 'the\\nconsecutivepostagger', 21136: 'maxentclassifier', 21137: 'naivebayesclassifier', 21138: 'class\\n', 21139: 'wrapper', 21140: 'that\\nturns', 21141: 'sequences;', 21142: 'the\\nparse()', 21143: 'consecutivenpchunktagger(nltk', 21144: 'npchunk_features(untagged_sent,', 21145: 'history)', 21146: '.maxentclassifier', 21147: '.train(', 21148: \"algorithm='megam',\", 21149: 'trace=0)\\n\\n', 21150: 'npchunk_features(sentence,', 21151: 'history)\\n\\nclass', 21152: 'consecutivenpchunker(nltk', 21153: '.chunkparseri):', 21154: '[[((w,t),c)', 21155: '(w,t,c)', 21156: 'consecutivenpchunktagger(tagged_sents)\\n\\n', 21157: '.tag(sentence)\\n', 21158: '[(w,t,c)', 21159: '((w,t),c)', 21160: 'tagged_sents]\\n', 21161: '(code_classifier_chunker', 21162: 'classifier\\n\\n\\nthe', 21163: 'by\\ndefining', 21164: 'the\\npart-of-speech', 21165: 'our\\nclassifier-based', 21166: 'history):\\n', 21167: 'sentence[i]\\n', 21168: '{pos:', 21169: 'pos}\\n>>>', 21170: 'consecutivenpchunker(train_sents)\\n>>>', 21171: 'print(chunker', 21172: '.2%\\n\\n\\n\\nwe', 21173: 'this\\nfeature', 21174: 'adjacent\\ntags,', 21175: 'bigram\\nchunker', 21176: 'prevword,', 21177: 'prevpos', 21178: '<start>,', 21179: 'prevpos:', 21180: 'prevpos}\\n>>>', 21181: '.2%\\n', 21182: '.5%\\n\\n\\n\\nnext,', 21183: 'we\\nhypothesized', 21184: 'find\\nthat', 21185: 'indeed', 21186: \"chunker's\", 21187: 'performance,\\nby', 21188: '10%\\nreduction', 21189: 'rate)', 21190: '.5%\\n', 21191: '.7%\\n\\n\\n\\nfinally,', 21192: 'of\\nadditional', 21193: 'lookahead', 21194: ',\\npaired', 21195: 'tags-since-dt,', 21196: 'been\\nencountered', 21197: 'beginning\\nof', 21198: 'len(sentence)-1:\\n', 21199: 'nextword,', 21200: 'nextpos', 21201: '<end>,', 21202: '<end>\\n', 21203: 'sentence[i+1]\\n', 21204: 'pos,\\n', 21205: 'word,\\n', 21206: 'prevpos,\\n', 21207: 'nextpos:', 21208: 'nextpos,', 21209: 'prevpos+pos:', 21210: '%s+%s', 21211: '(prevpos,', 21212: 'pos),', 21213: 'pos+nextpos:', 21214: 'nextpos),\\n', 21215: 'tags-since-dt:', 21216: 'tags_since_dt(sentence,', 21217: 'i)}', 21218: '\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 21219: 'sentence[:i]:\\n', 21220: \"'dt':\\n\", 21221: '.add(pos)\\n', 21222: \"'+'\", 21223: '.join(sorted(tags))\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 21224: '96', 21225: '88', 21226: '91', 21227: '.8%\\n\\n\\n\\n\\nnote\\nyour', 21228: 'function\\nnpchunk_features,', 21229: '.\\n\\n\\n\\n\\n4\\xa0\\xa0\\xa0recursion', 21230: 'structure\\n\\n4', 21231: '.1\\xa0\\xa0\\xa0building', 21232: 'cascaded', 21233: 'chunkers\\nso', 21234: 'consist\\nof', 21235: 'as\\nnp', 21236: 'of\\narbitrary', 21237: 'multi-stage', 21238: 'grammar\\ncontaining', 21239: 'has\\npatterns', 21240: 'and\\nsentences', 21241: 'four-stage', 21242: 'create\\nstructures', 21243: '.\\n\\n\\n\\n\\n\\n\\xa0\\n\\ngrammar', 21244: '{<dt|jj|nn', 21245: 'nn\\n', 21246: 'pp:', 21247: '{<in><np>}', 21248: 'prepositions', 21249: 'np\\n', 21250: 'vp:', 21251: '{<vb', 21252: '.*><np|pp|clause>+$}', 21253: 'arguments\\n', 21254: 'clause:', 21255: '{<np><vp>}', 21256: 'vp\\n', 21257: '\\ncp', 21258: '[(mary,', 21259: '(saw,', 21260: 'nn),\\n', 21261: '(sit,', 21262: 'vb),', 21263: '(on,', 21264: '(mat,', 21265: '.parse(sentence))\\n(s\\n', 21266: 'mary/nn)\\n', 21267: 'saw/vbd\\n', 21268: '(clause\\n', 21269: 'cat/nn)\\n', 21270: 'sit/vb', 21271: 'mat/nn)))))\\n\\n\\nexample', 21272: '(code_cascaded_chunker', 21273: 'pp,', 21274: 's\\n\\nunfortunately', 21275: 'misses', 21276: 'headed', 21277: 'has\\nother', 21278: 'this\\nchunker', 21279: 'to\\nidentify', 21280: '[(john,', 21281: '(thinks,', 21282: 'vbz),', 21283: '(mary,', 21284: 'vb),\\n', 21285: 'john/nnp)\\n', 21286: 'thinks/vbz\\n', 21287: 'saw/vbd', 21288: '[_saw-vbd]\\n', 21289: 'mat/nn)))))\\n\\n\\n\\nthe', 21290: 'its\\npatterns:', 21291: 'run:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 21292: '.regexpparser(grammar,', 21293: 'loop=2)\\n>>>', 21294: '(vp\\n', 21295: 'mat/nn)))))))\\n\\n\\n\\n\\nnote\\nthis', 21296: 'cascading', 21297: 'deep', 21298: 'however,\\ncreating', 21299: 'cascade', 21300: 'comes\\na', 21301: '.\\nalso,', 21302: 'depth\\n(no', 21303: 'cascade),', 21304: '.2\\xa0\\xa0\\xa0trees\\na', 21305: 'connected', 21306: 'reachable\\nby', 21307: 'distinguished', 21308: 'standardly', 21309: 'upside-down):\\n\\n', 21310: '(4)\\nwe', 21311: \"'family'\", 21312: 'metaphor', 21313: 'talk', 21314: 'tree:', 21315: 'the\\nparent', 21316: 'vp;', 21317: 'conversely', 21318: 'child\\nof', 21319: 'also,', 21320: 'both\\nchildren', 21321: 'siblings', 21322: 'specifying\\ntrees:\\n\\n\\n\\n\\n\\xa0\\n\\n(s\\n', 21323: 'alice)\\n', 21324: '(v', 21325: 'chased)\\n', 21326: '(np\\n', 21327: '(det', 21328: 'the)\\n', 21329: '(n', 21330: 'rabbit))))\\n\\n\\n\\nalthough', 21331: 'encode\\nany', 21332: 'homogeneous', 21333: 'hierarchical', 21334: 'discourse', 21335: 'structure)', 21336: 'children:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 21337: 'tree1', 21338: \".tree('np',\", 21339: \"['alice'])\\n>>>\", 21340: 'print(tree1)\\n(np', 21341: 'alice)\\n>>>', 21342: 'tree2', 21343: \"'rabbit'])\\n>>>\", 21344: 'print(tree2)\\n(np', 21345: 'rabbit)\\n\\n\\n\\nwe', 21346: 'tree3', 21347: \".tree('vp',\", 21348: \"['chased',\", 21349: 'tree2])\\n>>>', 21350: 'tree4', 21351: \".tree('s',\", 21352: '[tree1,', 21353: 'tree3])\\n>>>', 21354: 'print(tree4)\\n(s', 21355: 'alice)', 21356: 'chased', 21357: 'rabbit)))\\n\\n\\n\\nhere', 21358: 'objects:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 21359: 'print(tree4[1])\\n(vp', 21360: 'rabbit))\\n>>>', 21361: 'tree4[1]', 21362: \".label()\\n'vp'\\n>>>\", 21363: \".leaves()\\n['alice',\", 21364: \"'chased',\", 21365: \"'rabbit']\\n>>>\", 21366: \"tree4[1][1][1]\\n'rabbit'\\n\\n\\n\\nthe\", 21367: 'representation\\nof', 21368: 'zoom', 21369: 'out,\\nto', 21370: 'subtrees,', 21371: 'graphical\\nrepresentation', 21372: 'postscript', 21373: 'inclusion', 21374: 'document)', 21375: '\\n\\n\\n\\n\\n\\n\\n4', 21376: '.3\\xa0\\xa0\\xa0tree', 21377: 'traversal\\nit', 21378: 'traverse', 21379: 'traverse(t):\\n', 21380: 'try:\\n', 21381: '.label()\\n', 21382: 'attributeerror:\\n', 21383: 'print(t,', 21384: '.node', 21385: 'defined\\n', 21386: \"print('(',\", 21387: '.label(),', 21388: 't:\\n', 21389: 'traverse(child)\\n', 21390: \"print(')',\", 21391: ')\\n\\n', 21392: \".tree('(s\", 21393: \"rabbit)))')\\n\", 21394: 'traverse(t)\\n', 21395: 'rabbit', 21396: ')\\n\\n\\nexample', 21397: '(code_traverse', 21398: 'tree\\n\\n\\nnote\\nwe', 21399: 'duck', 21400: 't\\nis', 21401: 'defined)', 21402: '.\\n\\n\\n\\n\\n5\\xa0\\xa0\\xa0named', 21403: 'recognition\\nat', 21404: 'entities\\n(nes)', 21405: 'that\\nrefer', 21406: 'individuals,', 21407: 'organizations,', 21408: 'persons,\\ndates,', 21409: 'used\\ntypes', 21410: 'self-explanatory,', 21411: 'facility:\\nhuman-made', 21412: 'artifacts', 21413: 'domains', 21414: 'civil\\nengineering;', 21415: 'gpe:', 21416: 'geo-political', 21417: 'city,', 21418: 'state/province,', 21419: '.\\n\\n\\n\\n\\n\\n\\nne', 21420: 'type\\nexamples\\n\\n\\n\\norganization\\ngeorgia-pacific', 21421: 'who\\n\\nperson\\neddy', 21422: 'bonte,', 21423: 'obama\\n\\nlocation\\nmurray', 21424: 'river,', 21425: 'mount', 21426: 'everest\\n\\ndate\\njune,', 21427: '2008-06-29\\n\\ntime\\ntwo', 21428: '1:30', 21429: '.m', 21430: '.\\n\\nmoney\\n175', 21431: 'dollars,', 21432: 'gbp', 21433: '.40\\n\\npercent\\ntwenty', 21434: 'pct,', 21435: '.75', 21436: '%\\n\\nfacility\\nwashington', 21437: 'monument,', 21438: 'stonehenge\\n\\ngpe\\nsouth', 21439: 'east', 21440: 'asia,', 21441: 'midlothian\\n\\n\\ntable', 21442: 'entity\\n\\n\\nthe', 21443: '(ner)', 21444: 'all\\ntextual', 21445: 'into\\ntwo', 21446: 'sub-tasks:', 21447: 'ne,', 21448: 'its\\ntype', 21449: '.\\nwhile', 21450: 'prelude', 21451: 'identifying\\nrelations', 21452: '(qa),', 21453: 'the\\nprecision', 21454: 'recovering', 21455: 'pages,', 21456: 'but\\njust', 21457: 'most\\nqa', 21458: 'information\\nretrieval,', 21459: 'isolate', 21460: 'minimal', 21461: 'the\\ndocument', 21462: 'was\\nthe', 21463: 'us?,', 21464: 'was\\nretrieved', 21465: 'passage:\\n\\n', 21466: '(5)the', 21467: 'washington', 21468: 'monument', 21469: 'prominent', 21470: 'in\\nwashington,', 21471: \"city's\", 21472: 'attractions', 21473: 'was\\nbuilt', 21474: 'honor', 21475: 'george', 21476: 'washington,', 21477: 'to\\nindependence', 21478: 'became', 21479: '.\\nanalysis', 21480: 'be\\nof', 21481: 'x\\nis', 21482: 'type\\nperson', 21483: 'the\\npassage', 21484: 'washington,\\nnamed', 21485: 'them\\nhas', 21486: 'entities?', 21487: 'to\\nlook', 21488: 'gazetteer,\\nor', 21489: 'alexandria', 21490: 'gazetteer', 21491: 'the\\ngetty', 21492: 'this\\nblindly', 21493: 'runs', 21494: 'story:', 21495: 'every\\nword', 21496: 'error-prone;', 21497: 'but\\nthese', 21498: '.\\n\\nobserve', 21499: 'countries,\\nand', 21500: 'sanchez', 21501: 'dominican', 21502: 'republic\\nand', 21503: 'vietnam', 21504: 'gazetteer,', 21505: \"won't\\nbe\", 21506: 'organizations\\ncome', 21507: 'existence', 21508: 'day,', 21509: 'deal\\nwith', 21510: 'contemporary', 21511: 'entries,', 21512: 'many\\nnamed', 21513: 'thus\\nmay', 21514: 'date\\nand', 21515: 'respectively,', 21516: 'person;\\nconversely', 21517: 'dior', 21518: 'more\\nlikely', 21519: 'yankee', 21520: 'be\\nordinary', 21521: 'modifier', 21522: 'of\\ntype', 21523: 'infielders', 21524: 'posed', 21525: 'multi-word', 21526: 'like\\nstanford', 21527: 'university,', 21528: 'names\\nsuch', 21529: 'cecil', 21530: 'escondido', 21531: 'village', 21532: 'conference\\nservice', 21533: 'recognition,', 21534: 'multi-token\\nsequences', 21535: '.\\nnamed', 21536: 'well-suited', 21537: 'of\\nclassifier-based', 21538: 'sentence\\nusing', 21539: '(conll2002)', 21540: 'data:\\n\\neddy', 21541: 'b-per\\nbonte', 21542: 'i-per\\nis', 21543: 'o\\nwoordvoerder', 21544: 'o\\nvan', 21545: 'prep', 21546: 'o\\ndiezelfde', 21547: 'o\\nhogeschool', 21548: 'b-org\\n', 21549: 'punc', 21550: 'o\\n\\nin', 21551: 'representation,', 21552: 'its\\npart-of-speech', 21553: 'new\\nsentences;', 21554: '.conlltags2tree()', 21555: 'entities,\\naccessed', 21556: '.ne_chunk()', 21557: 'the\\nparameter', 21558: 'binary=true', 21559: 'just\\ntagged', 21560: 'ne;', 21561: 'gpe', 21562: '.tagged_sents()[22]\\n>>>', 21563: '.ne_chunk(sent,', 21564: 'binary=true))', 21565: 'the/dt\\n', 21566: '(ne', 21567: './nnp)\\n', 21568: 'is/vbz\\n', 21569: 'one/cd\\n', 21570: 'according/vbg\\n', 21571: 'to/to\\n', 21572: 'brooke/nnp', 21573: 'mossman/nnp)\\n', 21574: '.)\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 21575: '.ne_chunk(sent))', 21576: '(gpe', 21577: '(person', 21578: '.)\\n\\n\\n\\n\\n\\n\\n\\n6\\xa0\\xa0\\xa0relation', 21579: 'extraction\\n\\nonce', 21580: 'extract\\nthe', 21581: 'indicated', 21582: 'will\\ntypically', 21583: 'of\\nnamed', 21584: 'approaching', 21585: 'all\\ntriples', 21586: '(x,', 21587: 'α,', 21588: 'y),', 21589: 'entities\\nof', 21590: 'α', 21591: 'that\\nintervenes', 21592: 'to\\npull', 21593: 'relation\\nthat', 21594: 'strings\\nthat', 21595: 'expression\\n(?!\\\\b', 21596: '.+ing\\\\b)', 21597: 'to\\ndisregard', 21598: 'supervising', 21599: 'transition\\nof,', 21600: 'gerund', 21601: \".compile(r'\", 21602: '.*\\\\bin\\\\b(?!\\\\b', 21603: \".+ing)')\\n>>>\", 21604: '.ieer', 21605: \".parsed_docs('nyt_19980315'):\\n\", 21606: 'rel', 21607: '.sem', 21608: \".extract_rels('org',\", 21609: \"'loc',\", 21610: 'doc,\\n', 21611: \"corpus='ieer',\", 21612: 'in):\\n', 21613: '.rtuple(rel))\\n[org:', 21614: \"'whyy']\", 21615: \"'philadelphia']\\n[org:\", 21616: \"'mcglashan\", 21617: '&amp;', 21618: \"sarrail']\", 21619: \"'firm\", 21620: \"in'\", 21621: \"'san\", 21622: \"mateo']\\n[org:\", 21623: \"'freedom\", 21624: \"forum']\", 21625: \"'arlington']\\n[org:\", 21626: \"'brookings\", 21627: \"institution']\", 21628: \"'washington']\\n[org:\", 21629: \"'idealab']\", 21630: 'self-described', 21631: 'incubator', 21632: \"'los\", 21633: \"angeles']\\n[org:\", 21634: \"'open\", 21635: \"text']\", 21636: \"'waterloo']\\n[org:\", 21637: \"'wgbh']\", 21638: \"'boston']\\n[org:\", 21639: \"'bastille\", 21640: \"opera']\", 21641: \"'paris']\\n[org:\", 21642: \"'omnicom']\", 21643: \"york']\\n[org:\", 21644: \"'ddb\", 21645: \"needham']\", 21646: \"'kaplan\", 21647: \"group']\", 21648: \"'bbdo\", 21649: \"south']\", 21650: \"'atlanta']\\n[org:\", 21651: \"'atlanta']\\n\\n\\n\\nsearching\", 21652: 'well,\\nthough', 21653: '[org:', 21654: 'house\\ntransportation', 21655: 'committee]', 21656: 'secured', 21657: 'new\\nyork];', 21658: 'string-based', 21659: 'of\\nexcluding', 21660: 'filler', 21661: 'conll2002', 21662: 'entity\\nannotation', 21663: 'devise\\npatterns', 21664: 'next\\nexample', 21665: 'clause()', 21666: 'a\\nclausal', 21667: 'relsym', 21668: 'conll2002\\n>>>', 21669: 'vnv', 21670: '(\\n', 21671: 'is/v|', 21672: 'sing', 21673: 'was/v|', 21674: 'zijn', 21675: \"('be')\\n\", 21676: 'werd/v|', 21677: 'wordt/v', 21678: 'worden', 21679: \"('become)\\n\", 21680: 'anything\\n', 21681: 'van/prep', 21682: 'van', 21683: \"('of')\\n\", 21684: '.compile(vnv,', 21685: '.verbose)\\n>>>', 21686: \".chunked_sents('ned\", 21687: \".train'):\\n\", 21688: \".extract_rels('per',\", 21689: \"'org',\", 21690: \"corpus='conll2002',\", 21691: 'pattern=van):\\n', 21692: '.clause(rel,', 21693: 'relsym=van))', 21694: \"\\nvan(cornet_d'elzius,\", 21695: \"'buitenlandse_handel')\\nvan('johan_rottiers',\", 21696: \"'kardinaal_van_roey_instituut')\\nvan('annie_lennox',\", 21697: \"'eurythmics')\\n\\n\\n\\n\\nnote\\nyour\", 21698: 'by\\nprint(nltk', 21699: '.rtuple(rel,', 21700: 'lcon=true,', 21701: 'rcon=true))', 21702: 'you\\nthe', 21703: 'intervene', 21704: 'and\\nalso', 21705: '10-word\\nwindow', 21706: 'to\\nfigure', 21707: \"van('annie_lennox',\", 21708: \"'eurythmics')\", 21709: 'hit', 21710: '.\\n\\n\\n\\n\\n\\n7\\xa0\\xa0\\xa0summary\\n\\ninformation', 21711: 'unrestricted\\ntext', 21712: 'to\\npopulate', 21713: 'well-organized', 21714: 'segmenting,', 21715: 'tokenizing,', 21716: 'determine\\nwhether', 21717: '.\\nentity', 21718: 'chunkers,', 21719: 'which\\nsegment', 21720: 'appropriate\\nentity', 21721: 'person,\\nlocation,', 21722: 'money,', 21723: '(geo-political', 21724: '.\\nchunkers', 21725: 'rule-based', 21726: 'the\\nregexpparser', 21727: 'nltk;', 21728: 'consecutivenpchunker', 21729: 'this\\nchapter', 21730: 'very\\nimportant', 21731: 'flat\\ndata', 21732: 'allowed', 21733: 'overlap,\\nthey', 21734: '.\\nrelation', 21735: 'rule-based\\nsystems', 21736: 'intervening', 21737: 'using\\nmachine-learning', 21738: 'learn\\nsuch', 21739: '.\\n\\n\\n\\n8\\xa0\\xa0\\xa0further', 21740: 'the\\nchunking', 21741: 'due', 21742: 'pioneering', 21743: 'by\\nabney', 21744: 'cass', 21745: 'in\\nhttp://www', 21746: '.vinartus', 21747: '.net/spa/97a', 21748: '.pdf', 21749: 'stopwords,\\naccording', 21750: '1975', 21751: 'ross', 21752: 'tukey', 21753: 'bio', 21754: 'format)', 21755: 'for\\nnp', 21756: '(ramshaw', 21757: 'marcus,', 21758: 'np\\nbracketing', 21759: 'conference', 21760: 'learning\\n(conll)', 21761: 'was\\nadopted', 21762: 'annotating', 21763: '.\\nsection', 21764: 'mining', 21765: 'medicine,', 21766: 'see\\n(ananiadou', 21767: 'mcnaught,', 21768: '.\\n\\n\\n\\n\\n\\n\\n9\\xa0\\xa0\\xa0exercises\\n\\n☼', 21769: 'i,\\no', 21770: 'necessary?', 21771: 'what\\nproblem', 21772: 'tags\\nexclusively?\\n☼', 21773: 'head', 21774: 'nouns,\\ne', 21775: 'many/jj', 21776: 'researchers/nns,', 21777: 'two/cd', 21778: 'weeks/nns,', 21779: 'both/dt', 21780: 'positions/nns', 21781: 'generalizing', 21782: 'singular\\nnoun', 21783: '.\\n☼\\npick', 21784: '.\\ninspect', 21785: 'sequences\\nthat', 21786: '.regexpparser', 21787: '.\\ndiscuss', 21788: '.\\n☼\\nan', 21789: '.\\ndevelop', 21790: 'single\\nchunk,', 21791: 'solely', 21792: '.\\ndetermine', 21793: 'sequences)', 21794: 'chinks\\nwith', 21795: 'and\\nsimplicity', 21796: 'on\\nchunk', 21797: 'gerunds,\\ne', 21798: 'receiving/vbg', 21799: 'end/nn,', 21800: 'assistant/nn', 21801: 'managing/vbg', 21802: 'editor/nn', 21803: 'using\\nsome', 21804: 'devising', 21805: 'coordinated', 21806: 'phrases,\\ne', 21807: 'july/nnp', 21808: 'august/nnp,\\nall/dt', 21809: 'your/prp$', 21810: 'managers/nns', 21811: 'supervisors/nns,\\ncompany/nn', 21812: 'courts/nns', 21813: 'adjudicators/nns', 21814: 'internal\\ninconsistencies,', 21815: 'approach\\nwill', 21816: '.)\\nevaluate', 21817: 'corpus,\\nand', 21818: 'chunkscore', 21819: '.missed()', 21820: '.incorrect()\\nmethods', 21821: 'chunker\\ndiscussed', 21822: '.\\n\\n\\n◑\\ndevelop', 21823: 'a\\nregular-expression', 21824: 'regexpchunk', 21825: 'any\\ncombination', 21826: 'chinking,', 21827: 'merging', 21828: 'in\\n12/cd', 21829: 'or/cc', 21830: 'so/rb', 21831: 'cases/vbz', 21832: 'correction', 21833: 'of\\ntagger', 21834: 'erroneous\\noutput', 21835: 'chunked\\nnoun', 21836: '.\\n◑\\nthe', 21837: '.\\nstudy', 21838: 'more?\\n★\\napply', 21839: 'tags\\nto', 21840: '(determiner)', 21841: 'occurs\\nat', 21842: '.\\n★\\nwe', 21843: 'establish\\nan', 21844: 'n-grams,\\nn-grams', 21845: '.\\napply', 21846: '.\\n★\\npick', 21847: 'functions\\nto', 21848: 'type:\\nlist', 21849: '.\\ncount', 21850: 'in\\norder', 21851: 'frequency;', 21852: 'frequency)\\nand', 21853: 'for\\ndeveloping', 21854: '.\\n\\n\\n★\\nthe', 21855: 'the\\nphrase:\\n[every/dt', 21856: 'time/nn]', 21857: '[she/prp]', 21858: 'sees/vbz', 21859: '[a/dt', 21860: 'newspaper/nn]\\ncontains', 21861: 'will\\nincorrectly', 21862: 'two:', 21863: '[every/dt', 21864: 'time/nn', 21865: 'she/prp]', 21866: 'chunk-internal', 21867: 'tags\\ntypically', 21868: 'then\\ndevise', 21869: 'and\\nre-evaluate', 21870: 'of\\ntuples,', 21871: 'of\\nnoun', 21872: 'prepositions,\\ne', 21873: 'cat', 21874: 'mat', 21875: \"('sat',\", 21876: \"'np')\", 21877: '.\\n★\\nthe', 21878: 'text\\nthat', 21879: 'brackets,\\nand', 21880: 'using:\\nfor', 21881: '.treebank_chunk', 21882: '.chunked_sents(fileid)', 21883: 'trees,\\njust', 21884: '.chunked_sents()', 21885: '.tree', 21886: '.pprint()', 21887: '.tree2conllstr()\\ncan', 21888: 'chunk2brackets()', 21889: 'chunk2iob()', 21890: 'single\\nchunk', 21891: 'command-line', 21892: 'utilities', 21893: 'bracket2iob', 21894: 'iob2bracket', 21895: '.py\\nthat', 21896: '(resp)', 21897: 'other\\nformat', 21898: '(obtain', 21899: '.)\\n\\n\\n★\\nan', 21900: 'current\\npart-of-speech', 21901: 'of\\nprevious', 21902: '.\\n★\\nconsider', 21903: 'example,\\nboth', 21904: 'adjectives\\n(in', 21905: 'english)', 21906: 'in\\ntwo', 21907: 'grows?\\nif', 21908: 'speculate', 21909: 'acst8', 21910: 'structure\\n\\n\\n\\n\\n\\n8', 21911: 'structure\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nearlier', 21912: 'them,\\nanalyze', 21913: 'categories,\\nand', 21914: 'scratch', 21915: 'surface', 21916: 'constraints\\nthat', 21917: 'govern', 21918: 'famous', 21919: 'cope', 21920: 'finite', 21921: 'their\\nstructures', 21922: 'sentences?\\nhow', 21923: 'trees?\\nhow', 21924: 'parsers', 21925: 'tree?\\n\\nalong', 21926: 'fundamentals', 21927: 'and\\nsee', 21928: 'easier\\nto', 21929: '.\\n\\n1\\xa0\\xa0\\xa0some', 21930: 'dilemmas\\n\\n1', 21931: '.1\\xa0\\xa0\\xa0linguistic', 21932: 'possibilities\\nprevious', 21933: 'analyse', 21934: 'text\\ncorpora,', 21935: 'stressed', 21936: 'growing\\ndaily', 21937: 'closely,', 21938: 'thought\\nexperiment', 21939: 'gigantic', 21940: 'everything\\nthat', 21941: 'uttered', 21942: 'last\\n50', 21943: 'language\\nof', 21944: 'english?', 21945: 'answer\\nno', 21946: 'search\\nthe', 21947: 'is\\neasy', 21948: 'as\\nnew', 21949: 'img\\n(http://www', 21950: '.telegraph', 21951: '.uk/sport/2387900/new-man-at-the-of-img', 21952: '.html),\\nspeakers', 21953: 'and\\ntherefore', 21954: '.\\naccordingly,', 21955: 'argue\\nthat', 21956: 'big\\nset', 21957: 'imaginary', 21958: 'speakers\\nof', 21959: 'judgements', 21960: 'reject\\nsome', 21961: '.\\nequally,', 21962: 'agree', 21963: 'perfectly\\ngood', 21964: 'property\\nthat', 21965: '.usain', 21966: 'bolt', 21967: '100m', 21968: 'record\\n\\n', 21969: 'jamaica', 21970: 'observer', 21971: 'reported', 21972: 'usain', 21973: '.andre', 21974: '.i', 21975: 'andre', 21976: 'record\\n\\nif', 21977: 'like\\nandre', 21978: 'sentence\\nand', 21979: 'like\\ns', 21980: 'ingenuity', 21981: 'can\\nconstruct', 21982: 'impressive', 21983: 'winnie', 21984: 'pooh', 21985: 'milne,\\nin', 21986: 'water:\\n\\n[you', 21987: \"piglet's\", 21988: 'joy', 21989: 'ship', 21990: 'of\\nhim', 21991: 'after-years', 21992: 'liked', 21993: 'very\\ngreat', 21994: 'terrible', 21995: 'flood,', 21996: 'had\\nreally', 21997: 'half-hour', 21998: 'imprisonment,', 21999: 'when\\nowl,', 22000: 'flown', 22001: 'comfort\\nhim,', 22002: 'aunt', 22003: 'laid\\na', 22004: \"seagull's\", 22005: 'egg', 22006: 'mistake,', 22007: 'rather\\nlike', 22008: 'listening', 22009: 'his\\nwindow', 22010: 'hope,', 22011: 'quietly', 22012: 'naturally,\\nslipping', 22013: 'was\\nonly', 22014: 'hanging', 22015: 'toes,', 22016: 'moment,', 22017: 'luckily,', 22018: 'sudden\\nloud', 22019: 'squawk', 22020: 'owl,', 22021: 'story,', 22022: 'being\\nwhat', 22023: 'said,', 22024: 'woke', 22025: 'to\\njerk', 22026: 'himself', 22027: 'interesting,', 22028: 'did\\nshe?', 22029: 'saw\\nthe', 22030: 'ship,', 22031: 'brain', 22032: '(captain,', 22033: 'robin;', 22034: '1st', 22035: 'mate,', 22036: 'bear)\\ncoming', 22037: 'rescue', 22038: 'begins\\ns', 22039: 'language\\nprovides', 22040: 'constructions', 22041: 'extend\\nsentences', 22042: 'indefinitely', 22043: 'length\\nthat', 22044: 'before:', 22045: 'concoct', 22046: 'an\\nentirely', 22047: 'before\\nin', 22048: 'language\\nwill', 22049: 'a\\nlanguage', 22050: 'closely\\nintertwined', 22051: 'observed', 22052: 'texts?', 22053: 'it\\nsomething', 22054: 'implicit', 22055: 'competent\\nspeakers', 22056: 'sentences?', 22057: 'combination\\nof', 22058: 'two?', 22059: 'stand', 22060: 'issue,', 22061: 'will\\nintroduce', 22062: 'framework\\nof', 22063: 'which\\na', 22064: 'an\\nenormous', 22065: 'a\\ngrammar', 22066: 'the\\nmembers', 22067: 'grammars', 22068: 'productions\\nof', 22069: 'this,\\nto', 22070: 'meanings\\nof', 22071: '.2\\xa0\\xa0\\xa0ubiquitous', 22072: 'ambiguity\\na', 22073: 'well-known', 22074: '(2),\\nfrom', 22075: 'groucho', 22076: 'marx', 22077: 'movie,', 22078: 'animal', 22079: 'crackers', 22080: '(1930):\\n\\n\\n', 22081: '(2)while', 22082: 'hunting', 22083: 'africa,', 22084: 'shot', 22085: 'elephant', 22086: 'pajamas', 22087: 'pajamas,', 22088: 'phrase:\\ni', 22089: 'we\\nneed', 22090: 'grammar:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 22091: 'groucho_grammar', 22092: '.cfg', 22093: '.fromstring(\\n', 22094: \"'i'\\n\", 22095: 'pp\\n', 22096: \"'an'\", 22097: \"'my'\\n\", 22098: \"'elephant'\", 22099: \"'pajamas'\\n\", 22100: \"'shot'\\n\", 22101: \"'in'\\n\", 22102: ')\\n\\n\\n\\nthis', 22103: 'ways,\\ndepending', 22104: 'pajamas\\ndescribes', 22105: \"'shot',\", 22106: \"'elephant',\", 22107: \"'pajamas']\\n>>>\", 22108: '.chartparser(groucho_grammar)\\n>>>', 22109: '.parse(sent):\\n', 22110: 'print(tree)\\n', 22111: '.\\n(s\\n', 22112: 'shot)', 22113: 'an)', 22114: 'elephant)))\\n', 22115: '(p', 22116: 'in)', 22117: 'my)', 22118: 'pajamas)))))\\n(s\\n', 22119: 'shot)\\n', 22120: 'elephant)', 22121: 'pajamas))))))\\n\\n\\n\\nthe', 22122: 'depict', 22123: 'as\\ntrees,', 22124: '(3b):\\n\\n', 22125: '.\\n\\nnotice', 22126: 'words;\\ne', 22127: 'gun', 22128: 'sentence,\\nand', 22129: 'camera', 22130: 'turn:\\nconsider', 22131: 'different\\ninterpretations:', 22132: 'fighting', 22133: '.\\nvisiting', 22134: 'relatives', 22135: 'tiresome', 22136: 'individual\\nwords', 22137: 'blame?', 22138: 'ambiguity?\\n\\nthis', 22139: 'and\\ncomputational', 22140: 'investigating', 22141: 'linguistic\\nphenomena', 22142: 'well-formedness', 22143: 'ill-formedness', 22144: 'the\\nphrase', 22145: 'formal\\nmodels', 22146: 'reliably\\nrecognize', 22147: 'contains?', 22148: 'simple\\nquestions', 22149: 'whom?', 22150: \".\\n\\n\\n\\n2\\xa0\\xa0\\xa0what's\", 22151: 'syntax?\\n\\n2', 22152: '.1\\xa0\\xa0\\xa0beyond', 22153: 'n-grams\\nwe', 22154: 'seems\\nperfectly', 22155: 'rapidly\\ndegenerates', 22156: 'by\\ncomputing', 22157: \"childrens'\", 22158: 'the\\nadventures', 22159: 'buster', 22160: '.org/files/22816/22816', 22161: '.txt):\\n\\n', 22162: 'roared', 22163: 'slip', 22164: 'back\\n\\n', 22165: 'worst', 22166: 'clumsy', 22167: 'whoever', 22168: 'light\\n\\nyou', 22169: 'intuitively', 22170: 'word-salad,', 22171: 'you\\nprobably', 22172: 'one\\nbenefit', 22173: 'framework\\nand', 22174: 'look\\nat', 22175: 'coordinate\\nstructure,', 22176: 'coordinating\\nconjunction', 22177: 'an\\ninformal', 22178: 'simplified)', 22179: 'coordination', 22180: 'works\\nsyntactically:\\ncoordinate', 22181: 'structure:\\n\\nif', 22182: 'grammatical\\ncategory', 22183: 'a\\nphrase', 22184: '(noun\\nphrases)', 22185: 'conjoined', 22186: 'second,\\ntwo', 22187: 'aps', 22188: '(adjective', 22189: 'phrases)', 22190: 'an\\nap', 22191: 'part)', 22192: '.on', 22193: '(ap', 22194: 'looking)', 22195: 'conjoin', 22196: 'ap,', 22197: 'is\\nwhy', 22198: 'ideas,', 22199: '.\\nconstituent', 22200: 'combine\\nwith', 22201: 'words\\nforms', 22202: 'substitutability', 22203: 'well-formed', 22204: 'a\\nshorter', 22205: 'rendering', 22206: 'ill-formed', 22207: 'clarify\\nthis', 22208: 'idea,', 22209: '(6)the', 22210: 'fine', 22211: 'fat', 22212: 'brook', 22213: 'substitute', 22214: 'bear\\nindicates', 22215: 'cannot\\nreplace', 22216: '.\\n\\n\\n', 22217: '.*the', 22218: 'sequences\\nby', 22219: 'preserves', 22220: 'grammaticality', 22221: 'sequence\\nthat', 22222: 'end\\nup', 22223: 'sequences:', 22224: 'replace\\nparticular', 22225: 'brook)', 22226: 'it);', 22227: 'grammatical\\ntwo-word', 22228: 'added\\ngrammatical', 22229: 'vp,', 22230: 'phrase,\\nverb', 22231: 'categories:\\nthis', 22232: 'reproduces', 22233: 'grammatical\\ncategories', 22234: '(np),', 22235: '(vp),\\nprepositional', 22236: '(pp),', 22237: 'nominals', 22238: '(nom)', 22239: 'topmost', 22240: 'an\\ns', 22241: 'flip', 22242: 'standard\\nphrase', 22243: '(8)', 22244: 'called\\na', 22245: 'constituents', 22246: 'of\\ns', 22247: '(8)\\nas', 22248: 'sentence\\ncan', 22249: 'constituents,', 22250: 'further\\nsubdivided', 22251: '.\\n\\nnote\\nas', 22252: '4\\ncan', 22253: 'bounded', 22254: \"methods\\naren't\", 22255: 'applicable', 22256: '.\\n\\n\\n\\n\\n3\\xa0\\xa0\\xa0context', 22257: 'grammar\\n\\n3', 22258: '.1\\xa0\\xa0\\xa0a', 22259: \"grammar\\n\\nlet's\", 22260: 'context-free', 22261: 'by\\nconvention,', 22262: 'left-hand-side', 22263: 'the\\nstart-symbol', 22264: 'all\\nwell-formed', 22265: 'in\\nnltk,', 22266: '.grammar\\nmodule', 22267: 'admitted', 22268: '.\\n\\n\\n\\n\\n\\xa0\\n\\ngrammar1', 22269: 'walked\\n', 22270: 'mary', 22271: 'bob', 22272: 'my\\n', 22273: 'telescope', 22274: 'park\\n', 22275: 'with\\n', 22276: ')\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 22277: 'rd_parser', 22278: '.recursivedescentparser(grammar1)\\n>>>', 22279: 'print(tree)\\n(s', 22280: 'mary)', 22281: 'saw)', 22282: 'bob)))\\n\\n\\nexample', 22283: '(code_cfg1', 22284: 'grammar\\n\\nthe', 22285: 'productions', 22286: 'categories,\\nas', 22287: '.\\n\\n\\n\\n\\n\\n\\n\\nsymbol\\nmeaning\\nexample\\n\\n\\n\\ns\\nsentence\\nthe', 22288: 'walked\\n\\nnp\\nnoun', 22289: 'phrase\\na', 22290: 'dog\\n\\nvp\\nverb', 22291: 'phrase\\nsaw', 22292: 'park\\n\\npp\\nprepositional', 22293: 'phrase\\nwith', 22294: 'telescope\\n\\ndet\\ndeterminer\\nthe\\n\\nn\\nnoun\\ndog\\n\\nv\\nverb\\nwalked\\n\\np\\npreposition\\nin\\n\\n\\ntable', 22295: 'categories\\n\\n\\na', 22296: 'the\\nrighthand', 22297: 'productions\\nvp', 22298: 'descent', 22299: 'demo:', 22300: 'against\\nthe', 22301: 'the\\nrecursive', 22302: '.rdparser(),\\nshown', 22303: 'can\\nedit', 22304: 'menu)', 22305: '.\\nchange', 22306: 'parsed,', 22307: 'and\\nrun', 22308: 'autostep', 22309: 'button', 22310: 'park', 22311: 'to\\nthose', 22312: '(9)\\n', 22313: 'is\\nsaid', 22314: 'structurally', 22315: 'the\\npp', 22316: 'attached', 22317: 'places\\nin', 22318: 'interpretation\\nis', 22319: 'happened\\nin', 22320: 'np,\\nthen', 22321: 'park,', 22322: 'agent', 22323: 'dog)\\nmight', 22324: 'sitting', 22325: 'balcony', 22326: 'apartment', 22327: 'overlooking', 22328: 'the\\npark', 22329: '.2\\xa0\\xa0\\xa0writing', 22330: 'grammars\\nif', 22331: 'cfgs,', 22332: 'will\\nfind', 22333: 'text\\nfile,', 22334: 'mygrammar', 22335: 'and\\nparse', 22336: 'grammar1', 22337: \".load('file:mygrammar\", 22338: \".cfg')\\n>>>\", 22339: 'print(tree)\\n\\n\\n\\nmake', 22340: \"'file:mygrammar\", 22341: \".cfg'\", 22342: 'print(tree)', 22343: 'because\\nyour', 22344: 'case,\\ncall', 22345: 'on:', 22346: '=\\nnltk', 22347: '.recursivedescentparser(grammar1,', 22348: 'trace=2)', 22349: 'check\\nwhat', 22350: 'currently', 22351: 'p\\nin', 22352: '.productions():', 22353: 'print(p)', 22354: 'cfgs', 22355: 'combine\\ngrammatical', 22356: 'righthand', 22357: \"'of'\", 22358: 'disallowed', 22359: 'in\\naddition,', 22360: \"'new\\nyork',\", 22361: \"'new_york'\\ninstead\", 22362: '.3\\xa0\\xa0\\xa0recursion', 22363: 'structure\\na', 22364: 'hand\\nside', 22365: 'production,', 22366: 'nom', 22367: 'nominals)', 22368: 'category\\nnom,', 22369: 'indirect', 22370: 'the\\ncombination', 22371: 'productions,', 22372: '.\\n\\n\\n\\n\\n\\xa0\\n\\ngrammar2', 22373: 'propn\\n', 22374: 'n\\n', 22375: 'propn', 22376: \"'buster'\", 22377: \"'chatterer'\", 22378: \"'joe'\\n\", 22379: \"'a'\\n\", 22380: \"'bear'\", 22381: \"'squirrel'\", 22382: \"'tree'\", 22383: \"'fish'\", 22384: \"'log'\\n\", 22385: \"'angry'\", 22386: \"'frightened'\", 22387: \"'little'\", 22388: \"'tall'\\n\", 22389: \"'chased'\", 22390: \"'saw'\", 22391: \"'said'\", 22392: \"'thought'\", 22393: \"'was'\", 22394: \"'put'\\n\", 22395: \"'on'\\n\", 22396: '(code_cfg2', 22397: 'grammar\\n\\nto', 22398: 'following\\ntrees', 22399: '(10a)', 22400: 'nominal', 22401: 'phrases,\\nwhile', 22402: '(10b)', 22403: '(10)\\n', 22404: \".\\n\\nwe've\", 22405: \"there's\\nno\", 22406: 'parsing\\nsentences', 22407: 'deeply', 22408: '.\\nbeware', 22409: 'recursivedescentparser', 22410: 'handle\\nleft-recursive', 22411: 'y;', 22412: 'will\\nreturn', 22413: '.\\n\\n\\n\\n4\\xa0\\xa0\\xa0parsing', 22414: 'grammar\\n\\na', 22415: 'the\\nproductions', 22416: 'more\\nconstituent', 22417: 'conform', 22418: 'specification', 22419: '—\\nit', 22420: 'licensed', 22421: 'grammar\\nto', 22422: 'fringe', 22423: 'against\\na', 22424: 'linguists\\nto', 22425: 'psycholinguistic', 22426: 'processing,\\nhelping', 22427: 'processing\\ncertain', 22428: 'point;\\nfor', 22429: 'questions\\nsubmitted', 22430: 'question-answering', 22431: 'undergo', 22432: 'algorithms,\\na', 22433: 'parsing,\\nand', 22434: 'shift-reduce', 22435: 'called\\nleft-corner', 22436: 'programming\\ntechnique', 22437: '.1\\xa0\\xa0\\xa0recursive', 22438: 'parsing\\nthe', 22439: 'specification\\nof', 22440: 'subgoals', 22441: 'top-level', 22442: 'vp\\nproduction', 22443: 'subgoals:\\nfind', 22444: 'sub-sub-goals,', 22445: 'np\\nand', 22446: 'left-hand', 22447: 'eventually,', 22448: 'expansion\\nprocess', 22449: 'such\\nsubgoals', 22450: 'and\\nsucceed', 22451: 'parser\\nmust', 22452: 'above\\nprocess', 22453: 's),', 22454: 'node\\nis', 22455: 'expands', 22456: 'downwards\\n(hence', 22457: 'descent)', 22458: '.rdparser()', 22459: '.\\nsix', 22460: 'parser:', 22461: 'a\\ntree', 22462: 's;', 22463: 'consults', 22464: 'enlarge', 22465: 'tree;', 22466: 'when\\na', 22467: 'encountered,', 22468: 'input;\\nafter', 22469: 'backtracks', 22470: 'parses', 22471: '.\\n\\nduring', 22472: 'several\\npossible', 22473: 'it\\ntries', 22474: 'work\\nit', 22475: 'backtracks,', 22476: 'it\\ngets', 22477: 'dog,', 22478: 'complete\\nparse', 22479: 'any\\ndangling', 22480: 'backtrack', 22481: 'other\\nchoices', 22482: 'parser:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 22483: \"'mary\", 22484: \"dog'\", 22485: 'dog))))\\n\\n\\n\\n\\nnote\\nrecursivedescentparser()', 22486: 'trace', 22487: 'steps\\nthat', 22488: '.\\n\\nrecursive', 22489: 'first,\\nleft-recursive', 22490: 'send', 22491: 'it\\ninto', 22492: 'infinite', 22493: 'wastes', 22494: 'time\\nconsidering', 22495: 'input\\nsentence', 22496: 'backtracking', 22497: 'parsed\\nconstituents', 22498: 'rebuilt', 22499: 'example,\\nbacktracking', 22500: 'subtree\\ncreated', 22501: 'proceeds', 22502: 'with\\nvp', 22503: 'all\\nover', 22504: '.\\nrecursive', 22505: '.\\ntop-down', 22506: 'be,\\nbefore', 22507: 'input!', 22508: 'called\\nbottom-up', 22509: '.2\\xa0\\xa0\\xa0shift-reduce', 22510: 'parsing\\na', 22511: 'shift-reduce\\nparser', 22512: 'them\\nwith', 22513: 'to\\nan', 22514: 'pushes', 22515: 'a\\nstack', 22516: '.1);', 22517: 'shift', 22518: 'match\\nthe', 22519: 'production,\\nthen', 22520: 'popped', 22521: 'left-hand\\nside', 22522: 'stack;\\nreducing', 22523: 'are\\npushed', 22524: 'is\\nconsumed', 22525: 'parse\\ntree', 22526: 'pops', 22527: 'partial', 22528: 'the\\ngraphical', 22529: '.srparser()', 22530: 'shifting', 22531: 'stack;', 22532: 'production;', 22533: 'succeeds', 22534: 'input\\nis', 22535: '.\\n\\nnltk', 22536: 'shiftreduceparser(),', 22537: 'simple\\nimplementation', 22538: 'not\\nimplement', 22539: 'backtracking,', 22540: 'guaranteed', 22541: 'parse\\nfor', 22542: 'exists', 22543: 'at\\nmost', 22544: 'parse,', 22545: 'an\\noptional', 22546: 'verbosely', 22547: 'the\\nparser', 22548: 'sr_parser', 22549: '.shiftreduceparser(grammar1)\\n>>>', 22550: '(s', 22551: 'dog))))\\n\\n\\n\\n\\nnote\\nyour', 22552: 'mode', 22553: 'reduce\\noperations,', 22554: 'sr_parse', 22555: '.shiftreduceparser(grammar1,', 22556: 'trace=2)\\n\\na', 22557: 'parse,\\neven', 22558: 'remains,', 22559: 'items\\nwhich', 22560: 'because\\nthere', 22561: 'undone', 22562: 'parser\\n(although', 22563: 'choices)', 22564: 'parser:\\n(a)', 22565: 'reduction', 22566: 'possible\\n(b)', 22567: 'policies', 22568: 'resolving', 22569: 'such\\nconflicts', 22570: 'conflicts', 22571: 'by\\nshifting', 22572: 'reductions', 22573: 'address\\nreduce-reduce', 22574: 'favoring', 22575: 'removes\\nthe', 22576: 'shift-reduce\\nparser,', 22577: 'lr', 22578: 'parser,', 22579: 'compilers', 22580: 'parsers\\nis', 22581: 'sub-structure', 22582: 'once,\\ne', 22583: 'np(det(the),', 22584: 'n(man))', 22585: 'stack\\na', 22586: 'the\\nvp', 22587: 'left-corner', 22588: 'parser\\none', 22589: 'it\\ngoes', 22590: 'left-recursive', 22591: 'grammar\\nproductions', 22592: 'blindly,', 22593: 'top-down\\napproaches', 22594: '.\\ngrammar', 22595: 'saw\\nmary:\\n\\n', 22596: '(11)\\nrecall', 22597: '(defined', 22598: 'np:\\n\\n', 22599: '(12)\\n', 22600: '.np', 22601: 'n\\n\\n', 22602: 'pp\\n\\n', 22603: 'bob\\n\\n\\nsuppose', 22604: '(11),', 22605: 'decide\\nwhich', 22606: 'to\\napply', 22607: 'obviously,', 22608: '(12c)', 22609: 'choice!', 22610: 'you\\nknow', 22611: 'pointless', 22612: '(12a)', 22613: '(12b)', 22614: 'instead?', 22615: 'because\\nneither', 22616: 'is\\njohn', 22617: 'successful\\nparse', 22618: 'mary,', 22619: 'more\\ngenerally,', 22620: '⇒*\\nb', 22621: '(13)\\na', 22622: 'trapped\\nin', 22623: 'preprocesses', 22624: 'the\\ncontext-free', 22625: 'two\\ncells,', 22626: 'non-terminal,', 22627: 'the\\ncollection', 22628: 'corners', 22629: 'non-terminal', 22630: 'grammar2', 22631: '.\\n\\n\\n\\n\\n\\n\\ncategory\\nleft-corners', 22632: '(pre-terminals)\\n\\n\\n\\ns\\nnp\\n\\nnp\\ndet,', 22633: 'propn\\n\\nvp\\nv\\n\\npp\\np\\n\\n\\ntable', 22634: 'left-corners', 22635: 'grammar2\\n\\n\\neach', 22636: 'pre-terminal\\ncategories', 22637: '.4\\xa0\\xa0\\xa0well-formed', 22638: 'tables\\nthe', 22639: 'suffer', 22640: 'in\\nboth', 22641: 'completeness', 22642: 'remedy', 22643: 'will\\napply', 22644: '.7,\\ndynamic', 22645: 're-uses', 22646: 'when\\nappropriate,', 22647: 'technique\\ncan', 22648: 'store\\npartial', 22649: 'as\\nnecessary', 22650: 'introduce\\nthe', 22651: 'section;', 22652: 'pajamas\\njust', 22653: 'it\\nup', 22654: 'subconstituent', 22655: 'a\\nwell-formed', 22656: 'wfst', 22657: 'contiguous', 22658: 'record\\nwhat', 22659: 'specified\\nspans', 22660: 'reminiscent', 22661: 'another\\nway', 22662: 'data\\nstructure', 22663: 'edge', 22664: 'wfst,', 22665: 'words\\nby', 22666: 'triangular', 22667: 'matrix:\\nthe', 22668: 'vertical', 22669: 'axis', 22670: 'denote', 22671: 'substring,\\nwhile', 22672: 'horizontal', 22673: 'position\\n(thus', 22674: 'coordinates', 22675: '2))', 22676: 'unique\\nlexical', 22677: 'category,', 22678: '.\\nmore', 22679: 'is\\na0a1', 22680: 'grammar\\ncontains', 22681: 'ai,', 22682: '`i`+1)', 22683: 'warning/2', 22684: '(ch08', 22685: '900);', 22686: 'backlink\\ninline', 22687: 'start-string', 22688: 'end-string', 22689: 'what\\ncategory', 22690: '.productions(rhs=text[1])\\n[v', 22691: \"'shot']\\n\\n\\n\\nfor\", 22692: '(n-1)', 22693: 'matrix\\nas', 22694: 'it\\nwith', 22695: 'init_wfst()\\nfunction', 22696: 'display()\\nto', 22697: 'pretty-print', 22698: 'init_wfst(tokens,', 22699: 'grammar):\\n', 22700: 'numtokens', 22701: 'len(tokens)\\n', 22702: '[[none', 22703: 'range(numtokens+1)]', 22704: 'range(numtokens+1)]\\n', 22705: 'range(numtokens):\\n', 22706: '.productions(rhs=tokens[i])\\n', 22707: 'wfst[i][i+1]', 22708: 'productions[0]', 22709: '.lhs()\\n', 22710: 'wfst\\n\\ndef', 22711: 'complete_wfst(wfst,', 22712: 'trace=false):\\n', 22713: 'dict((p', 22714: '.rhs(),', 22715: '.lhs())', 22716: '.productions())\\n', 22717: 'range(2,', 22718: 'numtokens+1):\\n', 22719: 'range(numtokens+1-span):\\n', 22720: 'span\\n', 22721: 'mid', 22722: 'range(start+1,', 22723: 'end):\\n', 22724: 'nt1,', 22725: 'nt2', 22726: 'wfst[start][mid],', 22727: 'wfst[mid][end]\\n', 22728: 'nt1', 22729: '(nt1,nt2)', 22730: 'index:\\n', 22731: 'wfst[start][end]', 22732: 'index[(nt1,nt2)]\\n', 22733: 'trace:\\n', 22734: 'print([%s]', 22735: '%3s', 22736: '[%s]', 22737: '==>', 22738: '(start,', 22739: 'mid,', 22740: 'nt2,', 22741: 'index[(nt1,nt2)],', 22742: 'end))\\n', 22743: 'display(wfst,', 22744: 'tokens):\\n', 22745: \"print('\\\\nwfst\", 22746: '.join((%-4d', 22747: 'len(wfst))))\\n', 22748: 'range(len(wfst)-1):\\n', 22749: 'print(%d', 22750: 'len(wfst)):\\n', 22751: 'print(%-4s', 22752: '(wfst[i][j]', 22753: 'wfst0', 22754: 'groucho_grammar)\\n>>>', 22755: 'display(wfst0,', 22756: 'tokens)\\nwfst', 22757: '7\\n0', 22758: 'n\\n>>>', 22759: 'wfst1', 22760: 'complete_wfst(wfst0,', 22761: 'display(wfst1,', 22762: 's\\n1', 22763: 'vp\\n2', 22764: 'pp\\n5', 22765: 'np\\n6', 22766: 'n\\n\\n\\nexample', 22767: '(code_wfst', 22768: 'acceptor', 22769: 'table\\n\\nreturning', 22770: 'det\\nin', 22771: 'elephant,', 22772: 'elephant?\\nwe', 22773: '.\\nconsulting', 22774: '.\\n\\nmore', 22775: 'j)', 22776: 'find\\nnonterminal', 22777: '(k,', 22778: 'complete_wfst(),\\nwe', 22779: 'constructed:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 22780: 'groucho_grammar,', 22781: 'trace=true)\\n[2]', 22782: '[3]', 22783: '[4]', 22784: '[2]', 22785: '[4]\\n[5]', 22786: '[6]', 22787: '[7]', 22788: '[5]', 22789: '[7]\\n[1]', 22790: '[1]', 22791: '[4]\\n[4]', 22792: '[7]\\n[0]', 22793: '[0]', 22794: '[4]\\n[1]', 22795: '[7]\\n\\n\\n\\nfor', 22796: 'at\\nwfst[2][3]', 22797: 'wfst[3][4],', 22798: 'to\\nwfst[2][4]', 22799: '.\\n\\nnote\\nto', 22800: 'hand\\nsides,', 22801: 'trade-off:', 22802: 'lookup\\non', 22803: 'of\\nproductions', 22804: 'non-terminals', 22805: 'once\\nwe', 22806: \"up!\\n\\nwfst's\", 22807: 'is\\nstrictly', 22808: 'speaking', 22809: 'a\\ngrammar,', 22810: 'non-lexical', 22811: 'form,\\nwe', 22812: 'requirement', 22813: 'wasteful,', 22814: 'propose', 22815: '.\\n\\nfinally,', 22816: 'structural', 22817: 'readings)', 22818: 'vp\\nin', 22819: '7)', 22820: 'twice,', 22821: 'np\\nreading,', 22822: 'different\\nhypotheses,', 22823: \"didn't\\nmatter\", 22824: '.)\\nchart', 22825: 'slighly', 22826: 'interesting\\nalgorithms', 22827: 'details)', 22828: '.chartparser()', 22829: '.\\n\\n\\n\\n\\n5\\xa0\\xa0\\xa0dependencies', 22830: 'grammar\\nphrase', 22831: 'complementary\\napproach,', 22832: 'focusses', 22833: 'words\\nrelate', 22834: 'asymmetric', 22835: 'that\\nholds', 22836: 'dependents', 22837: 'tensed', 22838: 'is\\neither', 22839: 'head,', 22840: 'of\\ndependencies', 22841: 'directed', 22842: 'graph,', 22843: 'the\\nnodes', 22844: 'arcs', 22845: 'dependency\\nrelations', 22846: 'heads', 22847: 'a\\ndependency', 22848: 'arrows', 22849: 'dependents;\\nlabels', 22850: 'as\\nsubject,', 22851: 'grammatical\\nfunction', 22852: 'example,\\ni', 22853: 'sbj', 22854: '(subject)', 22855: 'sentence),', 22856: 'nmod', 22857: '(noun', 22858: 'of\\nelephant)', 22859: 'therefore,\\ndependency', 22860: 'grammatical\\nfunctions', 22861: 'bare\\ndependency', 22862: 'dependency:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 22863: 'groucho_dep_grammar', 22864: '.dependencygrammar', 22865: \"'shot'\", 22866: \"'i'\", 22867: \"'pajamas'\", 22868: ')\\n>>>', 22869: 'print(groucho_dep_grammar)\\ndependency', 22870: 'productions\\n', 22871: \"'elephant'\\n\", 22872: \"'an'\\n\", 22873: \"'my'\\n\\n\\n\\na\", 22874: 'projective', 22875: 'are\\nwritten', 22876: 'words\\nwithout', 22877: 'crossing', 22878: 'its\\ndescendents', 22879: '(dependents', 22880: 'dependents,', 22881: 'is\\nprojective,', 22882: 'a\\nprojective', 22883: 'how\\ngroucho_dep_grammar', 22884: 'capturing\\nthe', 22885: 'phrase\\nstructure', 22886: 'pdp', 22887: '.projectivedependencyparser(groucho_dep_grammar)\\n>>>', 22888: \"pajamas'\", 22889: '.parse(sent)\\n>>>', 22890: 'trees:\\n', 22891: 'print(tree)\\n(shot', 22892: '(elephant', 22893: '(pajamas', 22894: 'my))))\\n(shot', 22895: 'my)))\\n\\n\\n\\nthese', 22896: 'trees,\\nwhere', 22897: '(14)\\nin', 22898: 'english,\\nnon-projective', 22899: '.\\n\\n\\nvarious', 22900: 'proposed', 22901: 'construction', 22902: 'important\\nare', 22903: 'following:\\n\\nh', 22904: 'c;', 22905: 'alternatively,', 22906: 'the\\nexternal', 22907: '.\\nh', 22908: 'or\\noptional', 22909: 'agreement\\nor', 22910: 'government)', 22911: 'immediate\\nconstituents', 22912: 'implicitly\\nappealing', 22913: 'phrase\\nis', 22914: 'preposition;', 22915: 'a\\ndependent', 22916: 'carries', 22917: 'other\\ntypes', 22918: 'from\\ndependency', 22919: 'grammars,', 22920: 'embody', 22921: 'of\\ndependency', 22922: 'capture\\ndependencies,', 22923: 'frameworks', 22924: 'increasingly\\nadopted', 22925: 'formalisms', 22926: '.1\\xa0\\xa0\\xa0valency', 22927: 'lexicon\\nlet', 22928: 'like\\n(15d)', 22929: '(15)\\n', 22930: 'squirrel', 22931: 'frightened', 22932: '.chatterer', 22933: 'angry', 22934: '.joe', 22935: 'fish', 22936: 'log', 22937: '.\\n\\n\\nthese', 22938: 'productions:\\n\\n\\n\\n\\n\\n\\nvp', 22939: 'adj\\nwas\\n\\nvp', 22940: 'np\\nsaw\\n\\nvp', 22941: 's\\nthought\\n\\nvp', 22942: 'pp\\nput\\n\\n\\ntable', 22943: 'heads\\n\\n\\n\\nthat', 22944: 'adj,', 22945: 'a\\nfollowing', 22946: 'and\\ns', 22947: 'complements', 22948: 'respective', 22949: 'strong\\nconstraints', 22950: 'with\\n(15d),', 22951: '(16d)', 22952: 'ill-formed:\\n\\n', 22953: '(16)\\n', 22954: '.*chatterer', 22955: '.*joe', 22956: '.\\n\\n\\nnote\\nwith', 22957: 'imagination,', 22958: 'to\\ninvent', 22959: 'and\\ncomplements', 22960: 'above\\nexamples', 22961: 'neutral', 22962: '.\\n\\n\\nin', 22963: 'tradition', 22964: 'said\\nto', 22965: 'valencies', 22966: 'valency', 22967: 'restrictions', 22968: 'just\\napplicable', 22969: '.\\n\\nwithin', 22970: 'various\\ntechniques', 22971: 'excluding', 22972: 'the\\nungrammatical', 22973: 'cfg,', 22974: 'constraining\\ngrammar', 22975: 'co-occur\\nwith', 22976: 'of\\nverbs', 22977: 'subcategories,', 22978: 'transitive', 22979: 'np\\nobject', 22980: 'complement;', 22981: 'subcategorized', 22982: 'np\\ndirect', 22983: 'namely\\ntv', 22984: 'verb),', 22985: 'productions:\\n\\nvp', 22986: 'tv', 22987: 'np\\ntv', 22988: \"'saw'\\n\\nnow\", 22989: '*joe', 22990: 'excluded', 22991: 'listed\\nthought', 22992: 'tv,', 22993: 'chatterer', 22994: 'subcategories', 22995: '.\\n\\n\\n\\n\\n\\n\\n\\nsymbol\\nmeaning\\nexample\\n\\n\\n\\niv\\nintransitive', 22996: 'verb\\nbarked\\n\\ntv\\ntransitive', 22997: 'verb\\nsaw', 22998: 'man\\n\\ndatv\\ndative', 22999: 'verb\\ngave', 23000: 'man\\n\\nsv\\nsentential', 23001: 'verb\\nsaid', 23002: 'barked\\n\\n\\ntable', 23003: 'subcategories\\n\\n\\nvalency', 23004: 'further\\nin', 23005: '.\\ncomplements', 23006: 'contrasted', 23007: 'adjuncts),\\nalthough', 23008: 'phrases,\\nadjectives', 23009: 'unlike\\ncomplements,', 23010: 'optional,', 23011: 'iterated,', 23012: 'are\\nnot', 23013: 'adverb', 23014: 'modifer', 23015: 'the\\nsentence', 23016: '(17d):\\n\\n', 23017: '(17)\\n', 23018: 'attachment,', 23019: 'have\\nillustrated', 23020: 'grammars,\\ncorresponds', 23021: '.2\\xa0\\xa0\\xa0scaling', 23022: 'up\\nso', 23023: 'toy', 23024: 'that\\nillustrate', 23025: 'obvious\\nquestion', 23026: 'scaled', 23027: 'cover\\nlarge', 23028: 'construct\\nsuch', 23029: 'hand?', 23030: 'is:', 23031: 'very\\nhard', 23032: 'devices', 23033: 'that\\ngive', 23034: 'succinct', 23035: 'extremely\\ndifficult', 23036: 'many\\nproductions', 23037: 'modularize', 23038: 'that\\none', 23039: 'in\\nturn', 23040: 'distribute', 23041: 'grammar\\nwriting', 23042: 'the\\ngrammar', 23043: 'constructions,\\nthere', 23044: 'analyses', 23045: 'are\\nadmitted', 23046: 'increases\\nwith', 23047: '.\\ndespite', 23048: 'collaborative\\nprojects', 23049: 'in\\ndeveloping', 23050: 'the\\nlexical', 23051: '(lfg)', 23052: 'pargram', 23053: 'project,\\nthe', 23054: 'head-driven', 23055: '(hpsg)', 23056: 'lingo', 23057: 'framework,\\nand', 23058: 'lexicalized', 23059: 'adjoining', 23060: 'xtag', 23061: '.\\n\\n\\n\\n6\\xa0\\xa0\\xa0grammar', 23062: 'development\\nparsing', 23063: 'above\\nonly', 23064: 'realistic', 23065: 'language?', 23066: 'will\\nsee', 23067: 'treebanks,', 23068: 'developing\\nbroad-coverage', 23069: '.\\n\\n6', 23070: '.1\\xa0\\xa0\\xa0treebanks', 23071: 'grammars\\nthe', 23072: 'reader,\\nwhich', 23073: 'treebank\\n>>>', 23074: \".parsed_sents('wsj_0001\", 23075: \".mrg')[0]\\n>>>\", 23076: 'print(t)\\n(s\\n', 23077: '(np-sbj\\n', 23078: '(nnp', 23079: 'pierre)', 23080: 'vinken))\\n', 23081: '(,', 23082: ',)\\n', 23083: '(adjp', 23084: '(cd', 23085: '61)', 23086: '(nns', 23087: 'years))', 23088: '(jj', 23089: 'old))\\n', 23090: ',))\\n', 23091: '(md', 23092: 'will)\\n', 23093: '(vb', 23094: 'join)\\n', 23095: '(dt', 23096: '(nn', 23097: 'board))\\n', 23098: '(pp-clr\\n', 23099: 'as)\\n', 23100: 'nonexecutive)', 23101: 'director)))\\n', 23102: '(np-tmp', 23103: 'nov', 23104: '29))))\\n', 23105: '.))\\n\\n\\n\\nwe', 23106: '.1\\nuses', 23107: 'sentential', 23108: 's,\\nthis', 23109: 'verbs\\nthat', 23110: 'expansion', 23111: 'filter(tree):\\n', 23112: 'child_nodes', 23113: '[child', 23114: 'tree\\n', 23115: 'isinstance(child,', 23116: '.tree)]\\n', 23117: '(tree', 23118: \"'vp')\", 23119: \"('s'\", 23120: 'child_nodes)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 23121: '[subtree', 23122: '.parsed_sents()\\n', 23123: '.subtrees(filter)]\\n', 23124: \"[tree('vp',\", 23125: \"[tree('vbn',\", 23126: \"['named']),\", 23127: \"tree('s',\", 23128: \"[tree('np-sbj',\", 23129: '.]),', 23130: '(code_sentential_complement', 23131: 'complements\\n\\nthe', 23132: '.ppattach\\nis', 23133: 'preposition\\nand', 23134: '.ppattach', 23135: \".attachments('training')\\n>>>\", 23136: 'defaultdict(set))\\n>>>', 23137: 'entries:\\n', 23138: '.noun1', 23139: '.prep', 23140: '.noun2\\n', 23141: 'table[key][entry', 23142: '.attachment]', 23143: '.add(entry', 23144: '.verb)\\n', 23145: 'sorted(table):\\n', 23146: 'len(table[key])', 23147: 'print(key,', 23148: \"'n:',\", 23149: \"sorted(table[key]['n']),\", 23150: \"'v:',\", 23151: \"sorted(table[key]['v']))\\n\\n\\n\\namongst\", 23152: 'find\\noffer-from-group', 23153: 'n:', 23154: \"['rejected']\", 23155: 'v:', 23156: \"['received'],\\nwhich\", 23157: 'received', 23158: 'separate\\npp', 23159: 'rejected', 23160: 'pe08\\ncross-framework', 23161: 'prepared', 23162: 'of\\ncomparing', 23163: 'downloading\\nthe', 23164: 'large_grammars', 23165: 'package\\n(e', 23166: '-m', 23167: '.downloader', 23168: 'large_grammars)', 23169: 'sinica', 23170: 'corpus,\\nconsisting', 23171: 'the\\nacademia', 23172: '.parsed_sents()[3450]', 23173: '\\n\\n\\n\\n\\n\\n\\n6', 23174: '.2\\xa0\\xa0\\xa0pernicious', 23175: 'ambiguity\\nunfortunately,', 23176: 'an\\nastronomical', 23177: 'rate', 23178: 'word\\nfish', 23179: 'sentence\\nfish', 23180: 'fish,', 23181: '.\\n(try', 23182: 'police', 23183: '.)\\nhere', 23184: 'sbar\\n', 23185: 'sbar', 23186: \"'fish'\\n\", 23187: ')\\n\\n\\n\\nnow', 23188: 'fish\\nfish,', 23189: 'amongst', 23190: \"'fish\", 23191: 'fish\\nfish', 23192: 'fishing', 23193: \"themselves'\", 23194: 'nltk\\nchart', 23195: '[fish]', 23196: '.chartparser(grammar)\\n>>>', 23197: '.parse(tokens):\\n', 23198: 'fish)', 23199: '(sbar', 23200: 'fish))))\\n(s', 23201: 'fish)))', 23202: 'fish))\\n\\n\\n\\nas', 23203: 'trees:\\n1;', 23204: '2;', 23205: '5;', 23206: '14;', 23207: '42;', 23208: '132;', 23209: '429;', 23210: '1,430;', 23211: '4,862;', 23212: '16,796;', 23213: '58,786;', 23214: '208,012;', 23215: '.\\n(these', 23216: 'exercise\\nin', 23217: '23,', 23218: 'length\\nof', 23219: 'sentence\\nof', 23220: '1012', 23221: 'parses,', 23222: 'sentence\\n(1),\\nwhich', 23223: 'effortlessly', 23224: 'either!\\nnote', 23225: '.\\n(church', 23226: 'patil,', 23227: '1982)', 23228: '(18)', 23229: 'catalan\\nnumbers', 23230: '(18)put', 23231: 'ambiguity;', 23232: 'ambiguity?\\nas', 23233: 'broad-coverage', 23234: 'we\\nare', 23235: 'of\\nspeech', 23236: 'is\\nonly', 23237: 'a\\nbroad-coverage', 23238: 'a),\\ndog', 23239: '(meaning', 23240: 'closely),', 23241: 'runs\\nis', 23242: 'runs)', 23243: 'be\\nreferred', 23244: \"'ate'\", 23245: 'spelled', 23246: 'three\\nletters;', 23247: 'overwhelmed', 23248: 'even\\ncomplete', 23249: 'gibberish', 23250: 'reading,', 23251: 'of\\ni', 23252: '(klavans', 23253: 'resnik,', 23254: 'salad', 23255: 'a\\ngrammatical', 23256: 'a\\nhundredth', 23257: 'hectare', 23258: 'sq', 23259: 'm),', 23260: 'are\\nnouns', 23261: 'designating', 23262: 'coordinates,', 23263: 'i:', 23264: 'drawing', 23265: 'paddocks,', 23266: 'being\\none', 23267: 'coordinates;\\nthe', 23268: '(after', 23269: 'abney)', 23270: '.\\n\\n\\neven', 23271: 'unlikely,', 23272: 'tree\\nfor', 23273: 'be\\nunambiguous,', 23274: 'other\\nreadings', 23275: 'anticipated', 23276: 'abney', 23277: 'explains)', 23278: 'this\\nambiguity', 23279: 'unavoidable,', 23280: 'horrendous', 23281: 'inefficiency', 23282: 'in\\nparsing', 23283: 'seemingly', 23284: 'innocuous', 23285: 'by\\nprobabilistic', 23286: 'rank\\nthe', 23287: '.3\\xa0\\xa0\\xa0weighted', 23288: 'grammar\\n\\nas', 23289: 'ambiguity\\nis', 23290: '.\\nchart', 23291: 'multiple\\nparses', 23292: 'sheer', 23293: 'weighted', 23294: 'and\\nprobabilistic', 23295: 'effective\\nsolution', 23296: 'notion', 23297: 'of\\ngrammaticality', 23298: 'gradient', 23299: 'given)\\nand', 23300: 'recipient)', 23301: '(19)', 23302: 'dative', 23303: 'in\\n(19a),', 23304: '(19)\\n', 23305: '.kim', 23306: 'bone', 23307: 'dog\\n\\n', 23308: 'bone\\n\\nin', 23309: '(19b),\\nthe', 23310: 'pronoun,', 23311: 'construction:\\n\\n', 23312: '(20)\\n', 23313: 'heebie-jeebies', 23314: '(*prepositional', 23315: 'dative)\\n\\n', 23316: '(double', 23317: 'object)\\n\\nusing', 23318: 'sample,', 23319: 'of\\nprepositional', 23320: 'involving\\ngive,', 23321: 'give(t):\\n', 23322: \"'vp'\", 23323: 't[1]', 23324: \"'np'\\\\\\n\", 23325: '(t[2]', 23326: \"'pp-dtv'\", 23327: 't[2]', 23328: \"'np')\\\\\\n\", 23329: \"('give'\", 23330: '.leaves()', 23331: \"'gave'\", 23332: '.leaves())\\ndef', 23333: 'sent(t):\\n', 23334: '.join(token', 23335: 'token[0]', 23336: \"'*-0')\\ndef\", 23337: 'print_node(t,', 23338: 'width):\\n', 23339: '%s:', 23340: '%\\\\\\n', 23341: '(sent(t[0]),', 23342: 'sent(t[1]),', 23343: 'sent(t[2]))\\n', 23344: 'len(output)', 23345: 'width:\\n', 23346: 'output[:width]', 23347: 'print(output)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 23348: '.parsed_sents():\\n', 23349: '.subtrees(give):\\n', 23350: '72)\\ngave', 23351: 'chefs', 23352: 'standing', 23353: 'ovation\\ngive', 23354: 'advertisers', 23355: 'discounts', 23356: 'ad', 23357: 'sp', 23358: 'pp-dtv:', 23359: 'politicians\\ngave', 23360: 'help\\ngive', 23361: 'np:\\ngive', 23362: 'french', 23363: 'federal', 23364: 'judges', 23365: 'raise\\ngive', 23366: 'consumers', 23367: 'straight', 23368: 'scoop', 23369: 'waste', 23370: 'crisis\\ngave', 23371: 'mitsui', 23372: 'high-tech', 23373: 'medical', 23374: 'product\\ngive', 23375: 'mitsubishi', 23376: 'glass', 23377: 'industry\\ngive', 23378: 'rates', 23379: 'receiving', 23380: 'foster', 23381: 'savings', 23382: 'institution', 23383: 'gift', 23384: 'suspend', 23385: 'trading', 23386: 'futu', 23387: '.\\ngave', 23388: 'quick', 23389: 'approval', 23390: '.18', 23391: 'billion', 23392: 'supplemental', 23393: 'appr', 23394: 'transportation', 23395: 'power\\ngive', 23396: 'heebie-jeebies\\ngive', 23397: 'holders', 23398: 'obligation', 23399: 'cal', 23400: 'thomas', 23401: '``', 23402: 'qualified', 23403: 'rating', 23404: 'line-item', 23405: 'veto', 23406: 'power\\n\\n\\nexample', 23407: '(code_give', 23408: 'sample\\n\\nwe', 23409: 'tendency', 23410: 'appear\\nfirst', 23411: 'like\\ngive', 23412: 'raise,', 23413: 'animacy', 23414: 'may\\nplay', 23415: 'contributing\\nfactors,', 23416: 'surveyed', 23417: '(bresnan', 23418: 'hay,', 23419: 'preferences', 23420: 'pcfg)', 23421: 'free\\ngrammar', 23422: 'associates', 23423: 'corresponding\\ncontext', 23424: 'pcfg', 23425: 'product\\nof', 23426: 'specially\\nformatted', 23427: 'productions,\\nwhere', 23428: '.pcfg', 23429: '[1', 23430: '.0]\\n', 23431: '[0', 23432: '.4]\\n', 23433: 'iv', 23434: '.3]\\n', 23435: 'datv', 23436: \"'telescopes'\", 23437: '.8]\\n', 23438: \"'jack'\", 23439: '.2]\\n', 23440: 'print(grammar)\\ngrammar', 23441: '(start', 23442: 's)\\n', 23443: '.2]\\n\\n\\nexample', 23444: '(code_pcfg1', 23445: '(pcfg)\\n\\nit', 23446: 'line,\\ne', 23447: '.4]', 23448: '.3]', 23449: 'a\\nprobability', 23450: 'impose', 23451: 'constraint\\nthat', 23452: 'have\\nprobabilities', 23453: 'obeys', 23454: 'constraint:', 23455: 's,\\nthere', 23456: '.0;', 23457: 'vp,\\n0', 23458: '.4+0', 23459: '.3+0', 23460: '.3=1', 23461: '.8+0', 23462: '.2=1', 23463: 'parse()', 23464: 'probabilities:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 23465: 'viterbi_parser', 23466: '.viterbiparser(grammar)\\n>>>', 23467: \".parse(['jack',\", 23468: \"'saw',\", 23469: \"'telescopes']):\\n\", 23470: 'jack)', 23471: '(tv', 23472: 'telescopes)))', 23473: '(p=0', 23474: '.064)\\n\\n\\n\\nnow', 23475: 'probabilities,', 23476: 'matters\\nthat', 23477: '.\\n\\n\\n\\n7\\xa0\\xa0\\xa0summary\\n\\nsentences', 23478: 'organization\\nthat', 23479: 'constituent\\nstructure', 23480: 'are:', 23481: 'heads,', 23482: 'characterization', 23483: 'sentences;\\nwe', 23484: 'grammar\\nuses', 23485: 'α1\\n', 23486: 'αn', 23487: 'dependents\\nare', 23488: '.\\nsyntactic', 23489: 'analysis\\n(e', 23490: 'ambiguity)', 23491: 'grammatically\\nwell-formed', 23492: 'recursively\\nexpands', 23493: 'grammar\\nproductions,', 23494: 'cannot\\nhandle', 23495: 'pp)', 23496: 'inefficient', 23497: 'expands\\ncategories', 23498: 'and\\nin', 23499: 'shifts', 23500: 'onto\\na', 23501: 'right\\nhand', 23502: 'exists,', 23503: 'substructure', 23504: 'without\\nchecking', 23505: 'globally', 23506: '.\\n\\n\\n\\n\\n8\\xa0\\xa0\\xa0further', 23507: 'the\\nparsing', 23508: 'a\\ngeneral', 23509: '(radford,', 23510: '1988)', 23511: 'a\\ngentle', 23512: 'be\\nrecommended', 23513: 'to\\nunbounded', 23514: 'used\\nterm', 23515: 'grammar,\\nthough', 23516: '(chomsky,', 23517: '1965)', 23518: 'x-bar', 23519: 'is\\ndue', 23520: '(jacobs', 23521: 'rosenbaum,', 23522: '1970),', 23523: 'explored', 23524: '(jackendoff,', 23525: '1977)\\n(the', 23526: 'primes', 23527: \"chomsky's\", 23528: 'typographically', 23529: 'demanding', 23530: '.)\\n(burton-roberts,', 23531: '1997)', 23532: 'practically', 23533: 'oriented', 23534: 'constituency', 23535: 'exemplification', 23536: '(huddleston', 23537: 'pullum,', 23538: 'up-to-date', 23539: 'of\\nsyntactic', 23540: 'phenomena', 23541: 'english;\\nsections', 23542: '.1-3', 23543: 'techniques\\nfor', 23544: 'ambiguity;\\nchapter', 23545: 'parsing;\\nchapter', 23546: 'chomsky', 23547: 'complexity\\nof', 23548: '.\\n(levin,', 23549: '1993)', 23550: 'classes,\\naccording', 23551: 'ongoing', 23552: 'large-scale', 23553: 'grammars,\\ne', 23554: 'lfg', 23555: 'http://www2', 23556: '.parc', 23557: '.com/istl/groups/nltt/pargram/,\\nthe', 23558: 'hpsg', 23559: '.delph-in', 23560: '.net/matrix/\\nand', 23561: '.edu/~xtag/', 23562: '.\\n\\n\\n9\\xa0\\xa0\\xa0exercises\\n\\n☼', 23563: 'never\\nbeen', 23564: 'before?', 23565: '(take', 23566: 'partner', 23567: 'you\\nabout', 23568: 'language?\\n\\n☼', 23569: 'prohibition', 23570: 'sentence-initial\\nhowever', 23571: '.\\ndo', 23572: 'construction?\\n\\n☼', 23573: 'kim', 23574: 'dana', 23575: 'everyone', 23576: 'cheered', 23577: 'and\\nand', 23578: 'interpretations', 23579: 'import\\nthe', 23580: 'help(tree)', 23581: 'phrase\\nold', 23582: 'women\\nencode', 23583: 'labeled\\nbracketing', 23584: '.tree()', 23585: 'draw()', 23586: 'the\\ndepth', 23587: 'have\\ndepth', 23588: 'depth\\nof', 23589: 'children,', 23590: 'milne', 23591: 'piglet,', 23592: 'underlining', 23593: 's\\n(e', 23594: 'when:lx`', 23595: '.\\ndraw', 23596: 'are\\nthe', 23597: 'long\\nsentence?\\n\\n☼', 23598: 'demo,', 23599: 'are\\nmore', 23600: 'length?\\n\\n☼', 23601: 'chart-parser', 23602: 'with\\ndifferent', 23603: 'invocation', 23604: 'strategies', 23605: 'strategy\\nthat', 23606: 'describe\\nthe', 23607: 'steps,', 23608: 'terms\\nof', 23609: 'chart)', 23610: 'grammar?', 23611: 'prospects', 23612: 'for\\nsignificant', 23613: 'boosts', 23614: 'cleverer', 23615: 'invocation\\nstrategies?\\n\\n☼', 23616: 'paper,', 23617: 'descent\\nparser', 23618: 'edges\\nfrom', 23619: 'why?\\n\\n☼', 23620: 'words:\\nbuffalo', 23621: 'buffalo', 23622: 'grammatically', 23623: 'at\\nhttp://en', 23624: '.org/wiki/buffalo_buffalo_buffalo_buffalo_buffalo_buffalo_buffalo_buffalo', 23625: 'page,', 23626: 'suitable\\ngrammar', 23627: 'listener', 23628: 'hearing\\nthis', 23629: 'sentence?\\nhow', 23630: 'longer?\\n(more', 23631: '.org/wiki/list_of_homophonous_phrases)', 23632: 'demo\\nby', 23633: 'change\\nthe', 23634: 'np\\npp', 23635: 'button,', 23636: 'happens?\\n\\n◑', 23637: 'as\\nintransitive,', 23638: 'pp\\ncomplement', 23639: 'the\\npreceding', 23640: 'lee', 23641: 'ran', 23642: 'home', 23643: 'tasks:\\n\\nwrite', 23644: 'verb\\nexhibits', 23645: 'attachments,', 23646: 'preposition,', 23647: 'stay', 23648: '.\\ndevise', 23649: 'parser\\ncompared', 23650: 'performance\\nusing', 23651: 'this)', 23652: 'top-down,', 23653: 'left-corner\\nparsers', 23654: 'test\\nsentences', 23655: 'each\\nparser', 23656: 'all\\nthree', 23657: '3-by-3', 23658: 'of\\ntimes,', 23659: 'totals', 23660: 'garden', 23661: 'computational\\nwork', 23662: 'with\\nprocessing', 23663: 'sentences?\\nhttp://en', 23664: '.org/wiki/garden_path_sentence\\n\\n◑', 23665: 'the\\ndraw_trees()', 23666: 'out:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 23667: '.draw', 23668: 'draw_trees\\n>>>', 23669: 'draw_trees(tree1,', 23670: 'tree2,', 23671: 'tree3)', 23672: '\\n\\n\\n\\n\\n◑', 23673: 'positions,', 23674: '100\\nsentences', 23675: 'treebank;', 23676: 'view,\\nlimit', 23677: 'subtrees', 23678: 'height', 23679: 'corpus\\nand', 23680: 'influence', 23681: 'claimed', 23682: 'regularities\\nthat', 23683: 'phrase\\nin', 23684: 'based\\non', 23685: 'n-grams?\\n\\nwhat', 23686: 'youngish', 23687: 'nikolay', 23688: 'parfenovich\\nalso', 23689: 'a\\nsincere', 23690: 'liking', 23691: 'discriminated-against', 23692: 'procurator', 23693: '.\\n(dostoevsky:', 23694: 'brothers', 23695: 'karamazov)\\n\\n\\n◑', 23696: 'bracketing', 23697: 'non-terminal\\nlabels', 23698: 'pierre\\nvinken', 23699: 'produce:\\n[[[nnp', 23700: 'nnp]np', 23701: '[adjp', 23702: '[cd', 23703: 'nns]np', 23704: 'jj]adjp', 23705: ',]np-sbj', 23706: '[vb', 23707: '[dt', 23708: 'nn]np', 23709: '[in', 23710: 'nn]np]pp-clr', 23711: '[nnp', 23712: 'cd]np-tmp]vp', 23713: '.]s\\nconsecutive', 23714: 'extremely', 23715: 'find?', 23716: 'construction(s)\\nare', 23717: 'sentences?\\n\\n◑', 23718: 'init_wfst()', 23719: 'complete_wfst()', 23720: 'of\\nnon-terminal', 23721: 'why\\nparsing', 23722: 'n3,', 23723: 'n\\nis', 23724: '.treebank\\nand', 23725: '.productions()', 23726: 'discard\\nthe', 23727: 'side,\\nand', 23728: 'collapsed,', 23729: 'but\\nmore', 23730: 'in\\nenglish', 23731: 'sibling', 23732: 'to\\nthis', 23733: 'subject?\\n\\n★', 23734: '.start()', 23735: 'grammar;\\ngrammar', 23736: '.productions(lhs)', 23737: 'grammar\\nthat', 23738: 'side;', 23739: '.rhs()', 23740: 'right-hand', 23741: 'backtracking,\\nso', 23742: 'ascent', 23743: 'backtracking\\nat', 23744: '.org/wiki/backtracking\\n\\n★\\nas', 23745: 'this\\nfor', 23746: 'gave,', 23747: 'patterns\\nsuch', 23748: 'following:\\n\\ngave', 23749: 'np\\ngave', 23750: 'up\\ngave', 23751: 'np\\n\\n\\nuse', 23752: 'complementation', 23753: 'verb\\nof', 23754: 'task\\nis', 23755: '.)\\nidentify', 23756: 'near-synonyms,', 23757: 'the\\ndumped/filled/loaded', 23758: 'verbs\\nbe', 23759: 'substituted', 23760: 'constraints?\\ndiscuss', 23761: 'inheriting', 23762: 'parsei', 23763: 'complete)', 23764: 'includes\\na', 23765: 'acst9', 23766: 'grammars\\n\\n\\n\\n\\n\\n9', 23767: 'grammars\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nnatural', 23768: 'gain\\nmore', 23769: 'flexibility,', 23770: 's,\\nnp', 23771: 'atomic', 23772: 'decompose', 23773: 'like\\ndictionaries,', 23774: 'to\\ngain', 23775: 'productions?\\nwhat', 23776: 'them\\ncomputationally?\\nwhat', 23777: 'capture\\nwith', 23778: 'grammars?\\n\\nalong', 23779: 'as\\nagreement,', 23780: 'subcategorization,', 23781: 'unbounded', 23782: '.\\n\\n1\\xa0\\xa0\\xa0grammatical', 23783: 'features\\nin', 23784: 'chap-data-intensive,', 23785: 'simple,', 23786: 'a\\nword,', 23787: 'predicted\\nby', 23788: 'extractors,', 23789: 'record\\nfeatures', 23790: 'detected,', 23791: 'to\\ndeclare', 23792: 'a\\nvery', 23793: \"{'cat':\", 23794: \"'np',\", 23795: \"'orth':\", 23796: \"'kim',\", 23797: \"'ref':\", 23798: \"'k'}\\n>>>\", 23799: \"'rel':\", 23800: \"'chase'}\\n\\n\\n\\nthe\", 23801: 'cat\\n(grammatical', 23802: 'category)', 23803: 'orth', 23804: '(orthography,', 23805: 'spelling)', 23806: 'each\\nhas', 23807: 'feature:', 23808: \"kim['ref']\", 23809: 'the\\nreferent', 23810: \"chase['rel']\", 23811: 'by\\nchase', 23812: 'pairings', 23813: 'and\\nvalues', 23814: 'alternative\\nnotations', 23815: '.\\nfeature', 23816: 'grammatical\\nentities', 23817: 'exhaustive,', 23818: 'further\\nproperties', 23819: 'what\\nsemantic', 23820: 'chase,\\nthe', 23821: 'plays', 23822: 'of\\npatient', 23823: \"'sbj'\", 23824: \"'obj'\", 23825: 'placeholders\\nwhich', 23826: 'filled', 23827: 'arguments:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 23828: \"chase['agt']\", 23829: \"'sbj'\\n>>>\", 23830: \"chase['pat']\", 23831: \"'obj'\\n\\n\\n\\nif\", 23832: 'lee,', 23833: 'bind', 23834: \"verb's\", 23835: 'patient', 23836: 'linking', 23837: 'the\\nref', 23838: 'the\\nsimple-minded', 23839: 'the\\nverb', 23840: 'feature\\nstructure', 23841: 'lee\\n>>>', 23842: \"'lee',\", 23843: \"'l'}\\n>>>\", 23844: 'lex2fs(word):\\n', 23845: 'fs', 23846: '[kim,', 23847: 'chase]:\\n', 23848: \"fs['orth']\", 23849: 'fs\\n>>>', 23850: 'subj,', 23851: 'obj', 23852: 'lex2fs(tokens[0]),', 23853: 'lex2fs(tokens[1]),', 23854: 'lex2fs(tokens[2])\\n>>>', 23855: \"verb['agt']\", 23856: \"subj['ref']\\n>>>\", 23857: \"verb['pat']\", 23858: \"obj['ref']\\n>>>\", 23859: \"['orth',\", 23860: \"'rel',\", 23861: \"'agt',\", 23862: \"'pat']:\\n\", 23863: 'print(%-5s', 23864: '=>', 23865: 'verb[k]))\\north', 23866: 'chased\\nrel', 23867: 'chase\\nagt', 23868: 'k\\npat', 23869: 'l\\n\\n\\n\\nthe', 23870: 'adopted', 23871: 'surprise,', 23872: '(src)', 23873: 'object,\\nthe', 23874: 'experiencer', 23875: '(exp):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 23876: \"'surprised',\", 23877: \"'surprise',\\n\", 23878: \"'src':\", 23879: \"'sbj',\", 23880: \"'exp':\", 23881: \"'obj'}\\n\\n\\n\\nfeature\", 23882: 'way\\nin', 23883: 'hoc', 23884: 'be\\nexpanded', 23885: 'this\\nin', 23886: 'generic', 23887: 'principled', 23888: 'the\\nphenomenon', 23889: 'agreement;', 23890: 'agreement\\nconstraints', 23891: 'elegantly', 23892: 'illustrate\\ntheir', 23893: 'kind,', 23894: 'briefly\\nlook', 23895: 'view,', 23896: 'feature\\nstructures', 23897: 'offered', 23898: 'expressiveness', 23899: 'opens\\nup', 23900: 'spectrum', 23901: 'sophisticated\\naspects', 23902: '.1\\xa0\\xa0\\xa0syntactic', 23903: 'agreement\\nthe', 23904: 'is\\ngrammatical', 23905: 'signal', 23906: '.)\\n\\n', 23907: '.this', 23908: '.*these', 23909: 'dog\\n\\n\\n', 23910: '.these', 23911: 'dogs\\n\\n', 23912: '.*this', 23913: 'dogs\\n\\nin', 23914: 'singular\\nor', 23915: 'demonstrative', 23916: 'varies:\\nthis', 23917: '(singular)', 23918: '(plural)', 23919: '(2b)', 23920: 'demonstratives', 23921: 'phrase:\\neither', 23922: 'similar\\nconstraint', 23923: 'predicates:\\n\\n', 23924: 'runs\\n\\n', 23925: 'run\\n\\n\\n', 23926: 'dogs', 23927: 'run\\n\\n', 23928: 'runs\\n\\n\\nhere', 23929: 'co-vary\\nwith', 23930: 'co-variance', 23931: 'that\\npresent', 23932: 'inflected', 23933: 'person\\nsingular,', 23934: 'number,\\nas', 23935: '.\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\nsingular\\nplural\\n\\n1st', 23936: 'per\\ni', 23937: 'run\\nwe', 23938: 'run\\n\\n2nd', 23939: 'per\\nyou', 23940: 'run\\nyou', 23941: 'run\\n\\n3rd', 23942: 'per\\nhe/she/it\\nruns\\nthey', 23943: 'run\\n\\n\\ntable', 23944: 'paradigm', 23945: 'verbs\\n\\n\\nwe', 23946: 'explicit\\nas', 23947: 'ex-runs', 23948: 'ex-run', 23949: 'agrees', 23950: 'sg', 23951: 'pl', 23952: '.)\\n\\nsystem', 23953: '(ch09', 23954: '252)\\nerror', 23955: 'directive:', 23956: '257)\\nerror', 23957: 'a\\ncontext-free', 23958: '(5)\\ns', 23959: 'vp\\nnp', 23960: 'n\\nvp', 23961: 'v\\n\\ndet', 23962: \"'this'\\nn\", 23963: \"'dog'\\nv\", 23964: \"'runs'\\n\\n\\ngrammar\", 23965: 'runs;\\nhowever,', 23966: 'dogs\\nrun', 23967: 'blocking', 23968: '*this', 23969: 'run\\nand', 23970: '*these', 23971: 'to\\nadd', 23972: 'grammar:\\n\\n', 23973: '(6)\\ns', 23974: 'np_sg', 23975: 'vp_sg\\ns', 23976: 'np_pl', 23977: 'vp_pl\\nnp_sg', 23978: 'det_sg', 23979: 'n_sg\\nnp_pl', 23980: 'det_pl', 23981: 'n_pl\\nvp_sg', 23982: 'v_sg\\nvp_pl', 23983: 'v_pl\\n\\ndet_sg', 23984: \"'this'\\ndet_pl\", 23985: \"'these'\\nn_sg\", 23986: \"'dog'\\nn_pl\", 23987: \"'dogs'\\nv_sg\", 23988: \"'runs'\\nv_pl\", 23989: \"'run'\\n\\n\\nin\", 23990: 'two\\nproductions,', 23991: 'subject\\nnps', 23992: 'vps,', 23993: 'plural\\nsubject', 23994: 'vps', 23995: 'counterparts', 23996: 'grammar,\\nthis', 23997: 'aesthetically\\nunappealing', 23998: 'reasonable\\nsubset', 23999: 'constructions,', 24000: 'doubling', 24001: 'grammar\\nsize', 24002: 'unattractive', 24003: 'same\\napproach', 24004: 'agreement,', 24005: 'for\\nboth', 24006: 'grammar\\nbeing', 24007: 'to\\navoid', 24008: 'show\\nthat', 24009: 'cost\\nof', 24010: 'blowing', 24011: '.2\\xa0\\xa0\\xa0using', 24012: 'constraints\\nwe', 24013: 'spoke', 24014: 'informally', 24015: 'properties;', 24016: \"let's\\nmake\", 24017: 'explicit:\\n\\n', 24018: '(7)\\nn[num=pl]\\n\\n\\nin', 24019: '(7),', 24020: '(grammatical)', 24021: '(short', 24022: \"for\\n'number')\", 24023: \"for\\n'plural')\", 24024: 'annotations', 24025: 'entries:\\n\\n', 24026: '(8)\\ndet[num=sg]', 24027: \"'this'\\ndet[num=pl]\", 24028: \"'these'\\n\\nn[num=sg]\", 24029: \"'dog'\\nn[num=pl]\", 24030: \"'dogs'\\nv[num=sg]\", 24031: \"'runs'\\nv[num=pl]\", 24032: \"'run'\\n\\n\\ndoes\", 24033: 'all?', 24034: 'more\\nverbose', 24035: 'become\\nmore', 24036: 'use\\nthese', 24037: 'constraints:\\n\\n', 24038: '(9)\\ns', 24039: 'np[num=?n]', 24040: 'vp[num=?n]\\nnp[num=?n]', 24041: 'det[num=?n]', 24042: 'n[num=?n]\\nvp[num=?n]', 24043: 'v[num=?n]\\n\\n\\nwe', 24044: '?n', 24045: 'num;', 24046: 'pl,', 24047: 'whatever\\nvalue', 24048: 'num,\\nvp', 24049: \"it's\\nhelpful\", 24050: 'lexical\\nproductions', 24051: 'admit', 24052: '(trees', 24053: 'of\\ndepth', 24054: 'one):\\n\\n', 24055: '(11)\\n', 24056: 'vp[num=?n]', 24057: 'num\\nvalues', 24058: 'n[num=?n]', 24059: 'will\\npermit', 24060: '(11a)', 24061: '(11b)', 24062: 'be\\ncombined,', 24063: '(13a)', 24064: '(13b)', 24065: 'are\\nprohibited', 24066: 'differ\\nin', 24067: 'feature;', 24068: 'incompatibility', 24069: 'is\\nindicated', 24070: '.\\n\\nproduction', 24071: 'v[num=?n]', 24072: 'says\\nthat', 24073: 'the\\nnum', 24074: 'for\\nexpanding', 24075: 'we\\nderive', 24076: 'head\\nnoun', 24077: \"vp's\\nhead\", 24078: '(14)\\ngrammar', 24079: 'this\\nand', 24080: 'noun\\nrespectively', 24081: 'choosy\\nabout', 24082: 'add\\ntwo', 24083: 'and\\nplural', 24084: 'the\\n\\ndet[num=sg]', 24085: \"'some'\", 24086: \"'any'\\ndet[num=pl]\", 24087: \"'any'\\n\\nhowever,\", 24088: 'to\\nleave', 24089: 'underspecified', 24090: 'letting', 24091: 'agree\\nin', 24092: 'variable\\nvalue', 24093: 'result:\\n\\ndet[num=?n]', 24094: \"'any'\\n\\nbut\", 24095: 'economical,', 24096: 'any\\nspecification', 24097: 'explicitly', 24098: 'another\\nvalue', 24099: '.\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 24100: \".show_cfg('grammars/book_grammars/feat0\", 24101: \".fcfg')\\n%\", 24102: 's\\n#', 24103: '###################\\n#', 24104: 'productions\\n#', 24105: 'productions\\ns', 24106: 'vp[num=?n]\\n#', 24107: 'productions\\nnp[num=?n]', 24108: 'n[num=?n]\\nnp[num=?n]', 24109: 'propn[num=?n]\\nnp[num=?n]', 24110: 'n[num=?n]\\nnp[num=pl]', 24111: 'n[num=pl]\\n#', 24112: 'productions\\nvp[tense=?t,', 24113: 'num=?n]', 24114: 'iv[tense=?t,', 24115: 'num=?n]\\nvp[tense=?t,', 24116: 'tv[tense=?t,', 24117: 'np\\n#', 24118: '###################\\ndet[num=sg]', 24119: \"'this'\", 24120: \"'every'\\ndet[num=pl]\", 24121: \"'these'\", 24122: \"'all'\\ndet\", 24123: \"'several'\\npropn[num=sg]->\", 24124: \"'kim'\", 24125: \"'jody'\\nn[num=sg]\", 24126: \"'girl'\", 24127: \"'car'\", 24128: \"'child'\\nn[num=pl]\", 24129: \"'dogs'\", 24130: \"'girls'\", 24131: \"'cars'\", 24132: \"'children'\\niv[tense=pres,\", 24133: 'num=sg]', 24134: \"'disappears'\", 24135: \"'walks'\\ntv[tense=pres,\", 24136: \"'sees'\", 24137: \"'likes'\\niv[tense=pres,\", 24138: 'num=pl]', 24139: \"'disappear'\", 24140: \"'walk'\\ntv[tense=pres,\", 24141: \"'see'\", 24142: \"'like'\\niv[tense=past]\", 24143: \"'disappeared'\", 24144: \"'walked'\\ntv[tense=past]\", 24145: \"'liked'\\n\\n\\nexample\", 24146: '(code_feat0cfg', 24147: 'grammar\\n\\nnotice', 24148: 'for\\nexample,\\nv[tense=pres,', 24149: '%start', 24150: 'directive', 24151: 'the\\nstart', 24152: 'grammar,\\nit', 24153: 'edited,\\ntested', 24154: \"named\\n'feat0\", 24155: \".fcfg'\", 24156: 'own\\ncopy', 24157: 'experimentation', 24158: '.load()', 24159: 'chart\\nparser', 24160: 'feature-based', 24161: 'load_parser', 24162: 'function\\n', 24163: 'a\\nchart', 24164: \"parser's\\nparse()\", 24165: 'trees;\\ntrees', 24166: 'and\\nwill', 24167: \"'kim\", 24168: 'likes', 24169: \"children'\", 24170: \"load_parser('grammars/book_grammars/feat0\", 24171: \".fcfg',\", 24172: '.\\n|', 24173: '.like', 24174: '.chil', 24175: '.|\\nleaf', 24176: 'init', 24177: 'rule:\\n|[----]', 24178: '.|', 24179: '[0:1]', 24180: \"'kim'\\n|\", 24181: '[----]', 24182: '[1:2]', 24183: \"'likes'\\n|\", 24184: '[----]|', 24185: '[2:3]', 24186: \"'children'\\nfeature\", 24187: \"propn[num='sg']\", 24188: '*\\nfeature', 24189: \"np[num='sg']\", 24190: 'rule:\\n|[---->', 24191: 's[]', 24192: '{?n:', 24193: \"'sg'}\\nfeature\", 24194: 'rule:\\n|', 24195: \"tv[num='sg',\", 24196: \"tense='pres']\", 24197: \"'likes'\", 24198: '[---->', 24199: 'vp[num=?n,', 24200: 'tense=?t]', 24201: 'tv[num=?n,', 24202: 'np[]', 24203: \"'sg',\", 24204: '?t:', 24205: \"'pres'}\\nfeature\", 24206: \"n[num='pl']\", 24207: \"'children'\", 24208: \"np[num='pl']\", 24209: '[---->|', 24210: \"'pl'}\\nfeature\", 24211: '[---------]|', 24212: '[1:3]', 24213: \"vp[num='sg',\", 24214: 'rule:\\n|[==============]|', 24215: '[0:3]', 24216: \"vp[num='sg']\", 24217: '*\\n(s[]\\n', 24218: \"(np[num='sg']\", 24219: \"(propn[num='sg']\", 24220: 'kim))\\n', 24221: \"(vp[num='sg',\", 24222: \"tense='pres']\\n\", 24223: \"(tv[num='sg',\", 24224: 'likes)\\n', 24225: \"(np[num='pl']\", 24226: \"(n[num='pl']\", 24227: 'children))))\\n\\n\\nexample', 24228: '(code_featurecharttrace', 24229: 'parser\\n\\nthe', 24230: 'for\\npresent', 24231: 'which\\nbears', 24232: 'approach\\nto', 24233: 'compile\\nout', 24234: 'admissible', 24235: 'by\\ncontrast,', 24236: 'the\\nunderspecified', 24237: 'flow\\nupwards', 24238: 'associated\\nwith', 24239: 'bindings', 24240: 'dictionaries)', 24241: \"{?n:\\n'sg',\", 24242: \"'pres'}\", 24243: 'assembles', 24244: 'building,', 24245: 'to\\ninstantiate', 24246: 'nodes;', 24247: 'underspecified\\nvp[num=?n,', 24248: 'becomes\\ninstantiated', 24249: \"tv[num='sg',\\ntense='pres']\", 24250: '?t', 24251: 'one)', 24252: 'trees:', 24253: 'print(tree)\\n(s[]\\n', 24254: 'children))))\\n\\n\\n\\n\\n\\n1', 24255: '.3\\xa0\\xa0\\xa0terminology\\nso', 24256: 'and\\npl', 24257: 'atomic\\n—', 24258: 'decomposed', 24259: 'subparts', 24260: 'special\\ncase', 24261: 'that\\njust', 24262: 'as\\ncan,', 24263: 'may,', 24264: 'feature\\naux', 24265: 'v[tense=pres,', 24266: 'aux=+]', 24267: \"'can'\\nmeans\", 24268: 'pres', 24269: 'and\\n+', 24270: 'aux', 24271: 'adopted\\nconvention', 24272: 'abbreviates', 24273: 'boolean\\nfeatures', 24274: 'f;', 24275: 'aux=+', 24276: 'aux=-,', 24277: '+aux', 24278: 'and\\n-aux', 24279: 'any\\nother', 24280: '(15)', 24281: 'productions:\\n\\n', 24282: '(15)\\nv[tense=pres,', 24283: '+aux]', 24284: \"'can'\\nv[tense=pres,\", 24285: \"'may'\\n\\nv[tense=pres,\", 24286: '-aux]', 24287: \"'walks'\\nv[tense=pres,\", 24288: \"'likes'\\n\\n\\nwe\", 24289: 'attaching', 24290: 'to\\nsyntactic', 24291: 'radical', 24292: 'category\\n—', 24293: '—\\nas', 24294: 'bundle', 24295: 'n[num=sg]', 24296: 'speech\\ninformation', 24297: 'as\\npos=n', 24298: 'therefore\\nis', 24299: '[pos=n,', 24300: 'atomic-valued', 24301: 'group\\ntogether', 24302: 'a\\ndistinguished', 24303: 'agr', 24304: 'case,\\nwe', 24305: '(16)', 24306: 'depicts', 24307: 'format\\nknown', 24308: '(avm)', 24309: '(16)\\n[pos', 24310: ']\\n[', 24311: ']\\n[agr', 24312: '[per', 24313: ']]\\n[', 24314: '[num', 24315: '[gnd', 24316: 'fem', 24317: ']]\\n\\n\\n\\n\\nfigure', 24318: 'matrix\\n\\nin', 24319: 'approaches\\nfor', 24320: 'avms;', 24321: '.\\nathough', 24322: 'rendered', 24323: 'less\\nvisually', 24324: 'pleasing,', 24325: 'stick', 24326: 'it\\ncorresponds', 24327: '.\\n\\non', 24328: 'no\\nparticular', 24329: 'significance', 24330: 'to:\\n\\n', 24331: '(17)\\n[agr', 24332: ']\\n[pos', 24333: ']\\n\\n\\nonce', 24334: 'possibility', 24335: 'agr,', 24336: 'are\\nbundled', 24337: '(18)\\ns', 24338: 'np[agr=?n]', 24339: 'vp[agr=?n]\\nnp[agr=?n]', 24340: 'propn[agr=?n]\\nvp[tense=?t,', 24341: 'agr=?n]', 24342: 'cop[tense=?t,', 24343: 'adj\\n\\ncop[tense=pres,', 24344: 'agr=[num=sg,', 24345: 'per=3]]', 24346: \"'is'\\npropn[agr=[num=sg,\", 24347: \"'kim'\\nadj\", 24348: \"'happy'\\n\\n\\n\\n\\n\\n2\\xa0\\xa0\\xa0processing\", 24349: 'structures\\nin', 24350: 'be\\nconstructed', 24351: 'the\\nfundamental', 24352: 'the\\ninformation', 24353: 'the\\nfeatstruct()', 24354: 'or\\nintegers', 24355: 'fs1', 24356: \".featstruct(tense='past',\", 24357: \"num='sg')\\n>>>\", 24358: 'print(fs1)\\n[', 24359: \"'sg'\", 24360: \"'past'\", 24361: ']\\n\\n\\n\\na', 24362: 'dictionary,\\nand', 24363: '.featstruct(per=3,', 24364: \"num='pl',\", 24365: \"gnd='fem')\\n>>>\", 24366: \"print(fs1['gnd'])\\nfem\\n>>>\", 24367: \"fs1['case']\", 24368: \"'acc'\\n\\n\\n\\nwe\", 24369: 'as\\ndiscussed', 24370: 'fs2', 24371: \".featstruct(pos='n',\", 24372: 'agr=fs1)\\n>>>', 24373: 'print(fs2)\\n[', 24374: \"'acc'\", 24375: 'gnd', 24376: \"'fem'\", 24377: \"'pl'\", 24378: ']\\n>>>', 24379: \"print(fs2['agr'])\\n[\", 24380: \"print(fs2['agr']['per'])\\n3\\n\\n\\n\\nan\", 24381: 'feature-value', 24382: 'format\\nfeature=value,', 24383: 'structures:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 24384: \".featstruct([pos='n',\", 24385: 'agr=[per=3,', 24386: \"gnd='fem']]))\\n[\", 24387: ']\\n\\n\\n\\nfeature', 24388: 'inherently', 24389: 'objects;', 24390: 'are\\ngeneral', 24391: 'structure:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 24392: \".featstruct(name='lee',\", 24393: \"telno='01\", 24394: '42', 24395: \"96',\", 24396: 'age=33))\\n[', 24397: \"'lee'\", 24398: 'telno', 24399: \"'01\", 24400: \"96'\", 24401: ']\\n\\n\\n\\nin', 24402: 'this\\nto', 24403: 'divert', 24404: 'language,\\nbut', 24405: 'lay', 24406: 'groundwork', 24407: 'tight!\\nit', 24408: 'graphs;', 24409: 'acyclic', 24410: '(dags)', 24411: '.\\n(19)', 24412: 'avm', 24413: '(19)\\nthe', 24414: 'arcs,', 24415: '.\\njust', 24416: 'complex:\\n\\n', 24417: '(20)\\nwhen', 24418: 'graphs,', 24419: 'of\\npaths', 24420: 'arcs\\nthat', 24421: 'as\\ntuples', 24422: \"('address',\", 24423: \"'street')\", 24424: 'value\\nin', 24425: '(20)', 24426: \"'rue\", 24427: \"pascal'\", 24428: 'spouse', 24429: \"and\\nkim's\", 24430: \"lee's\", 24431: '(21)', 24432: '(21)\\nhowever,', 24433: 'address\\ninformation', 24434: 'same\\nsub-graph', 24435: 'arcs:\\n\\n', 24436: '(22)\\nin', 24437: \"('address')\", 24438: '(22)', 24439: \"('spouse',\", 24440: \"'address')\", 24441: 'dags\\nsuch', 24442: 'sharing', 24443: 'or\\nreentrancy', 24444: 'reentrancy', 24445: 'matrix-style', 24446: 'representations,', 24447: 'will\\nprefix', 24448: 'structure\\nwith', 24449: 'notation\\n->(1),', 24450: \".featstruct([name='lee',\", 24451: 'address=(1)[number=74,', 24452: \"street='rue\", 24453: \"pascal'],\\n\", 24454: \"spouse=[name='kim',\", 24455: 'address->(1)]]))\\n[', 24456: ']\\n\\n\\n\\nthe', 24457: 'a\\ncoindex', 24458: \".featstruct([a='a',\", 24459: \"b=(1)[c='c'],\", 24460: 'd->(1),', 24461: 'e->(1)]))\\n[', 24462: \"'c'\", 24463: ']\\n\\n\\n\\n\\n\\n\\n2', 24464: '.1\\xa0\\xa0\\xa0subsumption', 24465: 'unification\\nit', 24466: 'partial\\ninformation', 24467: 'order\\nfeature', 24468: 'example,\\n(23a)', 24469: '(23b),', 24470: '(23c)', 24471: '(23)\\n', 24472: '.\\n[number', 24473: '74]\\n\\n\\n\\n', 24474: ']\\n[street', 24475: \"pascal']\\n\\n\\n\\n\", 24476: \"pascal']\\n[city\", 24477: \"'paris'\", 24478: ']\\n\\n\\n\\nthis', 24479: 'subsumption;', 24480: 'fs0', 24481: 'subsumes', 24482: '⊑', 24483: 'subsumption', 24484: 'reentrancy,', 24485: 'careful\\nabout', 24486: 'subsumption:', 24487: 'if\\nfs0', 24488: 'fs1,', 24489: 'the\\npaths', 24490: 'reentrancies', 24491: 'subsumes\\n(22),', 24492: 'on\\nfeature', 24493: 'are\\nincommensurable', 24494: '(24)', 24495: 'subsumed\\nby', 24496: '(23a)', 24497: '(24)\\n[telno', 24498: '01', 24499: '96]\\n\\n\\nso', 24500: 'structure?\\nfor', 24501: 'should\\nconsist', 24502: 'a\\ncity', 24503: '(25b)', 24504: '(25a)', 24505: 'to\\nyield', 24506: '(25c)', 24507: '(25)\\n', 24508: '.\\n\\nmerging', 24509: 'called\\nunification', 24510: 'unify()', 24511: '.featstruct(number=74,', 24512: \"pascal')\\n>>>\", 24513: \".featstruct(city='paris')\\n>>>\", 24514: 'print(fs1', 24515: '.unify(fs2))\\n[', 24516: ']\\n\\n\\n\\nunification', 24517: 'formally', 24518: '(partial)', 24519: 'operation:\\nfs0', 24520: '⊔\\nfs1', 24521: '.\\nunification', 24522: 'symmetric,', 24523: 'so\\nfs0', 24524: '⊔\\nfs0', 24525: 'print(fs2', 24526: '.unify(fs1))\\n[', 24527: ']\\n\\n\\n\\n\\n\\nif', 24528: 'unify', 24529: 'subsumption\\nrelationship,', 24530: 'unification', 24531: 'two:\\n\\n', 24532: '(26)if', 24533: 'fs0\\n⊔', 24534: 'fs1\\nfor', 24535: 'unifying', 24536: '(23b)', 24537: 'π,\\nbut', 24538: 'π', 24539: 'distinct\\natom', 24540: \".featstruct(a='a')\\n>>>\", 24541: \".featstruct(a='b')\\n>>>\", 24542: '.unify(fs1)\\n>>>', 24543: 'print(fs2)\\nnone\\n\\n\\n\\nnow,', 24544: 'interacts', 24545: 'structure-sharing,\\nthings', 24546: '.featstruct([name=lee,\\n', 24547: 'address=[number=74,\\n', 24548: 'spouse=', 24549: '[name=kim,\\n', 24550: \"pascal']]])\\n>>>\", 24551: 'print(fs0)\\n[', 24552: ']\\n\\n\\n\\nwhat', 24553: 'specification\\nfor', 24554: 'city?', 24555: '.featstruct([spouse', 24556: '[address', 24557: '[city', 24558: 'paris]]])\\n>>>', 24559: '.unify(fs0))\\n[', 24560: ']\\n\\n\\n\\nby', 24561: 'unified', 24562: 'structure-sharing', 24563: 'graph\\n(22)):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 24564: '.featstruct([name=lee,', 24565: 'spouse=[name=kim,', 24566: 'address->(1)]])\\n>>>', 24567: ']\\n\\n\\n\\nrather', 24568: 'address,\\nwe', 24569: 'some\\npath', 24570: 'π,', 24571: 'stated\\nusing', 24572: '?x', 24573: '.featstruct([address1=[number=74,', 24574: \"pascal']])\\n>>>\", 24575: '.featstruct([address1=?x,', 24576: 'address2=?x])\\n>>>', 24577: 'address1', 24578: 'address2', 24579: ']\\n\\n\\n\\n\\n\\n\\n3\\xa0\\xa0\\xa0extending', 24580: 'grammar\\nin', 24581: 'explore\\na', 24582: 'issues,', 24583: 'benefits\\nof', 24584: '.1\\xa0\\xa0\\xa0subcategorization\\nin', 24585: 'augmented', 24586: 'labels\\niv', 24587: 'intransitive', 24588: 'verbs\\nrespectively', 24589: 'the\\nfollowing:\\n\\n', 24590: '(27)\\nvp', 24591: 'iv\\nvp', 24592: 'np\\n\\n\\nalthough', 24593: 'v,\\nthey', 24594: 'nonterminal', 24595: 'distinct\\nfrom', 24596: \"doesn't\\nlet\", 24597: 'say\\nall', 24598: 'tense,\\nsince', 24599: 'walk,', 24600: 'iv,', 24601: 'iv\\nby', 24602: 'object\\nor', 24603: 'complement?\\na', 24604: 'framework\\ncalled', 24605: 'generalized', 24606: '(gpsg),', 24607: 'solve\\nthis', 24608: 'lexical\\ncategories', 24609: 'subcat', 24610: 'subcategorization\\nclass', 24611: 'gpsg', 24612: 'for\\nsubcat,', 24613: 'adopts', 24614: 'mnemonic', 24615: 'namely\\nintrans,', 24616: 'trans', 24617: 'clause:\\n\\n', 24618: '(28)\\nvp[tense=?t,', 24619: 'v[subcat=intrans,', 24620: 'tense=?t,', 24621: 'v[subcat=trans,', 24622: 'np\\nvp[tense=?t,', 24623: 'v[subcat=clause,', 24624: 'sbar\\n\\nv[subcat=intrans,', 24625: 'tense=pres,', 24626: \"'walks'\\nv[subcat=trans,\", 24627: \"'likes'\\nv[subcat=clause,\", 24628: \"'says'\", 24629: \"'claims'\\n\\nv[subcat=intrans,\", 24630: \"'walk'\\nv[subcat=trans,\", 24631: \"'like'\\nv[subcat=clause,\", 24632: \"'say'\", 24633: \"'claim'\\n\\nv[subcat=intrans,\", 24634: 'tense=past,', 24635: \"'walked'\\nv[subcat=trans,\", 24636: \"'liked'\\nv[subcat=clause,\", 24637: \"'claimed'\\n\\n\\nwhen\", 24638: 'v[subcat=trans],', 24639: 'can\\ninterpret', 24640: 'pointer', 24641: 'in\\nwhich', 24642: 'v[subcat=trans]', 24643: 'a\\nvp', 24644: 'correspondence', 24645: 'lexical\\nheads', 24646: 'lexical\\ncategories;', 24647: 'subcat\\nvalue', 24648: 'required,', 24649: 'walk', 24650: 'belong', 24651: 'in\\nvps', 24652: 'subcat=intrans\\non', 24653: 'a\\nsubcat=trans', 24654: 'category\\nsbar', 24655: 'the\\ncomplement', 24656: 'like\\nchildren', 24657: '(29)\\nsbar', 24658: 'comp', 24659: 's\\ncomp', 24660: \"'that'\\n\\n\\nthe\", 24661: '(30)\\nan', 24662: 'framework\\nknown', 24663: 'categorial', 24664: 'patr\\nand', 24665: 'using\\nsubcat', 24666: 'encodes', 24667: 'of\\narguments', 24668: 'with)', 24669: 'like\\nput', 24670: '(put', 24671: 'table)', 24672: '(31):\\n\\n\\n', 24673: '(31)\\nv[subcat=<np,', 24674: 'pp>]\\n\\n\\nthis', 24675: 'everything\\nelse', 24676: 'comprises', 24677: 'the\\nsubcategorized-for', 24678: 'combined\\nwith', 24679: 'complements,', 24680: 'requirements', 24681: 'discharged,', 24682: 'is\\nneeded', 24683: 'traditionally\\nthought', 24684: '(32)\\nv[subcat=<np>]\\n\\n\\nfinally,', 24685: 'verbal', 24686: 'no\\nrequirements', 24687: 'hence', 24688: 'subcat\\nwhose', 24689: '(33)', 24690: 'these\\ncategory', 24691: '(33)\\n\\n\\n3', 24692: '.2\\xa0\\xa0\\xa0heads', 24693: 'revisited\\n\\nwe', 24694: 'factoring', 24695: 'subcategorization\\ninformation', 24696: 'more\\ngeneralizations', 24697: 'this\\nkind', 24698: 'following:', 24699: 'of\\nphrases', 24700: 'similarly,\\nns', 24701: 'nps,\\nas', 24702: 'adjectives)', 24703: 'aps,', 24704: 'and\\nps', 24705: 'prepositions)', 24706: 'pps', 24707: '.\\nnot', 24708: 'coordinate\\nphrases', 24709: 'bell)', 24710: '—\\nnevertheless,', 24711: 'formalism', 24712: 'head-child', 24713: 'features\\n(as', 24714: 'tv)', 24715: '.\\nx-bar', 24716: 'addresses\\nthis', 24717: 'abstracting', 24718: 'phrasal', 24719: 'is\\nusual', 24720: \"n'\", 24721: 'up,\\ncorresponding', 24722: 'nom,', 24723: \"while\\nn''\", 24724: '(34a)', 24725: 'a\\nrepresentative', 24726: '(34b)', 24727: 'counterpart', 24728: '(34)\\n', 24729: \"n'\\nand\", 24730: \"n''\", 24731: '(phrasal)', 24732: 'projections', 24733: \"n''\\nis\", 24734: 'maximal', 24735: 'projection,', 24736: 'the\\nzero', 24737: 'projection', 24738: 'that\\ndirectly', 24739: 'always\\nplaced', 24740: 'are\\nplaced', 24741: \"x'\", 24742: 'the\\nconfiguration', 24743: \"p''\", 24744: '(35)', 24745: 'contrasts', 24746: 'that\\nof', 24747: '(35)\\nthe', 24748: '(36)', 24749: 'encoded\\nusing', 24750: 'is\\nachieved', 24751: 'n[bar=1]', 24752: '(36)\\ns', 24753: 'n[bar=2]', 24754: 'v[bar=2]\\nn[bar=2]', 24755: 'n[bar=1]\\nn[bar=1]', 24756: 'p[bar=2]\\nn[bar=1]', 24757: 'n[bar=0]', 24758: 'n[bar=0]xs\\n\\n\\n\\n\\n3', 24759: '.3\\xa0\\xa0\\xa0auxiliary', 24760: 'inversion\\ninverted', 24761: 'is\\nswitched', 24762: 'interrogatives', 24763: \"after\\n'negative'\", 24764: 'adverbs:\\n\\n', 24765: '(37)\\n', 24766: '.do', 24767: 'children?\\n\\n', 24768: '.can', 24769: 'jody', 24770: 'walk?\\n\\n\\n', 24771: '(38)\\n', 24772: '.rarely', 24773: '.never', 24774: 'pre-subject', 24775: 'position:\\n\\n', 24776: '(39)\\n', 24777: '.*like', 24778: '.*walks', 24779: 'jody?\\n\\n\\n', 24780: '(40)\\n', 24781: '.*rarely', 24782: '.*never', 24783: '.\\n\\nverbs', 24784: 'positioned', 24785: 'auxiliaries,', 24786: 'do,\\ncan', 24787: 'and\\nshall', 24788: 'production:\\n\\n', 24789: '(41)\\ns[+inv]', 24790: 'v[+aux]', 24791: 'vp\\n\\n\\nthat', 24792: '[+inv]', 24793: 'auxiliary\\nverb', 24794: 'depending\\non', 24795: '(42)', 24796: 'an\\ninverted', 24797: '(42)\\n\\n\\n3', 24798: '.4\\xa0\\xa0\\xa0unbounded', 24799: 'constructions\\nconsider', 24800: 'contrasts:\\n\\n', 24801: '(43)\\n', 24802: '.you', 24803: '.*you', 24804: '(44)\\n', 24805: 'slot', 24806: 'complement,', 24807: 'while\\nput', 24808: '.\\n(43)', 24809: '(44)', 24810: 'obligatory:\\nomitting', 24811: 'ungrammaticality', 24812: '(45)', 24813: '(46)\\nillustrate', 24814: '(45)\\n', 24815: 'music,', 24816: '(46)\\n', 24817: '.which', 24818: 'slot?\\n\\n', 24819: 'into?\\n\\nthat', 24820: 'word\\nwho', 24821: '(45a),', 24822: 'preposed', 24823: 'music', 24824: '(45b),', 24825: 'card/slot', 24826: '(46)', 24827: 'to\\nsay', 24828: '–', 24829: 'where\\nthe', 24830: 'are\\nsometimes', 24831: 'underscore:\\n\\n', 24832: '(47)\\n', 24833: '__', 24834: '__?\\n\\nso,', 24835: 'gap', 24836: 'conversely,\\nfillers', 24837: '(48)\\n', 24838: '.*kim', 24839: 'hip-hop', 24840: '(49)\\n', 24841: '.*which', 24842: 'one?\\n\\nthe', 24843: 'co-occurence', 24844: 'termed', 24845: 'considerable', 24846: 'theoretical\\nlinguistics', 24847: 'nature', 24848: 'intervene\\nbetween', 24849: 'licenses;', 24850: 'answer\\nis', 24851: 'no:', 24852: 'and\\ngap', 24853: 'involving\\nsentential', 24854: '(50)', 24855: '(50)\\n', 24856: '.who', 24857: '__?\\n\\n', 24858: '__?\\n\\nsince', 24859: 'sentential\\ncomplements,', 24860: 'whole\\nsentence', 24861: 'constellation', 24862: 'an\\nunbounded', 24863: 'construction;', 24864: 'filler-gap\\ndependency', 24865: 'between\\nfiller', 24866: 'mechanisms', 24867: 'unbounded\\ndependencies', 24868: 'grammars;', 24869: 'to\\ngeneralized', 24870: 'involves\\nslash', 24871: 'y/xp;\\nwe', 24872: 'that\\nis', 24873: 'sub-constituent', 24874: 'xp', 24875: 'example,\\ns/np', 24876: 'of\\nslash', 24877: '(51)', 24878: '(51)\\nthe', 24879: '(treated', 24880: 'np[+wh])', 24881: 'a\\ncorresponding', 24882: 'gap-containing', 24883: 's/np', 24884: 'is\\nthen', 24885: 'percolated', 24886: 'vp/np', 24887: 'it\\nreaches', 24888: 'np/np', 24889: 'dependency\\nis', 24890: 'discharged', 24891: 'string,\\nimmediately', 24892: 'dominated', 24893: 'of\\nobject?', 24894: 'fortunately,', 24895: 'framework,\\nby', 24896: 'right\\nas', 24897: 'value;', 24898: 'reducible', 24899: 's[slash=np]', 24900: 'practice,\\nthis', 24901: 'illustrates\\nthe', 24902: 'principles', 24903: 'for\\ninverted', 24904: \".show_cfg('grammars/book_grammars/feat1\", 24905: '###################\\ns[-inv]', 24906: 'vp\\ns[-inv]/?x', 24907: 'vp/?x\\ns[-inv]', 24908: 's/np\\ns[-inv]', 24909: 'adv[+neg]', 24910: 's[+inv]\\ns[+inv]', 24911: 'vp\\ns[+inv]/?x', 24912: 'vp/?x\\nsbar', 24913: 's[-inv]\\nsbar/?x', 24914: 's[-inv]/?x\\nvp', 24915: '-aux]\\nvp', 24916: 'np\\nvp/?x', 24917: 'np/?x\\nvp', 24918: 'sbar\\nvp/?x', 24919: 'sbar/?x\\nvp', 24920: 'vp\\nvp/?x', 24921: 'vp/?x\\n#', 24922: '###################\\nv[subcat=intrans,', 24923: \"'walk'\", 24924: \"'sing'\\nv[subcat=trans,\", 24925: \"'claim'\\nv[+aux]\", 24926: \"'do'\", 24927: \"'can'\\nnp[-wh]\", 24928: \"'you'\", 24929: \"'cats'\\nnp[+wh]\", 24930: \"'who'\\nadv[+neg]\", 24931: \"'rarely'\", 24932: \"'never'\\nnp/np\", 24933: '->\\ncomp', 24934: \"'that'\\n\\n\\nexample\", 24935: '(code_slashcfg', 24936: 'and\\nlong-distance', 24937: 'dependencies,', 24938: 'categories\\n\\nthe', 24939: 'gap-introduction\\nproduction,', 24940: 's[-inv]', 24941: 'percolate', 24942: 'add\\nslashes', 24943: 'productions\\nthat', 24944: 'vp/?x', 24945: 'sbar/?x', 24946: 'slashed', 24947: 'and\\nsays', 24948: 'a\\nconstituent', 24949: 'sbar\\nchild', 24950: 'you\\nlike\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 24951: \"like'\", 24952: 'load_parser\\n>>>', 24953: \"load_parser('grammars/book_grammars/feat1\", 24954: \".fcfg')\\n>>>\", 24955: 'print(tree)\\n(s[-inv]\\n', 24956: '(np[+wh]', 24957: 'who)\\n', 24958: '(s[+inv]/np[]\\n', 24959: '(v[+aux]', 24960: 'do)\\n', 24961: '(np[-wh]', 24962: 'you)\\n', 24963: '(vp[]/np[]\\n', 24964: '(v[-aux,', 24965: \"subcat='clause']\", 24966: 'claim)\\n', 24967: '(sbar[]/np[]\\n', 24968: '(comp[]', 24969: 'that)\\n', 24970: '(s[-inv]/np[]\\n', 24971: '(vp[]/np[]', 24972: \"subcat='trans']\", 24973: 'like)', 24974: '(np[]/np[]', 24975: ')))))))\\n\\n\\n\\n\\na', 24976: '(52)', 24977: '(52)\\nthe', 24978: 'sentences\\nwithout', 24979: 'gaps:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 24980: \"cats'\", 24981: '(vp[]\\n', 24982: '(sbar[]\\n', 24983: '(s[-inv]\\n', 24984: '(vp[]', 24985: 'cats))))))\\n\\n\\n\\nin', 24986: 'admits', 24987: 'involve\\nwh', 24988: 'constructions:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 24989: \"'rarely\", 24990: \"sing'\", 24991: '(adv[+neg]', 24992: 'rarely)\\n', 24993: '(s[+inv]\\n', 24994: \"subcat='intrans']\", 24995: 'sing))))\\n\\n\\n\\n\\n\\n3', 24996: '.5\\xa0\\xa0\\xa0case', 24997: 'german\\ncompared', 24998: 'for\\nagreement', 24999: 'varies', 25000: 'with\\ncase,', 25001: '.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ncase\\nmasc\\nfem\\nneut\\nplural\\n\\nnom\\nder\\ndie\\ndas\\ndie\\n\\ngen\\ndes\\nder\\ndes\\nder\\n\\ndat\\ndem\\nder\\ndem\\nden\\n\\nacc\\nden\\ndie\\ndas\\ndie\\n\\n\\ntable', 25002: 'article\\n\\n\\nsubjects', 25003: 'nominative', 25004: 'verbs\\ngovern', 25005: 'accusative', 25006: 'are\\nexceptions', 25007: 'helfen', 25008: 'case:\\n\\n', 25009: '(53)\\nsystem', 25010: '1693)\\nerror', 25011: '1698)\\nerror', 25012: '1702)\\nerror', 25013: '1707)\\nerror', 25014: 'interaction', 25015: 'agreement\\n(comprising', 25016: \".show_cfg('grammars/book_grammars/german\", 25017: 's\\n', 25018: 'np[case=nom,', 25019: 'agr=?a]', 25020: 'vp[agr=?a]\\n', 25021: 'np[case=?c,', 25022: 'pro[case=?c,', 25023: 'agr=?a]\\n', 25024: 'det[case=?c,', 25025: 'n[case=?c,', 25026: 'vp[agr=?a]', 25027: 'iv[agr=?a]\\n', 25028: 'tv[objcase=?c,', 25029: 'np[case=?c]\\n', 25030: 'determiners\\n', 25031: 'masc\\n', 25032: 'det[case=nom,', 25033: 'agr=[gnd=masc,per=3,num=sg]]', 25034: \"'der'\\n\", 25035: 'det[case=dat,', 25036: \"'dem'\\n\", 25037: 'det[case=acc,', 25038: \"'den'\\n\", 25039: 'fem\\n', 25040: 'agr=[gnd=fem,per=3,num=sg]]', 25041: \"'die'\\n\", 25042: 'agr=[per=3,num=pl]]', 25043: 'n[agr=[gnd=masc,per=3,num=sg]]', 25044: \"'hund'\\n\", 25045: 'n[case=nom,', 25046: 'agr=[gnd=masc,per=3,num=pl]]', 25047: \"'hunde'\\n\", 25048: 'n[case=dat,', 25049: \"'hunden'\\n\", 25050: 'n[case=acc,', 25051: 'n[agr=[gnd=fem,per=3,num=sg]]', 25052: \"'katze'\\n\", 25053: 'n[agr=[gnd=fem,per=3,num=pl]]', 25054: \"'katzen'\\n\", 25055: 'pronouns\\n', 25056: 'pro[case=nom,', 25057: 'agr=[per=1,num=sg]]', 25058: \"'ich'\\n\", 25059: 'pro[case=acc,', 25060: \"'mich'\\n\", 25061: 'pro[case=dat,', 25062: \"'mir'\\n\", 25063: 'agr=[per=2,num=sg]]', 25064: \"'du'\\n\", 25065: 'agr=[per=3,num=sg]]', 25066: \"'er'\", 25067: \"'sie'\", 25068: 'agr=[per=1,num=pl]]', 25069: \"'wir'\\n\", 25070: \"'uns'\\n\", 25071: 'agr=[per=2,num=pl]]', 25072: \"'ihr'\\n\", 25073: \"'sie'\\n\", 25074: 'verbs\\n', 25075: 'iv[agr=[num=sg,per=1]]', 25076: \"'komme'\\n\", 25077: 'iv[agr=[num=sg,per=2]]', 25078: \"'kommst'\\n\", 25079: 'iv[agr=[num=sg,per=3]]', 25080: \"'kommt'\\n\", 25081: 'iv[agr=[num=pl,', 25082: 'per=1]]', 25083: \"'kommen'\\n\", 25084: 'per=2]]', 25085: 'tv[objcase=acc,', 25086: 'agr=[num=sg,per=1]]', 25087: \"'sehe'\", 25088: \"'mag'\\n\", 25089: 'agr=[num=sg,per=2]]', 25090: \"'siehst'\", 25091: \"'magst'\\n\", 25092: 'agr=[num=sg,per=3]]', 25093: \"'sieht'\", 25094: 'tv[objcase=dat,', 25095: \"'folge'\", 25096: \"'helfe'\\n\", 25097: \"'folgst'\", 25098: \"'hilfst'\\n\", 25099: \"'folgt'\", 25100: \"'hilft'\\n\", 25101: 'agr=[num=pl,per=1]]', 25102: \"'sehen'\", 25103: \"'moegen'\\n\", 25104: 'agr=[num=pl,per=2]]', 25105: \"'moegt'\\n\", 25106: 'agr=[num=pl,per=3]]', 25107: \"'folgen'\", 25108: \"'helfen'\\n\", 25109: \"'helft'\\n\", 25110: \"'helfen'\\n\\n\\nexample\", 25111: '(code_germancfg', 25112: 'objcase', 25113: 'governs', 25114: \"'ich\", 25115: 'folge', 25116: 'den', 25117: \"katzen'\", 25118: \"load_parser('grammars/book_grammars/german\", 25119: \"(np[agr=[num='sg',\", 25120: 'per=1],', 25121: \"case='nom']\\n\", 25122: \"(pro[agr=[num='sg',\", 25123: \"case='nom']\", 25124: 'ich))\\n', 25125: \"(vp[agr=[num='sg',\", 25126: 'per=1]]\\n', 25127: \"(tv[agr=[num='sg',\", 25128: \"objcase='dat']\", 25129: 'folge)\\n', 25130: \"(np[agr=[gnd='fem',\", 25131: 'per=3],', 25132: \"case='dat']\\n\", 25133: \"(det[agr=[num='pl',\", 25134: \"case='dat']\", 25135: 'den)\\n', 25136: \"(n[agr=[gnd='fem',\", 25137: 'katzen))))\\n\\n\\n\\nin', 25138: 'as\\nchallenging', 25139: 'idea\\nwhere', 25140: 'trace\\nparameter', 25141: 'load_parser()', 25142: 'failure:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 25143: \"katze'\", 25144: 'trace=2)\\n>>>', 25145: 'print(tree)\\n|', 25146: '.ich', 25147: '.fol', 25148: '.den', 25149: '.kat', 25150: 'rule:\\n|[---]', 25151: \"'ich'\\n|\", 25152: '[---]', 25153: \"'folge'\\n|\", 25154: \"'den'\\n|\", 25155: '[---]|', 25156: '[3:4]', 25157: \"'katze'\\nfeature\", 25158: \"pro[agr=[num='sg',\", 25159: \"'ich'\", 25160: \"np[agr=[num='sg',\", 25161: 'rule:\\n|[--->', 25162: 'np[agr=?a,', 25163: '{?a:', 25164: \"[num='sg',\", 25165: 'per=1]}\\nfeature', 25166: \"tv[agr=[num='sg',\", 25167: '[--->', 25168: 'tv[agr=?a,', 25169: 'objcase=?c]', 25170: 'np[case=?c]', 25171: '?c:', 25172: \"'dat'}\\nfeature\", 25173: \"det[agr=[gnd='masc',\", 25174: \"num='sg',\", 25175: \"case='acc']\", 25176: \"'den'\", 25177: '*\\n|', 25178: \"det[agr=[num='pl',\", 25179: 'case=?c]', 25180: 'det[agr=?a,', 25181: 'n[agr=?a,', 25182: \"[num='pl',\", 25183: \"[gnd='masc',\", 25184: \"'acc'}\\nfeature\", 25185: \"n[agr=[gnd='fem',\", 25186: \"'katze'\", 25187: '*\\n\\n\\n\\nthe', 25188: 'scanner', 25189: 'recognized', 25190: 'as\\nadmitting', 25191: 'categories:', 25192: \"num='sg',\\nper=3],\", 25193: 'katze', 25194: 'category\\nn[agr=[gnd=fem,', 25195: 'num=sg,', 25196: 'binding', 25197: '?a', 25198: 'det[case=?c,\\nagr=?a]', 25199: 'constraints,', 25200: 'the\\nagr', 25201: 'agr\\nvalues', 25202: 'den,', 25203: \"num='sg',\\nper=3]\", 25204: 'per=3]', 25205: '.\\n\\n\\n\\n4\\xa0\\xa0\\xa0summary\\n\\nthe', 25206: 'atomic\\nsymbols', 25207: 'capture\\nfine-grained', 25208: 'otherwise', 25209: 'massive\\nmultiplication', 25210: 'constraints\\nin', 25211: 'realization', 25212: 'feature\\nspecifications', 25213: 'inter-dependent', 25214: '.\\ntypically', 25215: 'level\\nand', 25216: 'constrain', 25217: 'sub-case', 25218: 'of\\natomic', 25219: 'as\\n[+/-', 25220: 'f]', 25221: '(either', 25222: 'or\\ncomplex)', 25223: 'be\\nre-entrant', 25224: '(or\\ntags)', 25225: 'avms', 25226: 'features\\ncorresponding', 25227: 'graph\\nrepresentation', 25228: '.\\nfs0', 25229: 'when\\nall', 25230: 'in\\nfs0', 25231: 'in\\nfs1', 25232: 'and\\nfs1,', 25233: 'successful,', 25234: 'combined\\ninformation', 25235: 'fs,', 25236: 'also\\nadds', 25237: \"π'\", 25238: 'wide\\nvariety', 25239: 'phenomena,', 25240: 'subcategorization,\\ninversion', 25241: '.\\n\\n\\n\\n5\\xa0\\xa0\\xa0further', 25242: 'including\\nfeature', 25243: 'suites', 25244: 'syntax:', 25245: 'see\\n(corbett,', 25246: 'earliest', 25247: 'designed\\nto', 25248: 'phonological', 25249: 'sound\\nlike', 25250: '/b/', 25251: '[+labial,', 25252: '+voice]', 25253: 'capture\\ngeneralizations', 25254: 'segments;', 25255: '/n/', 25256: 'gets\\nrealized', 25257: '/m/', 25258: '+labial', 25259: 'chomskyan', 25260: 'for\\nphenomena', 25261: 'generalizations', 25262: 'across\\nsyntactic', 25263: 'was\\nadvocated', 25264: '(gpsg;\\n(gazdar,', 25265: '1985)),', 25266: '.\\ncoming', 25267: 'perspective', 25268: 'linguistics,\\n(dahl', 25269: 'saint-dizier,', 25270: '1985)', 25271: 'be\\ncaptured', 25272: 'similar\\napproach', 25273: 'elaborated', 25274: '(grosz', 25275: 'stickel,', 25276: '1983)', 25277: 'patr-ii\\nformalism', 25278: 'lexical-functional', 25279: '(lfg;\\n(bresnan,', 25280: '1982))', 25281: 'f-structure', 25282: 'that\\nwas', 25283: 'primarily', 25284: 'and\\npredicate-argument', 25285: 'structure\\nparse', 25286: '(shieber,', 25287: 'this\\nphase', 25288: 'algebraic', 25289: 'researchers', 25290: 'attempted', 25291: 'negation', 25292: 'an\\nalternative', 25293: 'pioneered', 25294: '(kasper', 25295: 'rounds,', 25296: 'and\\n(johnson,', 25297: '1988),', 25298: 'argues', 25299: 'descriptions', 25300: 'of\\nfeature', 25301: 'these\\ndescriptions', 25302: 'as\\nconjunction,', 25303: 'over\\nfeature', 25304: 'description-oriented', 25305: 'was\\nintegral', 25306: '(huang', 25307: 'chen,', 25308: '1989),', 25309: 'later\\nversions', 25310: '(hpsg;\\n(sag', 25311: 'wasow,', 25312: '1999))', 25313: 'bibliography', 25314: 'be\\nfound', 25315: '.cl', 25316: '.uni-bremen', 25317: '.de/hpsg-bib/', 25318: 'to\\ncapture', 25319: 'example,\\nthere', 25320: 'permissible', 25321: 'for\\nnum', 25322: '[num=masc]', 25323: 'anomalous', 25324: 'say\\nthat', 25325: 'contain\\nspecifications', 25326: 'and\\ngnd,', 25327: 'as\\n[subcat=trans]', 25328: 'to\\nremedy', 25329: 'deficiency', 25330: 'stipulate', 25331: 'just\\nare', 25332: 'hierarchically,', 25333: 'more\\ninformative', 25334: 'subtype\\nof', 25335: 'num,', 25336: 'are\\nthemselves', 25337: 'only\\nper,', 25338: 'typed\\nfeature', 25339: '(emele', 25340: 'zajac,', 25341: '1990)', 25342: 'examination', 25343: 'foundations', 25344: '(carpenter,', 25345: '1992),', 25346: 'while\\n(copestake,', 25347: 'focuses', 25348: 'implementing', 25349: 'hpsg-oriented', 25350: 'copious', 25351: 'within\\nfeature', 25352: '(nerbonne,', 25353: 'netter,', 25354: 'pollard,', 25355: '1994)', 25356: 'good\\nstarting', 25357: 'while\\n(m{\\\\u}ller,', 25358: 'of\\ngerman', 25359: 'structures,\\nthe', 25360: 'integration', 25361: 'into\\nparsing', 25362: '.\\n\\n\\n6\\xa0\\xa0\\xa0exercises\\n\\n☼', 25363: 'am\\nhappy', 25364: '*you', 25365: 'or\\n*they', 25366: 'happy?', 25367: 'tense\\nparadigm', 25368: 'grammar\\n(6)', 25369: '(18)\\nas', 25370: 'variant', 25371: 'below:\\n\\n', 25372: '(54)\\n', 25373: 'sings', 25374: '.*boy', 25375: '(55)\\n', 25376: 'boys', 25377: '.boys', 25378: '(56)\\n', 25379: '(57)\\n', 25380: 'precious', 25381: '.water', 25382: 'subsumes()', 25383: '(28)', 25384: 'to\\nincorporate', 25385: 'the\\ntreatment', 25386: 'subcategorization', 25387: 'following\\nspanish', 25388: 'phrases:\\n\\nsystem', 25389: '2028)\\nerror', 25390: '.\\n\\n\\nsystem', 25391: '2033)\\nerror', 25392: '2038)\\nerror', 25393: '2043)\\nerror', 25394: 'earleychartparser', 25395: 'only\\nprints', 25396: '.\\n\\n\\n\\n\\n\\n\\xa0\\n\\nfs1', 25397: '.featstruct([a', 25398: '?x,', 25399: 'b=', 25400: '?x]])\\nfs2', 25401: '.featstruct([b', 25402: '[d', 25403: 'd]])\\nfs3', 25404: 'd]])\\nfs4', 25405: '(1)[b', 25406: 'b],', 25407: 'c->(1)])\\nfs5', 25408: '(1)[d', 25409: '?x],', 25410: '[e', 25411: '?x]', 25412: '])\\nfs6', 25413: 'd]])\\nfs7', 25414: 'd],', 25415: '[f', 25416: 'd]]])\\nfs8', 25417: '[b', 25418: '(1)]', 25419: '])\\nfs9', 25420: '[g', 25421: 'e]]])\\nfs10', 25422: '(1)])\\n\\n\\nexample', 25423: '(code_featstructures', 25424: 'structures\\n\\nwork', 25425: 'following\\nunifications', 25426: '.)\\n\\nfs1', 25427: 'fs2\\nfs1', 25428: 'fs3\\nfs4', 25429: 'fs5\\nfs5', 25430: 'fs6\\nfs5', 25431: 'fs7\\nfs8', 25432: 'fs9\\nfs8', 25433: 'fs10\\n\\ncheck', 25434: 'subsume', 25435: '[a=?x,', 25436: 'b=?x]', 25437: 'sharing,', 25438: 'unifying\\ntwo', 25439: 'can\\nhandle', 25440: 'verb-second', 25441: 'following:\\n\\n', 25442: '(58)heute', 25443: 'sieht', 25444: 'hund', 25445: 'different\\nsyntactic', 25446: '(levin,', 25447: 'patterns\\nof', 25448: 'loaded,', 25449: 'filled,', 25450: 'dumped\\nbelow', 25451: 'data?\\n\\n', 25452: '(59)\\n', 25453: 'cart', 25454: 'sand\\n\\n\\n', 25455: 'sand', 25456: 'cart\\n\\n\\n', 25457: 'dumped', 25458: 'cart\\n\\n\\n\\n★', 25459: 'paradigms', 25460: 'rarely', 25461: 'regular,', 25462: 'different\\nrealization', 25463: 'conjugation', 25464: 'the\\nlexeme', 25465: 'walks', 25466: 'the\\n3rd', 25467: 'singular,', 25468: 'of\\nperson', 25469: 'require\\nredundantly', 25470: 'morphological\\ncombinations', 25471: 'a\\nmethod', 25472: 'parent\\nnode', 25473: 'feature\\nthat', 25474: 'v\\nchild', 25475: '(gazdar,', 25476: 'are\\nsubcat', 25477: 'head\\nfeatures', 25478: 'predictable,', 25479: 'stated', 25480: 'explicitly\\nin', 25481: 'automatically\\naccounts', 25482: 'into\\nlist-valued', 25483: 'hpsg-style', 25484: 'of\\nsubcategorization,', 25485: 'whereby', 25486: 'the\\nconcatenation', 25487: \"complements'\", 25488: 'its\\nimmediate', 25489: 'with\\nunderspecified', 25490: '-->', 25491: 's/?x', 25492: '2002),\\nand', 25493: '.\\n\\n\\n\\n\\nabout', 25494: 'acst10', 25495: 'sentences\\n\\n\\n\\n\\n\\n10', 25496: 'sentences\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nwe', 25497: 'harness', 25498: 'the\\nmachinery', 25499: 'anything\\nsimilarly', 25500: 'sentences?\\nthe', 25501: 'computer\\ncan', 25502: 'representations?\\nhow', 25503: 'an\\nunlimited', 25504: 'to\\nstores', 25505: 'knowledge?\\n\\nalong', 25506: 'interrogating', 25507: 'facts\\nabout', 25508: '.\\n\\n1\\xa0\\xa0\\xa0natural', 25509: 'understanding\\n\\n1', 25510: '.1\\xa0\\xa0\\xa0querying', 25511: 'database\\nsuppose', 25512: 'answer:\\n\\n', 25513: 'athens', 25514: 'in?\\n\\n', 25515: '.greece', 25516: '.\\n\\nhow', 25517: 'program?', 25518: \"that\\nwe've\", 25519: 'new?\\nin', 25520: 'pretty\\nstraightforward', 25521: 'the\\nrepresentation', 25522: 'and\\ncountries', 25523: 'concrete,', 25524: 'database\\ntable', 25525: 'chat-80', 25526: 'system\\n(warren', 25527: 'pereira,', 25528: 'population', 25529: 'thousands,\\nbut', 25530: 'least\\nto', 25531: '1980s,', 25532: 'point\\nwhen', 25533: '(warren', 25534: '.\\n\\n\\n\\n\\n\\n\\n\\n\\ncity\\ncountry\\npopulation\\n\\n\\n\\nathens\\ngreece\\n1368\\n\\nbangkok\\nthailand\\n1178\\n\\nbarcelona\\nspain\\n1280\\n\\nberlin\\neast_germany\\n3481\\n\\nbirmingham\\nunited_kingdom\\n1112\\n\\n\\ntable', 25535: 'city_table:', 25536: 'cities,', 25537: 'countries', 25538: 'populations\\n\\n\\nthe', 25539: 'involves\\nwriting', 25540: '.\\n\\nnote\\nsql', 25541: '(structured', 25542: 'language)', 25543: 'for\\nretrieving', 25544: 'sql,\\nhttp://www', 25545: '.w3schools', 25546: '.com/sql/', 25547: 'online\\nreference', 25548: \"'greece':\\n\\n\", 25549: '(2)select', 25550: 'city_table', 25551: \"'athens'\\nthis\", 25552: 'column\\ncountry', 25553: \"is\\n'athens'\", 25554: 'query\\nsystem?', 25555: 'in\\n9', 25556: 'from\\nenglish', 25557: 'sql0', 25558: '.fcfg', 25559: 'assemble\\na', 25560: 'tandem', 25561: 'supplemented', 25562: 'recipe', 25563: 'for\\nconstructing', 25564: 'sem', 25565: 'these\\nrecipes', 25566: 'simple;', 25567: 'splice\\nthe', 25568: \".show_cfg('grammars/book_grammars/sql0\", 25569: 's\\ns[sem=(?np', 25570: '?vp)]', 25571: 'np[sem=?np]', 25572: 'vp[sem=?vp]\\nvp[sem=(?v', 25573: '?pp)]', 25574: 'iv[sem=?v]', 25575: 'pp[sem=?pp]\\nvp[sem=(?v', 25576: '?ap)]', 25577: 'ap[sem=?ap]\\nnp[sem=(?det', 25578: '?n)]', 25579: 'det[sem=?det]', 25580: 'n[sem=?n]\\npp[sem=(?p', 25581: '?np)]', 25582: 'p[sem=?p]', 25583: 'np[sem=?np]\\nap[sem=?pp]', 25584: 'a[sem=?a]', 25585: \"pp[sem=?pp]\\nnp[sem='country=greece']\", 25586: \"'greece'\\nnp[sem='country=china']\", 25587: \"'china'\\ndet[sem='select']\", 25588: \"'which'\", 25589: \"'what'\\nn[sem='city\", 25590: \"city_table']\", 25591: \"'cities'\\niv[sem='']\", 25592: \"'are'\\na[sem='']\", 25593: \"'located'\\np[sem='']\", 25594: \"'in'\\n\\n\\n\\nthis\", 25595: \"load_parser('grammars/book_grammars/sql0\", 25596: \"'what\", 25597: \"china'\\n>>>\", 25598: 'list(cp', 25599: '.parse(query', 25600: '.split()))\\n>>>', 25601: 'trees[0]', 25602: \".label()['sem']\\n>>>\", 25603: 's]\\n>>>', 25604: 'q', 25605: '.join(answer)\\n>>>', 25606: 'print(q)\\nselect', 25607: 'country=china\\n\\n\\n\\n\\nnote\\nyour', 25608: '.,\\ncp', 25609: 'trace=3),', 25610: 'examine\\nhow', 25611: 'added\\nto', 25612: '.db', 25613: 'and\\nretrieve', 25614: 'chat80\\n>>>', 25615: 'chat80', 25616: \".sql_query('corpora/city_database/city\", 25617: \".db',\", 25618: 'q)\\n>>>', 25619: 'rows:', 25620: 'print(r[0],', 25621: '\\ncanton', 25622: 'chungking', 25623: 'dairen', 25624: 'harbin', 25625: 'kowloon', 25626: 'mukden', 25627: 'peking', 25628: 'shanghai', 25629: 'sian', 25630: 'tientsin\\n\\n\\n\\nsince', 25631: 'data\\nin', 25632: 'query,', 25633: 'this\\nby', 25634: 'that\\nour', 25635: 'understands', 25636: 'sql,', 25637: 'is\\nable', 25638: 'database,', 25639: 'extension', 25640: 'understands\\nqueries', 25641: 'parallels\\nbeing', 25642: 'native', 25643: 'means:\\n\\n', 25644: '(3)margrietje', 25645: 'houdt', 25646: 'brunoke', 25647: 'know\\nhow', 25648: 'whole\\nsentence,', 25649: 'margrietje', 25650: 'loves\\nbrunoke\\nan', 25651: 'olga', 25652: 'take\\nthis', 25653: 'this\\nwould', 25654: 'herself', 25655: \"doesn't,\\nthen\", 25656: 'convince\\nher', 25657: '.fcfg,', 25658: 'earley', 25659: 'is\\ninstrumental', 25660: 'this\\ngrammar?', 25661: 'justification', 25662: 'meaning\\nrepresentations', 25663: 'the\\nnoun', 25664: 'correspond\\nrespectively', 25665: 'fragments', 25666: 'from\\ncity_table', 25667: 'criticism', 25668: 'grammar:', 25669: 'have\\nhard-wired', 25670: 'embarrassing', 25671: 'into\\nit', 25672: '.,\\ncity_table)', 25673: 'could\\nhave', 25674: 'different\\ntable', 25675: 'queries\\nwould', 25676: 'equally,', 25677: 'xml,', 25678: 'retrieving', 25679: 'same\\nresults', 25680: 'xml\\nquery', 25681: 'considerations', 25682: 'and\\ngeneric', 25683: 'sharpen', 25684: 'query\\nand', 25685: 'translation:\\n\\n', 25686: '.what', 25687: 'populations', 25688: '1,000,000?\\n\\n', 25689: '.select', 25690: \"'china'\", 25691: 'and\\npopulation', 25692: '1000\\n\\n\\nnote\\nyour', 25693: 'translate\\n(4a)', 25694: '(4b),', 25695: 'to\\nhandle', 25696: 'above\\n1,000,000', 25697: 'go\\nat', 25698: 'grammars/book_grammars/sql1', 25699: '(4a)', 25700: 'translated\\ninto', 25701: 'counterpart,', 25702: '(4b)', 25703: 'true\\ntogether:', 25704: 'talks', 25705: 'about\\nwhat', 25706: 'situation,', 25707: 'that\\ncond1', 25708: 'cond2', 25709: 'that\\ncondition', 25710: 'cond1', 25711: 'in\\ns', 25712: 'of\\nmeanings', 25713: 'is\\nindependent', 25714: 'classical', 25715: 'following\\nsections,', 25716: 'query\\nlanguage', 25717: 'abstract\\nand', 25718: 'our\\ntranslation', 25719: 'other\\nspecial-purpose', 25720: 'query\\ndatabases', 25721: 'methodology', 25722: '.2\\xa0\\xa0\\xa0natural', 25723: 'logic\\nwe', 25724: 'a\\nquery', 25725: 'but\\nthis', 25726: 'begged', 25727: 'back\\nfrom', 25728: 'of\\ntranslating', 25729: 'is\\nabout', 25730: 'this\\nfurther', 25731: 'margrietje\\nand', 25732: 'favourite', 25733: 'doll,', 25734: 'part,', 25735: 'this\\nbecause', 25736: 'margrietje,', 25737: 'to\\nbrunoke,', 25738: 'two\\nfundamental', 25739: 'notions', 25740: 'things\\nin', 25741: '(3)\\nis', 25742: 'loves', 25743: 'doll', 25744: 'brunoke,\\nhere', 25745: 'depiction', 25746: 'truth', 25747: 'a\\npowerful', 25748: 'at\\nsets', 25749: 'some\\nsituation', 25750: 'true,\\nwhile', 25751: '(7)', 25752: 'consistent,', 25753: 'are\\ninconsistent', 25754: '.sylvania', 25755: 'freedonia', 25756: '.freedonia', 25757: 'republic', 25758: '.no', 25759: 'sylvania', 25760: 'fictional', 25761: '(featured', 25762: 'the\\nmarx', 25763: \"brothers'\", 25764: '1933', 25765: 'soup)', 25766: 'emphasize', 25767: 'ability\\nto', 25768: 'true\\nor', 25769: 'you\\nshould', 25770: 'are\\ninconsistent,', 25771: 'population\\nof', 25772: 'which\\nboth', 25773: 'relation\\nexpressed', 25774: 'asymmetric,', 25775: 'be\\nable', 25776: 'inconsistent', 25777: '.\\nbroadly', 25778: 'speaking,', 25779: 'logic-based', 25780: 'semantics\\nfocus', 25781: 'our\\njudgments', 25782: 'consistency', 25783: 'inconsistency', 25784: 'logical\\nlanguage', 25785: 'a\\nresult,', 25786: 'reduced\\nto', 25787: 'symbolic', 25788: 'manipulation,', 25789: 'to\\ndevelop', 25790: 'logicians', 25791: 'formal\\nrepresentation', 25792: 'w\\nare', 25793: 'domain\\nd', 25794: 'about)', 25795: 'of\\nindividuals,', 25796: 'as\\nsets', 25797: 'three\\nchildren,', 25798: 'stefan,', 25799: 'klaus', 25800: 'evi,', 25801: 's,\\nk', 25802: '{s,', 25803: 'e}', 25804: 'denotes', 25805: 'the\\nset', 25806: 'stefan', 25807: 'klaus,', 25808: 'evi', 25809: 'graphical\\nrendering', 25810: 'd\\ncorresponding', 25811: 'boy,', 25812: 'is\\nrunning', 25813: '.\\n\\nlater', 25814: 'falsity', 25815: 'of\\nenglish', 25816: 'representing\\nmeaning', 25817: 'a\\nbroader', 25818: 'raised', 25819: 'in\\n5', 25820: 'sentence?', 25821: 'did?', 25822: 'asking', 25823: 'a\\ncomputer', 25824: 'think?', 25825: 'alan', 25826: 'famously', 25827: 'the\\nability', 25828: 'conversations', 25829: 'human\\n(turing,', 25830: '1950)', 25831: 'computer,\\nbut', 25832: 'partners', 25833: 'has\\nsuccessfully', 25834: 'imitated', 25835: 'human\\nin', 25836: 'imitation', 25837: 'popularly', 25838: 'known),', 25839: 'turing,', 25840: 'be\\nsaid', 25841: 'side-stepped', 25842: 'somehow', 25843: 'the\\ninternal', 25844: 'of\\nintelligence', 25845: 'reasoning,', 25846: 'specifics', 25847: \"turing's\", 25848: 'game,', 25849: 'rather\\nthe', 25850: 'proposal', 25851: 'judge', 25852: 'capacity', 25853: 'of\\nobservable', 25854: '.\\n\\n\\n\\n2\\xa0\\xa0\\xa0propositional', 25855: 'logic\\na', 25856: 'language\\nwhich', 25857: 'φ', 25858: 'truth-conditions', 25859: 'start\\noff', 25860: 'example:\\n\\n', 25861: '(8)[klaus', 25862: 'evi]', 25863: '[evi', 25864: 'away]', 25865: 'sub-sentences', 25866: 'ψ\\nrespectively,', 25867: 'and:', 25868: 'ψ', 25869: 'the\\nlogical', 25870: '.\\npropositional', 25871: 'represent\\njust', 25872: 'certain\\nsentential', 25873: 'connectives', 25874: 'such\\nconnectives', 25875: '.,\\nthen', 25876: 'formalization', 25877: 'propositional', 25878: 'the\\ncounterparts', 25879: 'boolean\\noperators', 25880: 'are\\npropositional', 25881: 'p,\\nq,', 25882: 'for\\nrepresenting', 25883: 'of\\nexploring', 25884: 'ascii\\nversions', 25885: 'operators:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 25886: '.boolean_ops()\\nnegation', 25887: '-\\nconjunction', 25888: '&\\ndisjunction', 25889: '|\\nimplication', 25890: '->\\nequivalence', 25891: '<->\\n\\n\\n\\nfrom', 25892: 'build\\nan', 25893: 'formulas', 25894: 'formulas,', 25895: 'for\\nshort)', 25896: 'a\\nformula', 25897: 'formula,', 25898: '-φ', 25899: 'and\\nψ', 25900: 'are\\n(φ', 25901: 'ψ)\\n(φ', 25902: '<->', 25903: 'ψ)', 25904: 'for\\nformulas', 25905: 'if\\nas', 25906: 'iff', 25907: '.\\n\\n\\n\\n\\n\\n\\n\\n\\nboolean', 25908: 'operator\\ntruth', 25909: 'conditions\\n\\n\\n\\nnegation', 25910: '(it', 25911: '.)\\n-φ', 25912: 's\\niff\\nφ', 25913: 's\\n\\nconjunction', 25914: '(and)\\n(φ', 25915: 's\\n\\ndisjunction', 25916: '(or)\\n(φ', 25917: 's\\n\\nimplication', 25918: '.)\\n(φ', 25919: 's\\n\\nequivalence', 25920: 'if)\\n(φ', 25921: 'straightforward,', 25922: 'for\\nimplication', 25923: 'departs', 25924: 'q)', 25925: 'false\\nwhen', 25926: '(say', 25927: 'p\\ncorresponds', 25928: 'moon', 25929: 'cheese)', 25930: 'is\\ntrue', 25931: 'four)', 25932: 'p\\n->', 25933: '.\\nnltks', 25934: 'into\\nvarious', 25935: 'subclasses', 25936: 'read_expr', 25937: '.expression', 25938: '.fromstring\\n>>>', 25939: \"read_expr('-(p\", 25940: \"q)')\\n<negatedexpression\", 25941: '-(p', 25942: 'q)>\\n>>>', 25943: \"read_expr('p\", 25944: \"q')\\n<andexpression\", 25945: '(r', 25946: \"q)')\\n<orexpression\", 25947: 'q))>\\n>>>', 25948: \"p')\\n<iffexpression\", 25949: '--p)>\\n\\n\\n\\nfrom', 25950: 'logics', 25951: 'performing\\ninference', 25952: 'sylvania,', 25953: 'case,\\nyou', 25954: 'of\\nfreedonia', 25955: 'the\\nnorth', 25956: 'assumptions\\nto', 25957: 'informally,', 25958: '(9)\\nsylvania', 25959: 'sylvania\\n\\n\\nan', 25960: 'which\\nits', 25961: 'premises', 25962: 'validity', 25963: '(9)', 25964: 'crucially', 25965: 'asymmetric\\nrelation:\\n\\n', 25966: '(10)if', 25967: 'of\\nx', 25968: '.\\nunfortunately,', 25969: 'logic:', 25970: 'smallest\\nelements', 25971: 'inside\\nthese', 25972: 'individuals', 25973: 'asymmetry', 25974: 'the\\npropositional', 25975: 'snf', 25976: 'freedonia\\nand', 25977: 'fns', 25978: 'freedonia\\nis', 25979: '-fns', 25980: 'not\\nas', 25981: 'one-place', 25982: 'implication', 25983: 'in\\n(10)', 25984: 'as\\n\\n', 25985: '(11)snf', 25986: '-fns\\nhow', 25987: 'argument?', 25988: 'first\\nsentence', 25989: 'snf,', 25990: 'also\\nthe', 25991: '(rather', 25992: 'poorly)', 25993: 'background\\nknowledge', 25994: '[a1,', 25995: 'an]', 25996: 'c\\nto', 25997: '.,\\nan]', 25998: '(9):\\n\\n', 25999: '(12)[snf,', 26000: '-fns]', 26001: '-fns\\nthis', 26002: 'argument:', 26003: 'situation\\ns,', 26004: 'would\\nconflict', 26005: 'each\\nother', 26006: 'equivalently,', 26007: '[snf,', 26008: '-fns,', 26009: 'fns]\\nis', 26010: '.\\narguments', 26011: 'proof', 26012: 'will\\nsay', 26013: 'proofs', 26014: \"with\\nnltk's\", 26015: 'the\\nthird-party', 26016: 'prover9', 26017: 'mechanism', 26018: 'lp', 26019: \"read_expr('snf')\\n>>>\", 26020: 'notfns', 26021: \"read_expr('-fns')\\n>>>\", 26022: \"read_expr('snf\", 26023: \"-fns')\\n>>>\", 26024: '.prover9()\\n>>>', 26025: '.prove(notfns,', 26026: \"r])\\ntrue\\n\\n\\n\\nhere's\", 26027: 'semantically\\nequivalent', 26028: '-snf', 26029: 'the\\ntwo-place', 26030: 'is\\ntrue,', 26031: 'cannot\\nalso', 26032: 'true;', 26033: 'consequently,\\n-fns', 26034: 'a\\nmodel,', 26035: 'for\\npropositional', 26036: 'false\\nto', 26037: 'inductively:', 26038: 'every\\npropositional', 26039: 'consulting', 26040: '.e,', 26041: \"formula's\", 26042: 'valuation', 26043: 'basic\\nexpressions', 26044: \".valuation([('p',\", 26045: 'true),', 26046: \"('q',\", 26047: \"('r',\", 26048: 'false)])\\n\\n\\n\\nwe', 26049: 'which\\nconsists', 26050: 'resulting\\nobject', 26051: 'expressions\\n(treated', 26052: \"val['p']\\ntrue\\n\\n\\n\\nas\", 26053: 'complicated\\nin', 26054: 'dom', 26055: 'and\\ng', 26056: 'declarations', 26057: '.assignment(dom)\\n\\n\\n\\nnow', 26058: 'val:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 26059: '.model(dom,', 26060: 'val)\\n\\n\\n\\nevery', 26061: 'of\\npropositional', 26062: 'logic;', 26063: 'initial\\ntruth', 26064: 'p,\\nq', 26065: \".evaluate('(p\", 26066: \"q)',\", 26067: 'g))\\ntrue\\n>>>', 26068: \".evaluate('-(p\", 26069: 'g))\\nfalse\\n>>>', 26070: \"r)',\", 26071: 'g))\\ntrue\\n\\n\\n\\n\\nnote\\nyour', 26072: 'turn:\\nexperiment', 26073: 'expected?\\n\\nup', 26074: 'into\\npropositional', 26075: 'confined', 26076: 'atomic\\nsentences', 26077: 'q,', 26078: 'their\\ninternal', 26079: 'of\\nlogical', 26080: 'subjects,', 26081: 'objects\\nand', 26082: 'wrong:', 26083: 'formalize\\narguments', 26084: '(9),', 26085: 'inside\\nbasic', 26086: 'logic\\nto', 26087: 'expressive,', 26088: '.\\n\\n\\n3\\xa0\\xa0\\xa0first-order', 26089: 'logic\\nin', 26090: 'language\\nexpressions', 26091: 'good\\nchoice', 26092: 'expressive', 26093: 'deal,', 26094: 'shelf\\nfor', 26095: 'constructed,', 26096: 'how\\nsuch', 26097: '.1\\xa0\\xa0\\xa0syntax\\nfirst-order', 26098: 'it\\nadds', 26099: 'propositions', 26100: 'are\\nanalyzed', 26101: 'closer\\nto', 26102: 'rules\\nfor', 26103: 'and\\nindividual', 26104: 'constants,', 26105: 'differing\\nnumbers', 26106: 'angus', 26107: 'be\\nformalized', 26108: 'walk(angus)', 26109: 'sees', 26110: 'bertie', 26111: 'as\\nsee(angus,', 26112: 'bertie)', 26113: 'unary\\npredicate,', 26114: 'predicate', 26115: 'the\\nsymbols', 26116: 'intrinsic', 26117: 'examples,\\nthere', 26118: 'and\\n(13b)', 26119: '.love(margrietje,', 26120: 'brunoke)\\n\\n', 26121: '.houden_van(margrietje,', 26122: 'brunoke)\\n\\n\\nby', 26123: 'substantive', 26124: 'lexical\\nsemantics', 26125: 'although\\nsome', 26126: 'an\\natomic', 26127: 'predication', 26128: 'see(angus,', 26129: 'particular\\nvaluation', 26130: 'see,\\nangus', 26131: 'non-logical', 26132: 'logical\\nconstants', 26133: 'operators)', 26134: 'same\\ninterpretation', 26135: 'special\\nstatus,', 26136: 'equality,', 26137: '=\\naj', 26138: 'constant,', 26139: 'for\\nindividual', 26140: 't2,', 26141: 'to\\nexpressions', 26142: 'montague', 26143: 'types:', 26144: 'entities,\\nwhile', 26145: 'have\\ntruth', 26146: 'complex\\ntypes', 26147: 'σ', 26148: 'τ,', 26149: '〈σ,\\nτ〉', 26150: \"from\\n'σ\", 26151: \"things'\", 26152: \"'τ\", 26153: '〈e,\\nt〉', 26154: 'to\\ntruth', 26155: 'unary', 26156: 'be\\nprocessed', 26157: 'expr', 26158: \"read_expr('walk(angus)',\", 26159: 'type_check=true)\\n>>>', 26160: '.argument\\n<constantexpression', 26161: 'angus>\\n>>>', 26162: '.argument', 26163: '.type\\ne\\n>>>', 26164: '.function\\n<constantexpression', 26165: 'walk>\\n>>>', 26166: '.function', 26167: '.type\\n<e,?>\\n\\n\\n\\nwhy', 26168: '<e,?>', 26169: 'example?', 26170: 'the\\ntype-checker', 26171: 'infer', 26172: 'case\\nit', 26173: 'walk,\\nsince', 26174: 'intending', 26175: 'walk\\nto', 26176: '<e,', 26177: 't>,', 26178: 'type-checker', 26179: 'knows,', 26180: 'this\\ncontext', 26181: 'e>', 26182: 't>', 26183: 'help\\nthe', 26184: 'type-checker,', 26185: 'signature,', 26186: 'non-logical\\nconstants:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 26187: 'sig', 26188: \"{'walk':\", 26189: \"'<e,\", 26190: \"t>'}\\n>>>\", 26191: 'signature=sig)\\n>>>', 26192: '.type\\ne\\n\\n\\n\\na', 26193: '〈e,', 26194: 't〉〉', 26195: 'of\\nsomething', 26196: 'make\\na', 26197: 'predicate,', 26198: 'combining\\ndirectly', 26199: 'the\\ntranslation', 26200: 'cyril', 26201: 'will\\ncombine', 26202: 'cyril)', 26203: 'variables\\nsuch', 26204: 'that\\nvariables', 26205: '.\\nindividual', 26206: 'to\\npersonal', 26207: 'he,', 26208: 'their\\ndenotation', 26209: '(14)', 26210: 'by\\npointing', 26211: '(14)he', 26212: 'disappeared', 26213: 'pronoun\\nhe,', 26214: 'uttering', 26215: '(15a)', 26216: 'to\\n(14)', 26217: 'coreferential', 26218: '(15b)', 26219: '.cyril', 26220: \"angus's\", 26221: '.\\n\\nconsider', 26222: '(16a)', 26223: 'np\\na', 26224: 'dog,\\nand', 26225: 'than\\ncoreference', 26226: 'dog,\\nthe', 26227: '(16b)', 26228: '.angus', 26229: '.\\n\\ncorresponding', 26230: '(17a),', 26231: 'formula\\n(17b)', 26232: '(we\\nignore', 26233: 'exposition', 26234: '.dog(x)', 26235: '∧', 26236: 'disappear(x)\\n\\nby', 26237: 'existential', 26238: 'quantifier', 26239: '∃x', 26240: \"('for\\nsome\", 26241: \"x')\", 26242: 'front', 26243: '(17b),', 26244: 'these\\nvariables,', 26245: '(18a),', 26246: '(18b)', 26247: 'more\\nidiomatically,', 26248: '(18c)', 26249: '(18)\\n', 26250: '.∃x', 26251: '.(dog(x)', 26252: 'disappear(x))\\n\\n', 26253: '.at', 26254: '(18a):\\n\\n', 26255: '(19)exists', 26256: 'disappear(x))\\nin', 26257: 'quantifier,', 26258: '∀x', 26259: \"('for\", 26260: \"all\\nx'),\", 26261: '.∀x', 26262: '.everything', 26263: 'disappears', 26264: '.every', 26265: '(20a):\\n\\n', 26266: '(21)all', 26267: 'disappear(x))\\nalthough', 26268: '(20a)', 26269: '(20c),', 26270: 'truth\\nconditions', 26271: \"aren't\", 26272: 'then\\nx', 26273: 'so\\nin', 26274: 'dogs,', 26275: 'still\\ncome', 26276: '(remember', 26277: 'p\\nis', 26278: 'argue', 26279: 'presuppose', 26280: 'the\\nexistence', 26281: 'presupposition', 26282: 'expression\\nastring', 26283: \".replace('ate',\", 26284: \"'8')\", 26285: 'every\\noccurrence', 26286: 'astring', 26287: \"'8',\", 26288: 'there\\nmay', 26289: 'quantifiers', 26290: 'what\\nhappens', 26291: 'following?:\\n\\n((exists', 26292: 'dog(x))', 26293: 'bark(x))\\n\\nthe', 26294: 'dog(x),', 26295: 'x\\nin', 26296: 'bark(x)', 26297: 'unbound', 26298: 'consequently', 26299: 'quantifier,\\nfor', 26300: 'formula:\\n\\nall', 26301: '.((exists', 26302: 'bark(x))\\n\\nin', 26303: 'in\\nφ', 26304: 'φ,', 26305: '.φ', 26306: 'bound,', 26307: 'process\\nstrings,', 26308: 'each\\ninstance', 26309: 'free()', 26310: \"read_expr('dog(cyril)')\", 26311: '.free()\\nset()\\n>>>', 26312: \"read_expr('dog(x)')\", 26313: \".free()\\n{variable('x')}\\n>>>\", 26314: \"read_expr('own(angus,\", 26315: \"cyril)')\", 26316: \"read_expr('exists\", 26317: \".dog(x)')\", 26318: \"read_expr('((some\", 26319: 'walk(x))', 26320: \"sing(x))')\", 26321: '.own(y,', 26322: \"x)')\", 26323: \".free()\\n{variable('y')}\\n\\n\\n\\n\\n\\n3\", 26324: '.2\\xa0\\xa0\\xa0first', 26325: 'proving\\nrecall', 26326: '(10):\\n\\n', 26327: '(22)if', 26328: 'predicates,', 26329: 'we\\ndid', 26330: 'properly', 26331: 'doubt', 26332: 'is\\nideal', 26333: 'formalizing', 26334: 'rules:\\n\\nall', 26335: '.(north_of(x,', 26336: 'y)', 26337: '-north_of(y,', 26338: 'x))\\n\\neven', 26339: 'better,', 26340: 'to\\nshow', 26341: 'proving', 26342: 'goal)', 26343: 'derived\\nby', 26344: '⊢', 26345: '(possibly', 26346: 'empty)', 26347: 'assumptions,\\nand', 26348: 'the\\ntheorem', 26349: 'required\\nproof', 26350: 'prover9\\ninstance', 26351: 'prove()', 26352: 'goal,', 26353: 'of\\nassumptions', 26354: \"read_expr('-north_of(f,\", 26355: \"s)')\", 26356: \"read_expr('north_of(s,\", 26357: \"f)')\", 26358: \"read_expr('all\", 26359: '(north_of(x,', 26360: \"x))')\", 26361: '.prover9()', 26362: 'r])', 26363: '\\ntrue\\n\\n\\n\\nhappily,', 26364: 'contrast,\\nit', 26365: 'north_of(f,', 26366: 'our\\nassumptions:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 26367: \"read_expr('north_of(f,\", 26368: \"s)')\\n>>>\", 26369: '.prove(fns,', 26370: 'r])\\nfalse\\n\\n\\n\\n\\n\\n3', 26371: '.3\\xa0\\xa0\\xa0summarizing', 26372: \"logic\\nwe'll\", 26373: 'restate', 26374: 'quantifiers;\\ntogether,', 26375: 'expressions\\ninvolved', 26376: 'that\\n〈en,', 26377: 't〉\\nis', 26378: 'arguments\\nof', 26379: 'expression\\nof', 26380: 'the\\narity', 26381: '.\\n\\n\\nif', 26382: '〈en,\\nt〉,\\nand', 26383: 'α1,', 26384: 'αn\\nare', 26385: 'then\\np(α1,', 26386: 'αn)', 26387: 'is\\nof', 26388: 'β', 26389: '(α', 26390: 'β)', 26391: 'and\\n(α', 26392: 'ψ),\\n(φ', 26393: 'and\\n(φ', 26394: 'then\\nexists', 26395: 'logic\\nmodule,', 26396: '.\\n\\n\\n\\n\\n\\n\\nexample\\ndescription\\n\\n\\n\\n=\\nequality\\n\\n!=\\ninequality\\n\\nexists\\nexistential', 26397: 'quantifier\\n\\nall\\nuniversal', 26398: 'quantifier\\n\\ne', 26399: '.free()\\nshow', 26400: 'e\\n\\ne', 26401: '.simplify()\\ncarry', 26402: 'β-reduction', 26403: 'e\\n\\n\\ntable', 26404: 'first\\norder', 26405: 'the\\nexpression', 26406: '.4\\xa0\\xa0\\xa0truth', 26407: 'model\\nwe', 26408: 'argued', 26409: 'if\\nwe', 26410: 'truth-conditional', 26411: 'semantics,', 26412: 'obvious\\nlimits', 26413: 'push', 26414: 'talk\\nabout', 26415: 'the\\nmeans', 26416: 'symbolic\\nmanner', 26417: 'limitation,', 26418: 'a\\nclearer', 26419: 'in\\nnltk', 26420: 'a\\npair', 26421: '〈d,', 26422: 'val〉,', 26423: 'an\\nnonempty', 26424: 'values\\nfrom', 26425: 'follows:\\n\\n\\nfor', 26426: 'l,\\nval(c)', 26427: 'arity', 26428: '≥', 26429: '0,\\nval(p)', 26430: 'dn', 26431: 'to\\n{true,', 26432: 'false}', 26433: 'val(p)', 26434: 'the\\np', 26435: '.)\\n\\n\\naccording', 26436: '(ii),', 26437: 'val(p)\\nwill', 26438: 'which\\nval(p)', 26439: 'follows:\\n\\n', 26440: '(23)s', 26441: '{s', 26442: 'f(s)', 26443: 'true}\\nsuch', 26444: 's\\n(as', 26445: '.\\nrelations', 26446: 'standard\\nset-theoretic', 26447: 'way:', 26448: 'bertie,', 26449: 'olive', 26450: 'cyril,\\nwhere', 26451: 'mnemonic\\nreasons,', 26452: 'labels\\nin', 26453: \"{'b',\", 26454: \"'c'}\\n\\n\\n\\nwe\", 26455: '.fromstring()', 26456: 'value\\ninto', 26457: 'b\\n', 26458: 'c\\n', 26459: '{b}\\n', 26460: '{o}\\n', 26461: '{c}\\n', 26462: '{o,', 26463: 'c}\\n', 26464: '{(b,', 26465: 'o),', 26466: '(c,', 26467: 'b),', 26468: '(o,', 26469: 'c)}\\n', 26470: '.valuation', 26471: '.fromstring(v)\\n>>>', 26472: \"print(val)\\n{'bertie':\", 26473: \"'b',\\n\", 26474: \"'boy':\", 26475: \"{('b',)},\\n\", 26476: \"'cyril':\", 26477: \"'c',\\n\", 26478: \"'dog':\", 26479: \"{('c',)},\\n\", 26480: \"'girl':\", 26481: \"{('o',)},\\n\", 26482: \"'olive':\", 26483: \"'o',\\n\", 26484: \"'see':\", 26485: \"{('o',\", 26486: \"'c'),\", 26487: \"('c',\", 26488: \"'b'),\", 26489: \"('b',\", 26490: \"'o')},\\n\", 26491: \"'walk':\", 26492: \"{('c',),\", 26493: \"('o',)}}\\n\\n\\n\\nso\", 26494: 'valuation,', 26495: 'of\\ntuples', 26496: 'olive,', 26497: 'and\\nolive', 26498: 'turn:\\ndraw', 26499: 'girl,\\ndog)', 26500: 'singleton', 26501: 'tuples,', 26502: 'convenience', 26503: 'predication\\nof', 26504: 'p(τ1,', 26505: 'τn),', 26506: 'where\\np', 26507: 'the\\ntuple', 26508: '(τ1,', 26509: 'τn)', 26510: \"'c')\", 26511: \"val['see']\\ntrue\\n>>>\", 26512: \"('b',)\", 26513: \"val['boy']\\ntrue\\n\\n\\n\\n\\n\\n3\", 26514: '.5\\xa0\\xa0\\xa0individual', 26515: 'assignments\\nin', 26516: 'variable\\nassignment', 26517: 'constructor,', 26518: 'of\\ndiscourse', 26519: 'any\\nbindings,', 26520: '(variable,\\nvalue)', 26521: 'valuations', 26522: '.assignment(dom,', 26523: \"[('x',\", 26524: \"('y',\", 26525: \"'c')])\\n>>>\", 26526: \"g\\n{'y':\", 26527: \"'x':\", 26528: \"'o'}\\n\\n\\n\\nin\", 26529: 'textbooks:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 26530: \"print(g)\\ng[c/y][o/x]\\n\\n\\n\\nlet's\", 26531: 'of\\nfirst-order', 26532: 'method\\nto', 26533: 'val)\\n>>>', 26534: \".evaluate('see(olive,\", 26535: \"y)',\", 26536: \"g)\\ntrue\\n\\n\\n\\nwhat's\", 26537: 'here?', 26538: 'to\\nour', 26539: 'examplle,', 26540: 'see(olive,', 26541: 'y,\\nrather', 26542: 'val,', 26543: 'value:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 26544: \"g['y']\\n'c'\\n\\n\\n\\nsince\", 26545: 'see\\nrelation,', 26546: 'can\\nsay', 26547: 'satisfies', 26548: 'to\\ng', 26549: \".evaluate('see(y,\", 26550: \"x)',\", 26551: 'g)\\nfalse\\n\\n\\n\\nin', 26552: '(though', 26553: 'logic),', 26554: 'assignments\\nare', 26555: 'variables\\napart', 26556: 'purge()', 26557: 'clears', 26558: 'all\\nbindings', 26559: '.purge()\\n>>>', 26560: 'g\\n{}\\n\\n\\n\\nif', 26561: 'to\\ng,', 26562: 'when\\nwe', 26563: 'function\\nfails', 26564: \"g)\\n'undefined'\\n\\n\\n\\nsince\", 26565: 'boolean\\noperators,', 26566: 'arbitrarily', 26567: 'composed', 26568: \".evaluate('see(bertie,\", 26569: 'olive)', 26570: 'boy(bertie)', 26571: \"-walk(bertie)',\", 26572: 'g)\\ntrue\\n\\n\\n\\nthe', 26573: 'a\\nmodel', 26574: '.6\\xa0\\xa0\\xa0quantification\\none', 26575: 'modern\\nlogic', 26576: 'satisfaction', 26577: 'quantified', 26578: \"let's\\nuse\", 26579: '(24)exists', 26580: '.(girl(x)', 26581: 'walk(x))\\nwhen', 26582: 'true?', 26583: 'domain,\\ni', 26584: 'individuals\\nhave', 26585: 'g[u/x]\\nsatisfies', 26586: '(25)', 26587: '(25)girl(x)', 26588: 'walk(x)\\nconsider', 26589: \".evaluate('exists\", 26590: \"walk(x))',\", 26591: 'g)\\ntrue\\n\\n\\n\\nevaluate()', 26592: 'in\\ndom', 26593: 'binds\\nx', 26594: 'u:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 26595: \".evaluate('girl(x)\", 26596: \"walk(x)',\", 26597: \".add('x',\", 26598: \"'o'))\\ntrue\\n\\n\\n\\none\", 26599: 'satisfiers()', 26600: 'this\\nreturns', 26601: 'an\\nassignment', 26602: 'examples:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 26603: 'fmla1', 26604: \"read_expr('girl(x)\", 26605: \"boy(x)')\\n>>>\", 26606: '.satisfiers(fmla1,', 26607: \"g)\\n{'b',\", 26608: \"'o'}\\n>>>\", 26609: 'fmla2', 26610: \"walk(x)')\\n>>>\", 26611: '.satisfiers(fmla2,', 26612: \"g)\\n{'c',\", 26613: 'fmla3', 26614: \"read_expr('walk(x)\", 26615: \"girl(x)')\\n>>>\", 26616: '.satisfiers(fmla3,', 26617: \"'o'}\\n\\n\\n\\nit's\", 26618: 'the\\nvalues', 26619: 'that\\nfmla2', 26620: '-girl(x)', 26621: 'walk(x),', 26622: 'satisfied\\nby', 26623: 'girl\\nor', 26624: '(bertie)', 26625: '(cyril)\\nare', 26626: 'girls,', 26627: 'satisfy\\nthe', 26628: 'o\\nsatisfies', 26629: 'disjuncts', 26630: 'fmla2,', 26631: 'universally\\nquantified', 26632: \".evaluate('all\", 26633: 'g)\\ntrue\\n\\n\\n\\nin', 26634: 'universally', 26635: 'u,', 26636: 'g[u/x]', 26637: 'pencil', 26638: 'using\\nm', 26639: '.evaluate(),', 26640: '&\\nwalk(x))', 26641: '.(boy(x)', 26642: 'you\\nunderstand', 26643: '.7\\xa0\\xa0\\xa0quantifier', 26644: 'ambiguity\\nwhat', 26645: 'quantifiers,', 26646: 'following?\\n\\n', 26647: '(26)everybody', 26648: 'admires', 26649: 'least)', 26650: '(26)', 26651: 'logic:\\n\\n', 26652: '(27)\\n', 26653: '.(person(x)', 26654: '.(person(y)', 26655: 'admire(x,y)))\\n\\n', 26656: '.exists', 26657: 'admire(x,y)))\\n\\ncan', 26658: 'these?', 26659: 'yes,', 26660: 'different\\nmeanings', 26661: '(27b)', 26662: 'logically', 26663: 'stronger', 26664: '(27a):', 26665: 'bruce,', 26666: 'admired', 26667: '.\\n(27a),', 26668: 'person\\nu,', 26669: \"u'\", 26670: 'u\\nadmires;', 26671: 'we\\ndistinguish', 26672: '(27a)', 26673: 'scope\\nof', 26674: '∀', 26675: 'than\\n∃,', 26676: '(27b),', 26677: 'reversed', 26678: '(26),', 26679: 'are\\nboth', 26680: 'claiming', 26681: 'is\\nambiguous', 26682: 'in\\n(27)', 26683: 'associating', 26684: 'two\\ndistinct', 26685: 'detail\\nhow', 26686: 'our\\nvaluation', 26687: 'bruce', 26688: 'elspeth', 26689: 'e\\n', 26690: 'julia', 26691: 'j\\n', 26692: 'matthew', 26693: 'm\\n', 26694: '{b,', 26695: 'j,', 26696: 'm}\\n', 26697: 'admire', 26698: '{(j,', 26699: '(b,', 26700: '(m,', 26701: 'e),', 26702: '(e,', 26703: 'm)}\\n', 26704: 'val2', 26705: '.fromstring(v2)\\n\\n\\n\\nthe', 26706: 'visualized', 26707: 'the\\nmapping', 26708: '(28)\\nin', 26709: '(28),', 26710: 'and\\ny', 26711: 'admires\\ny', 26712: '(bruce', 26713: 'vain),', 26714: 'admires\\nm', 26715: 'above\\nis', 26716: 'by\\nusing', 26717: 'dom2', 26718: '.domain\\n>>>', 26719: 'm2', 26720: '.model(dom2,', 26721: 'val2)\\n>>>', 26722: 'g2', 26723: '.assignment(dom2)\\n>>>', 26724: 'fmla4', 26725: \"read_expr('(person(x)\", 26726: 'admire(x,', 26727: \"y)))')\\n>>>\", 26728: '.satisfiers(fmla4,', 26729: \"g2)\\n{'e',\", 26730: \"'j'}\\n\\n\\n\\nthis\", 26731: 'fmla5', 26732: 'below;', 26733: 'no\\nsatisfiers', 26734: \"read_expr('(person(y)\", 26735: '.satisfiers(fmla5,', 26736: 'g2)\\nset()\\n\\n\\n\\nthat', 26737: 'everybody', 26738: 'fmla6,', 26739: 'a\\nperson,', 26740: 'fmla6', 26741: '.((x', 26742: 'julia)', 26743: '.satisfiers(fmla6,', 26744: \"g2)\\n{'b'}\\n\\n\\n\\n\\nnote\\nyour\", 26745: 'turn:\\ndevise', 26746: '(27a)\\ncomes', 26747: 'devise', 26748: '(27b)\\ncomes', 26749: '.8\\xa0\\xa0\\xa0model', 26750: 'building\\nwe', 26751: 'check\\nthe', 26752: 'building\\ntries', 26753: 'it\\nsucceeds,', 26754: 'an\\nexistence', 26755: 'mace4', 26756: 'builder', 26757: 'of\\nmace()', 26758: 'build_model()', 26759: 'an\\nanalogous', 26760: 'to\\ntreat', 26761: 'assumptions,', 26762: 'the\\ngoal', 26763: 'unspecified', 26764: 'both\\n[a,', 26765: 'c1]', 26766: '[a,', 26767: 'c2]', 26768: 'mace', 26769: 'succeeds\\nin', 26770: '[c1,', 26771: 'is\\ninconsistent', 26772: 'a3', 26773: '.(man(x)', 26774: \"walks(x))')\\n>>>\", 26775: \"read_expr('mortal(socrates)')\\n>>>\", 26776: \"read_expr('-mortal(socrates)')\\n>>>\", 26777: 'mb', 26778: '.mace(5)\\n>>>', 26779: 'print(mb', 26780: '.build_model(none,', 26781: '[a3,', 26782: 'c1]))\\ntrue\\n>>>', 26783: 'c2]))\\ntrue\\n>>>', 26784: 'c2]))\\nfalse\\n\\n\\n\\nwe', 26785: 'adjunct', 26786: 'is\\nlogically', 26787: 'derivable', 26788: '[s1,', 26789: 's2,', 26790: 'sn]', 26791: 'mace4,', 26792: 'a\\ncounterexample,', 26793: 'from\\ns', 26794: \"s'\", 26795: '=\\n[s1,', 26796: 'sn,', 26797: '-g]', 26798: 'then\\nmace4', 26799: 'counterexample', 26800: 'prover9\\nconcludes', 26801: 'if\\ng', 26802: 'provable', 26803: 'time\\nunsuccessfully', 26804: 'countermodel,', 26805: 'eventually\\ngive', 26806: '[there', 26807: 'a\\nwoman', 26808: 'loves,', 26809: 'adam', 26810: 'man,', 26811: 'eve', 26812: 'a\\nwoman]', 26813: 'false?', 26814: 'macecommand()', 26815: 'inspect\\nthe', 26816: 'a4', 26817: '(woman(y)', 26818: '(man(x)', 26819: \"love(x,y)))')\\n>>>\", 26820: 'a5', 26821: \"read_expr('man(adam)')\\n>>>\", 26822: 'a6', 26823: \"read_expr('woman(eve)')\\n>>>\", 26824: \"read_expr('love(adam,eve)')\\n>>>\", 26825: 'mc', 26826: '.macecommand(g,', 26827: 'assumptions=[a4,', 26828: 'a5,', 26829: 'a6])\\n>>>', 26830: '.build_model()\\ntrue\\n\\n\\n\\nso', 26831: 'yes:', 26832: 'countermodel', 26833: 'than\\neve', 26834: \"mace4's\", 26835: 'model,\\nconverted', 26836: 'print(mc', 26837: \".valuation)\\n{'c1':\", 26838: \"'adam':\", 26839: \"'a',\\n\", 26840: \"'eve':\", 26841: \"'love':\", 26842: \"{('a',\", 26843: \"'b')},\\n\", 26844: \"'man':\", 26845: \"{('a',)},\\n\", 26846: \"'woman':\", 26847: \"{('a',),\", 26848: \"('b',)}}\\n\\n\\n\\nthe\", 26849: 'you:', 26850: 'appropriate\\nkind', 26851: 'puzzling', 26852: 'the\\nc1', 26853: 'skolem', 26854: 'model\\nbuilder', 26855: 'knew\\nthat', 26856: 'open\\nformula', 26857: 'know\\nwhether', 26858: 'denotation', 26859: 'constant\\nanywhere', 26860: 'fly,\\nnamely', 26861: 'about\\nthe', 26862: 'eve,', 26863: 'decided', 26864: 'no\\nreason', 26865: 'denoting', 26866: 'mapped\\nto', 26867: 'woman\\ndenote', 26868: 'disjoint', 26869: 'denotations', 26870: 'this\\nillustrates', 26871: 'dramatically', 26872: 'to\\nbear', 26873: 'knows\\nnothing', 26874: 'builder\\nstill', 26875: 'accord', 26876: 'intuitions\\nabout', 26877: 'situation:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 26878: 'a7', 26879: \"-woman(x))')\\n>>>\", 26880: 'a6,', 26881: 'a7])\\n>>>', 26882: '.build_model()\\ntrue\\n>>>', 26883: \"'c')},\\n\", 26884: \"('b',)}}\\n\\n\\n\\non\", 26885: 'reflection,', 26886: 'that\\neve', 26887: 'discourse,', 26888: 'fact\\nis', 26889: 'further\\nassumption', 26890: '(woman(x)', 26891: '(x', 26892: 'y))', 26893: '.\\n\\n\\n\\n4\\xa0\\xa0\\xa0the', 26894: 'sentences\\n\\n4', 26895: '.1\\xa0\\xa0\\xa0compositional', 26896: 'grammar\\nat', 26897: 'of\\nbuilding', 26898: 'parse,\\nusing', 26899: 'this\\ntime,', 26900: 'logical\\nform', 26901: 'the\\nprinciple', 26902: 'compositionality', 26903: \"frege's\\nprinciple;\", 26904: '(gleitman', 26905: 'liberman,', 26906: '1995)', 26907: 'formulation', 26908: '.)\\nprinciple', 26909: 'compositionality:\\nthe', 26910: 'parts\\nand', 26911: 'complex\\nexpression', 26912: 'granted', 26913: 'parsed\\nagainst', 26914: 'entailed', 26915: 'integrate', 26916: 'representation\\nin', 26917: 'smoothly', 26918: '(29)\\nillustrates', 26919: 'would\\nlike', 26920: '(29)\\nin', 26921: '(29),', 26922: 'semantic\\nrepresentation', 26923: 'at\\nlower', 26924: 'special\\nmanner,', 26925: 'being\\nenclosed', 26926: 'us\\nthis', 26927: 'result?', 26928: 'then\\ncompose', 26929: 'its\\nchild', 26930: 'function\\napplication', 26931: 'of\\ncomposition', 26932: 'specific,', 26933: 'and\\nvp', 26934: 'sem\\nnodes', 26935: 'like\\n(30)', 26936: '(observe', 26937: 'a\\nvariable,', 26938: '(30)s[sem=<?vp(?np)>]', 26939: 'vp[sem=?vp]\\n(30)', 26940: '?np', 26941: 'subject\\nnp', 26942: '?vp', 26943: 'sem\\nvalue', 26944: 'to\\ndenote', 26945: 'its\\ndomain', 26946: '(30)', 26947: 'straightforward;', 26948: '.\\n\\nvp[sem=?v]', 26949: 'iv[sem=?v]\\nnp[sem=<cyril>]', 26950: \"'cyril'\\niv[sem=<\\\\x\", 26951: '.bark(x)>]', 26952: \"'barks'\\n\\nthe\", 26953: \"parent's\", 26954: 'the\\nhead', 26955: \"child's\", 26956: 'non-logical\\nconstants', 26957: 'and\\nbarks', 26958: 'barks', 26959: 'launching', 26960: 'compositional', 26961: 'kit,', 26962: 'λ', 26963: 'calculus', 26964: 'an\\ninvaluable', 26965: 'assemble', 26966: 'meaning\\nrepresentation', 26967: 'λ-calculus\\nin', 26968: 'that\\nmathematical', 26969: 'specifying\\nproperties', 26970: 'a\\ndocument', 26971: '(31),', 26972: 'we\\nglossed', 26973: 'element\\nof', 26974: '(31){w', 26975: 'p(w)}\\nit', 26976: 'that\\nwill', 26977: 'operator\\n(pronounced', 26978: 'lambda)', 26979: 'to\\n(31)', 26980: '(32)', 26981: 'trying\\nto', 26982: '(32)λw', 26983: '(v(w)', 26984: 'p(w))\\n\\nnote\\nλ', 26985: 'alonzo', 26986: 'church', 26987: 'represent\\ncomputable', 26988: 'mathematics\\nand', 26989: 'λ-calculus', 26990: '.\\n\\nλ', 26991: '(33a),', 26992: 'bind\\nthe', 26993: 'in\\n(33b)', 26994: 'in\\n(33c)', 26995: '(33)\\n', 26996: '.(walk(x)', 26997: 'chew_gum(x))\\n\\n', 26998: '.λx', 26999: '.\\\\x', 27000: 'chew_gum(x))\\n\\nremember', 27001: '\\\\),', 27002: 'strings\\n(3', 27003: '.4):\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 27004: \"read_expr(r'\\\\x\", 27005: \"chew_gum(x))')\\n>>>\", 27006: 'expr\\n<lambdaexpression', 27007: '\\\\x', 27008: 'chew_gum(x))>\\n>>>', 27009: \"print(read_expr(r'\\\\x\", 27010: \"chew_gum(y))'))\\n\\\\x\", 27011: 'chew_gum(y))\\n\\n\\n\\n\\nwe', 27012: 'an\\nexpression:', 27013: 'encounter\\nλ-abstracts,', 27014: 'their\\nmeaning', 27015: 'glosses', 27016: '(33b)', 27017: 'be\\nan', 27018: 'chews', 27019: 'gum', 27020: 'or\\nhave', 27021: 'chewing', 27022: 'suggested\\nthat', 27023: 'λ-abstracts', 27024: '(or\\nsubjectless', 27025: 'clauses),', 27026: 'in\\ntheir', 27027: 'its\\ntranslation', 27028: '.to', 27029: 'chew-gum', 27030: 'hard\\n\\n', 27031: '.hard(\\\\x', 27032: 'chew_gum(x)))\\n\\nso', 27033: 'with\\nfree', 27034: 'property\\nexpression', 27035: 'λx', 27036: 'being\\nan', 27037: 'official', 27038: 'abstracts', 27039: 'built:\\n\\n\\n', 27040: '(35)if', 27041: 'then\\n\\\\x', 27042: '.α', 27043: 'τ〉', 27044: '.\\n(34b)', 27045: 'property,', 27046: 'namely\\nthat', 27047: 'to\\nindividuals', 27048: '(36),\\n(33b)', 27049: 'predicated', 27050: 'gerald', 27051: '(36)\\\\x', 27052: 'chew_gum(x))', 27053: '(gerald)\\nnow', 27054: 'gum,\\nwhich', 27055: '(37)', 27056: '(37)(walk(gerald)', 27057: 'chew_gum(gerald))\\nwhat', 27058: '&\\nchew_gum(x))', 27059: '(walk(x)', 27060: 'α[β/x]', 27061: 'as\\nnotation', 27062: 'in\\nα', 27063: 'so:\\n\\n(walk(x)', 27064: 'chew_gum(x))[gerald/x]\\n\\nis', 27065: 'to\\n(37)', 27066: 'semantic\\nrepresentations,', 27067: 'operation\\nis', 27068: 'justified,', 27069: 'we\\nwant', 27070: 'α(β)', 27071: 'semantic\\nvalues', 27072: 'slight\\ncomplication', 27073: 'of\\nexpressions', 27074: 'simplify()', 27075: \"chew_gum(x))(gerald)')\\n>>>\", 27076: 'print(expr)\\n\\\\x', 27077: 'chew_gum(x))(gerald)\\n>>>', 27078: 'print(expr', 27079: '.simplify())', 27080: '\\n(walk(gerald)', 27081: 'chew_gum(gerald))\\n\\n\\n\\nalthough', 27082: 'the\\nλ', 27083: 'a\\nnecessary', 27084: 'restriction;', 27085: 'well-formed\\nexpression', 27086: 'λs', 27087: '(38)\\\\x', 27088: '.\\\\y', 27089: 'own(y,', 27090: 'x))\\njust', 27091: 'predicate,\\n(38)', 27092: 'predicate:', 27093: 'to\\ntwo', 27094: 'be\\nwritten', 27095: 'abbreviated', 27096: \"x))(cyril)')\", 27097: '.simplify())\\n\\\\y', 27098: '.(dog(cyril)', 27099: 'own(y,cyril))\\n>>>', 27100: 'x))(cyril,', 27101: \"angus)')\", 27102: '\\n(dog(cyril)', 27103: 'own(angus,cyril))\\n\\n\\n\\nall', 27104: 'variables:\\nx,', 27105: 'abstract,', 27106: 'say\\n\\\\x', 27107: '.walk(x)', 27108: 'the\\nargument', 27109: 'abstract?', 27110: 'this:\\n\\n\\\\y', 27111: '.y(angus)(\\\\x', 27112: '.walk(x))\\n\\nbut', 27113: 'stipulated', 27114: 'e,\\n\\\\y', 27115: '.y(angus)', 27116: 't〉!', 27117: 'allow\\nabstraction', 27118: 't〉,', 27119: 'as\\n\\\\p', 27120: '.p(angus)', 27121: '〈〈e,', 27122: 't〉', 27123: '\\\\p', 27124: '.p(angus)(\\\\x', 27125: '.walk(x))', 27126: 'legal,', 27127: '.walk(x)(angus)', 27128: 'walk(angus)\\nwhen', 27129: 'β-reduction,', 27130: 'with\\nvariables', 27131: '(39a)', 27132: 'and\\n(39b),', 27133: '.see(y,', 27134: 'x)\\n\\n', 27135: 'z)\\n\\nsuppose', 27136: 'λ-term', 27137: '.p(x)', 27138: 'terms:\\n\\n', 27139: '.\\\\p', 27140: '.p(x)(\\\\y', 27141: 'x))\\n\\n', 27142: 'z))\\n\\nwe', 27143: 'the\\nexistential', 27144: '(40a),', 27145: 'reduction,', 27146: 'results\\nwill', 27147: 'different:\\n\\n', 27148: '(41)\\n', 27149: '.see(x,', 27150: 'z)\\n\\n(41a)', 27151: 'him/herself,', 27152: 'whereas\\n(41b)', 27153: 'unspecified\\nindividual', 27154: 'forbid\\nthe', 27155: '(41a)', 27156: 'what\\nparticular', 27157: 'bound\\nby', 27158: '(40a)?', 27159: 'variable-binding', 27160: '(involving', 27161: '∀,\\n∃', 27162: 'λ),', 27163: 'bound\\nvariable', 27164: 'and\\nexists', 27165: '.p(y)', 27166: 'equivalent;', 27167: 'equivalents,\\nor', 27168: 'relabeling', 27169: 'bound\\nvariables', 27170: 'α-conversion', 27171: 'of\\nvariablebinderexpressions', 27172: 'using\\n==),', 27173: 'α-equivalence:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 27174: 'expr1', 27175: \".p(x)')\\n>>>\", 27176: 'print(expr1)\\nexists', 27177: '.p(x)\\n>>>', 27178: 'expr2', 27179: '.alpha_convert(nltk', 27180: \".variable('z'))\\n>>>\", 27181: 'print(expr2)\\nexists', 27182: '.p(z)\\n>>>', 27183: 'expr2\\ntrue\\n\\n\\n\\nwhen', 27184: 'application\\nf(a),', 27185: 'a\\nwhich', 27186: 'subterms', 27187: 'of\\nf', 27188: 'suppose,', 27189: 'that\\nx', 27190: 'the\\nsubterm', 27191: 'we\\nproduce', 27192: '.p(x),\\nsay,', 27193: 'z1', 27194: '.p(z1),', 27195: 'then\\ncarry', 27196: 'out\\nautomatically', 27197: 'the\\nresults', 27198: 'expr3', 27199: \"read_expr('\\\\p\", 27200: '.(exists', 27201: '.p(x))(\\\\y', 27202: \"x))')\\n>>>\", 27203: 'print(expr3)\\n(\\\\p', 27204: '.see(y,x))\\n>>>', 27205: 'print(expr3', 27206: '.simplify())\\nexists', 27207: '.see(z1,x)\\n\\n\\n\\n\\nnote\\nas', 27208: 'variable\\nnames;', 27209: 'z14', 27210: 'above\\nformula', 27211: 'an\\nillustration', 27212: '.\\n\\nafter', 27213: 'excursus,', 27214: 'english\\nsentences', 27215: '.3\\xa0\\xa0\\xa0quantified', 27216: 'nps\\nat', 27217: 'forgiven', 27218: 'thinking', 27219: 'all\\ntoo', 27220: 'surely', 27221: 'what\\nabout', 27222: 'instance?', 27223: 'want\\n(42a)', 27224: '(42b)', 27225: 'accomplished?\\n\\n', 27226: '(42)\\n', 27227: \"bark(x))\\n\\nlet's\", 27228: 'building\\ncomplex', 27229: 'bark', 27230: 'result\\nin', 27231: '(42b)?', 27232: \"subject's\", 27233: 'value\\nact', 27234: 'called\\ntype-raising', 27235: 'are\\nlooking', 27236: 'instantiating', 27237: '[sem=<?np(\\\\x', 27238: '.bark(x))>]\\nis', 27239: 'equivalent\\nto', 27240: '[sem=<exists', 27241: 'bark(x))>]', 27242: \".\\ndoesn't\", 27243: 'β-reduction\\nin', 27244: 'λ-calculus?', 27245: 'term\\nm', 27246: \"to\\n'bark'\", 27247: \"of\\n'bark'\", 27248: 'with\\nλ,', 27249: '(43)', 27250: '(43)\\\\p', 27251: 'p(x))\\nwe', 27252: 'in\\n(43)', 27253: \"'x'\", 27254: \"'y'\", 27255: 'signal\\nthat', 27256: 'an\\nindividual,', 27257: 'of\\n(43)', 27258: 'illustrate\\nfurther,', 27259: '(44)\\\\p', 27260: 'a\\nfurther', 27261: 'the\\nsemantics', 27262: '(43),', 27263: 'of\\ndog', 27264: '(45)\\\\q', 27265: '.(q(x)', 27266: 'p(x))\\napplying', 27267: 'applying\\nthat', 27268: 'us\\n\\\\p', 27269: 'p(x))(\\\\x', 27270: '.bark(x))', 27271: 'out\\nβ-reduction\\nyields', 27272: 'wanted,', 27273: '.4\\xa0\\xa0\\xa0transitive', 27274: 'verbs\\nour', 27275: 'transitive\\nverbs,', 27276: '(46)angus', 27277: 'chases', 27278: 'chase(angus,', 27279: 'x))', 27280: 'λ-abstraction', 27281: 'this\\nresult', 27282: 'require\\nthat', 27283: 'of\\nwhether', 27284: 'in\\nother', 27285: 'sticking', 27286: 'to\\n(43)', 27287: 'that\\nvps', 27288: 'regardless\\nof', 27289: 'transitive\\nverb', 27290: 'specifically,', 27291: 'vps\\nare', 27292: 'these\\nconstraints,', 27293: 'dog\\nwhich', 27294: 'trick', 27295: '(47)\\\\y', 27296: 'chase(y,', 27297: 'x))\\nthink', 27298: '(47)', 27299: 'that\\nfor', 27300: 'x;', 27301: 'more\\ncolloquially,', 27302: 'now\\nresolves', 27303: 'for\\nchases', 27304: 'allow\\n(47)', 27305: 'inverse', 27306: '(47),\\ngiving', 27307: '(48)', 27308: '(48)\\\\p', 27309: 'p(x))(\\\\z', 27310: '.chase(y,', 27311: 'z))\\n(48)', 27312: 'first;', 27313: 'from\\n(43)', 27314: '\\\\z', 27315: '.chase(y,z)', 27316: 'is\\nequivalent', 27317: 'np;', 27318: 'type\\n〈〈e,', 27319: '(49)x(\\\\z', 27320: 'z))\\nthe', 27321: 'of\\nvps,', 27322: 'ensure\\nthis', 27323: '(49)', 27324: 'by\\ngiving', 27325: '(50)\\\\x', 27326: '.x(\\\\x', 27327: 'x))\\nif', 27328: '(47),', 27329: 'along:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 27330: 'tvp', 27331: '.x(\\\\y', 27332: \".chase(x,y))')\\n>>>\", 27333: \"read_expr(r'(\\\\p\", 27334: \"p(x)))')\\n>>>\", 27335: '.applicationexpression(tvp,', 27336: 'np)\\n>>>', 27337: 'print(vp)\\n(\\\\x', 27338: '.chase(x,y)))(\\\\p', 27339: 'p(x)))\\n>>>', 27340: 'print(vp', 27341: '.simplify())\\n\\\\x', 27342: 'z2', 27343: '.(dog(z2)', 27344: 'chase(x,z2))\\n\\n\\n\\nin', 27345: 'also\\nneed', 27346: 'the\\nlatter', 27347: 'girl,', 27348: 'everything\\nproceeds', 27349: 'earlier\\non;', 27350: 'the\\nsemantic', 27351: 'have\\ncreated', 27352: 'these\\nhave', 27353: 'these\\ncannot', 27354: 'like\\n(47)', 27355: 'do\\nin', 27356: 're-interpret', 27357: 'are\\nfunction', 27358: 'required\\nλ', 27359: '(51)\\\\p', 27360: '.p(angus)\\n(51)', 27361: 'individual\\nconstant', 27362: 'of\\ntype-raising,', 27363: 'a\\nboolean-valued', 27364: 'an\\nequivalent', 27365: 'by\\nβ-reduction,', 27366: 'simple-sem', 27367: 'parsing\\nand', 27368: 'looking\\nat', 27369: \"load_parser('grammars/book_grammars/simple-sem\", 27370: 'trace=0)\\n>>>', 27371: \"'angus\", 27372: \"dog'\\n>>>\", 27373: 'print(tree', 27374: \".label()['sem'])\\nall\", 27375: '.(bone(z1)', 27376: 'give(angus,z1,z2)))\\n\\n\\n\\nnltk', 27377: 'inspect\\nsemantic', 27378: 'interpret_sents()', 27379: 'is\\nintended', 27380: 'it\\nbuilds', 27381: 'the\\ninput,', 27382: 'd[sent]', 27383: '(synrep,', 27384: 'semrep)', 27385: 'consisting\\nof', 27386: 'value\\nis', 27387: 'ambiguous;', 27388: 'sentence\\nin', 27389: \"['irene\", 27390: \"walks',\", 27391: \"'cyril\", 27392: 'bites', 27393: \"ankle']\\n>>>\", 27394: 'grammar_file', 27395: \"'grammars/book_grammars/simple-sem\", 27396: \".fcfg'\\n>>>\", 27397: '.interpret_sents(sents,', 27398: 'grammar_file):\\n', 27399: 'results:\\n', 27400: 'print(synrep)\\n(s[sem=<walk(irene)>]\\n', 27401: '(np[-loc,', 27402: 'sem=<\\\\p', 27403: '.p(irene)>]\\n', 27404: '(propn[-loc,', 27405: '.p(irene)>]', 27406: 'irene))\\n', 27407: 'sem=<\\\\x', 27408: '.walk(x)>]\\n', 27409: \"(iv[num='sg',\", 27410: '.walk(x)>,', 27411: \"tns='pres']\", 27412: 'walks)))\\n(s[sem=<exists', 27413: 'z3', 27414: '.(ankle(z3)', 27415: 'bite(cyril,z3))>]\\n', 27416: '.p(cyril)>]\\n', 27417: '.p(cyril)>]', 27418: 'cyril))\\n', 27419: 'bite(x,z3))>]\\n', 27420: '.bite(x,y))>,', 27421: 'bites)\\n', 27422: \"(np[num='sg',\", 27423: 'sem=<\\\\q', 27424: '.(ankle(x)', 27425: 'q(x))>]\\n', 27426: \"(det[num='sg',\", 27427: '.(p(x)', 27428: 'q(x))>]', 27429: 'an)\\n', 27430: \"(nom[num='sg',\", 27431: '.ankle(x)>]\\n', 27432: \"(n[num='sg',\", 27433: '.ankle(x)>]', 27434: 'ankle)))))\\n\\n\\n\\nwe', 27435: 'and\\nearlier', 27436: 'truth\\nvalue', 27437: 'as\\ndefined', 27438: 'evaluate_sents()', 27439: 'resembles\\ninterpret_sents()', 27440: 'triple', 27441: '(synrep,\\nsemrep,', 27442: 'synrep,\\nsemrep', 27443: 'simplicity,\\nthe', 27444: '.assignment(val', 27445: '.domain)\\n>>>', 27446: '.model(val', 27447: '.domain,', 27448: \"boy'\\n>>>\", 27449: '.evaluate_sents([sent],', 27450: 'grammar_file,', 27451: 'g)[0]\\n>>>', 27452: '(syntree,', 27453: 'semrep,', 27454: 'print(semrep)\\n', 27455: 'print(value)\\nall', 27456: 'z4', 27457: '.(boy(z4)', 27458: 'see(cyril,z4))\\ntrue\\n\\n\\n\\n\\n\\n\\n4', 27459: '.5\\xa0\\xa0\\xa0quantifier', 27460: 'revisited\\n\\none', 27461: 'is\\nsyntax-driven,', 27462: 'is\\nclosely', 27463: 'coupled', 27464: 'always\\nbe', 27465: '(53a),', 27466: '(53b)', 27467: '(52)every', 27468: '(53)\\n', 27469: '.(dog(y)', 27470: 'chase(x,y)))\\n\\n', 27471: 'chase(x,y)))\\n\\nthere', 27472: 'numerous', 27473: \"let's\\nbriefly\", 27474: 'scoped', 27475: \"scopings\\n\\nlet's\", 27476: 'top,', 27477: 'the\\nquantifier', 27478: 'thought\\nof', 27479: 'downwards,', 27480: 'plug', 27481: 'instantiation', 27482: 'of\\nφ', 27483: 'ψ,', 27484: \"'core'\", 27485: 'semantics,\\nnamely', 27486: 'chases\\ny', 27487: 'identical,', 27488: 'have\\nswapped', 27489: 'round', 27490: 'cooper', 27491: 'pair\\nconsisting', 27492: 'core', 27493: 'binding\\noperators', 27494: 'being\\nidentical', 27495: 'a\\ncooper-storage', 27496: '(52),', 27497: \"and\\nlet's\", 27498: 'chase(x,y)', 27499: 'in\\n(52),', 27500: '.\\n\\n\\\\p', 27501: 'p(y))(\\\\z2', 27502: '.chase(z1,z2))\\n\\nthen', 27503: 'p(x))(\\\\z1', 27504: 'chase(z1,x)))\\n\\nonce', 27505: 's-retrieval', 27506: 'possible\\norder', 27507: 'permutations\\nof', 27508: '.5),\\nthen', 27509: 'core+store\\nrepresentation', 27510: 'compositionally', 27511: 'lexical\\nrule', 27512: 'be\\nembedded', 27513: \"machinery,\\nlet's\", 27514: 'smiles', 27515: 'a\\nlexical', 27516: '(taken', 27517: 'grammar\\nstorage', 27518: '.fcfg)', 27519: '.\\n\\niv[sem=[core=<\\\\x', 27520: '.smile(x)>,', 27521: 'store=(/)]]', 27522: \"'smiles'\\n\\nthe\", 27523: '.\\n\\nnp[sem=[core=<@x>,', 27524: 'store=(<bo(\\\\p', 27525: '.p(cyril),@x)>)]]', 27526: \"'cyril'\\n\\nthe\", 27527: 'bo', 27528: 'subparts:', 27529: '(type-raised)\\nrepresentation', 27530: '@x,', 27531: 'the\\nneed', 27532: '@x', 27533: 'a\\nmetavariable,', 27534: 'individual\\nvariables', 27535: 'just\\npercolates', 27536: 'the\\ninteresting', 27537: '.\\n\\nvp[sem=?s]', 27538: 'iv[sem=?s]\\n\\ns[sem=[core=<?vp(?np)>,', 27539: 'store=(?b1+?b2)]]', 27540: '->\\n', 27541: 'np[sem=[core=?np,', 27542: 'store=?b1]]', 27543: 'vp[sem=[core=?vp,', 27544: 'store=?b2]]\\n\\nthe', 27545: \"the\\nvp's\", 27546: '.smile(x),', 27547: \"subject\\nnp's\", 27548: 'an\\ninstantiation', 27549: 'β-reduction,\\n<?vp(?np)>', 27550: '<smile(z3)>', 27551: 'when\\n@x', 27552: 'be\\ninstantiated', 27553: 'uniformly', 27554: \"np's\", 27555: 'z3,', 27556: 'yielding\\nthe', 27557: 'bo(\\\\p', 27558: '.p(cyril),z3)', 27559: '.\\n\\n(s[sem=[core=<smile(z3)>,', 27560: 'store=(bo(\\\\p', 27561: '.p(cyril),z3))]]\\n', 27562: '(np[sem=[core=<z3>,', 27563: '.p(cyril),z3))]]', 27564: 'cyril)\\n', 27565: '(vp[sem=[core=<\\\\x', 27566: 'store=()]]\\n', 27567: '(iv[sem=[core=<\\\\x', 27568: 'store=()]]', 27569: \"smiles)))\\n\\nlet's\", 27570: 'the\\nstorage', 27571: '.\\n\\ncore', 27572: '<chase(z1,z2)>\\nstore', 27573: '(bo(\\\\p', 27574: 'p(x)),z1),', 27575: 'p(x)),z2))\\n\\n\\nit', 27576: 'clearer', 27577: 'important\\npart', 27578: 's-retrieval,', 27579: 'them\\nsuccessively', 27580: '.(girl(x)\\n->', 27581: 'chase(z1,z2)', 27582: 'p(x)),\\nand', 27583: 'chase(z1,z2),', 27584: 'be\\nturned', 27585: 'λ-abstract', 27586: 'to\\nabstract', 27587: 'over?', 27588: 'us;', 27589: 'chaser', 27590: 'chasee', 27591: '.cooper_storage', 27592: 'turning\\nstorage-style', 27593: 'logical\\nforms', 27594: 'cooperstore', 27595: 'its\\nstore', 27596: 'cooper_storage', 27597: 'cs\\n>>>', 27598: \"'every\", 27599: 'cs', 27600: '.parse_with_bindops(sentence,', 27601: \"grammar='grammars/book_grammars/storage\", 27602: 'semrep', 27603: 'cs_semrep', 27604: '.cooperstore(semrep)\\n>>>', 27605: 'print(cs_semrep', 27606: '.core)\\nchase(z2,z4)\\n>>>', 27607: '.store:\\n', 27608: 'print(bo)\\nbo(\\\\p', 27609: 'p(x)),z2)\\nbo(\\\\p', 27610: 'p(x)),z4)\\n\\n\\n\\nfinally', 27611: 's_retrieve()', 27612: '.s_retrieve(trace=true)\\npermutation', 27613: '(\\\\p', 27614: 'p(x)))(\\\\z2', 27615: '.chase(z2,z4))\\n', 27616: 'p(x)))(\\\\z4', 27617: 'chase(x,z4)))\\npermutation', 27618: 'chase(z2,x)))\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 27619: '.readings:\\n', 27620: 'print(reading)\\nexists', 27621: '.(girl(z3)', 27622: 'chase(z3,x)))\\nall', 27623: '.(dog(z4)', 27624: 'chase(x,z4)))\\n\\n\\n\\n\\n\\n\\n5\\xa0\\xa0\\xa0discourse', 27625: 'semantics\\na', 27626: 'preceded\\nit', 27627: 'anaphoric', 27628: 'pronouns,', 27629: 'as\\nhe,', 27630: 'probably\\ninterpret', 27631: 'referring', 27632: 'angus\\nused', 27633: '.1\\xa0\\xa0\\xa0discourse', 27634: 'theory\\n\\nthe', 27635: 'quantification', 27636: 'single\\nsentences', 27637: 'a\\nquantifier', 27638: 'above,\\nand', 27639: 'owns', 27640: 'irene', 27641: 'own(angus,', 27642: 'x)', 27643: 'bite(x,', 27644: 'irene))\\n\\n\\nthat', 27645: 'binds\\nthe', 27646: 'theory\\n(drt)', 27647: 'of\\nproviding', 27648: 'which\\nseem', 27649: '(drs)', 27650: 'things\\nunder', 27651: 'drs', 27652: 'apply\\nto', 27653: 'referents,', 27654: 'open\\nformulas', 27655: '(54a)', 27656: 'drs;', 27657: 'processing\\nthe', 27658: 'of\\nprocessing', 27659: 'integrating', 27660: 'processed,', 27661: 'it\\ntriggers', 27662: 'referent,', 27663: 'u,\\nand', 27664: 'that\\nis,', 27665: 'drt,', 27666: 'task\\nof', 27667: 'involves\\nlinking', 27668: 'referent', 27669: 'drs,\\nand', 27670: 'about\\nanaphora', 27671: 'rise\\nto', 27672: 'and\\nthis', 27673: 'two-sentence', 27674: 'but\\nin', 27675: 'inquire', 27676: 'such\\nthat', 27677: 'named\\nangus,', 27678: 'named\\nirene', 27679: 'drss', 27680: 'computationally,', 27681: 'them\\ninto', 27682: 'conditions:\\n\\n([x,', 27683: 'y],', 27684: '[angus(x),', 27685: 'dog(y),', 27686: 'own(x,y)])\\n\\nthe', 27687: 'read_dexpr', 27688: '.drtexpression', 27689: 'drs1', 27690: \"read_dexpr('([x,\", 27691: 'own(x,', 27692: \"y)])')\", 27693: 'print(drs1)\\n([x,y],[angus(x),', 27694: 'own(x,y)])\\n\\n\\n\\n\\n\\nwe', 27695: '\\n\\n\\n\\n\\n\\nfigure', 27696: 'screenshot\\n\\nwhen', 27697: 'were\\ninterpreted', 27698: 'fol()', 27699: 'method\\nimplements', 27700: 'print(drs1', 27701: '.fol())\\nexists', 27702: '.(angus(x)', 27703: 'dog(y)', 27704: 'own(x,y))\\n\\n\\n\\nin', 27705: 'logic\\nexpressions,', 27706: 'drt', 27707: 'drs-concatenation', 27708: 'operator,\\nrepresented', 27709: 'drss\\nis', 27710: 'the\\nconditions', 27711: 'automatically\\nα-converts', 27712: 'name-clashes', 27713: 'drs2', 27714: \"read_dexpr('([x],\", 27715: '[walk(x)])', 27716: '([y],', 27717: \"[run(y)])')\\n>>>\", 27718: 'print(drs2)\\n(([x],[walk(x)])', 27719: '([y],[run(y)]))\\n>>>', 27720: 'print(drs2', 27721: '.simplify())\\n([x,y],[walk(x),', 27722: 'run(y)])\\n\\n\\n\\nwhile', 27723: 'atomic,', 27724: 'universal\\nquantification', 27725: 'drs3,', 27726: 'top-level\\ndiscourse', 27727: 'two\\nsub-drss,', 27728: 'use\\nfol()', 27729: 'drs3', 27730: \"read_dexpr('([],\", 27731: '[(([x],', 27732: '[dog(x)])', 27733: '([y],[ankle(y),', 27734: \"y)]))])')\\n>>>\", 27735: 'print(drs3', 27736: '.fol())\\nall', 27737: '.(ankle(y)', 27738: 'bite(x,y)))\\n\\n\\n\\nwe', 27739: 'be\\ninterpreted', 27740: 'sets\\nconstraints', 27741: 'possible\\nantecedents,', 27742: 'antecedent\\nis', 27743: 'candidates', 27744: '.drt_resolve_anaphora', 27745: 'similarly\\nconservative', 27746: 'strategy:', 27747: 'form\\npro(x),', 27748: 'resolve_anaphora()', 27749: 'a\\ncondition', 27750: 'antecedents', 27751: 'drs4', 27752: \"y)])')\\n>>>\", 27753: 'drs5', 27754: \"read_dexpr('([u,\", 27755: 'z],', 27756: '[pro(u),', 27757: 'irene(z),', 27758: 'bite(u,', 27759: \"z)])')\\n>>>\", 27760: 'drs6', 27761: 'drs5\\n>>>', 27762: 'print(drs6', 27763: '.simplify())\\n([u,x,y,z],[angus(x),', 27764: 'own(x,y),', 27765: 'pro(u),', 27766: 'bite(u,z)])\\n>>>', 27767: '.simplify()', 27768: '.resolve_anaphora())\\n([u,x,y,z],[angus(x),', 27769: '(u', 27770: '[x,y,z]),', 27771: 'bite(u,z)])\\n\\n\\n\\nsince', 27772: 'into\\nits', 27773: 'swapping', 27774: 'procedures\\nwhich', 27775: 'correct\\nantecedent', 27776: 'fully\\ncompatible', 27777: 'machinery', 27778: 'λ\\nabstraction,', 27779: 'semantic\\nrepresentations', 27780: 'this\\ntechnique', 27781: 'indefinites\\n(which', 27782: 'ease', 27783: 'comparison,\\nwe', 27784: 'indefinites', 27785: 'from\\nsimple-sem', 27786: '.\\n\\ndet[num=sg,sem=<\\\\p', 27787: '.(([x],[])', 27788: 'p(x)', 27789: \"'a'\\ndet[num=sg,sem=<\\\\p\", 27790: \"'a'\\n\\nto\", 27791: 'works,', 27792: 'subtree\\nfor', 27793: \".\\n\\n(np[num='sg',\", 27794: '.(([x],[dog(x)])', 27795: '.((([x],[])', 27796: 'p(x))', 27797: 'a)\\n', 27798: '.([],[dog(x)])>]\\n', 27799: '.([],[dog(x)])>]', 27800: 'dog)))))\\n\\nthe', 27801: 'to\\n\\\\x', 27802: '.([],[dog(x)])', 27803: '\\\\q', 27804: '([],[dog(x)])', 27805: '+\\nq(x));', 27806: 'simplification,', 27807: 'q(x))\\nas', 27808: 'to\\nload_parser()', 27809: 'be\\nparsed', 27810: 'drtparser', 27811: \"load_parser('grammars/book_grammars/drt\", 27812: 'logic_parser=nltk', 27813: '.drt', 27814: '.drtparser())\\n>>>', 27815: 'list(parser', 27816: \".parse('angus\", 27817: 'print(trees[0]', 27818: \".label()['sem']\", 27819: '.simplify())\\n([x,z2],[angus(x),', 27820: 'dog(z2),', 27821: 'own(x,z2)])\\n\\n\\n\\n\\n\\n5', 27822: '.2\\xa0\\xa0\\xa0discourse', 27823: 'processing\\nwhen', 27824: 'for\\ninterpretation,', 27825: 'in\\npart', 27826: 'the\\nmeaning', 27827: 'prior\\ndiscourse,', 27828: 'glaringly', 27829: 'the\\nprocessing', 27830: 'inference;', 27831: 'only\\nprocessed', 27832: 'omissions', 27833: 'redressed', 27834: 'the\\nmodule', 27835: '.inference', 27836: '.discourse', 27837: '.\\n\\nwhereas', 27838: 's1,', 27839: 'sn', 27840: 'of\\nsentences,', 27841: 'thread', 27842: 'sequence\\ns1-ri,', 27843: 'sn-rj\\nof', 27844: 'readings,', 27845: 'incrementally,', 27846: 'possible\\nthreads', 27847: 'example\\nignores', 27848: \".discoursetester(['a\", 27849: \"dances',\", 27850: \"person'])\\n>>>\", 27851: '.readings()\\n\\ns0', 27852: 'readings:\\n\\ns0-r0:', 27853: '.(student(x)', 27854: 'dance(x))\\n\\ns1', 27855: 'readings:\\n\\ns1-r0:', 27856: 'person(x))\\n\\n\\n\\nwhen', 27857: 'consistchk=true', 27858: 'checked\\nby', 27859: 'checker', 27860: 'thread,', 27861: 'of\\nadmissible', 27862: 'option\\nof', 27863: 'retracting', 27864: \".add_sentence('no\", 27865: 'consistchk=true)\\ninconsistent', 27866: 'discourse:', 27867: 'd0', 27868: \"['s0-r0',\", 27869: \"'s1-r0',\", 27870: \"'s2-r0']:\\n\", 27871: 's0-r0:', 27872: 'dance(x))\\n', 27873: 's1-r0:', 27874: 'person(x))\\n', 27875: 's2-r0:', 27876: '-exists', 27877: 'dance(x))\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 27878: \".retract_sentence('no\", 27879: 'verbose=true)\\ncurrent', 27880: 'are\\ns0:', 27881: 'dances\\ns1:', 27882: 'person\\n\\n\\n\\nin', 27883: 'informchk=true', 27884: 'φ\\nis', 27885: 'treats', 27886: 'existing\\nsentences', 27887: 'φ;', 27888: 'informative\\nif', 27889: \".add_sentence('a\", 27890: 'informchk=true)\\nsentence', 27891: \"dances'\", 27892: \"'exists\", 27893: \"dance(x))':\\nnot\", 27894: \"'d0'\\n\\n\\n\\nit\", 27895: 'as\\nbackground', 27896: 'inconsistent\\nreadings;', 27897: 'semantic\\nambiguity', 27898: 'invokes', 27899: 'glue\\nsemantics', 27900: 'configured', 27901: 'wide-coverage', 27902: 'malt', 27903: '(every', 27904: 'regexptagger\\n>>>', 27905: 'regexptagger(\\n', 27906: \"[('^(chases|runs)$',\", 27907: \"'vb'),\\n\", 27908: \"('^(a)$',\", 27909: \"'ex_quant'),\\n\", 27910: \"('^(every)$',\", 27911: \"'univ_quant'),\\n\", 27912: \"('^(dog|boy)$',\", 27913: \"'nn'),\\n\", 27914: \"('^(he)$',\", 27915: \"'prp')\\n\", 27916: 'rc', 27917: '.drtgluereadingcommand(depparser=nltk', 27918: '.maltparser(tagger=tagger))\\n>>>', 27919: \".discoursetester(['every\", 27920: \"boy',\", 27921: \"'he\", 27922: \"runs'],\", 27923: 'rc)\\n>>>', 27924: '([],[(([x],[dog(x)])', 27925: '([z3],[boy(z3),', 27926: 'chases(x,z3)]))])\\ns0-r1:', 27927: '([z4],[boy(z4),', 27928: '(([x],[dog(x)])', 27929: '([],[chases(x,z4)]))])\\n\\ns1', 27930: '([x],[pro(x),', 27931: 'runs(x)])\\n\\n\\n\\nthe', 27932: 'the\\nquantfier', 27933: 'scoping', 27934: 'pronoun\\nhe', 27935: 'pro(x)`', 27936: 'threads', 27937: 'that\\nresult:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 27938: '.readings(show_thread_readings=true)\\nd0:', 27939: \"'s1-r0']\", 27940: 'invalid:', 27941: 'anaphoraresolutionexception\\nd1:', 27942: \"['s0-r1',\", 27943: '([z6,z10],[boy(z6),', 27944: '->\\n([],[chases(x,z6)])),', 27945: '(z10', 27946: 'z6),', 27947: 'runs(z10)])\\n\\n\\n\\nwhen', 27948: 'd1,', 27949: 's0-r0,', 27950: 'out-scopes\\na', 27951: 'inadmissible', 27952: 'the\\npronoun', 27953: '(relettered', 27954: 'z24)', 27955: 'the\\nequation', 27956: '(z24', 27957: 'z20)', 27958: '.\\ninadmissible', 27959: 'filtered', 27960: 'parameter\\nfilter=true', 27961: '.readings(show_thread_readings=true,', 27962: 'filter=true)\\nd1:', 27963: '([z12,z15],[boy(z12),', 27964: '->\\n([],[chases(x,z12)])),', 27965: '(z17', 27966: 'z12),', 27967: 'runs(z15)])\\n\\n\\n\\nalthough', 27968: 'a\\nfeel', 27969: '.\\n\\n\\n\\n6\\xa0\\xa0\\xa0summary\\n\\nfirst', 27970: 'representing\\nnatural', 27971: 'is\\nflexible', 27972: 'and\\nthere', 27973: 'provers', 27974: 'order\\nlogic', 27975: '(equally,', 27976: 'semantics\\nwhich', 27977: 'believed', 27978: '.)\\nas', 27979: 'order\\nlogic,', 27980: 'by\\nexamining', 27981: 'compositionally,', 27982: 'we\\nsupplement', 27983: '.\\nβ-reduction', 27984: 'semantically\\nto', 27985: 'syntactically,', 27986: 'it\\ninvolves', 27987: 'valuation\\nwhich', 27988: 'are\\ninterpreted', 27989: 'n-ary', 27990: 'as\\nindividual', 27991: 'free\\nvariables', 27992: 'when\\ntheir', 27993: '.\\nquantifiers', 27994: 'constructing,', 27995: 'φ[x]', 27996: 'individuals\\nwhich', 27997: 'g\\nassigns', 27998: 'places\\nconstraints', 27999: 'variables;', 28000: 'the\\nvariables', 28001: 'or\\nfalse', 28002: 'by\\nbinding', 28003: 'quantifier)', 28004: 'equivalents', 28005: 'q1', 28006: 'q2,', 28007: 'outermost', 28008: 'wide\\nscope', 28009: 'q2)', 28010: 'frequently\\nambiguous', 28011: 'they\\ncontain', 28012: 'representation\\nby', 28013: 'the\\nsem', 28014: 'functional\\napplication', 28015: 'reading\\nconsult', 28016: 'the\\nprover9', 28017: 'two\\ninference', 28018: '(mccune,', 28019: 'and\\nlogic', 28020: 'other\\napproaches', 28021: 'in\\n(blackburn', 28022: 'bos,', 28023: '(dalrymple,', 28024: 'on\\nin', 28025: 'notably:\\n\\nevents,', 28026: 'aspect;\\nsemantic', 28027: 'roles;\\ngeneralized', 28028: 'most;\\nintensional', 28029: 'involving,', 28030: 'and\\nbelieve', 28031: '.\\n\\nwhile', 28032: 'dealt', 28033: 'language\\nfront-ends', 28034: '(androutsopoulos,', 28035: 'ritchie,', 28036: 'thanisch,', 28037: '(hodges,', 28038: '1977)', 28039: 'recommended', 28040: 'entertaining', 28041: 'insightful', 28042: 'text\\nwith', 28043: 'wide-ranging,', 28044: 'two-volume', 28045: 'contemporary\\nmaterial', 28046: 'and\\nintensional', 28047: '(gamut,', 28048: '1991)', 28049: '(kamp', 28050: 'reyle,', 28051: 'provides\\nthe', 28052: 'definitive', 28053: 'and\\ninteresting', 28054: 'tense,', 28055: 'and\\nmodality', 28056: 'language\\nconstructions', 28057: '(chierchia', 28058: 'mcconnell-ginet,', 28059: 'agnostic', 28060: 'while\\n(heim', 28061: 'kratzer,', 28062: '(larson', 28063: 'segal,', 28064: 'towards\\nintegrating', 28065: '.\\n(blackburn', 28066: 'computational\\nsemantics,', 28067: 'it\\nexpands', 28068: 'including\\nunderspecification', 28069: 'order\\ninference,', 28070: 'including\\ntreatments', 28071: '(lappin,', 28072: 'or\\n(benthem', 28073: 'meulen,', 28074: 'logic\\nand', 28075: '.\\nprovide', 28076: 'your\\ntranslation', 28077: 'sings,', 28078: 'sulks', 28079: '.\\ncyril', 28080: 'snow', 28081: 'rain', 28082: 'tofu', 28083: '.\\npat', 28084: 'cough', 28085: 'sneeze', 28086: 'into\\npredicate-argument', 28087: '.\\n\\nangus', 28088: 'hates', 28089: '.\\ntofu', 28090: 'taller', 28091: '.\\nbruce', 28092: 'fourlegged', 28093: 'friend', 28094: 'into\\nquantified', 28095: '.\\nangus', 28096: '.\\nnobody', 28097: '.\\nsomebody', 28098: 'coughs', 28099: 'sneezes', 28100: 'coughed', 28101: 'sneezed', 28102: 'somebody', 28103: '.\\nexactly', 28104: 'asleep', 28105: '.\\nquantified', 28106: '.\\n\\nfeed', 28107: 'capuccino', 28108: 'angus\\nbe', 28109: \"'war\", 28110: \"peace'\", 28111: 'pat\\nbe', 28112: 'loved', 28113: 'everyone\\nbe', 28114: 'detested', 28115: 'no-one\\n\\n\\n☼', 28116: 'statements:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 28117: 'e2', 28118: \"read_expr('pat')\\n>>>\", 28119: 'e3', 28120: '.applicationexpression(e1,', 28121: 'e2)\\n>>>', 28122: 'print(e3', 28123: '.love(pat,', 28124: 'y)\\n\\n\\n\\nclearly', 28125: 'e1', 28126: 'applicationexpression(e1,', 28127: 'e2)\\nto', 28128: 'β-convertible', 28129: 'e1\\nmust', 28130: 'to\\ne1,', 28131: 'all\\nsatisfied', 28132: '(up', 28133: 'variance)', 28134: 'of\\ne3', 28135: '.(love(pat,y)', 28136: 'love(y,pat))\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 28137: '.simplify())\\nwalk(fido)\\n\\n\\n\\n\\n☼', 28138: 'exercise,', 28139: 'yields\\nresults', 28140: \"read_expr('chase')\\n>>>\", 28141: 'chase(x,pat))\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 28142: 'chase(pat,x))\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 28143: \"read_expr('give')\\n>>>\", 28144: '.simplify())\\n\\\\x0', 28145: 'x1', 28146: '.(present(y)', 28147: 'give(x1,y,x0))\\n\\n\\n\\n\\n☼', 28148: \"read_expr('bark')\\n>>>\", 28149: 'bark(x))\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 28150: '.simplify())\\nbark(fido)\\n\\n\\n\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 28151: \"read_expr('\\\\\\\\p\", 28152: '(dog(x)', 28153: \"p(x))')\\n>>>\", 28154: '.simplify())\\nall', 28155: 'bark(x))\\n\\n\\n\\n\\n◑', 28156: 'into\\nformulas', 28157: 'an\\napproach,', 28158: 'quantified\\nformula', 28159: 'q(a,', 28160: 'are\\nexpressions', 28161: 'example,\\nall(a,', 28162: 'the\\ntruth', 28163: 'be\\ncomputed', 28164: '.evaluate', 28165: 'will\\ngive', 28166: 'domain\\nof', 28167: 'possible\\nsource', 28168: '.gutenberg:', 28169: '.txt,\\nburgess-busterbrown', 28170: 'and\\nedgeworth-parents', 28171: 'your\\nsentences', 28172: 'truth\\nor', 28173: 'technique\\nfor', 28174: 'query\\nof', 28175: '(p(x)', 28176: 'q(x)),', 28177: '(q(x)', 28178: 'of\\np', 28179: 'acst11', 28180: 'data\\n\\n\\n\\n\\n\\n11', 28181: 'data\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nstructured', 28182: 'nlp,\\nhowever,', 28183: 'face', 28184: 'obstacles', 28185: 'its\\ncoverage,', 28186: 'balance,', 28187: 'uses?\\nwhen', 28188: 'tool,\\nhow', 28189: 'format?\\nwhat', 28190: 'it?\\n\\nalong', 28191: 'the\\ntypical', 28192: 'workflow', 28193: 'lifecycle', 28194: 'from\\npractical', 28195: 'including\\ndata', 28196: 'collected', 28197: 'fieldwork,\\nlaboratory', 28198: 'crawling', 28199: '.\\n\\n1\\xa0\\xa0\\xa0corpus', 28200: 'study\\nthe', 28201: 'timit', 28202: 'be\\nwidely', 28203: '.\\ntimit', 28204: 'texas', 28205: 'instruments', 28206: 'mit,', 28207: 'which\\nit', 28208: 'acoustic-phonetic', 28209: 'to\\nsupport', 28210: 'timit\\nlike', 28211: 'sources,\\ntimit', 28212: 'dialects,', 28213: 'speakers,', 28214: 'of\\neight', 28215: 'dialect', 28216: 'regions,', 28217: 'ages', 28218: 'educational\\nbackgrounds', 28219: 'all\\nspeakers,', 28220: 'variation:\\n\\n', 28221: '.she', 28222: 'dark', 28223: 'suit', 28224: 'greasy', 28225: 'wash', 28226: 'year\\n\\n', 28227: \".don't\", 28228: 'oily', 28229: 'rag', 28230: 'that\\n\\nthe', 28231: 'phonetically', 28232: 'rich,', 28233: '(sounds)', 28234: 'diphones', 28235: '(phone', 28236: 'bigrams)', 28237: 'strikes', 28238: 'balance\\nbetween', 28239: 'across\\nspeakers,', 28240: 'maximal\\ncoverage', 28241: 'other\\nspeakers', 28242: 'comparability)', 28243: 'coverage)', 28244: 'usual\\nway,', 28245: '.timit)', 28246: '.timit', 28247: '.fileids()', 28248: 'the\\n160', 28249: 'recorded', 28250: 'identifier:', 28251: 'made\\nup', 28252: \"speaker's\", 28253: 'region,', 28254: 'gender,', 28255: 'identifier,', 28256: 'type,\\nand', 28257: 'phonetic', 28258: 'transcription', 28259: 'phones()\\nmethod', 28260: 'customary', 28261: 'access\\nmethods', 28262: 'offset=true', 28263: 'offsets\\nof', 28264: 'audio', 28265: \".phones('dr1-fvmh0/sa1')\\n>>>\", 28266: \"phonetic\\n['h#',\", 28267: \"'sh',\", 28268: \"'iy',\", 28269: \"'hv',\", 28270: \"'ae',\", 28271: \"'dcl',\", 28272: \"'ix',\", 28273: \"'aa',\", 28274: \"'kcl',\\n's',\", 28275: \"'ux',\", 28276: \"'tcl',\", 28277: \"'gcl',\", 28278: \"'aa',\\n'sh',\", 28279: \"'epi',\", 28280: \"'dx',\", 28281: \"'ax',\", 28282: \"'ao',\", 28283: \"'ih',\", 28284: \"'h#']\\n>>>\", 28285: \".word_times('dr1-fvmh0/sa1')\\n[('she',\", 28286: '7812,', 28287: '10610),', 28288: '10610,', 28289: '14496),', 28290: \"('your',\", 28291: '14496,', 28292: \"15791),\\n('dark',\", 28293: '15791,', 28294: '20720),', 28295: \"('suit',\", 28296: '20720,', 28297: '25647),', 28298: '25647,', 28299: \"26906),\\n('greasy',\", 28300: '26906,', 28301: '32668),', 28302: \"('wash',\", 28303: '32668,', 28304: '37890),', 28305: \"('water',\", 28306: '38531,', 28307: \"42417),\\n('all',\", 28308: '43091,', 28309: '46052),', 28310: \"('year',\", 28311: '46052,', 28312: '50522)]\\n\\n\\n\\nin', 28313: 'canonical\\npronunciation', 28314: 'utterance:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 28315: 'timitdict', 28316: '.transcription_dict()\\n>>>', 28317: \"timitdict['greasy']\", 28318: \"timitdict['wash']\", 28319: \"timitdict['water']\\n['g',\", 28320: \"'iy1',\", 28321: \"'ao1',\", 28322: \"'axr']\\n>>>\", 28323: \"phonetic[17:30]\\n['g',\", 28324: \"'ax']\\n\\n\\n\\nthis\", 28325: 'system\\nwould', 28326: 'dialect\\n(new', 28327: 'england)', 28328: 'demographic', 28329: 'speakers,\\npermitting', 28330: 'vocal,', 28331: \".spkrinfo('dr1-fvmh0')\\nspeakerinfo(id='vmh0',\", 28332: \"sex='f',\", 28333: \"dr='1',\", 28334: \"use='trn',\", 28335: \"recdate='03/11/86',\\nbirthdate='01/08/60',\", 28336: \"ht='5\\\\'05',\", 28337: \"race='wht',\", 28338: \"edu='bs',\\ncomments='best\", 28339: 'england', 28340: 'accent', 28341: \"far')\\n\\n\\n\\n\\n\\n1\", 28342: '.2\\xa0\\xa0\\xa0notable', 28343: 'features\\ntimit', 28344: 'annotation,', 28345: 'orthographic\\nlevels', 28346: 'levels,\\nincluding', 28347: 'syntactic,', 28348: 'given\\nlevel', 28349: 'disagreement', 28350: 'annotators,\\nsuch', 28351: 'variation,\\nfor', 28352: 'speaker\\ndemographics', 28353: 'to\\naccount', 28354: 'envisaged', 28355: 'created,\\nsuch', 28356: 'sociolinguistics', 28357: 'sharp', 28358: 'original\\nlinguistic', 28359: 'recording,', 28360: 'usually\\nhas', 28361: 'artifact', 28362: 'transformations\\nof', 28363: 'judgment', 28364: 'as\\nsimple', 28365: 'revision,', 28366: 'to\\nretain', 28367: 'doc,', 28368: 'train,', 28369: 'directories\\nat', 28370: 'directories', 28371: 'sub-directories,', 28372: 'per\\ndialect', 28373: 'region;', 28374: 'subdirectories,', 28375: 'speaker;\\nthe', 28376: 'aks0', 28377: 'listed,', 28378: 'showing\\n10', 28379: 'wav', 28380: 'accompanied', 28381: 'transcription,', 28382: 'word-aligned', 28383: 'transcription,\\nand', 28384: '20,000', 28385: 'are\\norganized', 28386: 'schematically', 28387: 'gives\\naway', 28388: 'transcriptions', 28389: 'associated\\ndata', 28390: '.\\nmoreover,', 28391: 'into\\nthe', 28392: '.\\neven', 28393: 'demographics', 28394: 'the\\nprimary', 28395: 'subfields', 28396: 'management,\\nnamely', 28397: 'techniques\\nfrom', 28398: '.3\\xa0\\xa0\\xa0fundamental', 28399: 'types\\n\\n\\nfigure', 28400: 'texts:', 28401: 'amid', 28402: 'diversity,\\nlexicons', 28403: '.\\n\\ndespite', 28404: 'complexity,', 28405: 'types,\\nnamely', 28406: 'fields,', 28407: 'as\\nshown', 28408: 'conventional\\ndictionary', 28409: 'wordlist,', 28410: 'record-structured', 28411: 'entries\\nvia', 28412: 'non-key', 28413: 'tabulations', 28414: '(known', 28415: 'paradigms)\\nto', 28416: 'variation,', 28417: \"timit's\", 28418: 'kind\\nof', 28419: 'event,\\nand', 28420: 'time-course', 28421: 'small\\nunit,', 28422: 'narrative', 28423: 'with\\nannotations', 28424: '.),', 28425: 'higher-level\\nconstituents', 28426: 'complexities', 28427: 'are\\ncollections', 28428: 'biased', 28429: 'relate\\nthe', 28430: 'wordnet\\ncontains', 28431: 'records,', 28432: 'incorporates', 28433: '(mini-texts)\\nto', 28434: 'usages', 28435: 'mid-point', 28436: 'substantial\\nfree-standing', 28437: '.\\n\\n\\n\\n2\\xa0\\xa0\\xa0the', 28438: 'life-cycle', 28439: 'corpus\\ncorpora', 28440: 'fully-formed,', 28441: 'preparation\\nand', 28442: 'needs\\nto', 28443: 'collected,', 28444: 'cleaned', 28445: 'documented,', 28446: 'systematic\\nstructure', 28447: 'applied,', 28448: 'requiring\\nspecialized', 28449: '.\\nsuccess', 28450: 'workflow\\ninvolving', 28451: 'converters', 28452: '.\\nquality', 28453: 'inconsistencies\\nin', 28454: 'highest\\npossible', 28455: 'the\\nscale', 28456: 'to\\nprepare,', 28457: 'person-years', 28458: 'the\\nlife-cycle', 28459: '.1\\xa0\\xa0\\xa0three', 28460: 'creation', 28461: 'scenarios\\nin', 28462: 'unfolds', 28463: 'over\\nin', 28464: \"creator's\", 28465: 'explorations', 28466: 'pattern\\ntypical', 28467: 'from\\nelicitation', 28468: 'gathered,', 28469: \"tomorrow's\", 28470: 'elicitation', 28471: 'corpus\\nis', 28472: 'serve\\nas', 28473: 'archival', 28474: 'computerization', 28475: 'an\\nobvious', 28476: 'boon', 28477: 'exemplified', 28478: 'popular\\nprogram', 28479: 'shoebox,', 28480: 'decades', 28481: 're-released', 28482: 'toolbox\\n(see', 28483: 'tools,', 28484: 'spreadsheets,', 28485: 'routinely\\nused', 28486: 'extract\\ndata', 28487: 'experimental', 28488: 'research\\nwhere', 28489: 'carefully-designed', 28490: 'subjects,\\nthen', 28491: 're-used\\nwithin', 28492: 'laboratory', 28493: 'more\\nwidely', 28494: 'the\\ncommon', 28495: 'management,', 28496: 'the\\npast', 28497: 'norm', 28498: 'government-funded', 28499: 'research\\nprograms', 28500: 'chapters;\\nwe', 28501: 'of\\ncuration', 28502: 'gather', 28503: '(anc)\\nand', 28504: 'british', 28505: '(bnc)', 28506: 'goal\\nhas', 28507: 'the\\nmany', 28508: 'scale,', 28509: 'reliance\\non', 28510: 'post-editing', 28511: 'to\\nfix', 28512: 'locate\\nand', 28513: '.2\\xa0\\xa0\\xa0quality', 28514: 'control\\ngood', 28515: 'data\\nare', 28516: 'just\\nas', 28517: 'mundane', 28518: '.\\nannotation', 28519: 'markup\\nconventions', 28520: 'regularly', 28521: 'difficult\\ncases,', 28522: 'devised', 28523: 'more\\nconsistent', 28524: 'the\\nprocedures,', 28525: 'covered\\nin', 28526: 'established,', 28527: 'possibly\\nwith', 28528: 'initialized,\\nannotated,', 28529: 'validated,', 28530: 'checked,', 28531: 'of\\nannotation,', 28532: 'specialists', 28533: 'uncertainty\\nor', 28534: 'adjudication', 28535: '.\\nlarge', 28536: 'annotators,', 28537: 'which\\nraises', 28538: 'consistently', 28539: 'perform?\\nwe', 28540: 'reveal', 28541: 'or\\ndiffering', 28542: 'paramount,', 28543: 'adjudicated\\nby', 28544: 'inter-annotator\\nagreement', 28545: 'double-annotating\\n10%', 28546: 'trained\\non', 28547: '.\\n\\ncaution!\\ncare', 28548: 'exercised', 28549: 'their\\ndifficulty', 28550: 'terrible\\nscore', 28551: 'exceptional', 28552: 'score\\nfor', 28553: 'kappa', 28554: 'coefficient', 28555: 'between\\ntwo', 28556: 'judgments,', 28557: 'correcting', 28558: 'expected\\nchance', 28559: 'annotated,\\nand', 28560: 'options', 28561: 'better\\nlevels', 28562: '50%,', 28563: 'get\\nk', 28564: '.333,', 28565: 'exist;', 28566: '.agreement)', 28567: 'sequence:', 28568: 'rectangles', 28569: 'characters,\\nwords,', 28570: 'short,', 28571: 'units;', 28572: 's1', 28573: 's2', 28574: 'close\\nagreement,', 28575: 's3', 28576: 'segmentations\\nof', 28577: 'segmentation,\\nnamed-entity', 28578: 'three\\npossible', 28579: 'programs)', 28580: 'exactly,', 28581: 's2\\nare', 28582: '.\\nwindowdiff', 28583: 'of\\ntwo', 28584: 'the\\ndata', 28585: 'awarding', 28586: 'credit', 28587: 'ones,', 28588: 'to\\nrecord', 28589: 'the\\nsegmentations', 28590: 'windowdiff', 28591: 'scorer', 28592: '00000010000000001000000\\n>>>', 28593: '00000001000000010000000\\n>>>', 28594: '00010000000000000001000\\n>>>', 28595: '.windowdiff(s1,', 28596: '3)\\n0', 28597: '.190', 28598: '.windowdiff(s2,', 28599: 's3,', 28600: '.571', 28601: '.\\n\\n\\n\\nin', 28602: 'slides', 28603: 'this\\nwindow,', 28604: 'differences\\nare', 28605: 'summed', 28606: 'shrink', 28607: 'sensitivity', 28608: 'the\\nmeasure', 28609: '.3\\xa0\\xa0\\xa0curation', 28610: 'evolution\\nas', 28611: 'published,', 28612: 'increasingly\\nlikely', 28613: 'investigations', 28614: 'balanced,', 28615: 'focused\\nsubsets', 28616: 'entirely\\ndifferent', 28617: 'switchboard', 28618: 'database,\\noriginally', 28619: 'research,\\nhas', 28620: 'studies', 28621: 'recognition,\\nword', 28622: 'pronunciation,', 28623: 'disfluency,', 28624: 'intonation', 28625: 'recycling', 28626: 'the\\ndesire', 28627: 'desire', 28628: 'material\\navailable', 28629: 'replication,', 28630: 'study\\nmore', 28631: 'naturalistic', 28632: 'possible\\notherwise', 28633: 'may\\ncount', 28634: 'file\\n(e', 28635: 'xml),', 28636: 'renaming', 28637: 'retokenizing', 28638: 'text,\\nselecting', 28639: 'enrich,', 28640: '.\\nmultiple', 28641: 'groups', 28642: 'independently,', 28643: 'illustrated\\nin', 28644: 'sources\\nof', 28645: 'onerous', 28646: 'evolution', 28647: 'time:', 28648: 'research\\ngroups', 28649: 'enriching', 28650: 'pieces;\\nlater', 28651: 'confronts\\nthe', 28652: 'aligning', 28653: 'of\\nany', 28654: 'created,', 28655: 'most\\nup-to-date', 28656: 'chaotic', 28657: 'centrally', 28658: 'curated,\\nand', 28659: 'committees', 28660: 'periodic\\nintervals,', 28661: 'submissions', 28662: 'third-parties,', 28663: 'publishing', 28664: 'releases\\nfrom', 28665: 'curated', 28666: 'impractical', 28667: 'publication', 28668: 'identifying\\nany', 28669: 'sub-part', 28670: 'entry,', 28671: 'unique\\nidentifier,', 28672: '(respectively)', 28673: '.\\nannotations,', 28674: 'segmentations,', 28675: 'using\\nthis', 28676: 'standoff', 28677: 'annotation)', 28678: 'and\\nmultiple', 28679: 'be\\ncompared', 28680: 'version\\nnumber', 28681: 'correspondences', 28682: 'editions', 28683: 'corpus\\nwould', 28684: '.\\n\\ncaution!\\nsometimes', 28685: 'revisions', 28686: 'merged,', 28687: 'constituents\\nmay', 28688: 'rearranged', 28689: 'one-to-one', 28690: 'between\\nold', 28691: 'break\\non', 28692: 'silently', 28693: 'identifiers\\nto', 28694: '.\\n\\n\\n\\n\\n3\\xa0\\xa0\\xa0acquiring', 28695: 'data\\n\\n3', 28696: '.1\\xa0\\xa0\\xa0obtaining', 28697: 'web\\nthe', 28698: 'already\\ndiscussed', 28699: 'feeds,', 28700: 'engine\\nresults', 28701: 'acl\\nspecial', 28702: '(sigwac)', 28703: 'resources\\nat', 28704: '.sigwac', 28705: 'documented,\\nstable,', 28706: 'reproducible', 28707: 'desired', 28708: 'website,', 28709: 'many\\nutilities', 28710: 'site,', 28711: 'as\\ngnu', 28712: 'wget', 28713: '.gnu', 28714: '.org/software/wget/', 28715: 'flexibility', 28716: 'crawler', 28717: 'used,\\nsuch', 28718: 'heritrix', 28719: 'http://crawler', 28720: '.archive', 28721: '.\\ncrawlers', 28722: 'look,', 28723: 'to\\nfollow,', 28724: 'a\\nbilingual', 28725: 'language,\\nthe', 28726: 'the\\ncorrespondence', 28727: 'downloaded\\npages', 28728: 'tempting\\nto', 28729: 'web-crawler,', 28730: 'with\\ndetecting', 28731: 'mime', 28732: 'urls,', 28733: 'avoiding', 28734: 'getting\\ntrapped', 28735: 'cyclic', 28736: 'latencies,', 28737: 'avoiding\\noverloading', 28738: 'banned', 28739: '.2\\xa0\\xa0\\xa0obtaining', 28740: 'processor', 28741: 'files\\nword', 28742: 'preparation\\nof', 28743: 'computational\\ninfrastructure', 28744: 'data\\nentry,', 28745: 'may\\nbe', 28746: 'lexical\\nentry', 28747: 'larger\\nproportion', 28748: 'programs?\\nmoreover,', 28749: 'validate', 28750: 'to\\nhelp', 28751: 'well-structured', 28752: 'the\\nquality', 28753: 'maximized', 28754: 'authoring', 28755: 'process?\\nconsider', 28756: 'field,', 28757: '20\\npossibilities,', 28758: 'rendered\\nin', 28759: '11-point', 28760: 'macro\\nfunctions', 28761: 'capable', 28762: 'verifying', 28763: 'exhaustive\\nmanual', 28764: 'be\\nsaved', 28765: 'non-proprietary', 28766: 'html,', 28767: 'can\\nsometimes', 28768: 'entry:\\nsleep', 28769: '[sli:p]', 28770: 'page,\\nthen', 28771: 'file:\\n\\n<p', 28772: 'class=msonormal>sleep\\n', 28773: '<span', 28774: \"style='mso-spacerun:yes'>\", 28775: '</span>\\n', 28776: '[<span', 28777: 'class=spelle>sli:p</span>]\\n', 28778: '<b><span', 28779: \"style='font-size:11\", 28780: \".0pt'>v\", 28781: '.</span></b>\\n', 28782: '<i>a', 28783: '.<o:p></o:p></i>\\n</p>\\n\\nobserve', 28784: 'paragraph,', 28785: 'the\\n<p>', 28786: \"<span\\nstyle='font-size:11\", 28787: \".0pt'>\", 28788: 'defines\\nthe', 28789: 'parts-of-speech,', 28790: 'legal_pos', 28791: 'all\\n11-point', 28792: 'dict', 28793: '.htm', 28794: 'set\\nused_pos', 28795: 'a\\nparenthesized', 28796: 'sub-expression;', 28797: 'this\\nsub-expression', 28798: '.findall', 28799: 'program\\nconstructs', 28800: 'illegal', 28801: 'parts-of-speech', 28802: 'used_pos', 28803: '-\\nlegal_pos:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 28804: \"set(['n',\", 28805: \"'det'])\\n>>>\", 28806: \".compile(r'font-size:11\", 28807: \".0pt'>([a-z\", 28808: '.]+)<)\\n>>>', 28809: 'open(dict', 28810: '.htm,', 28811: 'encoding=windows-1252)', 28812: 'set(re', 28813: '.findall(pattern,', 28814: 'document))\\n>>>', 28815: 'illegal_pos', 28816: '.difference(legal_pos)\\n>>>', 28817: \"print(list(illegal_pos))\\n['v\", 28818: \"'intrans']\\n\\n\\n\\nthis\", 28819: 'tip', 28820: 'iceberg', 28821: 'develop\\nsophisticated', 28822: 'files,\\nand', 28823: 'maintainer', 28824: 'correct\\nthe', 28825: 'formatted,', 28826: 'beautifulsoup', 28827: 'library,\\nextracts', 28828: 'pronunciations,', 28829: 'output\\nin', 28830: '(csv)', 28831: 'lexical_data(html_file,', 28832: 'encoding=utf-8):\\n', 28833: \"'_entry'\\n\", 28834: 'open(html_file,', 28835: 'encoding=encoding)', 28836: \".sub(r'<p',\", 28837: \"'<p',\", 28838: 'html)\\n', 28839: '.join(text', 28840: '.split())\\n', 28841: '.split(sep):\\n', 28842: \".count('\", 28843: '2:\\n', 28844: '3)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 28845: 'writer', 28846: '.writer(open(dict1', 28847: 'encoding=utf-8))\\n>>>', 28848: '.writerows(lexical_data(dict', 28849: 'encoding=windows-1252))\\n\\n\\nexample', 28850: '(code_html2csv', 28851: 'microsoft', 28852: 'values\\n\\n\\nwith', 28853: 'gzip', 28854: '.open(fn+', 28855: '.gz,wb)', 28856: 'f_out:\\nf_out', 28857: '.write(bytes(s,', 28858: \"'utf-8'))\\n\\n\\n\\n3\", 28859: '.3\\xa0\\xa0\\xa0obtaining', 28860: 'spreadsheets', 28861: 'databases\\nspreadsheets', 28862: 'spreadsheet,\\nwith', 28863: 'language\\n(cf', 28864: '.swadesh,', 28865: 'www', 28866: '.rosettaproject', 28867: '.org)', 28868: 'csv\\ncomma-separated', 28869: 'for\\npython', 28870: 'full-fledged', 28871: 'normalized,', 28872: 'validity\\nof', 28873: 'can\\nrequire', 28874: 'by\\ndeclaring', 28875: 'enumerated', 28876: 'type\\nor', 28877: 'data\\n(the', 28878: 'schema)', 28879: 'advance,', 28880: 'dominant', 28881: 'structuring', 28882: 'is\\nhighly', 28883: 'exploratory', 28884: 'obligatory\\nand', 28885: 'repeatable', 28886: 'relational\\ndatabase', 28887: 'however\\nif', 28888: 'optional\\nor', 28889: 'unworkable', 28890: 'database,\\nit', 28891: 'dump', 28892: 'results)\\nin', 28893: 'perform\\na', 28894: '.\\nselect', 28895: 'record\\nfor', 28896: 'identified,', 28897: 'headwords', 28898: 'example\\nsentences', 28899: 'file\\ndict', 28900: '.csv:\\n\\nsleep,sli:p,v', 28901: '.i,a', 28902: '.\\nwalk,wo:k,v', 28903: '.intr,progress', 28904: '.\\nwake,weik,intrans,cease', 28905: 'sleep\\n\\nnow', 28906: \".reader(open('dict\", 28907: \".csv'))\\n>>>\", 28908: '[(lexeme,', 28909: 'defn)', 28910: '(lexeme,', 28911: '_,', 28912: 'lexicon]\\n>>>', 28913: 'lexemes,', 28914: 'defns', 28915: 'zip(*pairs)\\n>>>', 28916: 'defn_words', 28917: 'defn', 28918: '.split())\\n>>>', 28919: 'sorted(defn_words', 28920: \".difference(lexemes))\\n['\", 28921: \"'body',\", 28922: \"'cease',\", 28923: \"'condition',\", 28924: \"'down',\", 28925: \"'each',\\n'foot',\", 28926: \"'lifting',\", 28927: \"'mind',\", 28928: \"'progress',\", 28929: \"'setting',\", 28930: \"'to']\\n\\n\\n\\nthis\", 28931: 'enrich', 28932: 'lexicon,\\nwork', 28933: '.4\\xa0\\xa0\\xa0converting', 28934: 'formats\\nannotated', 28935: 'arrives', 28936: 'format,\\nand', 28937: '.\\nconverting', 28938: 'discussed\\n(see', 28939: 'isomorphic', 28940: 'toolbox\\nformat', 28941: 'transliterate', 28942: 'the\\nentries', 28943: 'structure\\nof', 28944: 'required\\nprogram:', 28945: 'care\\nof', 28946: 'digested', 28947: 'necessary\\nto', 28948: '.8),\\nthen', 28949: 'maps\\nthe', 28950: 'corresponding\\nlexeme', 28951: ',\\nhaving', 28952: 'discarded', 28953: 'over\\nthe', 28954: 'idx', 28955: '.index((defn_word,', 28956: 'lexeme)', 28957: 'defn_word', 28958: '.word_tokenize(defn)', 28959: 'len(defn_word)', 28960: '.idx,', 28961: 'idx_file:\\n', 28962: 'sorted(idx):\\n', 28963: 'idx_words', 28964: '.join(idx[word])\\n', 28965: 'idx_line', 28966: '{}:', 28967: 'idx_words)', 28968: 'print(idx_line,', 28969: 'file=idx_file)\\n\\n\\n\\nthe', 28970: '.idx', 28971: 'larger\\ndictionary', 28972: 'lexemes', 28973: '.)\\n\\nbody:', 28974: 'sleep\\ncease:', 28975: 'wake\\ncondition:', 28976: 'sleep\\ndown:', 28977: 'walk\\neach:', 28978: 'walk\\nfoot:', 28979: 'walk\\nlifting:', 28980: 'walk\\nmind:', 28981: 'sleep\\nprogress:', 28982: 'walk\\nsetting:', 28983: 'walk\\nsleep:', 28984: 'wake\\n\\nin', 28985: 'single\\ncolumn', 28986: 'two-dimensional\\ntable', 28987: 'populate\\nan', 28988: 'off\\nthe', 28989: 'vexing', 28990: 'different\\ncoverage', 28991: 'unavoidably', 28992: 'file\\ncontaining', 28993: 'loosing', 28994: '\\\\lx', 28995: 'labor-intensive', 28996: 'inject\\nthe', 28997: 'round-tripping\\nproblem', 28998: 'propagate\\nthe', 28999: '.5\\xa0\\xa0\\xa0deciding', 29000: 'include\\npublished', 29001: 'typically\\ncontain', 29002: 'morphology,', 29003: 'prosody,', 29004: 'and\\nsemantic', 29005: 'relations\\nor', 29006: 'annotation\\nmay', 29007: 'given\\nlinguistic', 29008: 'structures;\\nand', 29009: 'provided\\nannotation', 29010: 'layers:\\n\\nword', 29011: 'tokenization:', 29012: 'unambiguously\\nidentify', 29013: 'version,\\nin', 29014: 'version,\\nmay', 29015: 'segmentation:', 29016: 'sentence\\nsegmentation', 29017: 'corpora\\ntherefore', 29018: '.\\nparagraph', 29019: 'elements\\n(headings,', 29020: '.\\npart', 29021: 'speech:', 29022: '.\\nshallow', 29023: 'semantics:', 29024: 'coreference', 29025: '.\\ndialogue', 29026: 'rhetorical', 29027: 'structure\\n\\nunfortunately,', 29028: 'classes\\nof', 29029: 'inline\\nannotation', 29030: 'special\\nsymbols', 29031: 'string\\nfly', 29032: 'fly/nn,', 29033: 'indicate\\nthat', 29034: 'standoff\\nannotation', 29035: 'instead\\ncreates', 29036: 'pointers\\nthat', 29037: 'might\\ncontain', 29038: '<token', 29039: 'id=8', 29040: \"pos='nn'/>,\", 29041: 'tokenization\\nitself', 29042: 'references\\nto', 29043: '.6\\xa0\\xa0\\xa0standards', 29044: 'tools\\nfor', 29045: 'widely\\nsupported', 29046: 'creation,', 29047: 'must\\ndevelop', 29048: 'help\\nto', 29049: 'adequate,', 29050: 'generally-accepted', 29051: 'standards', 29052: 'for\\nexpressing', 29053: 'without\\nsuch', 29054: 'standards,', 29055: 'developed,', 29056: 'forge', 29057: 'developing\\na', 29058: 'of\\nannotation', 29059: 'examples)', 29060: 'generality\\nof', 29061: 'must\\nbe', 29062: 'validated', 29063: 'rootedness,', 29064: 'connectedness,\\nand', 29065: 'acyclicity', 29066: 'program\\nwould', 29067: 'loaded,\\nbut', 29068: 'invalidate', 29069: 'obliterate', 29070: 'data\\nwas', 29071: 'one-off', 29072: 'scripts\\nto', 29073: 'formats;', 29074: 'scripts', 29075: 'litter', 29076: 'filespaces', 29077: 'many\\nnlp', 29078: 'systematic\\napproach,', 29079: 'founded', 29080: 'premise', 29081: 'format\\nshould', 29082: '(per', 29083: 'interface\\n\\ninstead', 29084: 'focussing', 29085: '.corpus)', 29086: 'elements,\\nor', 29087: '(child-id,', 29088: 'parent-id)', 29089: 'line,\\nor', 29090: 'notation,', 29091: 'allows\\napplication', 29092: 'data\\nusing', 29093: 'children(),', 29094: 'leaves(),', 29095: 'depth(),', 29096: 'and\\nso', 29097: 'within\\ncomputer', 29098: 'science,', 29099: 'viz', 29100: 'design,\\nand', 29101: '—\\nallows', 29102: 'end-user', 29103: 'model)\\nand', 29104: '(sql),', 29105: 'idiosyncrasies\\nof', 29106: 'innovations', 29107: 'filesystem', 29108: 'technologies\\nto', 29109: 'disturbing', 29110: 'interface\\ninsulates', 29111: 'dissemination,', 29112: 'is\\nexpedient', 29113: 'widely-used', 29114: 'software\\n—', 29115: 'supports\\nexisting', 29116: '.7\\xa0\\xa0\\xa0special', 29117: 'languages\\nthe', 29118: 'in\\nsignificance', 29119: 'treasure', 29120: 'embodied', 29121: '~7,000', 29122: 'respects,\\nin', 29123: 'oral', 29124: 'histories', 29125: 'legends,', 29126: 'grammatical\\nconstructions', 29127: 'nuances', 29128: '.\\nthreatened', 29129: 'remnant', 29130: 'cultures', 29131: 'subspecies\\naccording', 29132: 'therapeutic', 29133: 'languages\\nevolve', 29134: 'each\\none', 29135: 'pre-history', 29136: 'world,', 29137: 'variations\\nfrom', 29138: 'drive', 29139: 'breathtaking', 29140: 'and\\ndiversity,', 29141: 'colorful', 29142: 'tapestry', 29143: 'stretching\\nthrough', 29144: 'extinction', 29145: 'constructing\\nrich', 29146: 'records', 29147: 'facet', 29148: 'heritage', 29149: 'offer', 29150: 'effort?', 29151: 'developing\\ntaggers,', 29152: 'recognizers,', 29153: 'etc,\\nis', 29154: 'priority,', 29155: 'voiced', 29156: 'curating', 29157: 'focus\\non', 29158: 'texts\\nin', 29159: 'vexed', 29160: 'who\\nowns', 29161: 'sensitivities', 29162: 'texts,\\nthere', 29163: 'orthography', 29164: 'language\\nhas', 29165: 'literary', 29166: 'tradition,', 29167: 'punctuation\\nare', 29168: 'well-established', 29169: 'practice\\nto', 29170: 'collection,', 29171: 'continually', 29172: 'updating\\nthe', 29173: 'done\\nusing', 29174: 'lexicon)', 29175: '.\\nbetter', 29176: \"sil's\", 29177: 'fieldworks\\nprovide', 29178: 'themselves,\\na', 29179: 'obstacle', 29180: 'overriding', 29181: 'methods\\nthat', 29182: 'arbitrary\\nword', 29183: 'acute', 29184: 'that\\nincludes', 29185: 'prefixes', 29186: 'with\\nsemantic', 29187: 'domains,', 29188: '.\\npermitting', 29189: 'confusible', 29190: 'sequences,\\nand', 29191: 'notice\\nthat', 29192: 'cluster', 29193: 'consonants\\nis', 29194: 'the\\norder', 29195: 'consonants', 29196: \"[('ph',\", 29197: \"'f'),\", 29198: \"('ght',\", 29199: \"'t'),\", 29200: \"('^kn',\", 29201: \"('qu',\", 29202: \"'kw'),\\n\", 29203: \"('[aeiou]+',\", 29204: \"(r'(\", 29205: \".)\\\\1',\", 29206: \"r'\\\\1')]\\n>>>\", 29207: 'signature(word):\\n', 29208: 'patt,', 29209: 'repl', 29210: 'mappings:\\n', 29211: '.sub(patt,', 29212: 'repl,', 29213: \".findall('[^aeiou]+',\", 29214: '.join(char', 29215: 'sorted(piece))[:8]\\n>>>', 29216: \"signature('illefent')\\n'lfnt'\\n>>>\", 29217: \"signature('ebsekwieous')\\n'bskws'\\n>>>\", 29218: \"signature('nuculerr')\\n'nclr'\\n\\n\\n\\nnext,\", 29219: 'signatures', 29220: 'corrections', 29221: 'signature)', 29222: '.index((signature(w),', 29223: \"signatures[signature('nuculerr')]\\n['anicular',\", 29224: \"'inocular',\", 29225: \"'nucellar',\", 29226: \"'nuclear',\", 29227: \"'unicolor',\", 29228: \"'uniocular',\", 29229: \"'unocular']\\n\\n\\n\\nfinally,\", 29230: 'rank()', 29231: 'rank(word,', 29232: 'sorted((nltk', 29233: '.edit_distance(word,', 29234: 'w),', 29235: 'ranked]\\n>>>', 29236: 'fuzzy_spell(word):\\n', 29237: 'signature(word)\\n', 29238: 'signatures:\\n', 29239: 'signatures[sig])\\n', 29240: \"fuzzy_spell('illefent')\\n['olefiant',\", 29241: \"'oliphant',\", 29242: \"'elephanta']\\n>>>\", 29243: \"fuzzy_spell('ebsekwieous')\\n['obsequious']\\n>>>\", 29244: \"fuzzy_spell('nucular')\\n['anicular',\", 29245: \"'unocular',\", 29246: \"'unicolor']\\n\\n\\n\\nthis\", 29247: 'lexical\\ndata', 29248: 'standardized,', 29249: 'or\\nwhere', 29250: 'spellings', 29251: 'simple\\napplications', 29252: 'include:', 29253: 'to\\ndata,', 29254: 'gleaning', 29255: 'texts,\\nlocating', 29256: 'prevalent', 29257: 'or\\nexceptional', 29258: 'validation\\non', 29259: 'last\\nof', 29260: '.\\n\\n\\n\\n4\\xa0\\xa0\\xa0working', 29261: 'xml\\nthe', 29262: '(xml)', 29263: 'designing\\ndomain-specific', 29264: 'representing\\nannotated', 29265: 'predefined\\ntags,', 29266: 'xml\\npermits', 29267: 'section\\nwe', 29268: 'representing\\nlinguistic', 29269: 'using\\npython', 29270: '.1\\xa0\\xa0\\xa0using', 29271: 'structures\\nthanks', 29272: 'extensibility,', 29273: 'natural\\nchoice', 29274: '(2)\\n<entry>\\n', 29275: '<headword>whale</headword>\\n', 29276: '<pos>noun</pos>\\n', 29277: '<gloss>any', 29278: 'cetacean', 29279: 'mammals', 29280: 'streamlined\\n', 29281: 'breathing', 29282: 'blowhole', 29283: 'head</gloss>\\n</entry>\\n\\n\\nit', 29284: 'enclosed', 29285: '<gloss>', 29286: 'closing', 29287: '</gloss>;\\ntogether', 29288: 'nicely', 29289: 'whitespace,', 29290: 'could\\nequally', 29291: 'processing\\nxml', 29292: 'be\\nwell', 29293: 'formed,', 29294: 'tree)', 29295: '.\\nxml', 29296: 'layout\\ndoes', 29297: '(3)\\n<entry><headword>whale</headword><pos>noun</pos><gloss>any', 29298: 'the\\nlarger', 29299: 'streamlined', 29300: 'breathing\\nthrough', 29301: 'head</gloss><gloss>a', 29302: 'person;\\nimpressive', 29303: 'qualities</gloss></entry>\\n\\n\\na', 29304: 'wordnet,\\nusing', 29305: 'identifier\\ninside', 29306: '(4)\\n<entry>\\n', 29307: '<sense>\\n', 29308: 'head</gloss>\\n', 29309: '<synset>whale', 29310: '.02</synset>\\n', 29311: '</sense>\\n', 29312: '<gloss>a', 29313: 'person;', 29314: 'qualities</gloss>\\n', 29315: '<synset>giant', 29316: '.04</synset>\\n', 29317: '</sense>\\n</entry>\\n\\n\\nalternatively,', 29318: 'attribute,\\nwithout', 29319: '(5)\\n<entry>\\n', 29320: '<gloss', 29321: 'synset=whale', 29322: '.02>any', 29323: 'having\\n', 29324: 'synset=giant', 29325: '.04>a', 29326: 'or\\n', 29327: 'qualities</gloss>\\n</entry>\\n\\n\\nthis', 29328: \"arbitrary\\nthat's\", 29329: 'is!', 29330: 'attribute\\nnames,', 29331: 'them\\nout,', 29332: 'whose\\npresence', 29333: 'speech\\nis', 29334: 'past_tense', 29335: 'past_tense\\nelement', 29336: 'all\\nthis', 29337: 'schema,\\nwhich', 29338: 'akin', 29339: 'schema', 29340: 'xml\\nwe', 29341: 'complication,', 29342: 'permitting\\nan', 29343: 'program\\nthat', 29344: 'the\\ndata,', 29345: 'interrogate', 29346: 'data\\nmodeling', 29347: 'data,\\nthen', 29348: 'schema,', 29349: 'then\\nwrite', 29350: '.\\nsimilarly,', 29351: 'concerning\\ndata', 29352: 'wise', 29353: 'when\\nonly', 29354: 'cross-reference', 29355: 'was\\nrepresented', 29356: '<xref>headword</xref>', 29357: 'storage\\nof', 29358: 'break\\nif', 29359: '.\\nexistential', 29360: 'be\\nmodeled,', 29361: 'independently\\nof', 29362: 'entry\\nelement', 29363: 'many-to-many', 29364: 'abstracted', 29365: 'of\\nhierarchical', 29366: 'corresponding\\nsenses,', 29367: 'then\\nboth', 29368: 'sense)', 29369: 'split\\nacross', 29370: 'format\\naccompanied', 29371: 'panacea', 29372: 'elementtree', 29373: \"interface\\npython's\", 29374: 'access\\ndata', 29375: 'first\\nat', 29376: 'some\\nxml', 29377: 'headers', 29378: '.dtd,\\nfollowed', 29379: '.\\n(some', 29380: 'merchant_file', 29381: \".find('corpora/shakespeare/merchant\", 29382: 'open(merchant_file)', 29383: 'print(raw[:163])', 29384: '\\n<?xml', 29385: 'version=1', 29386: '.0?>\\n<?xml-stylesheet', 29387: 'type=text/css', 29388: 'href=shakes', 29389: '.css?>\\n<!--', 29390: '<!doctype', 29391: '.dtd>', 29392: '-->\\n<play>\\n<title>the', 29393: 'venice</title>\\n>>>', 29394: 'print(raw[1789:2006])', 29395: '\\n<title>act', 29396: 'i</title>\\n<scene><title>scene', 29397: 'venice', 29398: '.</title>\\n<stagedir>enter', 29399: 'antonio,', 29400: 'salarino,', 29401: 'salanio</stagedir>\\n<speech>\\n<speaker>antonio</speaker>\\n<line>in', 29402: 'sooth,', 29403: 'sad:</line>\\n\\n\\n\\nwe', 29404: 'see,\\nthe', 29405: 'title,', 29406: 'scene,', 29407: 'directions,', 29408: 'data,\\nusing', 29409: 'string)\\nand', 29410: 'structure;', 29411: 'index\\nto', 29412: 'child,', 29413: 'the\\ngetchildren()', 29414: '.etree', 29415: '.elementtree', 29416: 'elementtree\\n>>>', 29417: 'elementtree()', 29418: '.parse(merchant_file)', 29419: 'merchant\\n<element', 29420: \"'play'\", 29421: '0x10ac43d18>', 29422: '[_element-play]\\n>>>', 29423: 'merchant[0]\\n<element', 29424: \"'title'\", 29425: '0x10ac43c28>', 29426: '[_element-title]\\n>>>', 29427: 'merchant[0]', 29428: \".text\\n'the\", 29429: \"venice'\", 29430: '[_element-text]\\n>>>', 29431: '.getchildren()', 29432: '\\n[<element', 29433: '0x10ac43c28>,', 29434: '<element', 29435: \"'personae'\", 29436: '0x10ac43bd8>,\\n<element', 29437: \"'scndescr'\", 29438: '0x10b067f98>,', 29439: \"'playsubt'\", 29440: '0x10af37048>,\\n<element', 29441: \"'act'\", 29442: '0x10af37098>,', 29443: '0x10b936368>,\\n<element', 29444: '0x10b934b88>,', 29445: '0x10cfd8188>,\\n<element', 29446: '0x10cfadb38>]\\n\\n\\n\\nthe', 29447: 'personae,', 29448: 'description,', 29449: 'subtitle,', 29450: 'scenes,', 29451: 'iv:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 29452: 'merchant[-2][0]', 29453: \".text\\n'act\", 29454: \"iv'\\n>>>\", 29455: 'merchant[-2][1]\\n<element', 29456: \"'scene'\", 29457: '0x10cfd8228>\\n>>>', 29458: 'merchant[-2][1][0]', 29459: \".text\\n'scene\", 29460: 'justice', 29461: 'merchant[-2][1][54]\\n<element', 29462: \"'speech'\", 29463: '0x10cfb02c8>\\n>>>', 29464: 'merchant[-2][1][54][0]\\n<element', 29465: \"'speaker'\", 29466: '0x10cfb0318>\\n>>>', 29467: 'merchant[-2][1][54][0]', 29468: \".text\\n'portia'\\n>>>\", 29469: 'merchant[-2][1][54][1]\\n<element', 29470: \"'line'\", 29471: '0x10cfb0368>\\n>>>', 29472: 'merchant[-2][1][54][1]', 29473: '.text\\nthe', 29474: 'mercy', 29475: \"strain'd,\\n\\n\\n\\n\\nnote\\nyour\", 29476: 'turn:\\nrepeat', 29477: 'plays\\nincluded', 29478: 'romeo', 29479: 'juliet', 29480: 'macbeth;\\nfor', 29481: '.shakespeare', 29482: '.\\n\\nalthough', 29483: 'for\\nsub-elements', 29484: 'have\\nseveral', 29485: 'acts),', 29486: \".findall('act')\", 29487: 'such\\ntag-specific', 29488: 'nesting:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 29489: 'enumerate(merchant', 29490: \".findall('act')):\\n\", 29491: 'enumerate(act', 29492: \".findall('scene')):\\n\", 29493: 'enumerate(scene', 29494: \".findall('speech')):\\n\", 29495: \".findall('line'):\\n\", 29496: \"'music'\", 29497: 'str(line', 29498: '.text):\\n', 29499: 'print(act', 29500: '%d:', 29501: '(i+1,', 29502: 'j+1,', 29503: 'k+1,', 29504: '.text))\\nact', 29505: 'doth', 29506: 'choice;\\nact', 29507: 'fading', 29508: 'music:', 29509: 'comparison\\nact', 29510: 'then?', 29511: 'is\\nact', 29512: '23:', 29513: 'air', 29514: '.\\nact', 29515: 'sit', 29516: 'music\\nact', 29517: '24:', 29518: 'merry', 29519: 'hear', 29520: 'sweet', 29521: '25:', 29522: 'ears,\\nact', 29523: 'poet\\nact', 29524: 'himself,\\nact', 29525: 'trusted', 29526: '29:', 29527: 'madam,', 29528: '32:', 29529: 'musician', 29530: 'wren', 29531: '.\\n\\n\\n\\ninstead', 29532: 'navigating', 29533: 'hierarchy,', 29534: 'for\\nparticular', 29535: 'say:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 29536: 'counter\\n>>>', 29537: 'speaker_seq', 29538: \".findall('act/scene/speech/speaker')]\\n>>>\", 29539: 'speaker_freq', 29540: 'counter(speaker_seq)\\n>>>', 29541: 'top5', 29542: '.most_common(5)\\n>>>', 29543: \"top5\\n[('portia',\", 29544: '117),', 29545: \"('shylock',\", 29546: '79),', 29547: \"('bassanio',\", 29548: \"73),\\n('gratiano',\", 29549: '48),', 29550: \"('lorenzo',\", 29551: '47)]\\n\\n\\n\\nwe', 29552: 'dialogues', 29553: 'vocabulary\\nto', 29554: 'manageable', 29555: \"'oth')\\n>>>\", 29556: 'speaker,', 29557: 'top5:\\n', 29558: 'abbreviate[speaker]', 29559: 'speaker[:4]\\n', 29560: 'speaker_seq2', 29561: '[abbreviate[speaker]', 29562: 'speaker_seq]\\n>>>', 29563: '.conditionalfreqdist(nltk', 29564: '.bigrams(speaker_seq2))\\n>>>', 29565: 'anto', 29566: 'bass', 29567: 'grat', 29568: 'oth', 29569: 'shyl\\nanto', 29570: '12\\nbass', 29571: '16\\ngrat', 29572: '153', 29573: '25\\nport', 29574: '21\\nshyl', 29575: '0\\n\\n\\n\\nignoring', 29576: 'exchanges', 29577: 'people\\nother', 29578: 'oth),', 29579: 'largest', 29580: 'portia', 29581: 'bassanio', 29582: '.4\\xa0\\xa0\\xa0using', 29583: 'interface\\nfor', 29584: 'format\\nused', 29585: 'of\\ntechniques', 29586: 'supported\\nby', 29587: 'be\\napplied', 29588: '.xml()', 29589: 'toolbox\\nfile', 29590: 'file\\ncontains', 29591: \".xml('rotokas\", 29592: \".dic')\\n\\n\\n\\nthere\", 29593: 'by\\nindexes', 29594: 'thus\\nlexicon[3]', 29595: 'fourth\\nentry', 29596: 'zero);', 29597: 'lexicon[3][0]', 29598: 'field:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 29599: 'lexicon[3][0]\\n<element', 29600: \"'lx'\", 29601: '0x10b2f6958>\\n>>>', 29602: \".tag\\n'lx'\\n>>>\", 29603: \".text\\n'kaa'\\n\\n\\n\\nthe\", 29604: 'uses\\npaths', 29605: 'lx', 29606: 'ps', 29607: 'can\\nconveniently', 29608: 'record/lx', 29609: 'record/lx,', 29610: 'element,\\nnormalizing', 29611: '[lexeme', 29612: \".findall('record/lx')]\\n['kaa',\", 29613: \"'kaa',\", 29614: \"'kaakaaro',\", 29615: \"'kaakaaviko',\", 29616: \"'kaakaavo',\", 29617: \"'kaakaoko',\\n'kaakasi',\", 29618: \"'kaakau',\", 29619: \"'kaakauko',\", 29620: \"'kaakito',\", 29621: \"'kaakuupato',\", 29622: \"'kuvuto']\\n\\n\\n\\nlet's\", 29623: 'write()', 29624: 'of\\nelementtree', 29625: 'these\\nusing', 29626: 'output\\ndisplayed', 29627: 'screen,', 29628: 'stdout', 29629: '(standard', 29630: 'output),', 29631: 'sys\\n>>>', 29632: '.util', 29633: 'elementtree_indent\\n>>>', 29634: 'elementtree_indent(lexicon)\\n>>>', 29635: 'elementtree(lexicon[3])\\n>>>', 29636: '.write(sys', 29637: '.stdout,', 29638: \"encoding='unicode')\", 29639: '\\n<record>\\n', 29640: '<lx>kaa</lx>\\n', 29641: '<ps>n</ps>\\n', 29642: '<pt>masc</pt>\\n', 29643: '<cl>isi</cl>\\n', 29644: '<ge>cooking', 29645: 'banana</ge>\\n', 29646: '<tkp>banana', 29647: 'kukim</tkp>\\n', 29648: '<pt>itoo</pt>\\n', 29649: '<sf>flora</sf>\\n', 29650: '<dt>12/aug/2005</dt>\\n', 29651: '<ex>taeavi', 29652: 'iria', 29653: 'isi', 29654: 'kovopaueva', 29655: 'kaparapasia', 29656: '.</ex>\\n', 29657: '<xp>taeavi', 29658: 'bin', 29659: 'planim', 29660: 'gaden', 29661: 'kukim', 29662: 'tasol', 29663: 'paia', 29664: '.</xp>\\n', 29665: '<xe>taeavi', 29666: 'planted', 29667: 'cook', 29668: '.</xe>\\n</record>\\n\\n\\n\\n\\n\\n4', 29669: '.5\\xa0\\xa0\\xa0formatting', 29670: 'entries\\nwe', 29671: '<table>,', 29672: '<tr>', 29673: '(table', 29674: 'row),', 29675: 'and\\n<td>', 29676: 'data)', 29677: '<table>\\\\n\\n>>>', 29678: 'lexicon[70:80]:\\n', 29679: \".findtext('lx')\\n\", 29680: \".findtext('ps')\\n\", 29681: 'ge', 29682: \".findtext('ge')\\n\", 29683: '<tr><td>%s</td><td>%s</td><td>%s</td></tr>\\\\n', 29684: '(lx,', 29685: 'ps,', 29686: 'ge)\\n>>>', 29687: '</table>\\n>>>', 29688: 'print(html)\\n<table>\\n', 29689: '<tr><td>kakae</td><td>???</td><td>small</td></tr>\\n', 29690: '<tr><td>kakae</td><td>class</td><td>child</td></tr>\\n', 29691: '<tr><td>kakaevira</td><td>adv</td><td>small-like</td></tr>\\n', 29692: '<tr><td>kakapikoa</td><td>???</td><td>small</td></tr>\\n', 29693: '<tr><td>kakapikoto</td><td>n</td><td>newborn', 29694: 'baby</td></tr>\\n', 29695: '<tr><td>kakapu</td><td>v</td><td>place', 29696: 'sling', 29697: 'carrying</td></tr>\\n', 29698: '<tr><td>kakapua</td><td>n</td><td>sling', 29699: 'lifting</td></tr>\\n', 29700: '<tr><td>kakara</td><td>n</td><td>arm', 29701: 'band</td></tr>\\n', 29702: '<tr><td>kakarapaia</td><td>n</td><td>village', 29703: 'name</td></tr>\\n', 29704: '<tr><td>kakarau</td><td>n</td><td>frog</td></tr>\\n</table>\\n\\n\\n\\n\\n\\n\\n5\\xa0\\xa0\\xa0working', 29705: 'data\\ngiven', 29706: 'linguists,', 29707: 'further\\nmethods', 29708: 'previous\\nchapters,', 29709: 'counting,', 29710: 'co-occurrences,\\ncan', 29711: 'trivially\\ncompute', 29712: 'sum(len(entry)', 29713: 'len(lexicon)\\n13', 29714: '.635', 29715: 'documentary\\nlinguistics,', 29716: '.1\\xa0\\xa0\\xa0adding', 29717: 'entry\\nit', 29718: 'from\\nexisting', 29719: 'cv()', 29720: 'which\\nmaps', 29721: 'sequence,\\ne', 29722: 'kakapua', 29723: 'cvcvcvv', 29724: 'lowercase,\\nthen', 29725: '[^a-z]', 29726: 'consonant,', 29727: 'entry;', 29728: 'note\\nthe', 29729: 'subelement\\n\\ndef', 29730: 'cv(s):\\n', 29731: \".sub(r'[^a-z]',\", 29732: \"r'_',\", 29733: \".sub(r'[aeiou]',\", 29734: \"r'v',\", 29735: \".sub(r'[^v_]',\", 29736: \"r'c',\", 29737: '(s)\\n\\ndef', 29738: 'add_cv_field(entry):\\n', 29739: 'entry:\\n', 29740: \"'lx':\\n\", 29741: 'cv_field', 29742: 'subelement(entry,', 29743: \"'cv')\\n\", 29744: 'cv(field', 29745: '.text)\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 29746: 'add_cv_field(lexicon[53])\\n>>>', 29747: '.to_sfm_string(lexicon[53]))\\n\\\\lx', 29748: 'kaeviro\\n\\\\ps', 29749: 'v\\n\\\\pt', 29750: 'a\\n\\\\ge', 29751: 'lift', 29752: 'off\\n\\\\ge', 29753: 'off\\n\\\\tkp', 29754: 'antap\\n\\\\sc', 29755: 'motion\\n\\\\vx', 29756: '1\\n\\\\nt', 29757: 'plane\\n\\\\dt', 29758: '03/jun/2005\\n\\\\ex', 29759: 'pita', 29760: 'kaeviroroe', 29761: 'kepa', 29762: 'kekesia', 29763: 'oa', 29764: 'vuripierevo', 29765: 'kiuvu', 29766: '.\\n\\\\xp', 29767: 'antap', 29768: 'lukim', 29769: 'haus', 29770: 'win', 29771: 'bagarapim', 29772: '.\\n\\\\xe', 29773: 'peter', 29774: 'destroyed', 29775: '.\\n\\\\cv', 29776: 'cvvcvcv\\n\\n\\nexample', 29777: '(code_add_cv_field', 29778: 'entry\\n\\n\\nnote\\nif', 29779: 'updated,', 29780: 'in\\ncode-add-cv-field', 29781: 'would\\nbe', 29782: 'add_cv_field()', 29783: 'safer', 29784: 'such\\nprograms', 29785: 'analysis,\\nwithout', 29786: '.2\\xa0\\xa0\\xa0validating', 29787: 'lexicon\\nmany', 29788: 'fields\\nin', 29789: 'practicable', 29790: 'sequences,\\nwith', 29791: 'counter:\\n\\n\\n\\n\\n\\xa0\\n\\n>>>', 29792: 'field_sequences', 29793: \"counter(':'\", 29794: '.join(field', 29795: 'entry)', 29796: 'lexicon)\\n>>>', 29797: \".most_common()\\n[('lx:ps:pt:ge:tkp:dt:ex:xp:xe',\", 29798: '41),', 29799: \"('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe',\", 29800: \"37),\\n('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe',\", 29801: '27),', 29802: \"('lx:ps:pt:ge:tkp:nt:dt:ex:xp:xe',\", 29803: '20),', 29804: '.]\\n\\n\\n\\nafter', 29805: 'context\\nfree', 29806: '.2\\nuses', 29807: 'implicit\\nnested', 29808: 'the\\nleaves', 29809: 'iterate\\nover', 29810: 'conformance', 29811: '.\\nthose', 29812: 'grammar\\nit', 29813: \".fromstring('''\\n\", 29814: 'sem_field', 29815: 'examples\\n', 29816: 'root\\n', 29817: 'lx\\n', 29818: 'rt', 29819: 'ps\\n', 29820: 'tkp', 29821: 'eng\\n', 29822: 'dt\\n', 29823: 'sf\\n', 29824: 'ex_pidgin', 29825: 'ex_english', 29826: 'ex\\n', 29827: 'xp\\n', 29828: 'xe\\n', 29829: 'cmt', 29830: 'nt', 29831: \"''')\\n\\ndef\", 29832: 'validate_lexicon(grammar,', 29833: 'ignored_tags):\\n', 29834: '.recursivedescentparser(grammar)\\n', 29835: 'lexicon:\\n', 29836: 'marker_list', 29837: '[field', 29838: 'ignored_tags]\\n', 29839: 'list(rd_parser', 29840: '.parse(marker_list)):\\n', 29841: 'print(+,', 29842: '.join(marker_list))', 29843: 'print(-,', 29844: \".dic')[10:20]\\n>>>\", 29845: 'ignored_tags', 29846: \"['arg',\", 29847: \"'dcsv',\", 29848: \"'vx']\", 29849: 'ignored_tags)\\n-', 29850: 'lx:ps:ge:tkp:sf:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe\\n-', 29851: 'lx:rt:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe\\n-', 29852: 'lx:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe\\n-', 29853: 'lx:ps:ge:tkp:nt:sf:dt\\n-', 29854: 'lx:ps:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe\\n-', 29855: 'lx:ps:ge:ge:ge:tkp:cmt:dt:ex:xp:xe\\n-', 29856: 'lx:rt:ps:ge:ge:tkp:dt\\n-', 29857: 'lx:rt:ps:ge:eng:eng:eng:ge:tkp:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe\\n-', 29858: 'lx:rt:ps:ge:tkp:dt:ex:xp:xe\\n-', 29859: 'lx:ps:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe\\n\\n\\nexample', 29860: '(code_toolbox_validation', 29861: 'validating', 29862: 'grammar\\n\\nanother', 29863: '.),\\nsince', 29864: 'partial\\nstructures,', 29865: 'up\\na', 29866: 'lexfunc:', 29867: '{<lf>(<lv><ln|le>*)*}\\n', 29868: '{<rf|xv><xn|xe>*}\\n', 29869: 'sense:', 29870: '{<sn><ps><pn|gv|dv|gn|gp|dn|rn|ge|de|re>*<example>*<lexfunc>*}\\n', 29871: 'record:', 29872: '{<lx><hm><sense>+<dt>}\\n', 29873: 'toolboxdata\\n>>>', 29874: 'db', 29875: 'toolboxdata()\\n>>>', 29876: '.open(nltk', 29877: \".find('corpora/toolbox/iu_mien_samp\", 29878: \".db'))\\n>>>\", 29879: '.parse(grammar,', 29880: \"encoding='utf8')\\n>>>\", 29881: 'elementtree(lexicon)\\n>>>', 29882: 'open(iu_mien_samp', 29883: '.xml,', 29884: 'wb)', 29885: 'output:\\n', 29886: '.write(output)\\n\\n\\nexample', 29887: '(code_chunk_toolbox', 29888: 'lexicon:', 29889: 'of\\nentries', 29890: 'iu', 29891: 'mien,', 29892: 'record\\n\\n\\n\\n\\n6\\xa0\\xa0\\xa0describing', 29893: 'metadata\\nmembers', 29894: 'community', 29895: 'libraries\\ncommunity', 29896: 'metadata', 29897: 'aggregation', 29898: 'metadata?\\nthe', 29899: '.\\nmetadata', 29900: 'whether\\nit', 29901: 'physical', 29902: 'new,\\nthe', 29903: 'as\\ncollections', 29904: '.\\nlibrary', 29905: 'catalogs', 29906: 'metadata;\\nthey', 29907: 'discovery\\ntools', 29908: 'either\\nby', 29909: 'dublin', 29910: 'initiative', 29911: '1995', 29912: 'develop\\nconventions', 29913: 'discovery', 29914: 'broad,\\ninterdisciplinary', 29915: 'consensus', 29916: 'elements\\nthat', 29917: 'each\\nelement', 29918: 'repeatable:', 29919: 'creator,', 29920: 'subject,\\ndescription,', 29921: 'contributor,', 29922: 'format,\\nidentifier,', 29923: 'coverage,', 29924: 'that\\nexist', 29925: 'archives', 29926: '(oai)', 29927: 'framework\\nacross', 29928: 'scholarly', 29929: 'type,\\nincluding', 29930: 'recordings,', 29931: 'artifacts,\\ndigital', 29932: 'surrogates,', 29933: 'repository', 29934: 'server', 29935: 'offering\\npublic', 29936: 'archived', 29937: 'identifier,\\nand', 29938: 'additional\\nrecords', 29939: 'formats)', 29940: 'oai', 29941: 'protocol', 29942: 'search\\nservices', 29943: 'harvest', 29944: '.2\\xa0\\xa0\\xa0olac:', 29945: 'community\\nthe', 29946: '(olac)', 29947: 'international\\npartnership', 29948: 'institutions', 29949: 'a\\nworldwide', 29950: 'virtual', 29951: 'by:\\n(i)', 29952: 'archiving', 29953: 'and\\n(ii)', 29954: 'interoperating', 29955: 'repositories\\nand', 29956: 'services', 29957: 'housing', 29958: \".\\nolac's\", 29959: '.\\nolac', 29960: '.\\nuniform', 29961: 'ensured', 29962: 'limiting\\nthe', 29963: 'terms\\nfrom', 29964: 'controlled', 29965: 'set,\\na', 29966: 'descriptors', 29967: 'fundamental\\nproperties', 29968: 'and\\nlinguistic', 29969: 'record:\\n\\n<?xml', 29970: 'encoding=utf-8?>\\n<olac:olac', 29971: 'xmlns:olac=http://www', 29972: '.org/olac/1', 29973: '.1/\\n', 29974: 'xmlns=http://purl', 29975: '.org/dc/elements/1', 29976: 'xmlns:dcterms=http://purl', 29977: '.org/dc/terms/\\n', 29978: 'xmlns:xsi=http://www', 29979: '.w3', 29980: '.org/2001/xmlschema-instance\\n', 29981: 'xsi:schemalocation=http://www', 29982: '.1/olac', 29983: '.xsd>\\n', 29984: '<title>a', 29985: 'kayardild', 29986: 'notes', 29987: 'tangkic', 29988: '.</title>\\n', 29989: '<creator>evans,', 29990: 'nicholas', 29991: '.</creator>\\n', 29992: '<subject>kayardild', 29993: 'grammar</subject>\\n', 29994: '<subject', 29995: 'xsi:type=olac:language', 29996: 'olac:code=gyd>kayardild</subject>\\n', 29997: '<language', 29998: 'olac:code=en>english</language>\\n', 29999: '<description>kayardild', 30000: '(isbn', 30001: '3110127954)</description>\\n', 30002: '<publisher>berlin', 30003: 'mouton', 30004: 'de', 30005: 'gruyter</publisher>\\n', 30006: '<contributor', 30007: 'xsi:type=olac:role', 30008: 'olac:code=author>nicholas', 30009: 'evans</contributor>\\n', 30010: '<format>hardcover,', 30011: '837', 30012: 'pages</format>\\n', 30013: '<relation>related', 30014: '0646119966</relation>\\n', 30015: '<coverage>australia</coverage>\\n', 30016: '<type', 30017: 'xsi:type=olac:linguistic-type', 30018: 'olac:code=language_description/>\\n', 30019: 'xsi:type=dcterms:dcmitype>text</type>\\n</olac:olac>\\n\\nparticipating', 30020: 'xml\\nformat,', 30021: 'harvested', 30022: 'services\\nusing', 30023: 'infrastructure,\\nolac', 30024: 'describing\\nlanguage', 30025: 'extended\\nconsultation', 30026: 'see\\nhttp://www', 30027: '.org/rec/bpr', 30028: '.\\nsearching', 30029: 'others:\\n\\ncallhome', 30030: 'lexicon\\nhttp://www', 30031: '.org/item/oai:www', 30032: '.edu:ldc97l18\\nmultilex', 30033: '.org/item/oai:elra', 30034: '.icp', 30035: '.inpg', 30036: '.fr:m0001\\nslelex', 30037: 'siemens', 30038: '.fr:s0048\\n\\nsearching', 30039: 'korean', 30040: 'treebank,', 30041: 'lexicon,\\na', 30042: 'child-language', 30043: 'interlinear', 30044: 'glossed', 30045: 'software\\nincluding', 30046: 'analyzer', 30047: 'urls', 30048: 'form:\\noai:www', 30049: '.edu:ldc97l18', 30050: 'identifier,\\nusing', 30051: 'uri', 30052: 'registered', 30053: 'icann\\n(the', 30054: 'corporation', 30055: 'numbers)', 30056: 'oai:archive:local_id,\\nwhere', 30057: 'scheme,\\narchive', 30058: 'archive', 30059: '.edu,\\nand', 30060: 'local_id', 30061: 'ldc97l18', 30062: 'retrieve\\nthe', 30063: 'form:\\n\\nhttp://www', 30064: '.org/static-records/oai:archive:local_id\\n\\n\\n\\n6', 30065: '.3\\xa0\\xa0\\xa0disseminating', 30066: 'resources\\nthe', 30067: 'repository,\\nan', 30068: 'open-access', 30069: 'upload', 30070: 'and\\nsaved', 30071: \"nltk's\\ndownloader\", 30072: '.\\n\\n\\n\\n7\\xa0\\xa0\\xa0summary\\n\\nfundamental', 30073: 'texts\\nand', 30074: 'a\\nrecord', 30075: 'quality\\ncontrol,', 30076: '.\\ncorpus', 30077: 'representative\\nsample', 30078: 'one\\nsource', 30079: 'useful;', 30080: 'of\\nvariability', 30081: 'interchange', 30082: 'linguistic\\ndata,', 30083: 'shortcuts', 30084: 'projects;', 30085: 'can\\nwrite', 30086: 'curation', 30087: 'convert\\nthem', 30088: 'for\\ndocumenting', 30089: 'available:\\namerican', 30090: '(reppen,', 30091: 'ide,', 30092: 'suderman,', 30093: '2005),\\nbritish', 30094: '({bnc},', 30095: '1999),\\nthesaurus', 30096: 'linguae', 30097: 'graecae', 30098: '({tlg},', 30099: '1999),\\nchild', 30100: 'exchange', 30101: '(childes)', 30102: '(macwhinney,', 30103: '1995),\\ntimit', 30104: 'lamel,', 30105: 'william,', 30106: 'linguistics\\nthat', 30107: 'workshops', 30108: 'proceedings', 30109: 'are\\nsigwac,', 30110: 'promotes', 30111: 'has\\nsponsored', 30112: 'cleaneval', 30113: 'markup,\\nand', 30114: 'sigann,', 30115: 'encouraging', 30116: 'interoperability\\nof', 30117: '.\\nfull', 30118: '(buseman,', 30119: 'buseman,', 30120: '1996),\\nand', 30121: '.org/computing/ddp/', 30122: 'toolbox\\nare', 30123: '(tamanji,', 30124: 'hirotani,', 30125: '1999),', 30126: '(robinson,', 30127: 'aumann,', 30128: '.\\ndozens', 30129: 'some\\nsurveyed', 30130: '(bird', 30131: 'simons,', 30132: 'latech', 30133: 'on\\nlanguage', 30134: 'http://zvon', 30135: '.org/)\\nand', 30136: 'modes', 30137: 'include\\nolif', 30138: '.olif', 30139: '.net/\\nand', 30140: '.com/p/lift-standard/', 30141: 'survey', 30142: '.edu/annotation/', 30143: '(thompson', 30144: 'mckelvie,', 30145: 'called\\nannotation', 30146: '2001)', 30147: '(gold)', 30148: 'is\\ndocumented', 30149: '.linguistics-ontology', 30150: '(farghaly,', 30151: '2003)\\nmore', 30152: 'available\\nin', 30153: '(artstein', 30154: 'poesio,', 30155: '(pevzner', 30156: 'hearst,', 30157: 'robinson,', 30158: 'mien', 30159: 'greg', 30160: 'aumann', 30161: 'visit\\nhttp://www', 30162: '(simons', 30163: '.\\n\\n\\n9\\xa0\\xa0\\xa0exercises\\n\\n◑', 30164: 'the\\nentry', 30165: 'inserts', 30166: 'subelement', 30167: \"element('cv'),\\nassign\", 30168: 'insert()', 30169: '.)\\n\\n◑', 30170: 'deletes', 30171: 'sanitize', 30172: 'others,\\ne', 30173: 'entries\\nhaving', 30174: '(ps', 30175: 'field)', 30176: 'that\\noccurred', 30177: 'mistakes?\\n\\n◑', 30178: 'whole-word', 30179: 'reduplication', 30180: 'partial\\nreduplication', 30181: '.search()', 30182: 'following\\nregular', 30183: '.+)\\\\1\\n\\n◑', 30184: 'interesting\\nissue', 30185: 'this\\nprogram', 30186: 'syl', 30187: 'similarly', 30188: 'of\\nconsecutive', 30189: 'pt)', 30190: 'per\\nrow,', 30191: 'the\\nspreadsheet', 30192: 'in\\ntoolbox', 30193: 'gl', 30194: \"shakespeare's\", 30195: 'plays,', 30196: 'music,\\nreturning', 30197: 'acts,', 30198: 'speeches,', 30199: 'form\\n[(3,', 30200: '23),', 30201: 'indicates\\nact', 30202: 'length\\nfor', 30203: 'venice,', 30204: 'character,\\ne', 30205: \"cfd['portia'][12]\", 30206: 'portia\\nconsisting', 30207: 'cognates', 30208: 'edit-distance', 30209: 'three\\nfrom', 30210: 'xrf', 30211: 'referencing\\nthe', 30212: 'containing\\nw', 30213: 'toolbox-format', 30214: 'a\\ntree,', 30215: 'represented\\nas', 30216: '.:\\n\\n<s>\\n', 30217: '<np', 30218: 'type=sbj>\\n', 30219: '<np>\\n', 30220: '<nnp>pierre</nnp>\\n', 30221: '<nnp>vinken</nnp>\\n', 30222: '</np>\\n', 30223: '<comma>,</comma>\\n\\n\\n\\n\\n\\nabout', 30224: 'acstafterword:', 30225: 'challenge\\n\\n\\n\\n\\nafterword:', 30226: 'challenge\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nnatural', 30227: 'throws', 30228: 'including\\ntokenization,', 30229: 'extraction,\\nand', 30230: 'equipped', 30231: 'create\\nrobust', 30232: 'into\\ncomponents', 30233: 'exciting\\nendeavor', 30234: 'broader\\naudience', 30235: 'spite', 30236: 'with\\nfar', 30237: 'following\\nsentences', 30238: 'attest', 30239: 'riches', 30240: 'language:\\n\\n', 30241: '.overhead', 30242: 'drives', 30243: 'grey,', 30244: 'hiding', 30245: 'sun', 30246: '(william', 30247: 'faulkner,', 30248: 'dying,', 30249: '1935)\\n\\n', 30250: '.when', 30251: 'toaster', 30252: 'exhaust', 30253: 'fan', 30254: '(sign', 30255: 'dormitory', 30256: 'kitchen)\\n\\n', 30257: '.amiodarone', 30258: 'weakly', 30259: 'inhibited', 30260: 'cyp2c9,', 30261: 'cyp2d6,', 30262: 'cyp3a4-mediated', 30263: 'activities', 30264: 'ki', 30265: '.1-271', 30266: 'μm', 30267: '(medline,', 30268: 'pmid:', 30269: '10718780)\\n\\n', 30270: '.iraqi', 30271: 'arms', 30272: '(spoof', 30273: 'headline)\\n\\n', 30274: 'earnest', 30275: 'prayer', 30276: 'righteous', 30277: 'wonderful', 30278: '(james', 30279: '5:16b)\\n\\n', 30280: '.twas', 30281: 'brillig,', 30282: 'slithy', 30283: 'toves', 30284: 'gyre', 30285: 'gimble', 30286: 'wabe', 30287: '(lewis', 30288: 'carroll,', 30289: 'jabberwocky,', 30290: '1872)\\n\\n', 30291: '.there', 30292: 'afaik', 30293: ':smile:', 30294: '(internet', 30295: 'archive)\\n\\nother', 30296: 'disciplines\\nwhose', 30297: 'centers', 30298: 'disciplines', 30299: 'include\\ntranslation,', 30300: 'criticism,', 30301: 'philosophy,', 30302: 'anthropology', 30303: 'psychology', 30304: 'including\\nlaw,', 30305: 'hermeneutics,', 30306: 'forensics,', 30307: 'telephony,', 30308: 'pedagogy,', 30309: 'archaeology,', 30310: 'cryptanalysis', 30311: 'speech\\npathology', 30312: 'methodologies', 30313: 'gather\\nobservations,', 30314: 'hypotheses', 30315: 'to\\ndeepen', 30316: 'intellect', 30317: 'is\\nmanifested', 30318: 'interest\\nin', 30319: 'angles,', 30320: 'barely\\nscratched', 30321: 'itself,\\nthere', 30322: \"haven't\\nmentioned\", 30323: 'remarks', 30324: 'nlp,\\nincluding', 30325: 'directions', 30326: 'might\\nwant', 30327: 'well-supported', 30328: 'nltk,\\nand', 30329: 'rectify', 30330: 'contributing', 30331: 'software\\nand', 30332: 'a\\ncomputational', 30333: 'grew', 30334: 'dating', 30335: '1900s,', 30336: 'logic,\\nmost', 30337: 'manifested', 30338: 'frege,', 30339: 'russell,', 30340: 'wittgenstein,\\ntarski,', 30341: 'lambek', 30342: 'carnap', 30343: 'amenable', 30344: 'later\\ndevelopments', 30345: 'automata,', 30346: 'context-free\\nlanguages', 30347: 'pushdown', 30348: 'a\\nformal', 30349: 'in\\nsymbolic', 30350: 'rules\\nof', 30351: 'possibly,', 30352: 'set-theoretic\\nmodel;', 30353: 'given\\nsuch', 30354: 'becomes\\npossible', 30355: 'by\\ntranslating', 30356: 'example,\\nif', 30357: 'saw(j,m),', 30358: 'we\\n(implicitly', 30359: 'explicitly)', 30360: 'intepret', 30361: 'a\\nbinary', 30362: 'denoting\\nindividuals', 30363: 'birds', 30364: 'require\\nquantifiers,', 30365: '∀,', 30366: 'all:', 30367: '(bird(x)', 30368: 'fly(x))', 30369: 'provided\\nthe', 30370: 'inferences', 30371: 'of\\ncompositionality,', 30372: 'expression\\nis', 30373: 'of\\ncombination', 30374: 'between\\nsyntax', 30375: 'expression\\ncould', 30376: 'true\\nthat', 30377: 'proposition', 30378: 'not(p)', 30379: 'saw(j,', 30380: 'm)', 30381: 'mary\\nrecursively,', 30382: 'get\\nnot(saw(j,m))', 30383: 'outlined', 30384: 'with\\nnatural', 30385: 'relies', 30386: 'symbolic\\nrepresentations', 30387: 'nlp,\\nparticularly', 30388: 'starting\\npoint', 30389: 'practitioners', 30390: 'family\\nof', 30391: 'unification-based', 30392: 'feature-based)\\ngrammar', 30393: 'prolog\\nprogramming', 30394: 'grammar-based', 30395: 'eclipsed', 30396: '15–20', 30397: 'one\\nsignificant', 30398: 'although\\nearly', 30399: 'emulated', 30400: 'the\\nkind', 30401: 'typified\\nby', 30402: '(chomsky', 30403: 'halle,', 30404: '1968),\\nthis', 30405: 'hopelessly', 30406: 'dealing\\nwith', 30407: 'like\\nreal', 30408: 'from\\nlarge', 30409: 'accurate,\\nefficient', 30410: 'that\\nprogress', 30411: 'hugely', 30412: 'assisted', 30413: 'the\\nconstruction', 30414: 'quantitatively', 30415: 'measuring\\nperformance', 30416: 'nlp\\ncommunity', 30417: 'embraced', 30418: 'machine-learning', 30419: 'techniques\\nand', 30420: 'evaluation-led', 30421: '.\\n\\n\\ncontemporary', 30422: 'divides\\nthe', 30423: 'contrasting', 30424: 'section\\nrelate', 30425: 'metaphysical', 30426: 'debates', 30427: 'rationalism\\nversus', 30428: 'empiricism', 30429: 'realism', 30430: 'idealism', 30431: 'enlightenment', 30432: 'western', 30433: 'philosophy', 30434: 'these\\ndebates', 30435: 'backdrop', 30436: 'orthodox', 30437: 'divine', 30438: 'revelation', 30439: 'seventeenth', 30440: 'eighteenth', 30441: 'centuries,\\nphilosophers', 30442: 'sensory', 30443: 'has\\npriority', 30444: 'descartes', 30445: 'leibniz,', 30446: 'took\\nthe', 30447: 'rationalist', 30448: 'position,', 30449: 'asserting', 30450: 'origins', 30451: 'in\\nhuman', 30452: 'thought,', 30453: 'innate', 30454: 'implanted', 30455: 'our\\nminds', 30456: 'birth', 30457: 'of\\neuclidean', 30458: 'geometry', 30459: 'supernatural', 30460: 'contrast,\\nlocke', 30461: 'empiricist', 30462: 'of\\nknowledge', 30463: 'faculties,', 30464: 'reason\\nplays', 30465: 'reflecting', 30466: 'often-cited\\nevidence', 30467: \"galileo's\", 30468: 'on\\ncareful', 30469: 'motion', 30470: 'planets', 30471: 'the\\nsolar', 30472: 'heliocentric', 30473: 'geocentric', 30474: 'of\\nlinguistics,', 30475: 'debate', 30476: 'what\\nextent', 30477: 'experience,', 30478: 'language\\nfaculty,', 30479: 'nlp\\nthis', 30480: 'surfaces', 30481: 'priority', 30482: 'data\\nversus', 30483: 'introspection', 30484: 'concern,', 30485: 'enshrined', 30486: 'and\\nidealism,', 30487: '.\\nkant', 30488: 'manifestations', 30489: 'be\\nknown', 30490: 'realist', 30491: 'theoretical\\nconstruct', 30492: 'exists\\nindependently', 30493: 'perception', 30494: 'actually\\ncauses', 30495: 'idealist,', 30496: 'more\\nabstract', 30497: 'intrinsically\\nunobservable,', 30498: 'fictions', 30499: 'way\\nlinguists', 30500: 'betrays', 30501: 'while\\nnlp', 30502: 'occupy', 30503: 'territory', 30504: 'lean', 30505: 'the\\nidealist', 30506: 'theoretical\\nabstraction', 30507: 'sheds', 30508: 'alive', 30509: 'distinctions\\nbetween', 30510: 'processing,\\nbinary', 30511: 'classifications,', 30512: 'engineering\\ngoals', 30513: 'nuanced,', 30514: 'debate\\nis', 30515: 'polarized', 30516: 'the\\ndiscussions', 30517: 'a\\nbalancing', 30518: 'assume\\nthat', 30519: 'innately', 30520: 'analogical', 30521: 'memory-based\\nlearning', 30522: '(weak', 30523: 'rationalism),', 30524: 'identify\\nmeaningful', 30525: '(empiricism)', 30526: '.\\nstatistical', 30527: 'statistics\\nguide', 30528: 'grammar,\\ni', 30529: '.\\nsymbolic', 30530: 'features\\nfor', 30531: 'model,\\ni', 30532: 'circle', 30533: '.\\n\\n\\nnltk', 30534: 'roadmap\\nthe', 30535: 'work-in-progress,', 30536: 'being\\ncontinually', 30537: '(yet)', 30538: '.\\ncheck', 30539: 'developments', 30540: 'publication\\ndate', 30541: '.\\n\\n\\n\\n\\nphonology', 30542: 'morphology:\\n\\xa0computational', 30543: 'structures\\ntypically', 30544: 'suppletion', 30545: 'and\\nnon-concatenative', 30546: 'string\\nprocessing', 30547: 'high-performance', 30548: 'but\\nto', 30549: 'duplication', 30550: 'morphosyntactic\\nfeatures', 30551: 'morph', 30552: 'analyzers', 30553: '.\\n\\nhigh-performance', 30554: 'components:\\n\\xa0some', 30555: 'computationally', 30556: 'pure', 30557: 'python\\nimplementations', 30558: 'expense\\nonly', 30559: 'distribute\\ntrained', 30560: 'be\\nfreely', 30561: 'interfaces\\nto', 30562: 'the\\nreach', 30563: 'mapreduce', 30564: 'semantics:\\n\\xa0this', 30565: 'vibrant', 30566: 'encompassing\\ninheritance', 30567: 'ontologies,', 30568: 'expressions,\\netc,', 30569: 'conservative', 30570: 'information\\nfrom', 30571: 'in\\nword', 30572: '.\\n\\nnatural', 30573: 'generation:\\n\\xa0producing', 30574: 'meaning\\nis', 30575: 'nlp;', 30576: 'to\\nnlg', 30577: 'more\\ncontributions', 30578: '.\\n\\nlinguistic', 30579: 'fieldwork:\\n\\xa0a', 30580: 'of\\nendangered', 30581: 'heterogeneous', 30582: 'and\\nrapidly', 30583: 'evolving', 30584: 'fieldwork\\ndata', 30585: 'lexicon\\ninterchange', 30586: 'nltk,\\nhelping', 30587: 'curate', 30588: 'data,\\nwhile', 30589: 'liberating', 30590: 'spend', 30591: 'possible\\non', 30592: '.\\n\\nother', 30593: 'languages:\\n\\xa0improved', 30594: 'could\\ninvolve', 30595: 'areas:', 30596: 'obtaining', 30597: 'distribute\\nmore', 30598: 'collection;\\nwriting', 30599: 'language-specific', 30600: '.org/howto,\\nillustrating', 30601: 'language-specific\\nproblems', 30602: 'encodings,', 30603: 'segmentation,\\nand', 30604: 'language\\ncould', 30605: 'arrange', 30606: 'host', 30607: 'the\\nnltk', 30608: 'website;', 30609: 'discussions\\nto', 30610: 'the\\ntarget', 30611: '.\\n\\nnltk-contrib:many', 30612: 'the\\nnlp', 30613: 'housed', 30614: 'contrib\\npackage,', 30615: 'nltk_contrib', 30616: 'python,\\nrelevant', 30617: 'the\\nrest', 30618: 'imperfect', 30619: 'welcome,', 30620: 'probably\\nbe', 30621: '.\\n\\nteaching', 30622: 'materials:\\n\\xa0since', 30623: 'have\\naccompanied', 30624: 'to\\nfill', 30625: 'slides,\\nproblem', 30626: 'treatments', 30627: 'the\\ntopics', 30628: 'covered,', 30629: 'notify\\nthe', 30630: 'mainstream', 30631: 'departments,\\nor', 30632: 'is\\nsignificant', 30633: 'literature,\\ncomputer', 30634: 'curricula', 30635: '.\\n\\nonly', 30636: 'toolkit:as', 30637: 'preface,', 30638: 'tackled', 30639: 'nltk,\\npython,', 30640: 'external\\nnlp', 30641: '.\\n\\n\\n\\n\\n\\nenvoi', 30642: '.\\nlinguists', 30643: 'speak,\\nand', 30644: 'the\\nstudy', 30645: 'languages,\\na', 30646: 'profound', 30647: 'elusive', 30648: 'speak\\nas', 30649: 'scientists', 30650: 'many\\nprogramming', 30651: 'science\\nactually', 30652: 'be\\nimplemented', 30653: 'language,\\na', 30654: 'striving', 30655: 'for\\nfluency', 30656: 'unfortunate', 30657: 'concluded', 30658: 'that\\nnlp', 30659: 'text,\\nor', 30660: 'broadly,', 30661: 'language)\\nto', 30662: 'expedient,', 30663: 'end:\\nas', 30664: 'algorithms\\nfor', 30665: 'text,\\nas', 30666: 'serve\\nthe', 30667: 'society,\\nand', 30668: 'pathway', 30669: 'riches\\nof', 30670: 'present:', 30671: 'hacking!\\n\\n\\nabout', 30672: 'acst'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sentence = docs3[0]+docs3[1]+docs3[2]+docs3[3]+docs3[4]+docs3[5]+docs3[6]+docs3[7]+docs3[8]+docs3[9]+docs3[10]+docs3[11]+docs3[12]\n",
    "#print(sentence)\n",
    "#print('len(sentence) = ', len(sentence))\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"テキストに対する前処理。\n",
    "    「ゼロから作るDeepLearning2 自然言語処理辺」p.66より。\n",
    "\n",
    "    :param text:\n",
    "    :return:\n",
    "      courpus(list): id_to_wordのidに基づいたone-hot vector。\n",
    "      word_to_id(dict): 単語をkeyとして、idを参照する辞書。\n",
    "      id_to_word(dict): idをkeyとして、単語を参照する辞書。\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    text = text.replace('\"', '')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "corpus, word_to_id, id_to_word = preprocess(sentence)\n",
    "vocab_size = len(word_to_id)\n",
    "print(corpus)\n",
    "print(word_to_id)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preface\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "preface\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "this</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>book</th>\n",
       "      <th>about</th>\n",
       "      <th>natural</th>\n",
       "      <th>language</th>\n",
       "      <th>processing</th>\n",
       "      <th>.</th>\n",
       "      <th>by</th>\n",
       "      <th>...</th>\n",
       "      <th>end:\n",
       "as</th>\n",
       "      <th>algorithms\n",
       "for</th>\n",
       "      <th>text,\n",
       "as</th>\n",
       "      <th>serve\n",
       "the</th>\n",
       "      <th>society,\n",
       "and</th>\n",
       "      <th>pathway</th>\n",
       "      <th>riches\n",
       "of</th>\n",
       "      <th>present:</th>\n",
       "      <th>hacking!\n",
       "\n",
       "\n",
       "about</th>\n",
       "      <th>acst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>preface\\n\\n\\n\\n\\npreface\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nthis</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1</td>\n",
       "      <td>884</td>\n",
       "      <td>1802</td>\n",
       "      <td>27</td>\n",
       "      <td>60</td>\n",
       "      <td>54</td>\n",
       "      <td>114</td>\n",
       "      <td>46</td>\n",
       "      <td>1501</td>\n",
       "      <td>210</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>2</td>\n",
       "      <td>1802</td>\n",
       "      <td>3422</td>\n",
       "      <td>41</td>\n",
       "      <td>86</td>\n",
       "      <td>62</td>\n",
       "      <td>193</td>\n",
       "      <td>87</td>\n",
       "      <td>2682</td>\n",
       "      <td>426</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>about</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>86</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>106</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>natural</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>62</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>76</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <td>1</td>\n",
       "      <td>114</td>\n",
       "      <td>193</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>70</td>\n",
       "      <td>62</td>\n",
       "      <td>168</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>processing</th>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>87</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>62</td>\n",
       "      <td>12</td>\n",
       "      <td>63</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>1</td>\n",
       "      <td>1501</td>\n",
       "      <td>2682</td>\n",
       "      <td>46</td>\n",
       "      <td>106</td>\n",
       "      <td>76</td>\n",
       "      <td>168</td>\n",
       "      <td>63</td>\n",
       "      <td>10332</td>\n",
       "      <td>383</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>1</td>\n",
       "      <td>210</td>\n",
       "      <td>426</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>383</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>natural\\nlanguage</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>1</td>\n",
       "      <td>453</td>\n",
       "      <td>1248</td>\n",
       "      <td>11</td>\n",
       "      <td>68</td>\n",
       "      <td>19</td>\n",
       "      <td>60</td>\n",
       "      <td>34</td>\n",
       "      <td>1113</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>0</td>\n",
       "      <td>678</td>\n",
       "      <td>1114</td>\n",
       "      <td>15</td>\n",
       "      <td>40</td>\n",
       "      <td>21</td>\n",
       "      <td>60</td>\n",
       "      <td>18</td>\n",
       "      <td>899</td>\n",
       "      <td>147</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>142</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>110</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>0</td>\n",
       "      <td>533</td>\n",
       "      <td>1099</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>34</td>\n",
       "      <td>94</td>\n",
       "      <td>47</td>\n",
       "      <td>1690</td>\n",
       "      <td>131</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everyday\\ncommunication</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humans;</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>languages</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>151</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>english,</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hindi</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>or\\nportuguese</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0</td>\n",
       "      <td>896</td>\n",
       "      <td>1601</td>\n",
       "      <td>38</td>\n",
       "      <td>83</td>\n",
       "      <td>42</td>\n",
       "      <td>116</td>\n",
       "      <td>41</td>\n",
       "      <td>2301</td>\n",
       "      <td>217</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contrast</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0</td>\n",
       "      <td>1204</td>\n",
       "      <td>2350</td>\n",
       "      <td>36</td>\n",
       "      <td>103</td>\n",
       "      <td>64</td>\n",
       "      <td>162</td>\n",
       "      <td>66</td>\n",
       "      <td>2085</td>\n",
       "      <td>290</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>artificial</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>such</th>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>206</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>177</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as\\nprogramming</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>864</td>\n",
       "      <td>1538</td>\n",
       "      <td>41</td>\n",
       "      <td>75</td>\n",
       "      <td>65</td>\n",
       "      <td>157</td>\n",
       "      <td>68</td>\n",
       "      <td>1489</td>\n",
       "      <td>245</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speak,\\nand</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the\\nstudy</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>languages,\\na</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>profound</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elusive</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speak\\nas</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scientists</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>many\\nprogramming</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science\\nactually</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be\\nimplemented</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language,\\na</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>striving</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for\\nfluency</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unfortunate</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concluded</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that\\nnlp</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text,\\nor</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>broadly,</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language)\\nto</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expedient,</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end:\\nas</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algorithms\\nfor</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text,\\nas</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>serve\\nthe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>society,\\nand</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pathway</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>riches\\nof</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>present:</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hacking!\\n\\n\\nabout</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acst</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30673 rows × 30673 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    preface\\n\\n\\n\\n\\npreface\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nthis  \\\n",
       "preface\\n\\n\\n\\n\\npreface\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...                                                  0                                                      \n",
       "is                                                                                                  1                                                      \n",
       "a                                                                                                   2                                                      \n",
       "book                                                                                                1                                                      \n",
       "about                                                                                               1                                                      \n",
       "natural                                                                                             1                                                      \n",
       "language                                                                                            1                                                      \n",
       "processing                                                                                          1                                                      \n",
       ".                                                                                                   1                                                      \n",
       "by                                                                                                  1                                                      \n",
       "natural\\nlanguage                                                                                   1                                                      \n",
       "we                                                                                                  1                                                      \n",
       "mean                                                                                                1                                                      \n",
       "that                                                                                                0                                                      \n",
       "used                                                                                                0                                                      \n",
       "for                                                                                                 0                                                      \n",
       "everyday\\ncommunication                                                                             0                                                      \n",
       "humans;                                                                                             0                                                      \n",
       "languages                                                                                           0                                                      \n",
       "like                                                                                                0                                                      \n",
       "english,                                                                                            0                                                      \n",
       "hindi                                                                                               0                                                      \n",
       "or\\nportuguese                                                                                      0                                                      \n",
       "in                                                                                                  0                                                      \n",
       "contrast                                                                                            0                                                      \n",
       "to                                                                                                  0                                                      \n",
       "artificial                                                                                          0                                                      \n",
       "such                                                                                                0                                                      \n",
       "as\\nprogramming                                                                                     0                                                      \n",
       "and                                                                                                 0                                                      \n",
       "...                                                                                               ...                                                      \n",
       "speak,\\nand                                                                                         0                                                      \n",
       "the\\nstudy                                                                                          0                                                      \n",
       "languages,\\na                                                                                       0                                                      \n",
       "profound                                                                                            0                                                      \n",
       "elusive                                                                                             0                                                      \n",
       "speak\\nas                                                                                           0                                                      \n",
       "scientists                                                                                          0                                                      \n",
       "many\\nprogramming                                                                                   0                                                      \n",
       "science\\nactually                                                                                   0                                                      \n",
       "be\\nimplemented                                                                                     0                                                      \n",
       "language,\\na                                                                                        0                                                      \n",
       "striving                                                                                            0                                                      \n",
       "for\\nfluency                                                                                        0                                                      \n",
       "unfortunate                                                                                         0                                                      \n",
       "concluded                                                                                           0                                                      \n",
       "that\\nnlp                                                                                           0                                                      \n",
       "text,\\nor                                                                                           0                                                      \n",
       "broadly,                                                                                            0                                                      \n",
       "language)\\nto                                                                                       0                                                      \n",
       "expedient,                                                                                          0                                                      \n",
       "end:\\nas                                                                                            0                                                      \n",
       "algorithms\\nfor                                                                                     0                                                      \n",
       "text,\\nas                                                                                           0                                                      \n",
       "serve\\nthe                                                                                          0                                                      \n",
       "society,\\nand                                                                                       0                                                      \n",
       "pathway                                                                                             0                                                      \n",
       "riches\\nof                                                                                          0                                                      \n",
       "present:                                                                                            0                                                      \n",
       "hacking!\\n\\n\\nabout                                                                                 0                                                      \n",
       "acst                                                                                                0                                                      \n",
       "\n",
       "                                                      is     a  book  about  \\\n",
       "preface\\n\\n\\n\\n\\npreface\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...     1     2     1      1   \n",
       "is                                                   884  1802    27     60   \n",
       "a                                                   1802  3422    41     86   \n",
       "book                                                  27    41    10      6   \n",
       "about                                                 60    86     6     26   \n",
       "natural                                               54    62     4      4   \n",
       "language                                             114   193     9     11   \n",
       "processing                                            46    87     4      6   \n",
       ".                                                   1501  2682    46    106   \n",
       "by                                                   210   426    10     23   \n",
       "natural\\nlanguage                                      2     3     1      1   \n",
       "we                                                   453  1248    11     68   \n",
       "mean                                                   8     9     1      1   \n",
       "that                                                 678  1114    15     40   \n",
       "used                                                  98   142     2      6   \n",
       "for                                                  533  1099    21     35   \n",
       "everyday\\ncommunication                                1     1     0      0   \n",
       "humans;                                                1     1     0      0   \n",
       "languages                                             13    26     1      5   \n",
       "like                                                  51   151     2      7   \n",
       "english,                                               4    10     0      0   \n",
       "hindi                                                  1     1     0      0   \n",
       "or\\nportuguese                                         1     1     0      0   \n",
       "in                                                   896  1601    38     83   \n",
       "contrast                                               5     2     0      0   \n",
       "to                                                  1204  2350    36    103   \n",
       "artificial                                             3    10     1      0   \n",
       "such                                                  94   206     2      3   \n",
       "as\\nprogramming                                        0     0     0      0   \n",
       "and                                                  864  1538    41     75   \n",
       "...                                                  ...   ...   ...    ...   \n",
       "speak,\\nand                                            0     0     0      0   \n",
       "the\\nstudy                                             1     0     0      0   \n",
       "languages,\\na                                          1     0     0      0   \n",
       "profound                                               2     0     0      0   \n",
       "elusive                                                2     0     1      0   \n",
       "speak\\nas                                              1     0     0      0   \n",
       "scientists                                             0     0     0      0   \n",
       "many\\nprogramming                                      0     0     0      0   \n",
       "science\\nactually                                      0     0     0      0   \n",
       "be\\nimplemented                                        1     0     0      0   \n",
       "language,\\na                                           1     0     0      0   \n",
       "striving                                               1     0     1      0   \n",
       "for\\nfluency                                           1     0     1      0   \n",
       "unfortunate                                            1     0     0      1   \n",
       "concluded                                              1     0     0      1   \n",
       "that\\nnlp                                              1     0     0      1   \n",
       "text,\\nor                                              1     0     0      2   \n",
       "broadly,                                               1     0     0      2   \n",
       "language)\\nto                                          0     0     0      1   \n",
       "expedient,                                             0     1     0      0   \n",
       "end:\\nas                                               0     2     0      0   \n",
       "algorithms\\nfor                                        0     3     0      0   \n",
       "text,\\nas                                              0     1     0      0   \n",
       "serve\\nthe                                             0     2     0      0   \n",
       "society,\\nand                                          0     1     0      0   \n",
       "pathway                                                0     1     0      0   \n",
       "riches\\nof                                             0     1     0      0   \n",
       "present:                                               1     0     0      0   \n",
       "hacking!\\n\\n\\nabout                                    1     1     0      0   \n",
       "acst                                                   0     0     0      0   \n",
       "\n",
       "                                                    natural  language  \\\n",
       "preface\\n\\n\\n\\n\\npreface\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...        1         1   \n",
       "is                                                       54       114   \n",
       "a                                                        62       193   \n",
       "book                                                      4         9   \n",
       "about                                                     4        11   \n",
       "natural                                                   8       100   \n",
       "language                                                100        70   \n",
       "processing                                               30        62   \n",
       ".                                                        76       168   \n",
       "by                                                        7        23   \n",
       "natural\\nlanguage                                         1         2   \n",
       "we                                                       19        60   \n",
       "mean                                                      1         2   \n",
       "that                                                     21        60   \n",
       "used                                                      9        24   \n",
       "for                                                      34        94   \n",
       "everyday\\ncommunication                                   0         2   \n",
       "humans;                                                   0         1   \n",
       "languages                                                 7         6   \n",
       "like                                                      2         2   \n",
       "english,                                                  0         3   \n",
       "hindi                                                     0         1   \n",
       "or\\nportuguese                                            1         1   \n",
       "in                                                       42       116   \n",
       "contrast                                                  1         0   \n",
       "to                                                       64       162   \n",
       "artificial                                                2         1   \n",
       "such                                                      5        17   \n",
       "as\\nprogramming                                           1         0   \n",
       "and                                                      65       157   \n",
       "...                                                     ...       ...   \n",
       "speak,\\nand                                               0         0   \n",
       "the\\nstudy                                                0         0   \n",
       "languages,\\na                                             0         0   \n",
       "profound                                                  0         0   \n",
       "elusive                                                   0         0   \n",
       "speak\\nas                                                 0         0   \n",
       "scientists                                                0         0   \n",
       "many\\nprogramming                                         0         0   \n",
       "science\\nactually                                         0         0   \n",
       "be\\nimplemented                                           0         0   \n",
       "language,\\na                                              0         0   \n",
       "striving                                                  0         0   \n",
       "for\\nfluency                                              0         0   \n",
       "unfortunate                                               0         0   \n",
       "concluded                                                 0         0   \n",
       "that\\nnlp                                                 0         0   \n",
       "text,\\nor                                                 0         0   \n",
       "broadly,                                                  0         0   \n",
       "language)\\nto                                             1         0   \n",
       "expedient,                                                1         0   \n",
       "end:\\nas                                                  0         0   \n",
       "algorithms\\nfor                                           0         0   \n",
       "text,\\nas                                                 0         1   \n",
       "serve\\nthe                                                0         1   \n",
       "society,\\nand                                             0         2   \n",
       "pathway                                                   0         2   \n",
       "riches\\nof                                                0         1   \n",
       "present:                                                  0         1   \n",
       "hacking!\\n\\n\\nabout                                       0         1   \n",
       "acst                                                      0         0   \n",
       "\n",
       "                                                    processing      .   by  \\\n",
       "preface\\n\\n\\n\\n\\npreface\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...           1      1    1   \n",
       "is                                                          46   1501  210   \n",
       "a                                                           87   2682  426   \n",
       "book                                                         4     46   10   \n",
       "about                                                        6    106   23   \n",
       "natural                                                     30     76    7   \n",
       "language                                                    62    168   23   \n",
       "processing                                                  12     63   10   \n",
       ".                                                           63  10332  383   \n",
       "by                                                          10    383   94   \n",
       "natural\\nlanguage                                            2      4    2   \n",
       "we                                                          34   1113  172   \n",
       "mean                                                         1      9    3   \n",
       "that                                                        18    899  147   \n",
       "used                                                        11    110   29   \n",
       "for                                                         47   1690  131   \n",
       "everyday\\ncommunication                                      1      2    2   \n",
       "humans;                                                      0      2    2   \n",
       "languages                                                    0     43    9   \n",
       "like                                                         0    101   22   \n",
       "english,                                                     0     12    2   \n",
       "hindi                                                        0      1    1   \n",
       "or\\nportuguese                                               0      1    1   \n",
       "in                                                          41   2301  217   \n",
       "contrast                                                     0      5    4   \n",
       "to                                                          66   2085  290   \n",
       "artificial                                                   1      5    3   \n",
       "such                                                         6    177   36   \n",
       "as\\nprogramming                                              0      1    0   \n",
       "and                                                         68   1489  245   \n",
       "...                                                        ...    ...  ...   \n",
       "speak,\\nand                                                  0      2    0   \n",
       "the\\nstudy                                                   0      0    1   \n",
       "languages,\\na                                                0      0    1   \n",
       "profound                                                     0      0    1   \n",
       "elusive                                                      0      0    1   \n",
       "speak\\nas                                                    0      0    1   \n",
       "scientists                                                   0      0    0   \n",
       "many\\nprogramming                                            0      0    0   \n",
       "science\\nactually                                            0      0    0   \n",
       "be\\nimplemented                                              0      0    0   \n",
       "language,\\na                                                 0      0    0   \n",
       "striving                                                     0      0    0   \n",
       "for\\nfluency                                                 0      0    0   \n",
       "unfortunate                                                  0      0    0   \n",
       "concluded                                                    0      0    0   \n",
       "that\\nnlp                                                    0      0    0   \n",
       "text,\\nor                                                    0      0    0   \n",
       "broadly,                                                     0      0    0   \n",
       "language)\\nto                                                0      0    0   \n",
       "expedient,                                                   0      0    0   \n",
       "end:\\nas                                                     0      0    0   \n",
       "algorithms\\nfor                                              0      0    0   \n",
       "text,\\nas                                                    0      0    0   \n",
       "serve\\nthe                                                   0      0    0   \n",
       "society,\\nand                                                0      0    0   \n",
       "pathway                                                      0      0    0   \n",
       "riches\\nof                                                   0      2    0   \n",
       "present:                                                     0      2    0   \n",
       "hacking!\\n\\n\\nabout                                          0      2    0   \n",
       "acst                                                         0      0    0   \n",
       "\n",
       "                                                    ...  end:\\nas  \\\n",
       "preface\\n\\n\\n\\n\\npreface\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...  ...         0   \n",
       "is                                                  ...         0   \n",
       "a                                                   ...         2   \n",
       "book                                                ...         0   \n",
       "about                                               ...         0   \n",
       "natural                                             ...         0   \n",
       "language                                            ...         0   \n",
       "processing                                          ...         0   \n",
       ".                                                   ...         0   \n",
       "by                                                  ...         0   \n",
       "natural\\nlanguage                                   ...         0   \n",
       "we                                                  ...         0   \n",
       "mean                                                ...         0   \n",
       "that                                                ...         0   \n",
       "used                                                ...         0   \n",
       "for                                                 ...         0   \n",
       "everyday\\ncommunication                             ...         0   \n",
       "humans;                                             ...         0   \n",
       "languages                                           ...         0   \n",
       "like                                                ...         0   \n",
       "english,                                            ...         0   \n",
       "hindi                                               ...         0   \n",
       "or\\nportuguese                                      ...         0   \n",
       "in                                                  ...         0   \n",
       "contrast                                            ...         0   \n",
       "to                                                  ...         2   \n",
       "artificial                                          ...         0   \n",
       "such                                                ...         0   \n",
       "as\\nprogramming                                     ...         0   \n",
       "and                                                 ...         2   \n",
       "...                                                 ...       ...   \n",
       "speak,\\nand                                         ...         0   \n",
       "the\\nstudy                                          ...         0   \n",
       "languages,\\na                                       ...         0   \n",
       "profound                                            ...         0   \n",
       "elusive                                             ...         0   \n",
       "speak\\nas                                           ...         0   \n",
       "scientists                                          ...         0   \n",
       "many\\nprogramming                                   ...         0   \n",
       "science\\nactually                                   ...         0   \n",
       "be\\nimplemented                                     ...         0   \n",
       "language,\\na                                        ...         0   \n",
       "striving                                            ...         0   \n",
       "for\\nfluency                                        ...         0   \n",
       "unfortunate                                         ...         0   \n",
       "concluded                                           ...         0   \n",
       "that\\nnlp                                           ...         0   \n",
       "text,\\nor                                           ...         0   \n",
       "broadly,                                            ...         0   \n",
       "language)\\nto                                       ...         0   \n",
       "expedient,                                          ...         0   \n",
       "end:\\nas                                            ...         0   \n",
       "algorithms\\nfor                                     ...         1   \n",
       "text,\\nas                                           ...         0   \n",
       "serve\\nthe                                          ...         0   \n",
       "society,\\nand                                       ...         0   \n",
       "pathway                                             ...         0   \n",
       "riches\\nof                                          ...         0   \n",
       "present:                                            ...         0   \n",
       "hacking!\\n\\n\\nabout                                 ...         0   \n",
       "acst                                                ...         0   \n",
       "\n",
       "                                                    algorithms\\nfor  \\\n",
       "preface\\n\\n\\n\\n\\npreface\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...                0   \n",
       "is                                                                0   \n",
       "a                                                                 3   \n",
       "book                                                              0   \n",
       "about                                                             0   \n",
       "natural                                                           0   \n",
       "language                                                          0   \n",
       "processing                                                        0   \n",
       ".                                                                 0   \n",
       "by                                                                0   \n",
       "natural\\nlanguage                                                 0   \n",
       "we                                                                0   \n",
       "mean                                                              0   \n",
       "that                                                              0   \n",
       "used                                                              0   \n",
       "for                                                               0   \n",
       "everyday\\ncommunication                                           0   \n",
       "humans;                                                           0   \n",
       "languages                                                         0   \n",
       "like                                                              0   \n",
       "english,                                                          0   \n",
       "hindi                                                             0   \n",
       "or\\nportuguese                                                    0   \n",
       "in                                                                0   \n",
       "contrast                                                          0   \n",
       "to                                                                3   \n",
       "artificial                                                        0   \n",
       "such                                                              0   \n",
       "as\\nprogramming                                                   0   \n",
       "and                                                               2   \n",
       "...                                                             ...   \n",
       "speak,\\nand                                                       0   \n",
       "the\\nstudy                                                        0   \n",
       "languages,\\na                                                     0   \n",
       "profound                                                          0   \n",
       "elusive                                                           0   \n",
       "speak\\nas                                                         0   \n",
       "scientists                                                        0   \n",
       "many\\nprogramming                                                 0   \n",
       "science\\nactually                                                 0   \n",
       "be\\nimplemented                                                   0   \n",
       "language,\\na                                                      0   \n",
       "striving                                                          0   \n",
       "for\\nfluency                                                      0   \n",
       "unfortunate                                                       0   \n",
       "concluded                                                         0   \n",
       "that\\nnlp                                                         0   \n",
       "text,\\nor                                                         0   \n",
       "broadly,                                                          0   \n",
       "language)\\nto                                                     0   \n",
       "expedient,                                                        0   \n",
       "end:\\nas                                                          1   \n",
       "algorithms\\nfor                                                   0   \n",
       "text,\\nas                                                         1   \n",
       "serve\\nthe                                                        0   \n",
       "society,\\nand                                                     0   \n",
       "pathway                                                           0   \n",
       "riches\\nof                                                        0   \n",
       "present:                                                          0   \n",
       "hacking!\\n\\n\\nabout                                               0   \n",
       "acst                                                              0   \n",
       "\n",
       "                                                    text,\\nas  serve\\nthe  \\\n",
       "preface\\n\\n\\n\\n\\npreface\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...          0           0   \n",
       "is                                                          0           0   \n",
       "a                                                           1           2   \n",
       "book                                                        0           0   \n",
       "about                                                       0           0   \n",
       "natural                                                     0           0   \n",
       "language                                                    1           1   \n",
       "processing                                                  0           0   \n",
       ".                                                           0           0   \n",
       "by                                                          0           0   \n",
       "natural\\nlanguage                                           0           0   \n",
       "we                                                          0           0   \n",
       "mean                                                        0           0   \n",
       "that                                                        0           0   \n",
       "used                                                        0           0   \n",
       "for                                                         0           0   \n",
       "everyday\\ncommunication                                     0           0   \n",
       "humans;                                                     0           0   \n",
       "languages                                                   0           0   \n",
       "like                                                        0           0   \n",
       "english,                                                    0           0   \n",
       "hindi                                                       0           0   \n",
       "or\\nportuguese                                              0           0   \n",
       "in                                                          0           0   \n",
       "contrast                                                    0           0   \n",
       "to                                                          3           2   \n",
       "artificial                                                  0           0   \n",
       "such                                                        0           0   \n",
       "as\\nprogramming                                             0           0   \n",
       "and                                                         2           0   \n",
       "...                                                       ...         ...   \n",
       "speak,\\nand                                                 0           0   \n",
       "the\\nstudy                                                  0           0   \n",
       "languages,\\na                                               0           0   \n",
       "profound                                                    0           0   \n",
       "elusive                                                     0           0   \n",
       "speak\\nas                                                   0           0   \n",
       "scientists                                                  0           0   \n",
       "many\\nprogramming                                           0           0   \n",
       "science\\nactually                                           0           0   \n",
       "be\\nimplemented                                             0           0   \n",
       "language,\\na                                                0           0   \n",
       "striving                                                    0           0   \n",
       "for\\nfluency                                                0           0   \n",
       "unfortunate                                                 0           0   \n",
       "concluded                                                   0           0   \n",
       "that\\nnlp                                                   0           0   \n",
       "text,\\nor                                                   0           0   \n",
       "broadly,                                                    0           0   \n",
       "language)\\nto                                               0           0   \n",
       "expedient,                                                  0           0   \n",
       "end:\\nas                                                    0           0   \n",
       "algorithms\\nfor                                             1           0   \n",
       "text,\\nas                                                   0           1   \n",
       "serve\\nthe                                                  1           0   \n",
       "society,\\nand                                               0           1   \n",
       "pathway                                                     0           1   \n",
       "riches\\nof                                                  0           0   \n",
       "present:                                                    0           0   \n",
       "hacking!\\n\\n\\nabout                                         0           0   \n",
       "acst                                                        0           0   \n",
       "\n",
       "                                                    society,\\nand  pathway  \\\n",
       "preface\\n\\n\\n\\n\\npreface\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...              0        0   \n",
       "is                                                              0        0   \n",
       "a                                                               1        1   \n",
       "book                                                            0        0   \n",
       "about                                                           0        0   \n",
       "natural                                                         0        0   \n",
       "language                                                        2        2   \n",
       "processing                                                      0        0   \n",
       ".                                                               0        0   \n",
       "by                                                              0        0   \n",
       "natural\\nlanguage                                               0        0   \n",
       "we                                                              0        0   \n",
       "mean                                                            0        0   \n",
       "that                                                            0        0   \n",
       "used                                                            0        0   \n",
       "for                                                             0        1   \n",
       "everyday\\ncommunication                                         0        0   \n",
       "humans;                                                         0        0   \n",
       "languages                                                       0        0   \n",
       "like                                                            0        0   \n",
       "english,                                                        0        0   \n",
       "hindi                                                           0        0   \n",
       "or\\nportuguese                                                  0        0   \n",
       "in                                                              0        0   \n",
       "contrast                                                        0        0   \n",
       "to                                                              2        1   \n",
       "artificial                                                      0        0   \n",
       "such                                                            0        0   \n",
       "as\\nprogramming                                                 0        0   \n",
       "and                                                             0        0   \n",
       "...                                                           ...      ...   \n",
       "speak,\\nand                                                     0        0   \n",
       "the\\nstudy                                                      0        0   \n",
       "languages,\\na                                                   0        0   \n",
       "profound                                                        0        0   \n",
       "elusive                                                         0        0   \n",
       "speak\\nas                                                       0        0   \n",
       "scientists                                                      0        0   \n",
       "many\\nprogramming                                               0        0   \n",
       "science\\nactually                                               0        0   \n",
       "be\\nimplemented                                                 0        0   \n",
       "language,\\na                                                    0        0   \n",
       "striving                                                        0        0   \n",
       "for\\nfluency                                                    0        0   \n",
       "unfortunate                                                     0        0   \n",
       "concluded                                                       0        0   \n",
       "that\\nnlp                                                       0        0   \n",
       "text,\\nor                                                       0        0   \n",
       "broadly,                                                        0        0   \n",
       "language)\\nto                                                   0        0   \n",
       "expedient,                                                      0        0   \n",
       "end:\\nas                                                        0        0   \n",
       "algorithms\\nfor                                                 0        0   \n",
       "text,\\nas                                                       0        0   \n",
       "serve\\nthe                                                      1        1   \n",
       "society,\\nand                                                   0        1   \n",
       "pathway                                                         1        0   \n",
       "riches\\nof                                                      1        1   \n",
       "present:                                                        0        1   \n",
       "hacking!\\n\\n\\nabout                                             0        0   \n",
       "acst                                                            0        0   \n",
       "\n",
       "                                                    riches\\nof  present:  \\\n",
       "preface\\n\\n\\n\\n\\npreface\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...           0         0   \n",
       "is                                                           0         1   \n",
       "a                                                            1         0   \n",
       "book                                                         0         0   \n",
       "about                                                        0         0   \n",
       "natural                                                      0         0   \n",
       "language                                                     1         1   \n",
       "processing                                                   0         0   \n",
       ".                                                            2         2   \n",
       "by                                                           0         0   \n",
       "natural\\nlanguage                                            0         0   \n",
       "we                                                           0         0   \n",
       "mean                                                         0         0   \n",
       "that                                                         0         0   \n",
       "used                                                         0         0   \n",
       "for                                                          1         2   \n",
       "everyday\\ncommunication                                      0         0   \n",
       "humans;                                                      0         0   \n",
       "languages                                                    0         0   \n",
       "like                                                         0         0   \n",
       "english,                                                     0         0   \n",
       "hindi                                                        0         0   \n",
       "or\\nportuguese                                               0         0   \n",
       "in                                                           0         0   \n",
       "contrast                                                     0         0   \n",
       "to                                                           0         0   \n",
       "artificial                                                   0         0   \n",
       "such                                                         0         0   \n",
       "as\\nprogramming                                              0         0   \n",
       "and                                                          0         0   \n",
       "...                                                        ...       ...   \n",
       "speak,\\nand                                                  0         0   \n",
       "the\\nstudy                                                   0         0   \n",
       "languages,\\na                                                0         0   \n",
       "profound                                                     0         0   \n",
       "elusive                                                      0         0   \n",
       "speak\\nas                                                    0         0   \n",
       "scientists                                                   0         0   \n",
       "many\\nprogramming                                            0         0   \n",
       "science\\nactually                                            0         0   \n",
       "be\\nimplemented                                              0         0   \n",
       "language,\\na                                                 0         0   \n",
       "striving                                                     0         0   \n",
       "for\\nfluency                                                 0         0   \n",
       "unfortunate                                                  0         0   \n",
       "concluded                                                    0         0   \n",
       "that\\nnlp                                                    0         0   \n",
       "text,\\nor                                                    0         0   \n",
       "broadly,                                                     0         0   \n",
       "language)\\nto                                                0         0   \n",
       "expedient,                                                   0         0   \n",
       "end:\\nas                                                     0         0   \n",
       "algorithms\\nfor                                              0         0   \n",
       "text,\\nas                                                    0         0   \n",
       "serve\\nthe                                                   0         0   \n",
       "society,\\nand                                                1         0   \n",
       "pathway                                                      1         1   \n",
       "riches\\nof                                                   0         1   \n",
       "present:                                                     1         0   \n",
       "hacking!\\n\\n\\nabout                                          1         1   \n",
       "acst                                                         0         0   \n",
       "\n",
       "                                                    hacking!\\n\\n\\nabout  acst  \n",
       "preface\\n\\n\\n\\n\\npreface\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\...                    0     0  \n",
       "is                                                                    1     0  \n",
       "a                                                                     1     0  \n",
       "book                                                                  0     0  \n",
       "about                                                                 0     0  \n",
       "natural                                                               0     0  \n",
       "language                                                              1     0  \n",
       "processing                                                            0     0  \n",
       ".                                                                     2     0  \n",
       "by                                                                    0     0  \n",
       "natural\\nlanguage                                                     0     0  \n",
       "we                                                                    0     0  \n",
       "mean                                                                  0     0  \n",
       "that                                                                  0     0  \n",
       "used                                                                  0     0  \n",
       "for                                                                   2     0  \n",
       "everyday\\ncommunication                                               0     0  \n",
       "humans;                                                               0     0  \n",
       "languages                                                             0     0  \n",
       "like                                                                  0     0  \n",
       "english,                                                              0     0  \n",
       "hindi                                                                 0     0  \n",
       "or\\nportuguese                                                        0     0  \n",
       "in                                                                    0     0  \n",
       "contrast                                                              0     0  \n",
       "to                                                                    0     0  \n",
       "artificial                                                            0     0  \n",
       "such                                                                  0     0  \n",
       "as\\nprogramming                                                       0     0  \n",
       "and                                                                   0     0  \n",
       "...                                                                 ...   ...  \n",
       "speak,\\nand                                                           0     0  \n",
       "the\\nstudy                                                            0     0  \n",
       "languages,\\na                                                         0     0  \n",
       "profound                                                              0     0  \n",
       "elusive                                                               0     0  \n",
       "speak\\nas                                                             0     0  \n",
       "scientists                                                            0     0  \n",
       "many\\nprogramming                                                     0     0  \n",
       "science\\nactually                                                     0     0  \n",
       "be\\nimplemented                                                       0     0  \n",
       "language,\\na                                                          0     0  \n",
       "striving                                                              0     0  \n",
       "for\\nfluency                                                          0     0  \n",
       "unfortunate                                                           0     0  \n",
       "concluded                                                             0     0  \n",
       "that\\nnlp                                                             0     0  \n",
       "text,\\nor                                                             0     0  \n",
       "broadly,                                                              0     0  \n",
       "language)\\nto                                                         0     0  \n",
       "expedient,                                                            0     0  \n",
       "end:\\nas                                                              0     0  \n",
       "algorithms\\nfor                                                       0     0  \n",
       "text,\\nas                                                             0     0  \n",
       "serve\\nthe                                                            0     0  \n",
       "society,\\nand                                                         0     0  \n",
       "pathway                                                               0     0  \n",
       "riches\\nof                                                            1     0  \n",
       "present:                                                              1     0  \n",
       "hacking!\\n\\n\\nabout                                                   0     0  \n",
       "acst                                                                  0     0  \n",
       "\n",
       "[30673 rows x 30673 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    \"\"\"共起行列を作成。\n",
    "    「ゼロから作るDeepLearning2 自然言語処理辺」p.72より。\n",
    "\n",
    "    :param corpus(str): テキスト文。\n",
    "    :param vocab_size: 語彙数。\n",
    "    :param window_size: 共起判定の範囲。\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size+1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "    return co_matrix\n",
    "\n",
    "co_matrix = create_co_matrix(corpus, vocab_size, window_size=13)\n",
    "df = pd.DataFrame(co_matrix, index=word_to_id.keys(), columns=word_to_id.keys())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# similar() with co_matrix\n",
      "[query] natural language text write count\n",
      "natural [1.0, 0.91, 0.771, 0.738, 0.687]\n",
      "language [0.91, 1.0, 0.904, 0.869, 0.821]\n",
      "text [0.771, 0.904, 1.0, 0.92, 0.934]\n",
      "write [0.738, 0.869, 0.92, 1.0, 0.827]\n",
      "count [0.687, 0.821, 0.934, 0.827, 1.0]\n"
     ]
    }
   ],
   "source": [
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "def similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    \"\"\"\n",
    "    :param query(str): クエリ。\n",
    "    :param word_to_id(dict): 単語をkeyとして、idを参照する辞書。\n",
    "    :param id_to_word(dict): idをkeyとして、単語を参照する辞書。\n",
    "    :param word_matrix: 共起行列。\n",
    "    :param top(int): 上位何件まで表示させるか。今回は使用しない\n",
    "    :return: なし。\n",
    "    \"\"\"\n",
    "    if query not in word_to_id:\n",
    "        print('%s is not found' % query)\n",
    "        return\n",
    "    \n",
    "    result = []\n",
    "    print('[query] ' + query +\" language\" + \" text\"+ \" write\"+\" count\")\n",
    "    list=[\"natural\",\"language\",\"text\",\"write\",\"count\"]\n",
    "    for x in range(0,5):\n",
    "        query_id = word_to_id[list[x]]\n",
    "        query_vec = word_matrix[query_id]\n",
    "        \n",
    "        vocab_size = len(word_to_id)\n",
    "        similarity = np.zeros(vocab_size)\n",
    "        print(list[x]+\" \", end=\"\")\n",
    "        tmp=[]\n",
    "        for i in range(vocab_size):\n",
    "            natural=0\n",
    "            language=0\n",
    "            text=0\n",
    "            count=0\n",
    "            write=0\n",
    "            similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "            if  id_to_word[i] == \"natural\":\n",
    "                #print(round(similarity[i],3), end=\" \")\n",
    "                tmp.append(round(similarity[i],3))\n",
    "            if  id_to_word[i] == \"language\":\n",
    "                #print(round(similarity[i],3), end=\" \")\n",
    "                tmp.append(round(similarity[i],3))\n",
    "            if  id_to_word[i] == \"text\":\n",
    "                #print(round(similarity[i],3), end=\" \")\n",
    "                tmp.append(round(similarity[i],3))\n",
    "            if  id_to_word[i] == \"count\":\n",
    "                #print(round(similarity[i],3), end=\" \") \n",
    "                tmp.append(round(similarity[i],3))\n",
    "            if  id_to_word[i] == \"write\":\n",
    "                #print(round(similarity[i],3), end=\" \") \n",
    "                tmp.append(round(similarity[i],3))\n",
    "        \n",
    "        result.append(tmp)\n",
    "        print(result[x])\n",
    "\n",
    "\n",
    "\n",
    "print('\\n# similar() with co_matrix')\n",
    "user_query = \"natural\"\n",
    "similar(user_query, word_to_id, id_to_word, co_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "from pprint import pprint \n",
    "pprint(list(newsgroups_train.target_names))#全カテゴリーの表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(857, 18089)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "categories = ['alt.atheism', 'talk.religion.misc']#取得するカテゴリー\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                       categories=categories)#訓練データ取得\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                       categories=categories)#テストデータ取得\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176.6382730455076"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.nnz / float(vectors.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW + tfidfの適応\n",
    "vectors, vectorizer = bow_tfidf(newsgroups_train.data)#訓練データとテストデータの適応"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8372419272740816"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "# SVMの学習\n",
    "classifier = svm.LinearSVC(dual=False)#分類器\n",
    "classifier.fit(vectors , newsgroups_train.target)\n",
    "\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "# 予測\n",
    "pred = classifier.predict(vectors_test)\n",
    "\n",
    "metrics.f1_score(newsgroups_test.target, pred, average='macro')#精度表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[265  54]\n",
      " [ 38 213]]\n",
      "Normalized confusion matrix\n",
      "[[0.830721   0.169279  ]\n",
      " [0.15139442 0.84860558]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names = categories\n",
    "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = plot_confusion_matrix(classifier, vectors_test, newsgroups_test.target,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
